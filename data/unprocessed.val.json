[
  {
    "title": "Model Uncertainty in Classical Conditioning .",
    "entities": [
      "conditioning # t parameters",
      "# xed generative model",
      "spurious correlations",
      "bayesian model",
      "world model",
      "conditioning regimes",
      "model structure",
      "second-order conditioning",
      "conditioned inhibition",
      "reinforcer delivery"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "second-order conditioning -- CONJUNCTION -- conditioned inhibition"
    ],
    "abstract": "we develop a framework based on <method_3> averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments . traditional accounts of <otherscientificterm_0> within a <method_1> of <task_9> ; uncertainty over the <otherscientificterm_6> is not considered . we apply the theory to explain the puzzling relationship between <otherscientificterm_7> and <otherscientificterm_8> , two similar <otherscientificterm_5> that nonetheless result in strongly divergent behavioral outcomes . according to the theory , <otherscientificterm_7> results when limited experience leads animals to prefer a simpler <method_4> that produces <otherscientificterm_2> ; <otherscientificterm_8> results when a more complex model is justi # ed by additional experience .",
    "abstract_og": "we develop a framework based on bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments . traditional accounts of conditioning # t parameters within a # xed generative model of reinforcer delivery ; uncertainty over the model structure is not considered . we apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition , two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes . according to the theory , second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations ; conditioned inhibition results when a more complex model is justi # ed by additional experience ."
  },
  {
    "title": "Continuous Markov Random Fields for Robust Stereo Estimation .",
    "entities": [
      "middlebury high resolution imagery",
      "slanted plane mrf methods",
      "slanted 3d planes",
      "occlusion boundaries",
      "slanted-plane model",
      "kitti dataset",
      "hybrid mrf",
      "random variables",
      "inference"
    ],
    "types": "<material> <method> <otherscientificterm> <otherscientificterm> <method> <material> <method> <otherscientificterm> <task>",
    "relations": [
      "slanted 3d planes -- CONJUNCTION -- occlusion boundaries",
      "slanted-plane model -- COMPARE -- slanted plane mrf methods",
      "middlebury high resolution imagery -- EVALUATE-FOR -- slanted-plane model"
    ],
    "abstract": "in this paper we present a novel <method_4> which reasons jointly about <otherscientificterm_3> as well as depth . we formulate the problem as one of <task_8> in a <method_6> composed of both continuous -lrb- i.e. , <otherscientificterm_2> -rrb- and discrete -lrb- i.e. , <otherscientificterm_3> -rrb- <otherscientificterm_7> . this allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments , as well as potentials encoding which junctions are physically possible . our <method_4> outperforms the state-of-the-art on <material_0> -lsb- 1 -rsb- as well as in the more challenging <material_5> -lsb- 2 -rsb- , while being more efficient than existing <method_1> , taking on average 2 minutes to perform <task_8> on high resolution imagery .",
    "abstract_og": "in this paper we present a novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth . we formulate the problem as one of inference in a hybrid mrf composed of both continuous -lrb- i.e. , slanted 3d planes -rrb- and discrete -lrb- i.e. , occlusion boundaries -rrb- random variables . this allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments , as well as potentials encoding which junctions are physically possible . our slanted-plane model outperforms the state-of-the-art on middlebury high resolution imagery -lsb- 1 -rsb- as well as in the more challenging kitti dataset -lsb- 2 -rsb- , while being more efficient than existing slanted plane mrf methods , taking on average 2 minutes to perform inference on high resolution imagery ."
  },
  {
    "title": "A regularization framework for mobile social network analysis .",
    "entities": [
      "evolution of social network information",
      "learning and clustering communities",
      "real world data",
      "modalities of data",
      "mobile phone data",
      "mobile phone users",
      "social network analysis",
      "human activities",
      "dynamic scenarios",
      "regularization framework",
      "multimodal data",
      "graph"
    ],
    "types": "<task> <task> <material> <otherscientificterm> <material> <material> <task> <task> <task> <method> <material> <otherscientificterm>",
    "relations": [
      "regularization framework -- USED-FOR -- dynamic scenarios",
      "real world data -- EVALUATE-FOR -- regularization framework",
      "mobile phone data -- USED-FOR -- human activities",
      "mobile phone data -- USED-FOR -- social network analysis",
      "regularization framework -- USED-FOR -- evolution of social network information",
      "regularization framework -- USED-FOR -- modalities of data"
    ],
    "abstract": "mobile phone data provides rich dynamic information on <task_7> in <task_6> . in this paper , we represent data from two different <otherscientificterm_3> as a <otherscientificterm_11> and functions defined on the vertex set of the <otherscientificterm_11> . we propose a <method_9> for the joint utilization of these two <otherscientificterm_3> , which enables us to model <task_0> and efficiently classify relationships among <material_5> . simulations based on <material_2> demonstrate the potential application of our <method_9> in <task_8> , and present competitive results to baseline methods for combining <material_10> in the <task_1> .",
    "abstract_og": "mobile phone data provides rich dynamic information on human activities in social network analysis . in this paper , we represent data from two different modalities of data as a graph and functions defined on the vertex set of the graph . we propose a regularization framework for the joint utilization of these two modalities of data , which enables us to model evolution of social network information and efficiently classify relationships among mobile phone users . simulations based on real world data demonstrate the potential application of our regularization framework in dynamic scenarios , and present competitive results to baseline methods for combining multimodal data in the learning and clustering communities ."
  },
  {
    "title": "Reducing time-synchronous beam search effort using stage based look-ahead and language model rank based pruning .",
    "entities": [
      "so-call stage based look-ahead technique",
      "50k-word mandarin dictation task",
      "lm rank based pruning",
      "acoustic model look-ahead",
      "language model look-ahead",
      "large vocabulary speech recognition",
      "time-synchronous beam search",
      "word error rates",
      "hypothesis evaluating criteria",
      "look-ahead technique",
      "lexical tree",
      "phoneme node",
      "search effort",
      "search algorithm",
      "word-conditioned search",
      "look-ahead",
      "recognition"
    ],
    "types": "<method> <material> <method> <method> <method> <task> <task> <metric> <metric> <method> <otherscientificterm> <otherscientificterm> <metric> <method> <method> <otherscientificterm> <task>",
    "relations": [
      "language model look-ahead -- USED-FOR -- look-ahead technique",
      "acoustic model look-ahead -- HYPONYM-OF -- look-ahead technique",
      "language model look-ahead -- CONJUNCTION -- acoustic model look-ahead",
      "time-synchronous beam search -- USED-FOR -- large vocabulary speech recognition",
      "50k-word mandarin dictation task -- EVALUATE-FOR -- recognition",
      "acoustic model look-ahead -- USED-FOR -- time-synchronous beam search"
    ],
    "abstract": "in this paper , we present an efficient <method_9> based on both the <method_4> and the <method_3> , for the <task_6> in the <task_5> . in this <method_0> , two predicting processes with different <metric_8> are organized by stages according to the different requirements for pruning the unlikely surviving hypotheses . furthermore , in order to reduce the efforts for distributing the <method_3> over the <otherscientificterm_10> more effectively , the <method_2> is integrated with the extension of each new <otherscientificterm_11> . the <task_16> experiments performed on the <material_1> show that a reduction by 10 percents in the <metric_12> in comparison with the standard <method_14> using <method_3> only , and a reduction of 25 percents in the <metric_7> in comparison with the <method_13> without any <otherscientificterm_15> can be achieved .",
    "abstract_og": "in this paper , we present an efficient look-ahead technique based on both the language model look-ahead and the acoustic model look-ahead , for the time-synchronous beam search in the large vocabulary speech recognition . in this so-call stage based look-ahead technique , two predicting processes with different hypothesis evaluating criteria are organized by stages according to the different requirements for pruning the unlikely surviving hypotheses . furthermore , in order to reduce the efforts for distributing the acoustic model look-ahead over the lexical tree more effectively , the lm rank based pruning is integrated with the extension of each new phoneme node . the recognition experiments performed on the 50k-word mandarin dictation task show that a reduction by 10 percents in the search effort in comparison with the standard word-conditioned search using acoustic model look-ahead only , and a reduction of 25 percents in the word error rates in comparison with the search algorithm without any look-ahead can be achieved ."
  },
  {
    "title": "Reconstructing Occluded Surfaces Using Synthetic Apertures : Stereo , Focus and Robust Measures .",
    "entities": [
      "synthetic aperture focus",
      "synthetic apertures",
      "cost functions",
      "color medians",
      "classical shape",
      "multi-view stereo",
      "light fields",
      "3d reconstruction",
      "design space",
      "robustness",
      "occlusions",
      "images",
      "surfaces",
      "entropy",
      "oc-clusions",
      "occlusion",
      "accuracy",
      "ssd"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <metric> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <method>",
    "relations": [
      "images -- USED-FOR -- 3d reconstruction",
      "entropy -- USED-FOR -- multi-view stereo",
      "color medians -- USED-FOR -- multi-view stereo",
      "synthetic aperture focus -- USED-FOR -- classical shape",
      "color medians -- CONJUNCTION -- entropy",
      "cost functions -- USED-FOR -- 3d reconstruction"
    ],
    "abstract": "most algorithms for <task_7> from <material_11> use <otherscientificterm_2> based on <method_17> , which assume that the <otherscientificterm_12> being reconstructed are visible to all cameras . this makes it difficult to reconstruct objects which are partially occluded . recently , researchers working with large camera arrays have shown it is possible to '' see through '' <otherscientificterm_14> using a technique called synthetic aperture focus-ing . this suggests that we can design alternative <otherscientificterm_2> that are robust to <otherscientificterm_10> using <otherscientificterm_1> . our paper explores this <otherscientificterm_8> . we compare <otherscientificterm_4> from stereo with shape from <otherscientificterm_0> . we also describe two variants of <method_5> based on <otherscientificterm_3> and <otherscientificterm_13> that increase <metric_9> to <otherscientificterm_14> . we present an experimental comparison of these <otherscientificterm_2> on complex <otherscientificterm_6> , measuring their <metric_16> against the amount of <otherscientificterm_15> .",
    "abstract_og": "most algorithms for 3d reconstruction from images use cost functions based on ssd , which assume that the surfaces being reconstructed are visible to all cameras . this makes it difficult to reconstruct objects which are partially occluded . recently , researchers working with large camera arrays have shown it is possible to '' see through '' oc-clusions using a technique called synthetic aperture focus-ing . this suggests that we can design alternative cost functions that are robust to occlusions using synthetic apertures . our paper explores this design space . we compare classical shape from stereo with shape from synthetic aperture focus . we also describe two variants of multi-view stereo based on color medians and entropy that increase robustness to oc-clusions . we present an experimental comparison of these cost functions on complex light fields , measuring their accuracy against the amount of occlusion ."
  },
  {
    "title": "The voice-rate dialog system for consumer ratings .",
    "entities": [
      "toll-free phone number",
      "voice rate system",
      "review synthesis",
      "telephone playback",
      "dialog system",
      "robust name-matching",
      "voice-rate"
    ],
    "types": "<material> <method> <task> <task> <method> <task> <material>",
    "relations": [
      "voice-rate -- HYPONYM-OF -- dialog system",
      "review synthesis -- USED-FOR -- telephone playback"
    ],
    "abstract": "voice-rate is an experimental <method_4> that makes product and business ratings available to consumers via a <material_0> . by calling <material_6> , users can access the ratings of more than one million products , a quarter million local businesses -lrb- restaurants -rrb- , and three thousand national businesses . this paper describes the <method_1> , and solutions to three key technical challenges : <task_5> , efficient disambiguation , and <task_2> for <task_3> . <material_6> can be accessed by calling 1-877-456-data -lrb- toll-free -rrb- within the u.s.",
    "abstract_og": "voice-rate is an experimental dialog system that makes product and business ratings available to consumers via a toll-free phone number . by calling voice-rate , users can access the ratings of more than one million products , a quarter million local businesses -lrb- restaurants -rrb- , and three thousand national businesses . this paper describes the voice rate system , and solutions to three key technical challenges : robust name-matching , efficient disambiguation , and review synthesis for telephone playback . voice-rate can be accessed by calling 1-877-456-data -lrb- toll-free -rrb- within the u.s."
  },
  {
    "title": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks .",
    "entities": [
      "movement of the pen",
      "raw or pre-processed data",
      "raw on-line handwriting data",
      "sequence labelling algorithms",
      "proba-bilistic language model",
      "unconstrained on-line database",
      "on-line handwriting recognition",
      "sequence labelling tasks",
      "recurrent neural network",
      "raw data",
      "sequence labelling",
      "pre-processing",
      "hmm",
      "hmms"
    ],
    "types": "<otherscientificterm> <material> <material> <method> <method> <material> <task> <task> <method> <material> <task> <method> <method> <method>",
    "relations": [
      "raw or pre-processed data -- USED-FOR -- unconstrained on-line database",
      "on-line handwriting recognition -- USED-FOR -- sequence labelling tasks",
      "hmms -- HYPONYM-OF -- sequence labelling algorithms",
      "recurrent neural network -- USED-FOR -- sequence labelling"
    ],
    "abstract": "on-line handwriting recognition is unusual among <task_7> in that the underlying generator of the observed data , i.e. the <otherscientificterm_0> , is recorded directly . however , the <material_9> can be difficult to interpret because each letter is spread over many pen locations . as a consequence , sophisticated <method_11> is required to obtain inputs suitable for conventional <method_3> , such as <method_13> . in this paper we describe a system capable of directly transcribing <material_2> . the system consists of a <method_8> trained for <task_10> , combined with a <method_4> . in experiments on an <material_5> , we record excellent results using either <material_1> , well outperforming a state-of-the-art <method_12> in both cases .",
    "abstract_og": "on-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data , i.e. the movement of the pen , is recorded directly . however , the raw data can be difficult to interpret because each letter is spread over many pen locations . as a consequence , sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms , such as hmms . in this paper we describe a system capable of directly transcribing raw on-line handwriting data . the system consists of a recurrent neural network trained for sequence labelling , combined with a proba-bilistic language model . in experiments on an unconstrained on-line database , we record excellent results using either raw or pre-processed data , well outperforming a state-of-the-art hmm in both cases ."
  },
  {
    "title": "Source number estimation in reverberant conditions via full-band weighted , adaptive fuzzy c-means clustering .",
    "entities": [
      "adaptive fuzzy c-means clustering",
      "spatial feature vectors",
      "source number estimation",
      "adaptive variation",
      "quality measures",
      "real-world recordings",
      "microphone observations",
      "clustering algorithm",
      "full-band manner",
      "fuzzy c-means",
      "clusters"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <metric> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "microphone observations -- USED-FOR -- spatial feature vectors",
      "adaptive variation -- USED-FOR -- full-band manner"
    ],
    "abstract": "we introduce a novel approach for <task_2> through an <method_0> . <otherscientificterm_1> are extracted from <otherscientificterm_6> , weighted for reliability and then clustered in a <otherscientificterm_8> using an <otherscientificterm_3> on the <otherscientificterm_9> . a number of <metric_4> are combined to produce a weighted sum which is used to find the optimal number of <otherscientificterm_10> at each iteration of the <method_7> . experimental evaluations using <material_5> from a reverberant room -lrb- rt 60 = 390 ms -rrb- demonstrated encouraging performance in both even-and under-determined conditions .",
    "abstract_og": "we introduce a novel approach for source number estimation through an adaptive fuzzy c-means clustering . spatial feature vectors are extracted from microphone observations , weighted for reliability and then clustered in a full-band manner using an adaptive variation on the fuzzy c-means . a number of quality measures are combined to produce a weighted sum which is used to find the optimal number of clusters at each iteration of the clustering algorithm . experimental evaluations using real-world recordings from a reverberant room -lrb- rt 60 = 390 ms -rrb- demonstrated encouraging performance in both even-and under-determined conditions ."
  },
  {
    "title": "i , Poet : Automatic Chinese Poetry Composition through a Generative Summarization Framework under Constrained Optimization .",
    "entities": [
      "automatic chinese poetry composition",
      "classical ancient chinese poems",
      "song dynasty of china",
      "user specified writing intents",
      "tonal and rhythm requirements",
      "artificial intelligence assistance",
      "iterative term substitutions",
      "poetry composition task",
      "gener-ative summarization framework",
      "human judgments",
      "computational linguistics",
      "poetry formats",
      "generated poem",
      "linguistic rules",
      "optimization problem",
      "strict formats",
      "rouge scores",
      "optimization process"
    ],
    "types": "<task> <material> <material> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <method> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <metric> <method>",
    "relations": [
      "gener-ative summarization framework -- USED-FOR -- poetry composition task",
      "iterative term substitutions -- USED-FOR -- optimization process",
      "gener-ative summarization framework -- USED-FOR -- optimization problem",
      "optimization problem -- USED-FOR -- poetry composition task"
    ],
    "abstract": "part of the long lasting cultural heritage of china is the <material_1> which follow <otherscientificterm_15> and complicated <otherscientificterm_13> . <task_0> by programs is considered as a challenging problem in <otherscientificterm_10> and requires high <task_5> , and has not been well addressed . in this paper , we formulate the <task_7> as an <task_14> based on a <method_8> under several constraints . given the <otherscientificterm_3> , the system retrieves candidate terms out of a large poem corpus , and then orders these terms to fit into <otherscientificterm_11> , satisfying <otherscientificterm_4> . the <method_17> under constraints is conducted via <otherscientificterm_6> till convergence , and outputs the subset with the highest utility as the <material_12> . for experiments , we perform generation on large datasets of 61,960 classic poems from tang and <material_2> . a comprehensive evaluation , using both <material_9> and <metric_16> , has demonstrated the effectiveness of our proposed approach .",
    "abstract_og": "part of the long lasting cultural heritage of china is the classical ancient chinese poems which follow strict formats and complicated linguistic rules . automatic chinese poetry composition by programs is considered as a challenging problem in computational linguistics and requires high artificial intelligence assistance , and has not been well addressed . in this paper , we formulate the poetry composition task as an optimization problem based on a gener-ative summarization framework under several constraints . given the user specified writing intents , the system retrieves candidate terms out of a large poem corpus , and then orders these terms to fit into poetry formats , satisfying tonal and rhythm requirements . the optimization process under constraints is conducted via iterative term substitutions till convergence , and outputs the subset with the highest utility as the generated poem . for experiments , we perform generation on large datasets of 61,960 classic poems from tang and song dynasty of china . a comprehensive evaluation , using both human judgments and rouge scores , has demonstrated the effectiveness of our proposed approach ."
  },
  {
    "title": "Multi-Label Manifold Learning .",
    "entities": [
      "local topological structure",
      "smoothness assumption",
      "feature manifold",
      "label space",
      "label manifold",
      "multi-label learning",
      "euclidean space",
      "manifold"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "label manifold -- EVALUATE-FOR -- multi-label learning",
      "label space -- FEATURE-OF -- manifold",
      "label space -- FEATURE-OF -- multi-label learning"
    ],
    "abstract": "this paper gives an attempt to explore the <otherscientificterm_7> in the <otherscientificterm_3> for <task_5> . traditional <otherscientificterm_3> is logical , where no <otherscientificterm_7> exists . in order to study the <otherscientificterm_4> , the <otherscientificterm_3> should be extended to a <otherscientificterm_6> . however , the label man-ifold is not explicitly available from the training examples . fortunately , according to the <otherscientificterm_1> that the points close to each other are more likely to share a label , the <otherscientificterm_0> can be shared between the <otherscientificterm_2> and the <otherscientificterm_4> . based on this , we propose a novel method called ml 2 , i.e. , multi-label manifold learning , to reconstruct and exploit the <otherscientificterm_4> . to our best knowledge , it is one of the first attempts to explore the <otherscientificterm_7> in the <otherscientificterm_3> in <task_5> . extensive experiments show that the performance of <task_5> can be improved significantly with the <otherscientificterm_4> .",
    "abstract_og": "this paper gives an attempt to explore the manifold in the label space for multi-label learning . traditional label space is logical , where no manifold exists . in order to study the label manifold , the label space should be extended to a euclidean space . however , the label man-ifold is not explicitly available from the training examples . fortunately , according to the smoothness assumption that the points close to each other are more likely to share a label , the local topological structure can be shared between the feature manifold and the label manifold . based on this , we propose a novel method called ml 2 , i.e. , multi-label manifold learning , to reconstruct and exploit the label manifold . to our best knowledge , it is one of the first attempts to explore the manifold in the label space in multi-label learning . extensive experiments show that the performance of multi-label learning can be improved significantly with the label manifold ."
  },
  {
    "title": "Mobile Beamforming & Spatially Controlled Relay Communications .",
    "entities": [
      "single-source single-destination robotic relay networks",
      "2-stage stochastic programming formulation",
      "lower bound relaxation",
      "cooperative beam-forming framework",
      "stochastic decision making",
      "random causal csi",
      "nontrivial optimization problem",
      "total beamforming power",
      "spatiotemporal stochastic field",
      "stochastic motion planning",
      "communication medium",
      "control policy",
      "spatial controllers",
      "predictive character",
      "relay positions",
      "relay locations",
      "relay"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <method> <material> <task> <otherscientificterm> <method> <task> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "stochastic motion planning -- PART-OF -- single-source single-destination robotic relay networks",
      "random causal csi -- USED-FOR -- stochastic decision making",
      "cooperative beam-forming framework -- USED-FOR -- stochastic motion planning",
      "control policy -- USED-FOR -- stochastic decision making",
      "predictive character -- FEATURE-OF -- spatial controllers",
      "spatial controllers -- EVALUATE-FOR -- 2-stage stochastic programming formulation"
    ],
    "abstract": "we consider <task_9> in <task_0> , under a <method_3> . assuming that the <otherscientificterm_10> constitutes a <method_8> , we propose a <method_1> of the problem of specifying the positions of the relays , such that the expected reciprocal of their <otherscientificterm_7> is maximized . <method_4> is made on the basis of <material_5> . recognizing the intractability of the original problem , we propose a <otherscientificterm_2> , resulting to a <task_6> with respect to the <otherscientificterm_15> , which is equivalent to a small set of simple , tractable subproblems . our <method_1> results in <method_12> with a <otherscientificterm_13> ; at each time slot , the new <otherscientificterm_14> should be such that the expected power reciprocal at the next time slot is maximized . quite interestingly , the optimal <method_11> to the <method_4> is purely selective ; under a certain sense , only the best <material_16> should move .",
    "abstract_og": "we consider stochastic motion planning in single-source single-destination robotic relay networks , under a cooperative beam-forming framework . assuming that the communication medium constitutes a spatiotemporal stochastic field , we propose a 2-stage stochastic programming formulation of the problem of specifying the positions of the relays , such that the expected reciprocal of their total beamforming power is maximized . stochastic decision making is made on the basis of random causal csi . recognizing the intractability of the original problem , we propose a lower bound relaxation , resulting to a nontrivial optimization problem with respect to the relay locations , which is equivalent to a small set of simple , tractable subproblems . our 2-stage stochastic programming formulation results in spatial controllers with a predictive character ; at each time slot , the new relay positions should be such that the expected power reciprocal at the next time slot is maximized . quite interestingly , the optimal control policy to the stochastic decision making is purely selective ; under a certain sense , only the best relay should move ."
  },
  {
    "title": "Validation of acoustic models of auditory neural prostheses .",
    "entities": [
      "cochlear implant speech processor",
      "auditory neural prostheses",
      "noise bands",
      "acoustic models",
      "cochlear implant",
      "electrical stimulation",
      "percepts"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "acoustic models -- USED-FOR -- percepts"
    ],
    "abstract": "acoustic models have been used in numerous studies over the past thirty years to simulate the <otherscientificterm_6> elicited by <method_1> . in these <method_3> , incoming signals are processed the same way as in a <method_0> . the <otherscientificterm_6> that would be caused by <otherscientificterm_5> in a real <otherscientificterm_4> are simulated by modulating the amplitude of either <otherscientificterm_2> or sinusoids . despite their practical usefulness these <method_3> have never been convincingly validated . this study presents a tool to conduct such validation using subjects who have a <otherscientificterm_4> in one ear and have near perfect hearing in the other ear , allowing for the first time a direct perceptual comparison of the output of <method_3> to the stimulation provided by a <otherscientificterm_4> .",
    "abstract_og": "acoustic models have been used in numerous studies over the past thirty years to simulate the percepts elicited by auditory neural prostheses . in these acoustic models , incoming signals are processed the same way as in a cochlear implant speech processor . the percepts that would be caused by electrical stimulation in a real cochlear implant are simulated by modulating the amplitude of either noise bands or sinusoids . despite their practical usefulness these acoustic models have never been convincingly validated . this study presents a tool to conduct such validation using subjects who have a cochlear implant in one ear and have near perfect hearing in the other ear , allowing for the first time a direct perceptual comparison of the output of acoustic models to the stimulation provided by a cochlear implant ."
  },
  {
    "title": "Adequacy Analysis of Simulation-Based Assessment of Speech Recognition System .",
    "entities": [
      "speech recognition systems",
      "noisy speech recognition",
      "impulse response accuracies",
      "acoustic characteristics",
      "noisy conditions",
      "lombard effect",
      "lombard effects",
      "speech recognition"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "impulse response accuracies -- CONJUNCTION -- lombard effects",
      "lombard effect -- USED-FOR -- acoustic characteristics",
      "noisy conditions -- FEATURE-OF -- speech recognition systems",
      "lombard effects -- FEATURE-OF -- speech recognition"
    ],
    "abstract": "the adequacies of the simulation-based assessment of <method_0> under <otherscientificterm_4> are investigated and discussed . to evaluate the <method_0> in various environments , it is desirable to collect the test data uttered in the corresponding environments but it is not realistic since enormous works are required . to conduct evaluations of the <method_0> properly , it is promising to simulate evaluation experiments in the target environments as described below : comparatively small test data are collected , and test data of the target environment are generated by computing convolution of the impulse response of the target environment with the collected data . however , it is well known that changes of the <otherscientificterm_3> are caused by <otherscientificterm_5> , and so it is not necessarily obvious whether the simulation can precisely approximate the experiment in actual environment . this paper clarifies the condition to perform effective simulations of the <task_1> , focusing on the influence of <otherscientificterm_2> and <otherscientificterm_6> on the <task_7> performance .",
    "abstract_og": "the adequacies of the simulation-based assessment of speech recognition systems under noisy conditions are investigated and discussed . to evaluate the speech recognition systems in various environments , it is desirable to collect the test data uttered in the corresponding environments but it is not realistic since enormous works are required . to conduct evaluations of the speech recognition systems properly , it is promising to simulate evaluation experiments in the target environments as described below : comparatively small test data are collected , and test data of the target environment are generated by computing convolution of the impulse response of the target environment with the collected data . however , it is well known that changes of the acoustic characteristics are caused by lombard effect , and so it is not necessarily obvious whether the simulation can precisely approximate the experiment in actual environment . this paper clarifies the condition to perform effective simulations of the noisy speech recognition , focusing on the influence of impulse response accuracies and lombard effects on the speech recognition performance ."
  },
  {
    "title": "Concept segmentation and labeling for conversational speech .",
    "entities": [
      "conditional random fields",
      "manual transcription of spoken utterances",
      "stochastic finite state transducers",
      "genera-tive and discriminative models",
      "noisy automatic transcriptions",
      "spoken language understanding",
      "complementary learning models",
      "spoken dialog corpora",
      "automatic concept labeling",
      "noisy automatic transcription",
      "discriminative algorithm",
      "kernel methods",
      "manual transcriptions",
      "generative-discriminative model",
      "robustness",
      "media",
      "accuracy"
    ],
    "types": "<method> <material> <method> <method> <material> <task> <method> <material> <task> <task> <method> <method> <material> <method> <metric> <material> <metric>",
    "relations": [
      "complementary learning models -- USED-FOR -- spoken language understanding",
      "spoken language understanding -- USED-FOR -- automatic concept labeling",
      "generative-discriminative model -- USED-FOR -- noisy automatic transcriptions",
      "kernel methods -- USED-FOR -- discriminative algorithm"
    ],
    "abstract": "spoken language understanding performs <task_8> and segmentation of speech utterances . for this task , many approaches have been proposed based on both <method_3> . while all these methods have shown remarkable <metric_16> on <material_1> , <metric_14> to <task_9> is still an open issue . in this paper we study algorithms for <task_5> combining <method_6> : <method_2> produce a list of hypotheses , which are re-ranked using a <method_10> based on <method_11> . our experiments on two different <material_7> , <material_15> and luna , show that the combined <method_13> reaches the state-of-the-art such as <method_0> on <material_12> , and <method_13> is robust to <material_4> , outperforming , in some cases , the state-of-the-art .",
    "abstract_og": "spoken language understanding performs automatic concept labeling and segmentation of speech utterances . for this task , many approaches have been proposed based on both genera-tive and discriminative models . while all these methods have shown remarkable accuracy on manual transcription of spoken utterances , robustness to noisy automatic transcription is still an open issue . in this paper we study algorithms for spoken language understanding combining complementary learning models : stochastic finite state transducers produce a list of hypotheses , which are re-ranked using a discriminative algorithm based on kernel methods . our experiments on two different spoken dialog corpora , media and luna , show that the combined generative-discriminative model reaches the state-of-the-art such as conditional random fields on manual transcriptions , and generative-discriminative model is robust to noisy automatic transcriptions , outperforming , in some cases , the state-of-the-art ."
  },
  {
    "title": "Scalable and Robust Bayesian Inference via the Median Posterior .",
    "entities": [
      "geometric median of subset posterior distributions",
      "ad hoc techniques",
      "bayesian learning methods",
      "distributed computing environments",
      "aggregation step",
      "posterior distribution",
      "bayesian learning",
      "bayesian inference",
      "massive data",
      "stochastic approximation"
    ],
    "types": "<otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <task> <task> <material> <method>",
    "relations": [
      "geometric median of subset posterior distributions -- USED-FOR -- aggregation step",
      "bayesian learning methods -- USED-FOR -- massive data",
      "bayesian learning methods -- USED-FOR -- distributed computing environments"
    ],
    "abstract": "many <method_2> for <material_8> benefit from working with small subsets of observations . in particular , significant progress has been made in scalable <task_6> via <method_9> . however , <method_2> in <task_3> are often problem-or distribution-specific and use <method_1> . we propose a novel general approach to <task_7> that is scalable and robust to corruption in the data . our technique is based on the idea of splitting the data into several non-overlapping subgroups , evaluating the <otherscientificterm_5> given each independent subgroup , and then combining the results . our main contribution is the proposed <method_4> which is based on finding the <otherscientificterm_0> . presented theoretical and numerical results confirm the advantages of our approach .",
    "abstract_og": "many bayesian learning methods for massive data benefit from working with small subsets of observations . in particular , significant progress has been made in scalable bayesian learning via stochastic approximation . however , bayesian learning methods in distributed computing environments are often problem-or distribution-specific and use ad hoc techniques . we propose a novel general approach to bayesian inference that is scalable and robust to corruption in the data . our technique is based on the idea of splitting the data into several non-overlapping subgroups , evaluating the posterior distribution given each independent subgroup , and then combining the results . our main contribution is the proposed aggregation step which is based on finding the geometric median of subset posterior distributions . presented theoretical and numerical results confirm the advantages of our approach ."
  },
  {
    "title": "An EM-algorithm approach for the design of orthonormal bases adapted to sparse representations .",
    "entities": [
      "probabilistic interpretation of sezer 's algorithm",
      "missed detection rate",
      "sparse representations",
      "dictionary learning",
      "em algorithm",
      "optimization procedure"
    ],
    "types": "<method> <metric> <method> <task> <method> <method>",
    "relations": [
      "dictionary learning -- USED-FOR -- sparse representations",
      "missed detection rate -- EVALUATE-FOR -- probabilistic interpretation of sezer 's algorithm",
      "em algorithm -- USED-FOR -- optimization procedure"
    ],
    "abstract": "in this paper , we consider the problem of <task_3> for <method_2> . several algorithms dealing with this problem can be found in the literature . one of them , introduced by sezer et al. in -lsb- 1 -rsb- optimizes a dictionary made up of the union of orthonor-mal bases . in this paper , we propose a <method_0> and suggest a novel <method_5> based on the <method_4> . comparisons of the performance in terms of <metric_1> show a clear superiority of the proposed <method_0> .",
    "abstract_og": "in this paper , we consider the problem of dictionary learning for sparse representations . several algorithms dealing with this problem can be found in the literature . one of them , introduced by sezer et al. in -lsb- 1 -rsb- optimizes a dictionary made up of the union of orthonor-mal bases . in this paper , we propose a probabilistic interpretation of sezer 's algorithm and suggest a novel optimization procedure based on the em algorithm . comparisons of the performance in terms of missed detection rate show a clear superiority of the proposed probabilistic interpretation of sezer 's algorithm ."
  },
  {
    "title": "The PDAF based Active Contour .",
    "entities": [
      "d i r ectional approach of measurements",
      "directional-based m e asurement model",
      "directional smoothing operator",
      "gradient-based image potential",
      "active contour model",
      "estimation of motion",
      "velocity-based discrimination",
      "image motion",
      "pdaf approach",
      "walking leg",
      "image potential",
      "waving hand",
      "spatio-velocity space",
      "image clutter",
      "optical-ow"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "estimation of motion -- USED-FOR -- pdaf approach",
      "spatio-velocity space -- FEATURE-OF -- active contour model",
      "image potential -- CONJUNCTION -- optical-ow",
      "walking leg -- CONJUNCTION -- waving hand"
    ],
    "abstract": "we present a new <method_4> in <otherscientificterm_12> which is based o n t h e p r obability data association lter <method_8> . <method_4> employs a <method_1> of <otherscientificterm_10> and of <otherscientificterm_14> along the contour points . the proposed <method_0> is the basis for the <otherscientificterm_6> between measurements of the object to that of <otherscientificterm_13> . the <method_4> chooses the appropriate measurements which are most consistent with previous <otherscientificterm_5> in the sense of the <method_8> . in order to obtain reliable measurements of <otherscientificterm_7> and of <otherscientificterm_3> , we propose a <method_2> which is the basis for discriminating the objects measurements from that of <otherscientificterm_13> . the <method_2> was applied t o r eal world tracking problems such as a <otherscientificterm_9> and a <otherscientificterm_11> .",
    "abstract_og": "we present a new active contour model in spatio-velocity space which is based o n t h e p r obability data association lter pdaf approach . active contour model employs a directional-based m e asurement model of image potential and of optical-ow along the contour points . the proposed d i r ectional approach of measurements is the basis for the velocity-based discrimination between measurements of the object to that of image clutter . the active contour model chooses the appropriate measurements which are most consistent with previous estimation of motion in the sense of the pdaf approach . in order to obtain reliable measurements of image motion and of gradient-based image potential , we propose a directional smoothing operator which is the basis for discriminating the objects measurements from that of image clutter . the directional smoothing operator was applied t o r eal world tracking problems such as a walking leg and a waving hand ."
  },
  {
    "title": "Describing objects by their attributes .",
    "entities": [
      "feature selection method",
      "learning attributes",
      "recognition paradigm",
      "identity assignment",
      "learning attributes",
      "attribute-based framework",
      "recognition",
      "naming"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <task> <task>",
    "relations": [
      "feature selection method -- USED-FOR -- learning attributes"
    ],
    "abstract": "we propose to shift the goal of <task_6> from <task_7> to describing . doing so allows us not only to name familiar objects , but also : to report unusual aspects of a familiar object -lrb- '' spotty dog '' , not just '' dog '' -rrb- ; to say something about unfamiliar objects -lrb- '' hairy and four-legged '' , not just '' unknown '' -rrb- ; and to learn how to recognize new objects with few or no visual examples . rather than focusing on <task_3> , we make inferring attributes the core problem of <task_6> . these attributes can be semantic -lrb- '' spotty '' -rrb- or discriminative -lrb- '' dogs have it but sheep do not '' -rrb- . <otherscientificterm_4> presents a major new challenge : generalization across object categories , not just across instances within a category . in this paper , we also introduce a novel <method_0> for <otherscientificterm_1> that generalize well across categories . we support our claims by thorough evaluation that provides insights into the limitations of the standard <method_2> of <task_7> and demonstrates the new abilities provided by our <method_5> .",
    "abstract_og": "we propose to shift the goal of recognition from naming to describing . doing so allows us not only to name familiar objects , but also : to report unusual aspects of a familiar object -lrb- '' spotty dog '' , not just '' dog '' -rrb- ; to say something about unfamiliar objects -lrb- '' hairy and four-legged '' , not just '' unknown '' -rrb- ; and to learn how to recognize new objects with few or no visual examples . rather than focusing on identity assignment , we make inferring attributes the core problem of recognition . these attributes can be semantic -lrb- '' spotty '' -rrb- or discriminative -lrb- '' dogs have it but sheep do not '' -rrb- . learning attributes presents a major new challenge : generalization across object categories , not just across instances within a category . in this paper , we also introduce a novel feature selection method for learning attributes that generalize well across categories . we support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework ."
  },
  {
    "title": "Implementation of Learning-Based Dynamic Demand Response on a Campus Micro-Grid .",
    "entities": [
      "demand response",
      "polynomial time optimization algorithms",
      "real time automated dr",
      "d 2 r algorithms",
      "dr interval",
      "curtailment target",
      "curtailment maximization",
      "load curtailment",
      "sparse data",
      "curtailment requirements",
      "automated dr",
      "forecasting models"
    ],
    "types": "<task> <method> <material> <method> <otherscientificterm> <otherscientificterm> <task> <task> <material> <otherscientificterm> <task> <method>",
    "relations": [
      "forecasting models -- USED-FOR -- curtailment maximization",
      "d 2 r algorithms -- USED-FOR -- load curtailment",
      "sparse data -- USED-FOR -- curtailment maximization",
      "forecasting models -- USED-FOR -- sparse data"
    ],
    "abstract": "demand response -lrb- dr -rrb- allows utilities to curtail electricity consumption during peak demand periods . <material_2> can offer utilities a scalable solution for fine grained control of curtail-ment over small intervals for the duration of the entire dr event . in this work , we demonstrate a system for a real time automated dynamic dr -lrb- d 2 r -rrb- . our system has already been integrated with the electrical infrastructure of the university of southern california , which offers a unique environment to study the impact of <task_10> in a complex social and cultural environment including 170 buildings in a '' city-within-a-city '' scenario . our large scale information processing system coupled with accurate <method_11> for <material_8> and fast <method_1> for <task_6> provide the ability to adapt and respond to changing <otherscientificterm_9> in near real-time . our <method_3> automatically and dynamically select customers for <task_7> to guarantee the achievement of a <otherscientificterm_5> over a given <otherscientificterm_4> .",
    "abstract_og": "demand response -lrb- dr -rrb- allows utilities to curtail electricity consumption during peak demand periods . real time automated dr can offer utilities a scalable solution for fine grained control of curtail-ment over small intervals for the duration of the entire dr event . in this work , we demonstrate a system for a real time automated dynamic dr -lrb- d 2 r -rrb- . our system has already been integrated with the electrical infrastructure of the university of southern california , which offers a unique environment to study the impact of automated dr in a complex social and cultural environment including 170 buildings in a '' city-within-a-city '' scenario . our large scale information processing system coupled with accurate forecasting models for sparse data and fast polynomial time optimization algorithms for curtailment maximization provide the ability to adapt and respond to changing curtailment requirements in near real-time . our d 2 r algorithms automatically and dynamically select customers for load curtailment to guarantee the achievement of a curtailment target over a given dr interval ."
  },
  {
    "title": "Segmentation using superpixels : A bipartite graph partitioning approach .",
    "entities": [
      "unbalanced bipartite graph structure",
      "superpixels -lrb- image segments",
      "berkeley segmenta-tion database",
      "bipartite graph partitioning",
      "linear-time spectral algorithm",
      "segmentation algorithms",
      "guide segmentation",
      "multi-layer superpixels",
      "segmentation framework",
      "grouping cues",
      "superpixels"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <method> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "segmentation framework -- USED-FOR -- linear-time spectral algorithm",
      "berkeley segmenta-tion database -- EVALUATE-FOR -- segmentation framework",
      "segmentation framework -- USED-FOR -- unbalanced bipartite graph structure",
      "segmentation algorithms -- USED-FOR -- superpixels",
      "bipartite graph partitioning -- USED-FOR -- segmentation framework",
      "segmentation framework -- USED-FOR -- multi-layer superpixels"
    ],
    "abstract": "grouping cues can affect the performance of segmenta-tion greatly . in this paper , we show that <otherscientificterm_1> -rrb- can provide powerful grouping cues to <task_6> , where <otherscientificterm_10> can be collected easily by -lrb- over -rrb- - segmenting the image using any reasonable existing <method_5> . generated by different algorithms with varying parameters , <otherscientificterm_10> can capture diverse and multi-scale visual patterns of a natural image . successful integration of the cues from a large multitude of su-perpixels presents a promising yet not fully explored direction . in this paper , we propose a novel <method_8> based on <method_3> , which is able to aggregate <otherscientificterm_7> in a principled and very effective manner . computationally , <method_8> is tailored to <otherscientificterm_0> and leads to a highly efficient , <method_4> . our <method_8> achieves significantly better performance on the <material_2> compared to state-of-the-art techniques .",
    "abstract_og": "grouping cues can affect the performance of segmenta-tion greatly . in this paper , we show that superpixels -lrb- image segments -rrb- can provide powerful grouping cues to guide segmentation , where superpixels can be collected easily by -lrb- over -rrb- - segmenting the image using any reasonable existing segmentation algorithms . generated by different algorithms with varying parameters , superpixels can capture diverse and multi-scale visual patterns of a natural image . successful integration of the cues from a large multitude of su-perpixels presents a promising yet not fully explored direction . in this paper , we propose a novel segmentation framework based on bipartite graph partitioning , which is able to aggregate multi-layer superpixels in a principled and very effective manner . computationally , segmentation framework is tailored to unbalanced bipartite graph structure and leads to a highly efficient , linear-time spectral algorithm . our segmentation framework achieves significantly better performance on the berkeley segmenta-tion database compared to state-of-the-art techniques ."
  },
  {
    "title": "Tuning principal component weights to individualize HRTFs .",
    "entities": [
      "virtual auditory displays",
      "reduced order modeling technique",
      "horizontal plane hrtfs",
      "localization errors",
      "tuning procedure",
      "principal components",
      "spectral features",
      "low-dimensional models",
      "tuning pcws",
      "hrirs"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "low-dimensional models -- USED-FOR -- virtual auditory displays",
      "virtual auditory displays -- USED-FOR -- horizontal plane hrtfs",
      "reduced order modeling technique -- USED-FOR -- virtual auditory displays",
      "spectral features -- FEATURE-OF -- virtual auditory displays",
      "tuning pcws -- USED-FOR -- virtual auditory displays"
    ],
    "abstract": "prior research has investigated development of <method_0> using <method_7> of head related transfer functions -lrb- hrtfs -rrb- as a function of a finite number of <method_5> -lrb- pcs -rrb- and associated weights -lrb- <method_0> -rrb- . this paper investigates the effect of <method_0> on <otherscientificterm_2> derived from a database of <method_9> through analytical optimization experiments . the experiments investigate whether average hrtfs can be tuned to match individual hrtfs . results provide insight on the effect of <method_8> on <otherscientificterm_6> of the <method_0> . a <method_1> is used to compactly represent each <method_0> . subject testing results are provided , showing that a human can conduct the <method_4> and reduce <otherscientificterm_3> .",
    "abstract_og": "prior research has investigated development of virtual auditory displays using low-dimensional models of head related transfer functions -lrb- hrtfs -rrb- as a function of a finite number of principal components -lrb- pcs -rrb- and associated weights -lrb- virtual auditory displays -rrb- . this paper investigates the effect of virtual auditory displays on horizontal plane hrtfs derived from a database of hrirs through analytical optimization experiments . the experiments investigate whether average hrtfs can be tuned to match individual hrtfs . results provide insight on the effect of tuning pcws on spectral features of the virtual auditory displays . a reduced order modeling technique is used to compactly represent each virtual auditory displays . subject testing results are provided , showing that a human can conduct the tuning procedure and reduce localization errors ."
  },
  {
    "title": "Pranking with Ranking .",
    "entities": [
      "mistake bound model",
      "rank-prediction rule",
      "eachmovie dataset",
      "collaborative filtering",
      "online algorithm",
      "synthetic data",
      "online algorithms",
      "ranking instances",
      "rank",
      "ranking",
      "rating",
      "integer",
      "classification"
    ],
    "types": "<method> <otherscientificterm> <material> <task> <method> <material> <method> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "synthetic data -- CONJUNCTION -- eachmovie dataset",
      "online algorithms -- USED-FOR -- classification",
      "classification -- USED-FOR -- ranking",
      "online algorithms -- USED-FOR -- ranking"
    ],
    "abstract": "we discuss the problem of <task_7> . in our framework each instance is associated with a <otherscientificterm_8> or a <otherscientificterm_10> , which is an <otherscientificterm_11> from 1 to k . our goal is to find a <otherscientificterm_1> that assigns each instance a <otherscientificterm_8> which is as close as possible to the instance 's true <otherscientificterm_8> . we describe a simple and efficient <method_4> , analyze its performance in the <method_0> , and prove its correctness . we describe two sets of experiments , with <material_5> and with the <material_2> for <task_3> . in the experiments we performed , our algorithm outper-forms <method_6> for regression and <task_12> applied to <task_9> .",
    "abstract_og": "we discuss the problem of ranking instances . in our framework each instance is associated with a rank or a rating , which is an integer from 1 to k . our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance 's true rank . we describe a simple and efficient online algorithm , analyze its performance in the mistake bound model , and prove its correctness . we describe two sets of experiments , with synthetic data and with the eachmovie dataset for collaborative filtering . in the experiments we performed , our algorithm outper-forms online algorithms for regression and classification applied to ranking ."
  },
  {
    "title": "Optimum error nonlinearities for long adaptive filters .",
    "entities": [
      "adaptive lters",
      "optimum nonlinearity",
      "error nonlinearities",
      "estimation process",
      "steady-state error"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "error nonlinearities -- FEATURE-OF -- adaptive lters"
    ],
    "abstract": "in this paper , we consider the class of <otherscientificterm_0> with <otherscientificterm_2> . in particular , we derive an expression for the <otherscientificterm_1> that minimizes the <otherscientificterm_4> and attains the limit mandated by the cramer-rao bound of the underlying <method_3> .",
    "abstract_og": "in this paper , we consider the class of adaptive lters with error nonlinearities . in particular , we derive an expression for the optimum nonlinearity that minimizes the steady-state error and attains the limit mandated by the cramer-rao bound of the underlying estimation process ."
  },
  {
    "title": "A switched DPCM/subband coder for pre-echo reduction .",
    "entities": [
      "temporally varying bit allocation scheme",
      "adaptive subband coders",
      "adaptive subband coder",
      "human auditory system",
      "wavelet packet decomposition",
      "temporal masking properties",
      "switched dpcm/subband structure",
      "pre-echo artifact",
      "psychoacoustic modelling",
      "pre-echo problem",
      "transient signals",
      "stationary signals",
      "coder"
    ],
    "types": "<method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <material> <method>",
    "relations": [
      "wavelet packet decomposition -- CONJUNCTION -- psychoacoustic modelling",
      "wavelet packet decomposition -- USED-FOR -- adaptive subband coders",
      "coder -- USED-FOR -- pre-echo artifact",
      "adaptive subband coders -- USED-FOR -- stationary signals",
      "switched dpcm/subband structure -- USED-FOR -- pre-echo problem",
      "psychoacoustic modelling -- USED-FOR -- adaptive subband coders"
    ],
    "abstract": "recently , <method_1> based on <method_4> and <method_8> have been proposed to achieve transparent quality compression of audio signals -lsb- 1 -rsb- , -lsb- 2 -rsb- . while these <method_1> perform well for <material_11> , there is no special mechanism in the <method_12> to prevent the <otherscientificterm_7> when <otherscientificterm_10> are encoded . in this paper , we propose a <otherscientificterm_6> to remove the <task_9> . this is achieved through a novel <method_0> which is based on the <otherscientificterm_5> of the <method_3> . the proposed coder/decoder output is found to be free from the <otherscientificterm_7> even at a lower bitrate than the <method_2> .",
    "abstract_og": "recently , adaptive subband coders based on wavelet packet decomposition and psychoacoustic modelling have been proposed to achieve transparent quality compression of audio signals -lsb- 1 -rsb- , -lsb- 2 -rsb- . while these adaptive subband coders perform well for stationary signals , there is no special mechanism in the coder to prevent the pre-echo artifact when transient signals are encoded . in this paper , we propose a switched dpcm/subband structure to remove the pre-echo problem . this is achieved through a novel temporally varying bit allocation scheme which is based on the temporal masking properties of the human auditory system . the proposed coder/decoder output is found to be free from the pre-echo artifact even at a lower bitrate than the adaptive subband coder ."
  },
  {
    "title": "Land use classification of SAR images using a type II local discriminant basis for preprocessing .",
    "entities": [
      "synthetic aperture radar images",
      "classifying small image blocks",
      "wavelet packet decomposition",
      "land use classification",
      "decision process",
      "image blocks",
      "feature extraction",
      "classification algorithm",
      "feature vector",
      "discriminant coordinates",
      "spatial information",
      "image block"
    ],
    "types": "<material> <task> <method> <task> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "wavelet packet decomposition -- PART-OF -- image block",
      "spatial information -- PART-OF -- classification algorithm",
      "feature extraction -- USED-FOR -- land use classification",
      "discriminant coordinates -- PART-OF -- image block",
      "spatial information -- PART-OF -- decision process"
    ],
    "abstract": "in this paper , we present the application of the type ii local discriminant basis -lrb- ldb -rrb- technique to <task_6> for <task_3> in <material_0> . our <method_7> incorporates <otherscientificterm_10> into the <method_4> by <task_1> , instead of single pixels . a <otherscientificterm_8> composed of all the values in the <otherscientificterm_5> is large for even small <otherscientificterm_5> and , therefore , degrades the performance of many classi-fiers . the <method_7> greatly compresses the dimensionality of the <otherscientificterm_8> , by indicating the most <otherscientificterm_9> within the <method_2> of an <otherscientificterm_11> .",
    "abstract_og": "in this paper , we present the application of the type ii local discriminant basis -lrb- ldb -rrb- technique to feature extraction for land use classification in synthetic aperture radar images . our classification algorithm incorporates spatial information into the decision process by classifying small image blocks , instead of single pixels . a feature vector composed of all the values in the image blocks is large for even small image blocks and , therefore , degrades the performance of many classi-fiers . the classification algorithm greatly compresses the dimensionality of the feature vector , by indicating the most discriminant coordinates within the wavelet packet decomposition of an image block ."
  },
  {
    "title": "Towards Interactive Text Understanding .",
    "entities": [
      "semantics-based text authoring system",
      "automatic information extraction",
      "deep semantic analysis",
      "text understanding",
      "human intervention",
      "interactive approach"
    ],
    "types": "<method> <task> <method> <task> <otherscientificterm> <method>",
    "relations": [
      "interactive approach -- USED-FOR -- text understanding",
      "semantics-based text authoring system -- USED-FOR -- interactive approach",
      "interactive approach -- USED-FOR -- deep semantic analysis"
    ],
    "abstract": "this position paper argues for an <method_5> to <task_3> . the proposed <method_5> extends an existing <method_0> by using the input text as a source of information to assist the user in re-authoring its content . the <method_5> permits a reliable <method_2> by combining <task_1> with a minimal amount of <otherscientificterm_4> .",
    "abstract_og": "this position paper argues for an interactive approach to text understanding . the proposed interactive approach extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content . the interactive approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention ."
  },
  {
    "title": "A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling .",
    "entities": [
      "multi-level clustering hierarchical dirichlet process",
      "independent human physician clusterings",
      "mlc-hdp 's clustering",
      "hierarchical dirich-let process",
      "epilepsy literature",
      "modeling seizures",
      "brain activity",
      "clustering seizures",
      "epileptic seizures",
      "seizure-types",
      "channels-types",
      "mlc-hdp"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <material> <task> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "brain activity -- PART-OF -- hierarchical dirich-let process",
      "multi-level clustering hierarchical dirichlet process -- HYPONYM-OF -- hierarchical dirich-let process",
      "mlc-hdp 's clustering -- HYPONYM-OF -- epilepsy literature",
      "mlc-hdp 's clustering -- USED-FOR -- clustering seizures",
      "channels-types -- CONJUNCTION -- seizure-types",
      "mlc-hdp -- COMPARE -- hierarchical dirich-let process",
      "channels-types -- FEATURE-OF -- mlc-hdp 's clustering"
    ],
    "abstract": "driven by the multi-level structure of human intracranial electroencephalogram -lrb- ieeg -rrb- recordings of <material_8> , we introduce a new variant of a <method_3> -- the <method_0> -- that simultaneously clusters datasets on multiple levels . our <method_3> contains <otherscientificterm_6> recorded in typically more than a hundred individual channels for each seizure of each patient . the <method_2> clusters over <otherscientificterm_10> , <otherscientificterm_9> , and patient-types simultaneously . we describe this <method_3> and its implementation in detail . we also present the results of a simulation study comparing the <method_11> to a similar <method_3> , the <method_3> and finally demonstrate the <method_11> 's use in <task_5> across multiple patients . we find the <method_2> to be comparable to <otherscientificterm_1> . to our knowledge , the <method_2> is the first in the <material_4> capable of <task_7> within and between patients .",
    "abstract_og": "driven by the multi-level structure of human intracranial electroencephalogram -lrb- ieeg -rrb- recordings of epileptic seizures , we introduce a new variant of a hierarchical dirich-let process -- the multi-level clustering hierarchical dirichlet process -- that simultaneously clusters datasets on multiple levels . our hierarchical dirich-let process contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient . the mlc-hdp 's clustering clusters over channels-types , seizure-types , and patient-types simultaneously . we describe this hierarchical dirich-let process and its implementation in detail . we also present the results of a simulation study comparing the mlc-hdp to a similar hierarchical dirich-let process , the hierarchical dirich-let process and finally demonstrate the mlc-hdp 's use in modeling seizures across multiple patients . we find the mlc-hdp 's clustering to be comparable to independent human physician clusterings . to our knowledge , the mlc-hdp 's clustering is the first in the epilepsy literature capable of clustering seizures within and between patients ."
  },
  {
    "title": "Low-dimensional models of neural population activity in sensory cortical circuits .",
    "entities": [
      "nonlinear receptive field model",
      "idiosyncratic receptive field shapes",
      "neural population activity",
      "shared stimulus drive",
      "nonlinear stimulus inputs",
      "latent dynamical model",
      "online expectation maximization",
      "neural tuning properties",
      "fast estimation method",
      "ongoing cortical activity",
      "low-dimensional dynamical model",
      "local circuits",
      "computational neuroscience",
      "inference scales",
      "recording duration",
      "common noise",
      "visual cortex",
      "laplace approximations",
      "visual stimuli",
      "stimulus representations",
      "nonlinear inputs",
      "population size",
      "multi-neuron recordings",
      "multi-channel recordings",
      "temporal dynamics",
      "cortical dynamics",
      "neural responses",
      "statistical model",
      "cross-neural correlations",
      "features"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "nonlinear receptive field model -- USED-FOR -- idiosyncratic receptive field shapes",
      "shared stimulus drive -- CONJUNCTION -- common noise",
      "nonlinear receptive field model -- USED-FOR -- multi-channel recordings",
      "neural responses -- USED-FOR -- visual cortex",
      "nonlinear receptive field model -- USED-FOR -- temporal dynamics",
      "visual stimuli -- USED-FOR -- visual cortex",
      "online expectation maximization -- USED-FOR -- fast estimation method",
      "laplace approximations -- USED-FOR -- fast estimation method",
      "fast estimation method -- USED-FOR -- cross-neural correlations",
      "fast estimation method -- USED-FOR -- neural tuning properties",
      "features -- USED-FOR -- stimulus representations",
      "visual stimuli -- USED-FOR -- neural responses"
    ],
    "abstract": "neural responses in <otherscientificterm_16> are influenced by <otherscientificterm_18> and by ongoing spiking activity in <otherscientificterm_11> . an important challenge in <method_12> is to develop models that can account for both of these <otherscientificterm_29> in large <material_22> and to reveal how <method_19> interact with and depend on <otherscientificterm_25> . here we introduce a <method_27> of <otherscientificterm_2> that integrates a <method_0> with a <method_5> of <otherscientificterm_9> . this <method_0> captures <otherscientificterm_24> and correlations due to <otherscientificterm_3> as well as <otherscientificterm_15> . moreover , because the <otherscientificterm_4> are mixed by the ongoing dynamics , the <method_0> can account for a multiple <otherscientificterm_1> with a small number of <otherscientificterm_20> to a <method_10> . we introduce a <method_8> using <method_6> with <method_17> , for which <otherscientificterm_13> linearly in both <otherscientificterm_21> and <otherscientificterm_14> . we test this <method_0> to <material_23> from primary <otherscientificterm_16> and show that <method_8> accounts for <otherscientificterm_7> as well as <otherscientificterm_28> .",
    "abstract_og": "neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits . an important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics . here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity . this nonlinear receptive field model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise . moreover , because the nonlinear stimulus inputs are mixed by the ongoing dynamics , the nonlinear receptive field model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model . we introduce a fast estimation method using online expectation maximization with laplace approximations , for which inference scales linearly in both population size and recording duration . we test this nonlinear receptive field model to multi-channel recordings from primary visual cortex and show that fast estimation method accounts for neural tuning properties as well as cross-neural correlations ."
  },
  {
    "title": "Efficient iterative mean shift based cosine dissimilarity for multi-recording speaker clustering .",
    "entities": [
      "iterative mean shift algorithm",
      "speaker and cluster impurities",
      "nist sre 2008 datasets",
      "speech recognition",
      "euclidean distance",
      "speaker diarization",
      "speaker clustering",
      "cosine distance"
    ],
    "types": "<method> <otherscientificterm> <material> <task> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "speaker diarization -- CONJUNCTION -- speech recognition",
      "cosine distance -- USED-FOR -- iterative mean shift algorithm",
      "euclidean distance -- USED-FOR -- iterative mean shift algorithm"
    ],
    "abstract": "speaker clustering is an important task in many applications such as <task_5> as well as <task_3> . <method_6> can be done within a single multi-speaker recording -lrb- diarization -rrb- or for a set of different recordings . in this work we are interested by the former case and we propose a simple <method_0> to deal with this problem . traditionally , <method_0> is based on <otherscientificterm_4> . we propose to use the <otherscientificterm_7> in order to build a new version of <method_0> . we report results as measured by <otherscientificterm_1> on <material_2> .",
    "abstract_og": "speaker clustering is an important task in many applications such as speaker diarization as well as speech recognition . speaker clustering can be done within a single multi-speaker recording -lrb- diarization -rrb- or for a set of different recordings . in this work we are interested by the former case and we propose a simple iterative mean shift algorithm to deal with this problem . traditionally , iterative mean shift algorithm is based on euclidean distance . we propose to use the cosine distance in order to build a new version of iterative mean shift algorithm . we report results as measured by speaker and cluster impurities on nist sre 2008 datasets ."
  },
  {
    "title": "New results in low bitrate audio coding using a combined harmonic-wavelet representation .",
    "entities": [
      "total least squares",
      "harmonic - wavelet representation",
      "step -lrb- harmonic analysis",
      "wavelet ltering schemes",
      "harmonic analysis-synthesis scheme",
      "wavelet ltering scheme",
      "reconstructed harmonic signal",
      "audio signal quality",
      "m-band wavelet",
      "harmonic analysis-scheme",
      "prony algorithm",
      "audio coder",
      "harmonic analysis-synthesis",
      "audio frame",
      "encoder bitrates",
      "dier-ence",
      "residual"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "total least squares -- USED-FOR -- harmonic analysis-scheme",
      "total least squares -- CONJUNCTION -- prony algorithm",
      "audio signal quality -- EVALUATE-FOR -- audio coder",
      "encoder bitrates -- EVALUATE-FOR -- audio coder",
      "prony algorithm -- USED-FOR -- harmonic analysis-scheme",
      "wavelet ltering schemes -- USED-FOR -- audio coder",
      "wavelet ltering scheme -- USED-FOR -- reconstructed harmonic signal",
      "m-band wavelet -- USED-FOR -- residual"
    ],
    "abstract": "in this paper , we propose a new combined <method_1> for audio where a <method_4> is used , rst , to approximate each <otherscientificterm_13> as a sum of several sinusoids . then , the <otherscientificterm_15> between the original signal and the <otherscientificterm_6> is analyzed using a <method_5> . after each <method_2> & wavelet ltering -rrb- , parameters are quantized and encoded . compared to previously proposed methods , our <method_11> uses dierent <method_12> and <method_3> . we use the <method_0> - <method_10> for the <task_9> , and an <otherscientificterm_8> transform for analyzing the <otherscientificterm_16> . altogether , our proposed <method_11> is capable of delivering excellent <metric_7> at <otherscientificterm_14> of 60-70 kb/s .",
    "abstract_og": "in this paper , we propose a new combined harmonic - wavelet representation for audio where a harmonic analysis-synthesis scheme is used , rst , to approximate each audio frame as a sum of several sinusoids . then , the dier-ence between the original signal and the reconstructed harmonic signal is analyzed using a wavelet ltering scheme . after each step -lrb- harmonic analysis & wavelet ltering -rrb- , parameters are quantized and encoded . compared to previously proposed methods , our audio coder uses dierent harmonic analysis-synthesis and wavelet ltering schemes . we use the total least squares - prony algorithm for the harmonic analysis-scheme , and an m-band wavelet transform for analyzing the residual . altogether , our proposed audio coder is capable of delivering excellent audio signal quality at encoder bitrates of 60-70 kb/s ."
  },
  {
    "title": "Application of LDA to speaker recognition .",
    "entities": [
      "linear transformation of n-dimensional feature vectors",
      "linear discriminant analysis",
      "optimal speaker discriminative space",
      "feature extraction method",
      "speaker recognition task",
      "vector of features",
      "pattern classification problem",
      "m-dimensional space",
      "pattern classification",
      "speaker recognition",
      "feature space",
      "identification"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <task> <otherscientificterm> <task> <task> <otherscientificterm> <task>",
    "relations": [
      "linear discriminant analysis -- HYPONYM-OF -- feature extraction method",
      "pattern classification -- USED-FOR -- speaker recognition task",
      "linear discriminant analysis -- USED-FOR -- identification",
      "pattern classification problem -- USED-FOR -- speaker recognition",
      "linear discriminant analysis -- USED-FOR -- linear transformation of n-dimensional feature vectors"
    ],
    "abstract": "the <task_4> falls under the general problem of <task_8> . <task_9> as a <task_6> , its ultimate objective is design of a system that classifies the <otherscientificterm_5> in different classes by partitioning the <otherscientificterm_10> into <otherscientificterm_2> . <method_1> is a <method_3> that provides a <otherscientificterm_0> -lrb- or samples -rrb- into <otherscientificterm_7> -lrb- m < n -rrb- , so that samples belonging to the same class are close together but samples from different classes are far apart from each other . in this paper we discuss the issue of the application of <method_1> to our gaussian mixture model -lrb- gmm -rrb- based speaker <task_11> task . applying <method_1> improved the <task_11> performance .",
    "abstract_og": "the speaker recognition task falls under the general problem of pattern classification . speaker recognition as a pattern classification problem , its ultimate objective is design of a system that classifies the vector of features in different classes by partitioning the feature space into optimal speaker discriminative space . linear discriminant analysis is a feature extraction method that provides a linear transformation of n-dimensional feature vectors -lrb- or samples -rrb- into m-dimensional space -lrb- m < n -rrb- , so that samples belonging to the same class are close together but samples from different classes are far apart from each other . in this paper we discuss the issue of the application of linear discriminant analysis to our gaussian mixture model -lrb- gmm -rrb- based speaker identification task . applying linear discriminant analysis improved the identification performance ."
  },
  {
    "title": "Liaison and schwa deletion in French : an effect of lexical frequency and competition ? .",
    "entities": [
      "phonological processes of liaison",
      "lexical frequency",
      "neighbourhood density",
      "lexical neighbourhoods",
      "neighbourhood frequency",
      "speech corpus",
      "schwa deletion",
      "lexical variants",
      "lexical recognition",
      "french",
      "elision"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm>",
    "relations": [
      "phonological processes of liaison -- USED-FOR -- lexical variants",
      "phonological processes of liaison -- CONJUNCTION -- schwa deletion",
      "lexical frequency -- CONJUNCTION -- elision",
      "neighbourhood density -- CONJUNCTION -- neighbourhood frequency",
      "lexical frequency -- CONJUNCTION -- neighbourhood density"
    ],
    "abstract": "this study aims to determine whether the production of the <otherscientificterm_7> created by the <method_0> and <otherscientificterm_6> in <material_9> are conditioned by factors linked to <task_8> . we hypothesise that the realisation of these variants would be favoured for words which are lexically '' salient '' in term of frequency and in their <otherscientificterm_3> . this claim was tested by examining a <material_5> for the effects of <otherscientificterm_1> , <otherscientificterm_2> and <otherscientificterm_4> on the production of liaison -lrb- both in linking and linked words and their co-occurrence -rrb- and <otherscientificterm_10> . overall the results do not support our hypothesis : <otherscientificterm_1> and competition do not appear to influence strongly whether liaison and <otherscientificterm_10> are realised or not .",
    "abstract_og": "this study aims to determine whether the production of the lexical variants created by the phonological processes of liaison and schwa deletion in french are conditioned by factors linked to lexical recognition . we hypothesise that the realisation of these variants would be favoured for words which are lexically '' salient '' in term of frequency and in their lexical neighbourhoods . this claim was tested by examining a speech corpus for the effects of lexical frequency , neighbourhood density and neighbourhood frequency on the production of liaison -lrb- both in linking and linked words and their co-occurrence -rrb- and elision . overall the results do not support our hypothesis : lexical frequency and competition do not appear to influence strongly whether liaison and elision are realised or not ."
  },
  {
    "title": "An Extended Interpreted System Model for Epistemic Logics .",
    "entities": [
      "logic of knowledge and certainty",
      "sound and complete proof system",
      "interpreted perception system model",
      "perception system model",
      "interpreted system model",
      "s5 epistemic logics",
      "notion of knowledge",
      "computationally grounded model",
      "epis-temic logics",
      "computer processes",
      "kc logic",
      "knowledge modality",
      "s5"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "kc logic -- HYPONYM-OF -- logic of knowledge and certainty",
      "knowledge modality -- PART-OF -- computationally grounded model"
    ],
    "abstract": "the <method_4> offers a <method_7> , in terms of the states of <method_9> , to <otherscientificterm_5> . this paper extends the <method_4> , and provides a computationally grounded one , called the <method_2> , to those <otherscientificterm_8> other than <method_12> . it is usually assumed , in the <method_4> , that those parts of the environment that are visible to an agent are correctly perceived by the agent as a whole . the essential idea of the <method_2> is that an agent may have incorrect perception or observations to the visible parts of the environment and the agent may not be aware of this . the <otherscientificterm_6> can be defined so that an agent knows a statement iff the statement holds in those states that the agent can not distinguish -lrb- from the current state -rrb- by using only her correct observations . we establish a <otherscientificterm_0> , called <method_10> , with a <method_1> . the <otherscientificterm_11> in this <method_7> is s4 valid . it becomes <method_12> if we assume an agent always has correct observations ; and more interestingly , it can be s4 .2 or s4 .3 under other natural constraints on agents and their sensors to the environment .",
    "abstract_og": "the interpreted system model offers a computationally grounded model , in terms of the states of computer processes , to s5 epistemic logics . this paper extends the interpreted system model , and provides a computationally grounded one , called the interpreted perception system model , to those epis-temic logics other than s5 . it is usually assumed , in the interpreted system model , that those parts of the environment that are visible to an agent are correctly perceived by the agent as a whole . the essential idea of the interpreted perception system model is that an agent may have incorrect perception or observations to the visible parts of the environment and the agent may not be aware of this . the notion of knowledge can be defined so that an agent knows a statement iff the statement holds in those states that the agent can not distinguish -lrb- from the current state -rrb- by using only her correct observations . we establish a logic of knowledge and certainty , called kc logic , with a sound and complete proof system . the knowledge modality in this computationally grounded model is s4 valid . it becomes s5 if we assume an agent always has correct observations ; and more interestingly , it can be s4 .2 or s4 .3 under other natural constraints on agents and their sensors to the environment ."
  },
  {
    "title": "Robust dialogue-state dependent language modeling using leaving-one-out .",
    "entities": [
      "dialogue-state dependent language models",
      "train timetable information system",
      "automatic inquiry systems",
      "word error rate",
      "robust language models",
      "dialogue state",
      "language model",
      "interpolation weights",
      "speech recognition",
      "dutch corpus",
      "em-algorithm",
      "leaving-one-out"
    ],
    "types": "<method> <method> <task> <metric> <method> <otherscientificterm> <method> <otherscientificterm> <task> <material> <method> <material>",
    "relations": [
      "dialogue-state dependent language models -- USED-FOR -- automatic inquiry systems",
      "dialogue-state dependent language models -- USED-FOR -- speech recognition"
    ],
    "abstract": "the use of <method_0> in <task_2> can improve <task_8> and understanding if a reasonable prediction of the <otherscientificterm_5> is feasible . in this paper , the <otherscientificterm_5> is defined as the set of parameters which are contained in the <task_2> prompt . for each <otherscientificterm_5> a separate <method_6> is constructed . in order to obtain <method_4> despite the small amount of training data we propose to interpolate all of the <method_0> linearly for each <otherscientificterm_5> and to train the large number of resulting <otherscientificterm_7> with the <method_10> in combination with <material_11> . we present experimental results on a small <material_9> which has been recorded in the netherlands with a <method_1> and show that the perplexity and the <metric_3> can be reduced significantly .",
    "abstract_og": "the use of dialogue-state dependent language models in automatic inquiry systems can improve speech recognition and understanding if a reasonable prediction of the dialogue state is feasible . in this paper , the dialogue state is defined as the set of parameters which are contained in the automatic inquiry systems prompt . for each dialogue state a separate language model is constructed . in order to obtain robust language models despite the small amount of training data we propose to interpolate all of the dialogue-state dependent language models linearly for each dialogue state and to train the large number of resulting interpolation weights with the em-algorithm in combination with leaving-one-out . we present experimental results on a small dutch corpus which has been recorded in the netherlands with a train timetable information system and show that the perplexity and the word error rate can be reduced significantly ."
  },
  {
    "title": "Labeling audio-visual speech corpora and training an ANN/HMM audio-visual speech recognition system .",
    "entities": [
      "institute de la communication parl\u00e9e",
      "multi-stage labeling process",
      "audiovisual speech recognition",
      "audio database numbers95",
      "audiovisual recognition system",
      "video labeling",
      "audio database",
      "labeling process",
      "audio labeling",
      "bootstrap training",
      "audiovisual database",
      "numbers95"
    ],
    "types": "<method> <method> <task> <material> <method> <task> <material> <method> <task> <task> <material> <method>",
    "relations": [
      "audiovisual database -- USED-FOR -- multi-stage labeling process",
      "audio labeling -- COMPARE -- video labeling",
      "audiovisual database -- USED-FOR -- audiovisual speech recognition"
    ],
    "abstract": "we present a method to label an <material_10> and to setup a system for <task_2> based on a hybrid artificial neural network/hidden markov model -lrb- ann/hmm -rrb- approach . the <method_1> is presented on a new <material_10> recorded at the <method_0> . the <material_10> was generated via transposition of the <material_3> . for the labeling first a large subset of <method_11> is used to achieve a <task_9> of an <method_0> , which can then be employed to label the audio part of the <material_10> . this initial labeling is further improved via readapting the <method_0> to the new <material_10> and reperforming the labeling . from the <task_8> then the <task_5> is derived . tests at different signal to noise ratios -lrb- snr -rrb- are performed to demonstrate the efficiency of the <method_7> . furthermore ways to incorporate information from a large <material_6> into the final <method_4> were investigated .",
    "abstract_og": "we present a method to label an audiovisual database and to setup a system for audiovisual speech recognition based on a hybrid artificial neural network/hidden markov model -lrb- ann/hmm -rrb- approach . the multi-stage labeling process is presented on a new audiovisual database recorded at the institute de la communication parl\u00e9e . the audiovisual database was generated via transposition of the audio database numbers95 . for the labeling first a large subset of numbers95 is used to achieve a bootstrap training of an institute de la communication parl\u00e9e , which can then be employed to label the audio part of the audiovisual database . this initial labeling is further improved via readapting the institute de la communication parl\u00e9e to the new audiovisual database and reperforming the labeling . from the audio labeling then the video labeling is derived . tests at different signal to noise ratios -lrb- snr -rrb- are performed to demonstrate the efficiency of the labeling process . furthermore ways to incorporate information from a large audio database into the final audiovisual recognition system were investigated ."
  },
  {
    "title": "Microphone array speech recognition : experiments on overlapping speech in meetings .",
    "entities": [
      "speech in meetings",
      "microphone array geometry",
      "microphone array system",
      "close-talking lapel microphone",
      "table-top microphone",
      "close-talking microphones",
      "speech recognition",
      "speech processing",
      "speaker separation",
      "speech scenarios",
      "microphone arrays",
      "processing technique",
      "hands-free acquisition"
    ],
    "types": "<material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <task> <material> <otherscientificterm> <method> <task>",
    "relations": [
      "microphone arrays -- USED-FOR -- speech in meetings",
      "processing technique -- USED-FOR -- microphone arrays",
      "microphone array geometry -- USED-FOR -- microphone arrays"
    ],
    "abstract": "this paper investigates the use of <otherscientificterm_10> to acquire and recognise <material_0> . meetings pose several interesting problems for <task_7> , as they consist of multiple competing speakers within a small space , typically around a table . due to their ability to provide <task_12> and directional discrimination , <otherscientificterm_10> present a potential alternative to <otherscientificterm_5> in such an application . we first propose an appropriate <method_1> and improved <method_11> for this <otherscientificterm_10> , paying particular attention to <task_8> during possible overlap segments . data collection of a small vocabulary <task_6> corpus -lrb- numbers -rrb- was performed in a real meeting room for a single speaker , and several overlapping <material_9> . in <task_6> experiments on the acquired database , the performance of the <method_2> is compared to that of a <otherscientificterm_3> , and a single <otherscientificterm_4> .",
    "abstract_og": "this paper investigates the use of microphone arrays to acquire and recognise speech in meetings . meetings pose several interesting problems for speech processing , as they consist of multiple competing speakers within a small space , typically around a table . due to their ability to provide hands-free acquisition and directional discrimination , microphone arrays present a potential alternative to close-talking microphones in such an application . we first propose an appropriate microphone array geometry and improved processing technique for this microphone arrays , paying particular attention to speaker separation during possible overlap segments . data collection of a small vocabulary speech recognition corpus -lrb- numbers -rrb- was performed in a real meeting room for a single speaker , and several overlapping speech scenarios . in speech recognition experiments on the acquired database , the performance of the microphone array system is compared to that of a close-talking lapel microphone , and a single table-top microphone ."
  },
  {
    "title": "Direct image alignment of projector-camera systems with planar surfaces .",
    "entities": [
      "planar surfaces of diffuse reflectance properties",
      "real world objects",
      "spatial augmented reality",
      "real world surfaces",
      "direct alignment",
      "computer vision",
      "projector images",
      "projector-camera systems",
      "displayed content",
      "subpixel accuracy",
      "display",
      "illumination",
      "generalization"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <method> <material> <metric> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "computer vision -- USED-FOR -- projector-camera systems",
      "displayed content -- USED-FOR -- direct alignment"
    ],
    "abstract": "projector-camera systems use <otherscientificterm_5> to analyze their surroundings and <otherscientificterm_10> feedback directly onto <otherscientificterm_1> , as embodied by <otherscientificterm_2> . to be effective , the <otherscientificterm_10> must remain aligned even when the target object moves , but the added <otherscientificterm_11> causes problems for traditional algorithms . current solutions consider the <material_8> as interference and largely depend on channels orthogonal to visible light . they can not directly align <material_6> with <otherscientificterm_3> , even though this may be the actual goal . we propose instead to model the light emitted by projectors and reflected into cameras , and to consider the <material_8> as additional information useful for <task_4> . we implemented in software an algorithm that successfully executes on <otherscientificterm_0> at almost two frames per second with <metric_9> . although slow , our work proves the viability of the concept , paving the way for future optimization and <task_12> .",
    "abstract_og": "projector-camera systems use computer vision to analyze their surroundings and display feedback directly onto real world objects , as embodied by spatial augmented reality . to be effective , the display must remain aligned even when the target object moves , but the added illumination causes problems for traditional algorithms . current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light . they can not directly align projector images with real world surfaces , even though this may be the actual goal . we propose instead to model the light emitted by projectors and reflected into cameras , and to consider the displayed content as additional information useful for direct alignment . we implemented in software an algorithm that successfully executes on planar surfaces of diffuse reflectance properties at almost two frames per second with subpixel accuracy . although slow , our work proves the viability of the concept , paving the way for future optimization and generalization ."
  },
  {
    "title": "Joint kernel collaborative representation on Tensor manifold for face recognition .",
    "entities": [
      "gabor-based region covariance matrix",
      "collaborative representation-based classifier",
      "orl and feret datasets",
      "face feature descriptor",
      "tensor kernel crc",
      "kernel learning method",
      "kernel crc construction",
      "sparse representation-based classifier",
      "regionalized grcms",
      "face recognition",
      "tensor manifold",
      "geodesic distances",
      "grcm descriptor",
      "speedy computation",
      "vector-based classifiers",
      "disconnect"
    ],
    "types": "<method> <method> <material> <method> <method> <method> <task> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <task> <method> <otherscientificterm>",
    "relations": [
      "collaborative representation-based classifier -- USED-FOR -- sparse representation-based classifier",
      "tensor kernel crc -- CONJUNCTION -- regionalized grcms",
      "gabor-based region covariance matrix -- HYPONYM-OF -- face feature descriptor",
      "collaborative representation-based classifier -- HYPONYM-OF -- vector-based classifiers",
      "face feature descriptor -- USED-FOR -- face recognition",
      "regionalized grcms -- CONJUNCTION -- tensor kernel crc",
      "grcm descriptor -- CONJUNCTION -- vector-based classifiers"
    ],
    "abstract": "gabor-based region covariance matrix -lrb- <method_0> -rrb- is an emerging <method_3> , which has been shown promising for <task_9> . the <method_0> lies on <otherscientificterm_10> is inherently non-euclidean , hence a <otherscientificterm_15> exists between <method_12> and <method_14> , such as <method_1> . <method_1> is a strong alternative to <method_7> yet enjoys high efficiency . in this paper , we bridge <method_0> and <method_1> with <method_5> . we investigate several <otherscientificterm_11> on <otherscientificterm_10> that satisfy the mercer 's condition for <task_6> as well as for <task_13> . apart from that , we also devise two strategies to jointly combine the <method_8> with <method_4> . extensive experiments on the <material_2> are conducted to verify the efficacy of the proposed method .",
    "abstract_og": "gabor-based region covariance matrix -lrb- gabor-based region covariance matrix -rrb- is an emerging face feature descriptor , which has been shown promising for face recognition . the gabor-based region covariance matrix lies on tensor manifold is inherently non-euclidean , hence a disconnect exists between grcm descriptor and vector-based classifiers , such as collaborative representation-based classifier . collaborative representation-based classifier is a strong alternative to sparse representation-based classifier yet enjoys high efficiency . in this paper , we bridge gabor-based region covariance matrix and collaborative representation-based classifier with kernel learning method . we investigate several geodesic distances on tensor manifold that satisfy the mercer 's condition for kernel crc construction as well as for speedy computation . apart from that , we also devise two strategies to jointly combine the regionalized grcms with tensor kernel crc . extensive experiments on the orl and feret datasets are conducted to verify the efficacy of the proposed method ."
  },
  {
    "title": "Preduction : A Common Form of Induction and Analogy .",
    "entities": [
      "artificial intelligence",
      "logical formalization",
      "mathematical induction",
      "deduction",
      "induction",
      "preduction",
      "deduction"
    ],
    "types": "<material> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "deduction -- CONJUNCTION -- induction"
    ],
    "abstract": "deduction , <otherscientificterm_4> , and analogy pervade all our thinking . in contrast with <method_3> , understanding logical aspects of <otherscientificterm_4> and analogy is still an important and challenging issue of <material_0> . this paper describes a <method_1> , called production , of common conjectural reasoning of both <otherscientificterm_4> and analogy . by introduction of <otherscientificterm_5> , <method_1> is refined into `` <otherscientificterm_5> + <method_3> '' and -lrb- empirical -rrb- inductive reasoning is refined into `` <otherscientificterm_5> + <method_2> '' . we examine generality of <otherscientificterm_5> through applications to various examples on <otherscientificterm_4> and analogy .",
    "abstract_og": "deduction , induction , and analogy pervade all our thinking . in contrast with deduction , understanding logical aspects of induction and analogy is still an important and challenging issue of artificial intelligence . this paper describes a logical formalization , called production , of common conjectural reasoning of both induction and analogy . by introduction of preduction , logical formalization is refined into `` preduction + deduction '' and -lrb- empirical -rrb- inductive reasoning is refined into `` preduction + mathematical induction '' . we examine generality of preduction through applications to various examples on induction and analogy ."
  },
  {
    "title": "Subspace-Based Face Recognition in Analog VLSI .",
    "entities": [
      "on-chip compensation techniques",
      "analog-vlsi neural network",
      "orl database",
      "device mismatch",
      "user-programmed coefficients",
      "12x12-pixel images",
      "dimensionality-reduction network",
      "face recognition",
      "manhattan distances",
      "software implementation",
      "classification performance",
      "subspace methods",
      "lda",
      "coefficients",
      "classification",
      "pca"
    ],
    "types": "<method> <method> <material> <otherscientificterm> <otherscientificterm> <material> <method> <task> <otherscientificterm> <method> <metric> <method> <task> <otherscientificterm> <task> <method>",
    "relations": [
      "analog-vlsi neural network -- USED-FOR -- face recognition",
      "dimensionality-reduction network -- USED-FOR -- analog-vlsi neural network",
      "on-chip compensation techniques -- USED-FOR -- analog-vlsi neural network",
      "subspace methods -- USED-FOR -- analog-vlsi neural network",
      "user-programmed coefficients -- PART-OF -- analog-vlsi neural network",
      "classification -- EVALUATE-FOR -- analog-vlsi neural network",
      "on-chip compensation techniques -- USED-FOR -- device mismatch",
      "pca -- USED-FOR -- lda"
    ],
    "abstract": "we describe an <method_1> for <task_7> based on <method_11> . the <method_1> uses a <method_6> whose <otherscientificterm_13> can be either programmed or learned on-chip to perform <method_15> , or programmed to perform <task_12> . a second <method_1> with <otherscientificterm_4> performs <task_14> with <otherscientificterm_8> . the <method_1> uses <method_0> to reduce the effects of <otherscientificterm_3> . using the <material_2> with <material_5> , our <method_1> achieves up to 85 % <metric_10> -lrb- 98 % of an equivalent <method_9> -rrb- .",
    "abstract_og": "we describe an analog-vlsi neural network for face recognition based on subspace methods . the analog-vlsi neural network uses a dimensionality-reduction network whose coefficients can be either programmed or learned on-chip to perform pca , or programmed to perform lda . a second analog-vlsi neural network with user-programmed coefficients performs classification with manhattan distances . the analog-vlsi neural network uses on-chip compensation techniques to reduce the effects of device mismatch . using the orl database with 12x12-pixel images , our analog-vlsi neural network achieves up to 85 % classification performance -lrb- 98 % of an equivalent software implementation -rrb- ."
  },
  {
    "title": "Modeling Organization in Student Essays .",
    "entities": [
      "heuristic-based and learning-based approaches",
      "automated essay scoring",
      "natural language processing",
      "annotated corpus",
      "string kernels",
      "organization dimension",
      "technical errors",
      "alignment kernels",
      "modeling organization",
      "sequence alignment",
      "coherence"
    ],
    "types": "<method> <task> <task> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "sequence alignment -- CONJUNCTION -- alignment kernels",
      "coherence -- CONJUNCTION -- technical errors",
      "automated essay scoring -- HYPONYM-OF -- natural language processing",
      "alignment kernels -- CONJUNCTION -- string kernels"
    ],
    "abstract": "automated essay scoring is one of the most important educational applications of <task_2> . recently , researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as <otherscientificterm_10> , <otherscientificterm_6> , and relevance to prompt , but there is relatively little work on <task_8> . we present a new <material_3> and propose <method_0> to scoring essays along the <otherscientificterm_5> , utilizing techniques that involve <method_9> , <otherscientificterm_7> , and <method_4> .",
    "abstract_og": "automated essay scoring is one of the most important educational applications of natural language processing . recently , researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as coherence , technical errors , and relevance to prompt , but there is relatively little work on modeling organization . we present a new annotated corpus and propose heuristic-based and learning-based approaches to scoring essays along the organization dimension , utilizing techniques that involve sequence alignment , alignment kernels , and string kernels ."
  },
  {
    "title": "HD Maps : Fine-Grained Road Segmentation by Parsing Ground and Aerial Images .",
    "entities": [
      "fine grained segmentation categories",
      "stereo camera pair",
      "monocular aerial imagery",
      "ground images",
      "aerial images",
      "joint inference",
      "parking spots",
      "gps+imu systems",
      "sidewalk",
      "kitti"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <material> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "monocular aerial imagery -- CONJUNCTION -- ground images",
      "parking spots -- HYPONYM-OF -- fine grained segmentation categories",
      "stereo camera pair -- USED-FOR -- ground images",
      "sidewalk -- HYPONYM-OF -- fine grained segmentation categories",
      "parking spots -- CONJUNCTION -- sidewalk"
    ],
    "abstract": "in this paper we present an approach to enhance existing maps with <otherscientificterm_0> such as <otherscientificterm_6> and <otherscientificterm_8> , as well as the number and location of road lanes . towards this goal , we propose an efficient approach that is able to estimate these fine grained categories by doing <otherscientificterm_5> over both , <material_2> , as well as <material_3> taken from a <otherscientificterm_1> mounted on top of a car . important to this is reasoning about the alignment between the two types of imagery , as even when the measurements are taken with sophisticated <method_7> , this alignment is not sufficiently accurate . we demonstrate the effectiveness of our approach on a new dataset which enhances <method_9> -lsb- 8 -rsb- with <material_4> taken with a camera mounted on an airplane and flying around the city of karlsruhe , germany .",
    "abstract_og": "in this paper we present an approach to enhance existing maps with fine grained segmentation categories such as parking spots and sidewalk , as well as the number and location of road lanes . towards this goal , we propose an efficient approach that is able to estimate these fine grained categories by doing joint inference over both , monocular aerial imagery , as well as ground images taken from a stereo camera pair mounted on top of a car . important to this is reasoning about the alignment between the two types of imagery , as even when the measurements are taken with sophisticated gps+imu systems , this alignment is not sufficiently accurate . we demonstrate the effectiveness of our approach on a new dataset which enhances kitti -lsb- 8 -rsb- with aerial images taken with a camera mounted on an airplane and flying around the city of karlsruhe , germany ."
  },
  {
    "title": "A New Algorithm for Weighted Partial MaxSAT .",
    "entities": [
      "weighted partial maxsat solvers",
      "weighted partial maxsat solver",
      "sat solver"
    ],
    "types": "<method> <method> <method>",
    "relations": [
      "weighted partial maxsat solver -- COMPARE -- weighted partial maxsat solvers"
    ],
    "abstract": "we present and implement a <method_1> based on successive calls to a <method_2> . we prove the cor-rectness of our <method_1> and compare our <method_1> with other <method_0> .",
    "abstract_og": "we present and implement a weighted partial maxsat solver based on successive calls to a sat solver . we prove the cor-rectness of our weighted partial maxsat solver and compare our weighted partial maxsat solver with other weighted partial maxsat solvers ."
  },
  {
    "title": "Robust statistics on Riemannian manifolds via the geometric median .",
    "entities": [
      "geometric median of euclidean data",
      "geometric median of data",
      "non-positive sectional curvature",
      "positively curved manifolds",
      "3d rotation group",
      "geometric median computation",
      "manifold data",
      "weiszfeld procedure",
      "riemannian manifold",
      "tensor manifolds",
      "geometric median",
      "arbitrary manifold",
      "shape spaces",
      "euclidean spaces",
      "vision applications",
      "robustness",
      "manifolds",
      "manifold"
    ],
    "types": "<material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "3d rotation group -- HYPONYM-OF -- manifold data",
      "weiszfeld procedure -- USED-FOR -- geometric median of euclidean data",
      "3d rotation group -- CONJUNCTION -- tensor manifolds",
      "shape spaces -- HYPONYM-OF -- manifold data",
      "tensor manifolds -- CONJUNCTION -- shape spaces",
      "tensor manifolds -- HYPONYM-OF -- manifold data",
      "riemannian manifold -- USED-FOR -- geometric median of data",
      "non-positive sectional curvature -- FEATURE-OF -- manifolds"
    ],
    "abstract": "the <otherscientificterm_10> is a classic robust estimator of centrality for data in <otherscientificterm_13> . in this paper we formulate the <material_1> on a <otherscientificterm_8> as the minimizer of the sum of geodesic distances to the data points . we prove existence and uniqueness of the <otherscientificterm_10> on <otherscientificterm_16> with <otherscientificterm_2> and give sufficient conditions for uniqueness on <otherscientificterm_3> . generalizing the <method_7> for finding the <material_0> , we present an algorithm for computing the <otherscientificterm_10> on an <otherscientificterm_11> . we show that this algorithm converges to the unique solution when it exists . this method produces a robust central point for data lying on a <otherscientificterm_17> , and should have use in a variety of <task_14> involving <otherscientificterm_16> . we give examples of the <otherscientificterm_5> and demonstrate its <metric_15> for three types of <material_6> : the <otherscientificterm_4> , <otherscientificterm_9> , and <otherscientificterm_12> .",
    "abstract_og": "the geometric median is a classic robust estimator of centrality for data in euclidean spaces . in this paper we formulate the geometric median of data on a riemannian manifold as the minimizer of the sum of geodesic distances to the data points . we prove existence and uniqueness of the geometric median on manifolds with non-positive sectional curvature and give sufficient conditions for uniqueness on positively curved manifolds . generalizing the weiszfeld procedure for finding the geometric median of euclidean data , we present an algorithm for computing the geometric median on an arbitrary manifold . we show that this algorithm converges to the unique solution when it exists . this method produces a robust central point for data lying on a manifold , and should have use in a variety of vision applications involving manifolds . we give examples of the geometric median computation and demonstrate its robustness for three types of manifold data : the 3d rotation group , tensor manifolds , and shape spaces ."
  },
  {
    "title": "Localization of impulsive disturbances in archive audio signals using predictive matched filtering .",
    "entities": [
      "elimination of impulsive disturbances",
      "archive audio recordings",
      "multi-step-ahead prediction errors",
      "archive audio signals",
      "model-based signal predictor",
      "classical detection method",
      "repetitive shapes",
      "autoregres-sive modeling",
      "noise pulses",
      "click templates"
    ],
    "types": "<task> <material> <otherscientificterm> <material> <method> <method> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "noise pulses -- FEATURE-OF -- archive audio recordings",
      "autoregres-sive modeling -- USED-FOR -- classical detection method",
      "model-based signal predictor -- USED-FOR -- multi-step-ahead prediction errors",
      "archive audio signals -- USED-FOR -- elimination of impulsive disturbances",
      "model-based signal predictor -- USED-FOR -- click templates"
    ],
    "abstract": "the problem of <task_0> from <material_3> is considered and its new solution , called predictive matched filtering , is proposed . the new approach is based on the observation that a large percentage of <material_8> corrupting <material_1> have highly <otherscientificterm_6> that match several typical '' patterns '' , called <otherscientificterm_9> . to localize <material_8> , <otherscientificterm_9> can be correlated with the sequence of <otherscientificterm_2> yielded by the <method_4> . it is shown that predictive matched filtering is an efficient and com-putationally affordable disturbance localization technique -- when combined with the <method_5> based on <method_7> , it can significantly improve restoration results .",
    "abstract_og": "the problem of elimination of impulsive disturbances from archive audio signals is considered and its new solution , called predictive matched filtering , is proposed . the new approach is based on the observation that a large percentage of noise pulses corrupting archive audio recordings have highly repetitive shapes that match several typical '' patterns '' , called click templates . to localize noise pulses , click templates can be correlated with the sequence of multi-step-ahead prediction errors yielded by the model-based signal predictor . it is shown that predictive matched filtering is an efficient and com-putationally affordable disturbance localization technique -- when combined with the classical detection method based on autoregres-sive modeling , it can significantly improve restoration results ."
  },
  {
    "title": "Developments in large vocabulary , continuous speech recognition of German .",
    "entities": [
      "large vocabulary continuous speech recognition system",
      "oocial word error rate",
      "french and american english",
      "word error rate",
      "ger-eval95 test set",
      "sqale adjudication process",
      "system development",
      "limsi recognizerr1",
      "german language",
      "german"
    ],
    "types": "<method> <metric> <material> <metric> <material> <method> <task> <method> <material> <material>",
    "relations": [
      "word error rate -- EVALUATE-FOR -- large vocabulary continuous speech recognition system",
      "oocial word error rate -- EVALUATE-FOR -- large vocabulary continuous speech recognition system"
    ],
    "abstract": "in this paper we describe our <method_0> for the <material_8> , the development of which was partly carried out within the context of the european lre project 62-058 sqale . the <method_0> is the <method_7> -rsb- originally developed for <material_2> , which has been adapted to <material_9> . speciicities of <material_9> , as relevant to the <method_0> , are presented . these speciicities have been accounted for during the recognizer 's adaptation process . we present experimental results on a rst test set ger-dev95 to measure progress in <task_6> . results are given with the <method_0> using diierent acoustic model sets on two test sets ger-dev95 and ger-eval95 . this <method_0> achieved a <metric_3> of 17.3 % -lrb- <metric_1> of 16.1 % after <method_5> -rrb- on the <material_4> .",
    "abstract_og": "in this paper we describe our large vocabulary continuous speech recognition system for the german language , the development of which was partly carried out within the context of the european lre project 62-058 sqale . the large vocabulary continuous speech recognition system is the limsi recognizerr1 -rsb- originally developed for french and american english , which has been adapted to german . speciicities of german , as relevant to the large vocabulary continuous speech recognition system , are presented . these speciicities have been accounted for during the recognizer 's adaptation process . we present experimental results on a rst test set ger-dev95 to measure progress in system development . results are given with the large vocabulary continuous speech recognition system using diierent acoustic model sets on two test sets ger-dev95 and ger-eval95 . this large vocabulary continuous speech recognition system achieved a word error rate of 17.3 % -lrb- oocial word error rate of 16.1 % after sqale adjudication process -rrb- on the ger-eval95 test set ."
  },
  {
    "title": "Learning to Rank Using Privileged Information .",
    "entities": [
      "privileged information",
      "computer vision problems",
      "bounding boxes",
      "computer vision",
      "asymmetric distribution",
      "image tags",
      "object classification",
      "maximum-margin techniques",
      "rationales",
      "learning",
      "attributes"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "attributes -- CONJUNCTION -- bounding boxes",
      "bounding boxes -- CONJUNCTION -- image tags",
      "image tags -- CONJUNCTION -- rationales"
    ],
    "abstract": "many <task_1> have an <otherscientificterm_4> of information between training and test time . in this work , we study the case where we are given additional information about the training data , which however will not be available at test time . this situation is called <task_9> using <otherscientificterm_0> . we introduce two <method_7> that are able to make use of this additional source of information , and we show that the framework is applicable to several scenarios that have been studied in <task_3> before . experiments with <otherscientificterm_10> , <otherscientificterm_2> , <otherscientificterm_5> and <otherscientificterm_8> as additional information in <task_6> show promising results .",
    "abstract_og": "many computer vision problems have an asymmetric distribution of information between training and test time . in this work , we study the case where we are given additional information about the training data , which however will not be available at test time . this situation is called learning using privileged information . we introduce two maximum-margin techniques that are able to make use of this additional source of information , and we show that the framework is applicable to several scenarios that have been studied in computer vision before . experiments with attributes , bounding boxes , image tags and rationales as additional information in object classification show promising results ."
  },
  {
    "title": "A study of using locality preserving projections for feature extraction in speech recognition .",
    "entities": [
      "resource management data set",
      "principal components analysis",
      "linear discriminant analysis",
      "locality preserving projections",
      "automatic speech recognition",
      "word error rate",
      "manifold based dimensionality reduction algorithms",
      "manifold based dimensionality reduction algorithm",
      "lpp based dimensionality reduction",
      "linear di-mensionality reduction algorithms",
      "nonlin-ear embedding subspace",
      "batch mode implementation",
      "feature analysis",
      "input features",
      "mfcc features",
      "linear projection",
      "asr features",
      "local relations",
      "feature vectors",
      "unseen data"
    ],
    "types": "<material> <method> <method> <method> <task> <metric> <method> <method> <task> <method> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "automatic speech recognition -- HYPONYM-OF -- manifold based dimensionality reduction algorithm",
      "feature vectors -- USED-FOR -- nonlin-ear embedding subspace",
      "linear projection -- USED-FOR -- asr features",
      "automatic speech recognition -- USED-FOR -- feature vectors",
      "resource management data set -- EVALUATE-FOR -- lpp based dimensionality reduction",
      "linear discriminant analysis -- HYPONYM-OF -- linear di-mensionality reduction algorithms",
      "principal components analysis -- HYPONYM-OF -- linear di-mensionality reduction algorithms",
      "manifold based dimensionality reduction algorithms -- USED-FOR -- batch mode implementation",
      "automatic speech recognition -- COMPARE -- linear di-mensionality reduction algorithms",
      "feature analysis -- USED-FOR -- automatic speech recognition",
      "principal components analysis -- CONJUNCTION -- linear discriminant analysis",
      "linear projection -- USED-FOR -- manifold based dimensionality reduction algorithm"
    ],
    "abstract": "this paper presents a new approach to <task_12> in <task_4> based on <method_3> . <task_4> is a <method_7> which can be trained and applied as a <method_15> to <otherscientificterm_16> . conventional <method_6> are generally restricted to <task_11> and <task_4> is difficult in practice to apply them to <material_19> . it is argued that <task_4> can model <otherscientificterm_18> that are assumed to lie on a <otherscientificterm_10> by preserving <otherscientificterm_17> among <otherscientificterm_13> , so <task_4> has a potential advantage over conventional <method_9> like <method_1> and <method_2> . experimental results obtained on the <material_0> showed that when <task_8> was applied in the context of mel frequency cepstrum coefficient -lrb- mfcc -rrb- based <task_12> , a significant reduction of <metric_5> was obtained with respect to standard <otherscientificterm_14> .",
    "abstract_og": "this paper presents a new approach to feature analysis in automatic speech recognition based on locality preserving projections . automatic speech recognition is a manifold based dimensionality reduction algorithm which can be trained and applied as a linear projection to asr features . conventional manifold based dimensionality reduction algorithms are generally restricted to batch mode implementation and automatic speech recognition is difficult in practice to apply them to unseen data . it is argued that automatic speech recognition can model feature vectors that are assumed to lie on a nonlin-ear embedding subspace by preserving local relations among input features , so automatic speech recognition has a potential advantage over conventional linear di-mensionality reduction algorithms like principal components analysis and linear discriminant analysis . experimental results obtained on the resource management data set showed that when lpp based dimensionality reduction was applied in the context of mel frequency cepstrum coefficient -lrb- mfcc -rrb- based feature analysis , a significant reduction of word error rate was obtained with respect to standard mfcc features ."
  },
  {
    "title": "Groebner basis techniques in multidimensional multirate systems .",
    "entities": [
      "multidimensional multirate systems",
      "one-dimensional multirate systems",
      "md multirate systems theory",
      "euclidean algorithm",
      "groebner bases"
    ],
    "types": "<method> <method> <method> <method> <material>",
    "relations": [
      "euclidean algorithm -- USED-FOR -- one-dimensional multirate systems"
    ],
    "abstract": "the <method_3> is a frequently used tool in the analysis of <method_1> . this tool is however not available for <method_0> . in this paper we discus how groebner basis techniques can ll this gap . after presenting the relevant facts about <material_4> , we will show in a few examples how this technique can contribute to <method_2> .",
    "abstract_og": "the euclidean algorithm is a frequently used tool in the analysis of one-dimensional multirate systems . this tool is however not available for multidimensional multirate systems . in this paper we discus how groebner basis techniques can ll this gap . after presenting the relevant facts about groebner bases , we will show in a few examples how this technique can contribute to md multirate systems theory ."
  },
  {
    "title": "Semi-supervised noise dictionary adaptation for exemplar-based noise robust speech recognition .",
    "entities": [
      "continuous digits recognition",
      "noise robust asr",
      "speech exem-plars",
      "unknown noise",
      "low snrs",
      "speech features",
      "recognition errors",
      "noise exem-plars",
      "exemplar-based approaches",
      "training data",
      "noise exemplars"
    ],
    "types": "<task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "speech exem-plars -- CONJUNCTION -- noise exemplars"
    ],
    "abstract": "the <method_8> , which model signals as a sparse linear combination of exemplars of signals , are proved to have state-of-the-art performance in <task_1> , especially on <otherscientificterm_4> . however , since both the <otherscientificterm_2> and <otherscientificterm_10> are built from <material_9> and are fixed throughout the process of enhancing <otherscientificterm_5> , the conventional approach is especially weak for unknown types of noise . therefore , in this paper , we propose a semi-supervised approach which automatically adapt <otherscientificterm_7> to the target noise , while keeping the speech exemplars fixed . <task_0> experiments show that this approach is much more robust for <otherscientificterm_3> . the <metric_6> are reduced by 36.2 % .",
    "abstract_og": "the exemplar-based approaches , which model signals as a sparse linear combination of exemplars of signals , are proved to have state-of-the-art performance in noise robust asr , especially on low snrs . however , since both the speech exem-plars and noise exemplars are built from training data and are fixed throughout the process of enhancing speech features , the conventional approach is especially weak for unknown types of noise . therefore , in this paper , we propose a semi-supervised approach which automatically adapt noise exem-plars to the target noise , while keeping the speech exemplars fixed . continuous digits recognition experiments show that this approach is much more robust for unknown noise . the recognition errors are reduced by 36.2 % ."
  },
  {
    "title": "Neural System Model of Human Sound Localization .",
    "entities": [
      "psychophysical and neural system modeling approach",
      "spatial location of the sound source",
      "auditory image model of cochlear processing",
      "human subject 's head-related transfer functions",
      "temporal and spectral information",
      "broadband and bandpass stimuli",
      "human auditory localization process",
      "time-delay neural network",
      "directional acoustical cues",
      "variable bandwidth",
      "human-like localization",
      "system model",
      "center-frequency sounds",
      "bandpass noise",
      "neural network",
      "biological constraints",
      "sound stimuli",
      "hrtfs"
    ],
    "types": "<method> <otherscientificterm> <method> <material> <otherscientificterm> <material> <task> <method> <otherscientificterm> <otherscientificterm> <task> <method> <material> <otherscientificterm> <method> <otherscientificterm> <material> <method>",
    "relations": [
      "biological constraints -- PART-OF -- human auditory localization process",
      "psychophysical and neural system modeling approach -- CONJUNCTION -- neural network",
      "time-delay neural network -- USED-FOR -- spatial location of the sound source",
      "system model -- USED-FOR -- human auditory localization process",
      "hrtfs -- USED-FOR -- sound stimuli",
      "human-like localization -- USED-FOR -- broadband and bandpass stimuli",
      "temporal and spectral information -- USED-FOR -- time-delay neural network",
      "auditory image model of cochlear processing -- USED-FOR -- psychophysical and neural system modeling approach",
      "variable bandwidth -- CONJUNCTION -- center-frequency sounds",
      "time-delay neural network -- USED-FOR -- auditory image model of cochlear processing",
      "bandpass noise -- USED-FOR -- sound stimuli"
    ],
    "abstract": "this paper examines the role of <otherscientificterm_15> in the <task_6> . a <method_0> was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible `` realism constraints '' . the <otherscientificterm_8> , upon which <task_6> is based , were derived from the <material_3> -lrb- <method_17> -rrb- . <material_16> were generated by convolving <otherscientificterm_13> with the <method_17> and were presented to both the subject and the <method_0> . the input stimuli to the <method_0> was processed using the <method_2> . the <method_2> was then analyzed by a <method_7> which integrated <otherscientificterm_4> to determine the <otherscientificterm_1> . the combined <method_0> and <method_14> provided a <method_11> of the <task_6> . <task_10> performance was qualitatively achieved for <material_5> when the <method_11> incorporated frequency division -lrb- or tonotopicity -rrb- , and was trained using <otherscientificterm_9> and <material_12> .",
    "abstract_og": "this paper examines the role of biological constraints in the human auditory localization process . a psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible `` realism constraints '' . the directional acoustical cues , upon which human auditory localization process is based , were derived from the human subject 's head-related transfer functions -lrb- hrtfs -rrb- . sound stimuli were generated by convolving bandpass noise with the hrtfs and were presented to both the subject and the psychophysical and neural system modeling approach . the input stimuli to the psychophysical and neural system modeling approach was processed using the auditory image model of cochlear processing . the auditory image model of cochlear processing was then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source . the combined psychophysical and neural system modeling approach and neural network provided a system model of the human auditory localization process . human-like localization performance was qualitatively achieved for broadband and bandpass stimuli when the system model incorporated frequency division -lrb- or tonotopicity -rrb- , and was trained using variable bandwidth and center-frequency sounds ."
  },
  {
    "title": "Phase adjustment in waveform interpolation .",
    "entities": [
      "waveform interpolation speech coder",
      "slowly-evolving waveform",
      "rapidly-evolving waveform",
      "female and male speech",
      "natural speech quality",
      "quantized rew spectrum",
      "synthesized signal",
      "phase information",
      "rew spectrum",
      "noise sensitivity"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "slowly-evolving waveform -- CONJUNCTION -- rapidly-evolving waveform"
    ],
    "abstract": "this paper describes a method of improving the quality of the <method_0> by adjustment of the <otherscientificterm_7> . in <method_0> , a <method_1> and a <otherscientificterm_2> represent the periodic and the non-periodic part of the signal . the phase of the <otherscientificterm_6> is determined by the <method_1> and rew , and thus the correct quantiza-tion of these parameters are important to producing <metric_4> . a method is described , whereby the phase of the <otherscientificterm_6> is adjusted by modifying the <otherscientificterm_5> as a function of the fundamental frequency . this essentialy attempts to correct the discrepancies in phase that arise due to variation in pitch and also accounts for the difference in <metric_9> between <material_3> -lsb- 5 -rsb- . the overall effect would be the same if multiple codebooks -lrb- depending on pitch -rrb- were used to code the <otherscientificterm_8> . experimental results confirm that the new method results in significantly improved performance .",
    "abstract_og": "this paper describes a method of improving the quality of the waveform interpolation speech coder by adjustment of the phase information . in waveform interpolation speech coder , a slowly-evolving waveform and a rapidly-evolving waveform represent the periodic and the non-periodic part of the signal . the phase of the synthesized signal is determined by the slowly-evolving waveform and rew , and thus the correct quantiza-tion of these parameters are important to producing natural speech quality . a method is described , whereby the phase of the synthesized signal is adjusted by modifying the quantized rew spectrum as a function of the fundamental frequency . this essentialy attempts to correct the discrepancies in phase that arise due to variation in pitch and also accounts for the difference in noise sensitivity between female and male speech -lsb- 5 -rsb- . the overall effect would be the same if multiple codebooks -lrb- depending on pitch -rrb- were used to code the rew spectrum . experimental results confirm that the new method results in significantly improved performance ."
  },
  {
    "title": "Design of successive approximation lattice vector quantizers .",
    "entities": [
      "large vector quantization codebooks",
      "lattice vq -lrb- l v q -rrb-",
      "renement l v q stages",
      "rst lvq stage",
      "entropy code",
      "residual vq",
      "renement lattices",
      "entropy coding",
      "product codes",
      "residuals"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "lattice vq -lrb- l v q -rrb- -- USED-FOR -- entropy coding",
      "renement l v q stages -- USED-FOR -- residuals",
      "entropy code -- USED-FOR -- rst lvq stage",
      "lattice vq -lrb- l v q -rrb- -- CONJUNCTION -- product codes"
    ],
    "abstract": "two methods to overcome the problems with <method_0> are <method_1> and <otherscientificterm_8> . the approach described in this paper takes advantage of both methods by applying <method_5> with <method_1> at all stages . using <method_1> in conjunction with <method_7> is strongly motivated by the fact that entropy constrained but structurally unconstrained vq design leads to more equally sized vq cells . the <method_4> of the <otherscientificterm_3> should aim at exploiting the statistical properties of the source . the <otherscientificterm_2> quantize the <otherscientificterm_9> . simulations show that there exist certain scales of the <otherscientificterm_6> yielding extraordinary performance . we focus on the search of these scales .",
    "abstract_og": "two methods to overcome the problems with large vector quantization codebooks are lattice vq -lrb- l v q -rrb- and product codes . the approach described in this paper takes advantage of both methods by applying residual vq with lattice vq -lrb- l v q -rrb- at all stages . using lattice vq -lrb- l v q -rrb- in conjunction with entropy coding is strongly motivated by the fact that entropy constrained but structurally unconstrained vq design leads to more equally sized vq cells . the entropy code of the rst lvq stage should aim at exploiting the statistical properties of the source . the renement l v q stages quantize the residuals . simulations show that there exist certain scales of the renement lattices yielding extraordinary performance . we focus on the search of these scales ."
  },
  {
    "title": "Using Lookaheads with Optimal Best-First Search .",
    "entities": [
      "bidi-rectional pathmax",
      "best-first search",
      "depth-first search",
      "breadth-first search",
      "inconsistent heuristics",
      "lookahead phase",
      "time speedup",
      "lookahead depth",
      "memory",
      "bfs"
    ],
    "types": "<method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "best-first search -- CONJUNCTION -- depth-first search"
    ],
    "abstract": "we present an algorithm that exploits the complimentary benefits of <method_1> and <method_2> by performing limited dfs lookaheads from the frontier of <method_9> . we show that this continuum requires significantly less <otherscientificterm_8> than <method_9> . in addition , a <otherscientificterm_6> is also achieved when choosing the <otherscientificterm_7> correctly . we demonstrate this idea for <method_3> and for a * . additionally , we show that when using <method_4> , <method_0> , can be implemented very easily on top of the <otherscientificterm_5> . experimental results on several domains demonstrate the benefits of all our ideas .",
    "abstract_og": "we present an algorithm that exploits the complimentary benefits of best-first search and depth-first search by performing limited dfs lookaheads from the frontier of bfs . we show that this continuum requires significantly less memory than bfs . in addition , a time speedup is also achieved when choosing the lookahead depth correctly . we demonstrate this idea for breadth-first search and for a * . additionally , we show that when using inconsistent heuristics , bidi-rectional pathmax , can be implemented very easily on top of the lookahead phase . experimental results on several domains demonstrate the benefits of all our ideas ."
  },
  {
    "title": "Subband based voice conversion .",
    "entities": [
      "voice conversion system",
      "discrete wavelet transform",
      "segmental codebooks",
      "voice conversion method",
      "full-band based output",
      "subjective listening tests",
      "abx listening tests",
      "speaker transformation algorithm",
      "faster voice conversion",
      "film dubbing",
      "subband decomposition",
      "speech spectrum",
      "sampling rates",
      "sampling rate",
      "computational complexity",
      "conversion",
      "16khz",
      "looping"
    ],
    "types": "<task> <method> <method> <method> <otherscientificterm> <method> <material> <method> <task> <task> <task> <otherscientificterm> <metric> <metric> <metric> <task> <method> <task>",
    "relations": [
      "sampling rates -- EVALUATE-FOR -- voice conversion method",
      "film dubbing -- CONJUNCTION -- looping",
      "subband decomposition -- USED-FOR -- speech spectrum",
      "discrete wavelet transform -- USED-FOR -- subband decomposition",
      "discrete wavelet transform -- USED-FOR -- speech spectrum",
      "segmental codebooks -- USED-FOR -- speaker transformation algorithm",
      "subjective listening tests -- EVALUATE-FOR -- voice conversion method",
      "voice conversion method -- USED-FOR -- voice conversion system"
    ],
    "abstract": "a new <method_3> that improves the quality of the voice <task_15> output at higher <metric_12> is proposed . <method_7> using <method_2> is modified to process source and target speech spectra in different subbands . the new <method_3> ensures better <task_15> at <metric_12> above <method_16> . <method_1> is employed for <task_10> to estimate the <otherscientificterm_11> better with higher resolution . <task_8> is achieved since the <metric_14> decreases at a lower <metric_13> . a <task_0> is implemented using the proposed <method_3> with necessary tools . the performance of the proposed <method_3> is demonstrated by both <method_5> and applications to <task_9> and <task_17> . in <material_6> , the listeners preferred the subband based output by 92.1 % as compared to the <otherscientificterm_4> .",
    "abstract_og": "a new voice conversion method that improves the quality of the voice conversion output at higher sampling rates is proposed . speaker transformation algorithm using segmental codebooks is modified to process source and target speech spectra in different subbands . the new voice conversion method ensures better conversion at sampling rates above 16khz . discrete wavelet transform is employed for subband decomposition to estimate the speech spectrum better with higher resolution . faster voice conversion is achieved since the computational complexity decreases at a lower sampling rate . a voice conversion system is implemented using the proposed voice conversion method with necessary tools . the performance of the proposed voice conversion method is demonstrated by both subjective listening tests and applications to film dubbing and looping . in abx listening tests , the listeners preferred the subband based output by 92.1 % as compared to the full-band based output ."
  },
  {
    "title": "Robust and Efficient Foreground Analysis for Real-Time Video Surveillance .",
    "entities": [
      "real time video surveillance system",
      "mixture of gaussians models",
      "intensity and texture information",
      "fixed camera view",
      "gaussian mixture model",
      "static foreground regions",
      "color images",
      "static regions",
      "background subtraction",
      "motion information",
      "mmx optimization",
      "grayscale images",
      "foreground analysis",
      "background model",
      "gaussian mixtures",
      "tracking",
      "fragmentation",
      "shadows",
      "foreground"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <task> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "background model -- USED-FOR -- fixed camera view",
      "gaussian mixture model -- USED-FOR -- static foreground regions",
      "intensity and texture information -- USED-FOR -- shadows",
      "color images -- CONJUNCTION -- grayscale images",
      "gaussian mixture model -- USED-FOR -- foreground analysis",
      "gaussian mixtures -- USED-FOR -- background model"
    ],
    "abstract": "we present a new method to robustly and efficiently analyze <otherscientificterm_18> when we detect <otherscientificterm_13> for a <otherscientificterm_3> by using <method_1> and multiple cues . the <otherscientificterm_13> is modeled by three <method_14> as in the work of stauffer and grimson -lsb- 11 -rsb- . then the <otherscientificterm_2> are integrated to remove <otherscientificterm_17> and to enable the algorithm working for quick lighting changes . for <task_12> , the same <method_4> is employed to detect the <otherscientificterm_5> without using any <method_15> or <otherscientificterm_9> . then the whole <otherscientificterm_7> are pushed back to the <otherscientificterm_13> to avoid a common problem in <otherscientificterm_8> -- <otherscientificterm_16> -lrb- one object becomes multiple parts -rrb- . the method was tested on our <task_0> . it is robust and run about 130 fps for <material_6> and 150 fps for <material_11> at size 160x120 on a 2gb pentium iv machine with <method_10> .",
    "abstract_og": "we present a new method to robustly and efficiently analyze foreground when we detect background model for a fixed camera view by using mixture of gaussians models and multiple cues . the background model is modeled by three gaussian mixtures as in the work of stauffer and grimson -lsb- 11 -rsb- . then the intensity and texture information are integrated to remove shadows and to enable the algorithm working for quick lighting changes . for foreground analysis , the same gaussian mixture model is employed to detect the static foreground regions without using any tracking or motion information . then the whole static regions are pushed back to the background model to avoid a common problem in background subtraction -- fragmentation -lrb- one object becomes multiple parts -rrb- . the method was tested on our real time video surveillance system . it is robust and run about 130 fps for color images and 150 fps for grayscale images at size 160x120 on a 2gb pentium iv machine with mmx optimization ."
  },
  {
    "title": "A new speaker verification spoofing countermeasure based on local binary patterns .",
    "entities": [
      "local binary patterns",
      "spoofed , converted voice signals",
      "automatic speaker verification systems",
      "acoustic feature vectors",
      "detecting converted voice",
      "artificial signals",
      "voice conversion",
      "prior knowledge",
      "speech synthesis",
      "false acceptance"
    ],
    "types": "<method> <material> <method> <otherscientificterm> <task> <material> <task> <otherscientificterm> <material> <metric>",
    "relations": [
      "local binary patterns -- USED-FOR -- acoustic feature vectors",
      "speech synthesis -- CONJUNCTION -- artificial signals"
    ],
    "abstract": "this paper presents a new countermeasure for the protection of <method_2> from <material_1> . the new countermeasure is based on the analysis of a sequence of <otherscientificterm_3> using <method_0> . compared to existing approaches the new countermeasure is less reliant on <otherscientificterm_7> and affords robust protection from not only <task_6> , for which it is optimised , but also spoofing attacks from <material_8> and <material_5> , all of which otherwise provoke significant increases in <metric_9> . the work highlights the difficulty in <task_4> and also discusses the need for formal evaluations to develop new countermeasures which are less reliant on <otherscientificterm_7> and thus more reflective of practical use cases .",
    "abstract_og": "this paper presents a new countermeasure for the protection of automatic speaker verification systems from spoofed , converted voice signals . the new countermeasure is based on the analysis of a sequence of acoustic feature vectors using local binary patterns . compared to existing approaches the new countermeasure is less reliant on prior knowledge and affords robust protection from not only voice conversion , for which it is optimised , but also spoofing attacks from speech synthesis and artificial signals , all of which otherwise provoke significant increases in false acceptance . the work highlights the difficulty in detecting converted voice and also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge and thus more reflective of practical use cases ."
  },
  {
    "title": "Can tongue be recovered from face ? the answer of data-driven statistical models .",
    "entities": [
      "multi linear regression method",
      "jaw / lips / tongue tip synergy",
      "hidden markov models",
      "gaussian mixture models",
      "french corpus of articulatory data",
      "face-to-tongue articulatory inversion problem",
      "phonetic class distribution",
      "front high vowels",
      "centralisation effects",
      "coronal consonants",
      "electromagnetography",
      "hmms",
      "speech"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <material>",
    "relations": [
      "hidden markov models -- CONJUNCTION -- gaussian mixture models",
      "hidden markov models -- CONJUNCTION -- multi linear regression method",
      "hmms -- USED-FOR -- phonetic class distribution",
      "gaussian mixture models -- COMPARE -- hmms",
      "gaussian mixture models -- CONJUNCTION -- hmms",
      "gaussian mixture models -- USED-FOR -- phonetic class distribution"
    ],
    "abstract": "this study revisits the <task_5> in <material_12> . we compare the <method_0> with two more sophisticated methods based on <method_2> and <method_3> , using the same <material_4> acquired by <method_10> . <method_3> give overall results better than <method_11> , but <method_3> does poorly . <method_3> and <method_11> maintain the original <otherscientificterm_6> , though with some <otherscientificterm_8> , effects still much stronger with <method_3> . a detailed analysis shows that , if the <otherscientificterm_1> helps recovering <otherscientificterm_7> and <otherscientificterm_9> , the velars are not recovered at all . it is therefore not possible to recover reliably tongue from face .",
    "abstract_og": "this study revisits the face-to-tongue articulatory inversion problem in speech . we compare the multi linear regression method with two more sophisticated methods based on hidden markov models and gaussian mixture models , using the same french corpus of articulatory data acquired by electromagnetography . gaussian mixture models give overall results better than hmms , but gaussian mixture models does poorly . gaussian mixture models and hmms maintain the original phonetic class distribution , though with some centralisation effects , effects still much stronger with gaussian mixture models . a detailed analysis shows that , if the jaw / lips / tongue tip synergy helps recovering front high vowels and coronal consonants , the velars are not recovered at all . it is therefore not possible to recover reliably tongue from face ."
  },
  {
    "title": "Learning Image Matching by Simply Watching Video .",
    "entities": [
      "unsupervised learning based approach",
      "real-world video sequences",
      "empirically designed methods",
      "convolutional neural network",
      "inter-frame correspondences",
      "unsupervised manner",
      "temporal coherency",
      "image matching",
      "analysis-by-synthesis",
      "frame-interpolation",
      "cnn"
    ],
    "types": "<method> <material> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <method> <task> <method>",
    "relations": [
      "unsupervised learning based approach -- USED-FOR -- image matching",
      "convolutional neural network -- USED-FOR -- frame-interpolation",
      "unsupervised manner -- USED-FOR -- frame-interpolation",
      "frame-interpolation -- USED-FOR -- inter-frame correspondences",
      "unsupervised manner -- USED-FOR -- cnn",
      "cnn -- USED-FOR -- frame-interpolation",
      "unsupervised learning based approach -- COMPARE -- empirically designed methods"
    ],
    "abstract": "this work presents an <method_0> to the ubiquitous computer vision problem of <task_7> . we start from the insight that the problem of <task_9> implicitly solves for <otherscientificterm_4> . this permits the application of <method_8> : we firstly train and apply a <method_3> for <task_9> , then obtain correspondences by inverting the learned <method_10> . the key benefit behind this <method_0> is that the <method_10> for <task_9> can be trained in an <method_5> by exploiting the <otherscientificterm_6> that is naturally contained in <material_1> . the present <method_0> therefore learns <task_7> by simply '' watching videos '' . besides a promise to be more generally applicable , the presented <method_0> achieves surprising performance comparable to traditional <method_2> .",
    "abstract_og": "this work presents an unsupervised learning based approach to the ubiquitous computer vision problem of image matching . we start from the insight that the problem of frame-interpolation implicitly solves for inter-frame correspondences . this permits the application of analysis-by-synthesis : we firstly train and apply a convolutional neural network for frame-interpolation , then obtain correspondences by inverting the learned cnn . the key benefit behind this unsupervised learning based approach is that the cnn for frame-interpolation can be trained in an unsupervised manner by exploiting the temporal coherency that is naturally contained in real-world video sequences . the present unsupervised learning based approach therefore learns image matching by simply '' watching videos '' . besides a promise to be more generally applicable , the presented unsupervised learning based approach achieves surprising performance comparable to traditional empirically designed methods ."
  },
  {
    "title": "A practical , self-adaptive voice activity detector for speaker verification with noisy telephone and microphone data .",
    "entities": [
      "mel-frequency cepstral coefficients",
      "voice activity detector",
      "speech and nonspeech models",
      "likelihood ratio based vad",
      "speech enhancement preprocessing",
      "energy vad variants",
      "vad error analysis",
      "robust speaker verification",
      "noise-free conditions",
      "energy vad",
      "training labels",
      "utterance-by-utterance basis",
      "noisy conditions",
      "i-vector system",
      "speaker verification"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "voice activity detector -- USED-FOR -- robust speaker verification",
      "likelihood ratio based vad -- COMPARE -- energy vad variants",
      "vad error analysis -- CONJUNCTION -- speaker verification",
      "likelihood ratio based vad -- USED-FOR -- speech and nonspeech models",
      "vad error analysis -- EVALUATE-FOR -- likelihood ratio based vad"
    ],
    "abstract": "a <method_1> plays a vital role in <task_7> , where <method_9> is most commonly used . <method_1> works well in <otherscientificterm_8> but deteriorates in <otherscientificterm_12> . one way to tackle this is to introduce <method_4> . we study an alternative , <method_3> that trains <method_2> on an <otherscientificterm_11> from <method_0> . the <otherscientificterm_10> are obtained from enhanced <method_9> . as the <method_2> are retrained for each utterance , minimum assumptions of the background noise are made . according to both <method_6> and <task_14> results utilizing state-of-the-art <method_13> , the proposed <method_3> outperforms <method_5> by a wide margin . we provide open-source implementation of the <method_3> .",
    "abstract_og": "a voice activity detector plays a vital role in robust speaker verification , where energy vad is most commonly used . voice activity detector works well in noise-free conditions but deteriorates in noisy conditions . one way to tackle this is to introduce speech enhancement preprocessing . we study an alternative , likelihood ratio based vad that trains speech and nonspeech models on an utterance-by-utterance basis from mel-frequency cepstral coefficients . the training labels are obtained from enhanced energy vad . as the speech and nonspeech models are retrained for each utterance , minimum assumptions of the background noise are made . according to both vad error analysis and speaker verification results utilizing state-of-the-art i-vector system , the proposed likelihood ratio based vad outperforms energy vad variants by a wide margin . we provide open-source implementation of the likelihood ratio based vad ."
  },
  {
    "title": "Evaluation and design of variable step size adaptive algorithms .",
    "entities": [
      "variable step size adaptive algorithms",
      "transient and steady-state behaviors",
      "mean square error",
      "system identification applications",
      "learning plane",
      "algorithm optimization",
      "optimum trajectory",
      "step size"
    ],
    "types": "<method> <otherscientificterm> <metric> <task> <method> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "algorithm optimization -- USED-FOR -- system identification applications",
      "step size -- CONJUNCTION -- mean square error"
    ],
    "abstract": "this paper presents a new methodology for evaluation and design of <method_0> . the new methodology is based on a <method_4> , which combines the evolutions of both the <otherscientificterm_7> and the <metric_2> . it includes both <otherscientificterm_1> and can be used to compare performances of different algorithms against an <otherscientificterm_6> in the <method_4> . the new technique can also be used for <task_5> in <task_3> .",
    "abstract_og": "this paper presents a new methodology for evaluation and design of variable step size adaptive algorithms . the new methodology is based on a learning plane , which combines the evolutions of both the step size and the mean square error . it includes both transient and steady-state behaviors and can be used to compare performances of different algorithms against an optimum trajectory in the learning plane . the new technique can also be used for algorithm optimization in system identification applications ."
  },
  {
    "title": "A novel search algorithm for LSF VQ .",
    "entities": [
      "crvq-cs -lrb- constrained range vector quantization",
      "weighted euclidean distance measure",
      "weighted euclidean distance",
      "component searching -rrb-",
      "lsf vector quantizers",
      "euclidean distance measure",
      "search vq",
      "computational complexity"
    ],
    "types": "<method> <metric> <otherscientificterm> <method> <method> <metric> <method> <metric>",
    "relations": [
      "crvq-cs -lrb- constrained range vector quantization -- USED-FOR -- crvq-cs -lrb- constrained range vector quantization",
      "euclidean distance measure -- USED-FOR -- crvq-cs -lrb- constrained range vector quantization",
      "weighted euclidean distance -- USED-FOR -- lsf vector quantizers",
      "weighted euclidean distance -- USED-FOR -- crvq-cs -lrb- constrained range vector quantization",
      "weighted euclidean distance measure -- USED-FOR -- crvq-cs -lrb- constrained range vector quantization",
      "crvq-cs -lrb- constrained range vector quantization -- HYPONYM-OF -- crvq-cs -lrb- constrained range vector quantization"
    ],
    "abstract": "because classical fast vector quantization -lrb- <method_0> -rrb- algorithms ca n't be used in the <method_4> that use varying <otherscientificterm_2> , a novel <method_0> -- <method_0> based on <method_3> is presented in this paper . the <method_0> works well with the varying <otherscientificterm_2> and yields the same result as full <method_6> with reduced <metric_7> does . although the <method_0> is proposed for <method_0> using varying <metric_1> , <method_0> is also suitable for <method_0> using simple <metric_5> .",
    "abstract_og": "because classical fast vector quantization -lrb- crvq-cs -lrb- constrained range vector quantization -rrb- algorithms ca n't be used in the lsf vector quantizers that use varying weighted euclidean distance , a novel crvq-cs -lrb- constrained range vector quantization -- crvq-cs -lrb- constrained range vector quantization based on component searching -rrb- is presented in this paper . the crvq-cs -lrb- constrained range vector quantization works well with the varying weighted euclidean distance and yields the same result as full search vq with reduced computational complexity does . although the crvq-cs -lrb- constrained range vector quantization is proposed for crvq-cs -lrb- constrained range vector quantization using varying weighted euclidean distance measure , crvq-cs -lrb- constrained range vector quantization is also suitable for crvq-cs -lrb- constrained range vector quantization using simple euclidean distance measure ."
  },
  {
    "title": "Variational inference for conditional random fields .",
    "entities": [
      "structured variational inference",
      "conditional random fields",
      "factorized variational inference",
      "conditional random fields -lrb- crfs",
      "idiap human motion database",
      "variation inference methods",
      "human motion recognition",
      "contextual pattern classification",
      "variational inference methods",
      "state structure",
      "variational distribution",
      "classification accuracy",
      "indirect calculation",
      "variational distributions",
      "baseline crfs",
      "conditional probability",
      "viterbi approximation"
    ],
    "types": "<method> <method> <method> <method> <material> <method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <metric> <method> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "conditional random fields -lrb- crfs -- COMPARE -- baseline crfs",
      "conditional random fields -- USED-FOR -- contextual pattern classification",
      "structured variational inference -- USED-FOR -- state structure",
      "structured variational inference -- PART-OF -- conditional random fields -lrb- crfs",
      "variation inference methods -- USED-FOR -- conditional random fields -lrb- crfs",
      "viterbi approximation -- USED-FOR -- conditional random fields -lrb- crfs",
      "factorized variational inference -- CONJUNCTION -- structured variational inference",
      "viterbi approximation -- USED-FOR -- variation inference methods",
      "viterbi approximation -- USED-FOR -- baseline crfs",
      "classification accuracy -- EVALUATE-FOR -- conditional random fields -lrb- crfs",
      "classification accuracy -- EVALUATE-FOR -- structured variational inference",
      "variation inference methods -- USED-FOR -- baseline crfs",
      "factorized variational inference -- PART-OF -- conditional random fields -lrb- crfs"
    ],
    "abstract": "conditional random fields -lrb- <method_3> -rrb- have been popular for <task_7> . this paper presents two <method_8> for direct approximation of a <otherscientificterm_15> instead of <method_12> through <method_16> of a marginal probability . the <method_3> with the <method_2> and the <method_0> are proposed and investigated for <task_6> . in general , <method_0> assumes a factorization of <otherscientificterm_13> of individual states for representation of <otherscientificterm_15> while <method_0> preserves the <otherscientificterm_9> in the <otherscientificterm_10> . in the experiments on using <material_4> , we found that <method_3> using <method_5> performed better than <method_14> using <method_16> . <method_3> with <method_0> obtained higher <metric_11> than those with <method_0> .",
    "abstract_og": "conditional random fields -lrb- conditional random fields -lrb- crfs -rrb- have been popular for contextual pattern classification . this paper presents two variational inference methods for direct approximation of a conditional probability instead of indirect calculation through viterbi approximation of a marginal probability . the conditional random fields -lrb- crfs with the factorized variational inference and the structured variational inference are proposed and investigated for human motion recognition . in general , structured variational inference assumes a factorization of variational distributions of individual states for representation of conditional probability while structured variational inference preserves the state structure in the variational distribution . in the experiments on using idiap human motion database , we found that conditional random fields -lrb- crfs using variation inference methods performed better than baseline crfs using viterbi approximation . conditional random fields -lrb- crfs with structured variational inference obtained higher classification accuracy than those with structured variational inference ."
  },
  {
    "title": "Maintaining Coherent Perceptual Information Using Anchoring .",
    "entities": [
      "spatial and olfactory sensors",
      "correspondence between sensor data",
      "extended periods of time",
      "mobile robotic system",
      "priori symbolic concepts",
      "user requested tasks",
      "anchoring framework",
      "symbolic descriptions",
      "perceptual maintenance",
      "sensor-data"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <material> <method> <otherscientificterm> <task> <material>",
    "relations": [
      "spatial and olfactory sensors -- USED-FOR -- mobile robotic system",
      "anchoring framework -- USED-FOR -- perceptual maintenance",
      "mobile robotic system -- USED-FOR -- user requested tasks"
    ],
    "abstract": "the purpose of this paper is to address the problem of maintaining coherent perceptual information in a <method_3> working over <otherscientificterm_2> , interacting with a user and using multiple sensing modalities to gather information about the environment and specific objects . we present a <method_3> which is able to use <otherscientificterm_0> to patrol a corridor and execute <material_5> . to cope with <task_8> we present an extension of the <method_6> capable of maintaining the <material_1> and the <otherscientificterm_7> referring to objects . <method_3> is also capable of tracking and acquiring information from observations derived from <material_9> as well as information from a <otherscientificterm_4> . the general <method_3> is described and an experimental validation on a <method_3> is presented .",
    "abstract_og": "the purpose of this paper is to address the problem of maintaining coherent perceptual information in a mobile robotic system working over extended periods of time , interacting with a user and using multiple sensing modalities to gather information about the environment and specific objects . we present a mobile robotic system which is able to use spatial and olfactory sensors to patrol a corridor and execute user requested tasks . to cope with perceptual maintenance we present an extension of the anchoring framework capable of maintaining the correspondence between sensor data and the symbolic descriptions referring to objects . mobile robotic system is also capable of tracking and acquiring information from observations derived from sensor-data as well as information from a priori symbolic concepts . the general mobile robotic system is described and an experimental validation on a mobile robotic system is presented ."
  },
  {
    "title": "Burst deblurring : Removing camera shake through fourier burst accumulation .",
    "entities": [
      "inverse and inherently ill-posed deconvolution problem",
      "fourier burst accumulation algorithm",
      "real camera data",
      "fourier spectrum magnitude",
      "random nature",
      "image blur",
      "fourier domain",
      "on-board implementation",
      "digital cameras",
      "blur estimation",
      "inverse problem",
      "camera shake",
      "modality",
      "images",
      "photographer"
    ],
    "types": "<task> <method> <material> <metric> <otherscientificterm> <otherscientificterm> <material> <task> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "fourier burst accumulation algorithm -- USED-FOR -- on-board implementation",
      "real camera data -- EVALUATE-FOR -- fourier burst accumulation algorithm",
      "blur estimation -- CONJUNCTION -- inverse problem"
    ],
    "abstract": "numerous recent approaches attempt to remove <otherscientificterm_5> due to <otherscientificterm_11> , either with one or multiple input <material_13> , by explicitly solving an <task_0> . if the <method_14> takes a burst of <material_13> , a <otherscientificterm_12> available in virtually all modern <otherscientificterm_8> , we show that it is possible to combine them to get a clean sharp version . this is done without explicitly solving any <method_9> and subsequent <task_10> . the proposed algorithm is strikingly simple : it performs a weighted average in the <material_6> , with weights depending on the <metric_3> . the method 's rationale is that <otherscientificterm_11> has a <otherscientificterm_4> and therefore each image in the burst is generally blurred differently . experiments with <material_2> show that the proposed <method_1> achieves state-of-the-art results an order of magnitude faster , with simplicity for <task_7> on camera phones .",
    "abstract_og": "numerous recent approaches attempt to remove image blur due to camera shake , either with one or multiple input images , by explicitly solving an inverse and inherently ill-posed deconvolution problem . if the photographer takes a burst of images , a modality available in virtually all modern digital cameras , we show that it is possible to combine them to get a clean sharp version . this is done without explicitly solving any blur estimation and subsequent inverse problem . the proposed algorithm is strikingly simple : it performs a weighted average in the fourier domain , with weights depending on the fourier spectrum magnitude . the method 's rationale is that camera shake has a random nature and therefore each image in the burst is generally blurred differently . experiments with real camera data show that the proposed fourier burst accumulation algorithm achieves state-of-the-art results an order of magnitude faster , with simplicity for on-board implementation on camera phones ."
  },
  {
    "title": "Machine Translation Evaluation Meets Community Question Answering .",
    "entities": [
      "machine translation evaluation methods",
      "pairwise neural network architecture",
      "rich syntactic and semantic embeddings",
      "pairwise nn architecture",
      "community question answering",
      "mte features",
      "non-linear interactions",
      "ranking"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <task>",
    "relations": [
      "mte features -- CONJUNCTION -- rich syntactic and semantic embeddings"
    ],
    "abstract": "we explore the applicability of <method_0> to a very different problem : answer <task_7> in <task_4> . in particular , we adopt a <method_1> , which incorporates <method_5> , as well as <otherscientificterm_2> , and which efficiently models complex <otherscientificterm_6> . the evaluation results show state-of-the-art performance , with sizeable contribution from both the <method_5> and from the <method_3> .",
    "abstract_og": "we explore the applicability of machine translation evaluation methods to a very different problem : answer ranking in community question answering . in particular , we adopt a pairwise neural network architecture , which incorporates mte features , as well as rich syntactic and semantic embeddings , and which efficiently models complex non-linear interactions . the evaluation results show state-of-the-art performance , with sizeable contribution from both the mte features and from the pairwise nn architecture ."
  },
  {
    "title": "Interactive Construction of 3D Models from Panoramic Mosaics .",
    "entities": [
      "soft and hard linear constraints",
      "linearly-constrained least-squares problem",
      "texture-mapped 3d models",
      "panoramic image mosaics",
      "interactive modeling system",
      "partition constraints",
      "qr factorization",
      "panoramic mosaic",
      "geometrical constraints",
      "3d models",
      "transformation matrix"
    ],
    "types": "<otherscientificterm> <task> <method> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "qr factorization -- USED-FOR -- linearly-constrained least-squares problem",
      "panoramic image mosaics -- USED-FOR -- interactive modeling system",
      "panoramic image mosaics -- USED-FOR -- 3d models",
      "linearly-constrained least-squares problem -- USED-FOR -- interactive modeling system",
      "geometrical constraints -- USED-FOR -- 3d models",
      "interactive modeling system -- USED-FOR -- 3d models"
    ],
    "abstract": "this paper presents an <method_4> that constructs <method_9> from a collection of <material_3> . a <otherscientificterm_7> consists of a set of images taken around the same viewpoint , and a <otherscientificterm_10> associated with each input image . our <method_4> first recovers the camera pose for each mosaic from known line directions and points , and then constructs the <method_9> using all available <otherscientificterm_8> . we <otherscientificterm_5> into <otherscientificterm_0> so that the <method_4> can be formulated as a <task_1> , which can be solved efficiently using <method_6> . the results of extracting wire frame and <method_2> from single and multiple panoramas are presented .",
    "abstract_og": "this paper presents an interactive modeling system that constructs 3d models from a collection of panoramic image mosaics . a panoramic mosaic consists of a set of images taken around the same viewpoint , and a transformation matrix associated with each input image . our interactive modeling system first recovers the camera pose for each mosaic from known line directions and points , and then constructs the 3d models using all available geometrical constraints . we partition constraints into soft and hard linear constraints so that the interactive modeling system can be formulated as a linearly-constrained least-squares problem , which can be solved efficiently using qr factorization . the results of extracting wire frame and texture-mapped 3d models from single and multiple panoramas are presented ."
  },
  {
    "title": "Perception-based selective encryption of G. 729 speech .",
    "entities": [
      "low-complexity , perception-based partial encryption scheme",
      "widely-used speech coding algorithm",
      "formal listening tests",
      "mobile multimedia applications",
      "customer privacy",
      "content protection",
      "telephone-bandwidth speech",
      "full encryption",
      "low-power techniques",
      "speech-content protection",
      "wireless services",
      "cs-acelp",
      "encryp-tion"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <task> <material> <otherscientificterm> <method> <task> <method> <method> <method>",
    "relations": [
      "content protection -- EVALUATE-FOR -- cs-acelp",
      "content protection -- CONJUNCTION -- customer privacy",
      "low-power techniques -- USED-FOR -- mobile multimedia applications"
    ],
    "abstract": "mobile multimedia applications , the focus of many forthcoming <method_10> , increasingly demand <method_8> implementing <task_5> and <otherscientificterm_4> . in this paper a <method_0> for <material_6> is presented . speech compressed by a <method_1> , itu-t g. 729 <method_11> at 8 kb/s , is partitioned in two classes , one , the most perceptually relevant , to be encrypted , the other , to be left unprotected . <method_12> of about 45 % of the <method_11> achieves <task_5> equivalent to <otherscientificterm_7> of the <method_11> , as verified by both objective measures and <method_2> . low-power , portable devices can , therefore , implement very high levels of <task_9> at a fraction of the computational load of current techniques , freeing resources for other tasks and enabling longer battery life .",
    "abstract_og": "mobile multimedia applications , the focus of many forthcoming wireless services , increasingly demand low-power techniques implementing content protection and customer privacy . in this paper a low-complexity , perception-based partial encryption scheme for telephone-bandwidth speech is presented . speech compressed by a widely-used speech coding algorithm , itu-t g. 729 cs-acelp at 8 kb/s , is partitioned in two classes , one , the most perceptually relevant , to be encrypted , the other , to be left unprotected . encryp-tion of about 45 % of the cs-acelp achieves content protection equivalent to full encryption of the cs-acelp , as verified by both objective measures and formal listening tests . low-power , portable devices can , therefore , implement very high levels of speech-content protection at a fraction of the computational load of current techniques , freeing resources for other tasks and enabling longer battery life ."
  },
  {
    "title": "Multi-view 3D Models from Single Images with a Convolutional Network .",
    "entities": [
      "surface mesh",
      "cluttered background",
      "depth map",
      "arbitrary view",
      "3d representation",
      "depth maps",
      "point cloud",
      "rgb image",
      "convolutional network"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "convolutional network -- USED-FOR -- 3d representation"
    ],
    "abstract": "we present a <method_8> capable of inferring a <method_4> of a previously unseen object given a single image of this object . concretely , the <method_8> can predict an <material_7> and a <otherscientificterm_2> of the object as seen from an <otherscientificterm_3> . several of these <otherscientificterm_5> fused together give a full <otherscientificterm_6> of the object . the <otherscientificterm_6> can in turn be transformed into a <otherscientificterm_0> . the <method_8> is trained on renderings of synthetic 3d models of cars and chairs . <method_8> successfully deals with objects on <otherscientificterm_1> and generates reasonable predictions for real images of cars .",
    "abstract_og": "we present a convolutional network capable of inferring a 3d representation of a previously unseen object given a single image of this object . concretely , the convolutional network can predict an rgb image and a depth map of the object as seen from an arbitrary view . several of these depth maps fused together give a full point cloud of the object . the point cloud can in turn be transformed into a surface mesh . the convolutional network is trained on renderings of synthetic 3d models of cars and chairs . convolutional network successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars ."
  },
  {
    "title": "No-Regret Learning in Bayesian Games .",
    "entities": [
      "bayesian coarse correlated equilibrium",
      "no-regret learning dynamics",
      "incomplete information games",
      "coarse correlated equilibria",
      "near-optimal welfare",
      "incomplete information",
      "bayesian games",
      "bayesian game"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "no-regret learning dynamics -- USED-FOR -- bayesian coarse correlated equilibrium",
      "near-optimal welfare -- FEATURE-OF -- no-regret learning dynamics"
    ],
    "abstract": "recent price-of-anarchy analyses of games of complete information suggest that <otherscientificterm_3> , which characterize outcomes resulting from <otherscientificterm_1> , have <otherscientificterm_4> . this work provides two main technical results that lift this conclusion to games of <otherscientificterm_5> , a.k.a. , <material_6> . first , <otherscientificterm_4> in <material_6> follows directly from the smoothness-based proof of <otherscientificterm_4> in the same game when the private information is public . second , <otherscientificterm_1> converge to <otherscientificterm_0> in these <otherscientificterm_2> . these results are enabled by interpretation of a <method_7> as a stochastic game of complete information .",
    "abstract_og": "recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare . this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games . first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public . second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games . these results are enabled by interpretation of a bayesian game as a stochastic game of complete information ."
  },
  {
    "title": "Joint uncertainty decoding with the second order approximation for noise robust speech recognition .",
    "entities": [
      "vector taylor series",
      "joint uncertainty decoding algorithm",
      "joint uncertainty decoding",
      "aurora 2 database",
      "mathematically consistent framework",
      "second-order taylor expansion",
      "regression class",
      "front-end uncertainty",
      "regression classes",
      "computational cost",
      "recognition accuracy",
      "taylor expansion",
      "hmm mixture"
    ],
    "types": "<method> <method> <method> <material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <metric> <method> <otherscientificterm>",
    "relations": [
      "recognition accuracy -- EVALUATE-FOR -- joint uncertainty decoding algorithm",
      "aurora 2 database -- EVALUATE-FOR -- joint uncertainty decoding algorithm",
      "computational cost -- EVALUATE-FOR -- joint uncertainty decoding algorithm",
      "joint uncertainty decoding -- COMPARE -- vector taylor series",
      "taylor expansion -- COMPARE -- vector taylor series",
      "taylor expansion -- USED-FOR -- joint uncertainty decoding",
      "second-order taylor expansion -- USED-FOR -- joint uncertainty decoding algorithm",
      "aurora 2 database -- EVALUATE-FOR -- vector taylor series",
      "recognition accuracy -- EVALUATE-FOR -- vector taylor series",
      "taylor expansion -- USED-FOR -- hmm mixture",
      "computational cost -- EVALUATE-FOR -- vector taylor series"
    ],
    "abstract": "joint uncertainty decoding has recently achieved promising results by integrating the <otherscientificterm_7> into the back-end in a <method_4> . in this paper , <method_2> is compared with the widely used <method_0> . we show that the two methods are identical except that <method_2> applies the <method_11> on each <otherscientificterm_6> whereas <method_0> applies <method_11> to each <otherscientificterm_12> . the relatively rougher expansion points used in <method_2> make <method_11> computationally cheaper than <method_0> but inevitably worse on <metric_10> . to overcome this drawback , this paper proposes an improved <method_1> which employs <method_5> on each <otherscientificterm_6> in order to reduce the expansion errors . special considerations are further given to limit the overall <metric_9> by adopting different number of <otherscientificterm_8> for different orders in the <method_11> . experiments on the <material_3> show that the proposed <method_1> is able to beat <method_0> on <metric_10> and <metric_9> with relative improvement up to 6 % and 60 % , respectively .",
    "abstract_og": "joint uncertainty decoding has recently achieved promising results by integrating the front-end uncertainty into the back-end in a mathematically consistent framework . in this paper , joint uncertainty decoding is compared with the widely used vector taylor series . we show that the two methods are identical except that joint uncertainty decoding applies the taylor expansion on each regression class whereas vector taylor series applies taylor expansion to each hmm mixture . the relatively rougher expansion points used in joint uncertainty decoding make taylor expansion computationally cheaper than vector taylor series but inevitably worse on recognition accuracy . to overcome this drawback , this paper proposes an improved joint uncertainty decoding algorithm which employs second-order taylor expansion on each regression class in order to reduce the expansion errors . special considerations are further given to limit the overall computational cost by adopting different number of regression classes for different orders in the taylor expansion . experiments on the aurora 2 database show that the proposed joint uncertainty decoding algorithm is able to beat vector taylor series on recognition accuracy and computational cost with relative improvement up to 6 % and 60 % , respectively ."
  },
  {
    "title": "A Variational Principle for Model-based Morphing .",
    "entities": [
      "multidimensional data set",
      "boundary value problem",
      "mixture models",
      "arc length",
      "extremal motionj",
      "euler-lagrange equations"
    ],
    "types": "<material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "euler-lagrange equations -- USED-FOR -- extremal motionj"
    ],
    "abstract": "given a <material_0> and a model of its density , we consider how to define the optimal interpolation between two points . this is done by assigning a cost to each path through space , based on two competing goals-one to interpolate through regions of high density , the other to minimize <otherscientificterm_3> . from this path functional , we derive the <otherscientificterm_5> for <otherscientificterm_4> given two points , the desired interpolation is found by solving a <task_1> . we show that this interpolation can be done efficiently , in high dimensions , for gaussian , dirichlet , and <method_2> .",
    "abstract_og": "given a multidimensional data set and a model of its density , we consider how to define the optimal interpolation between two points . this is done by assigning a cost to each path through space , based on two competing goals-one to interpolate through regions of high density , the other to minimize arc length . from this path functional , we derive the euler-lagrange equations for extremal motionj given two points , the desired interpolation is found by solving a boundary value problem . we show that this interpolation can be done efficiently , in high dimensions , for gaussian , dirichlet , and mixture models ."
  },
  {
    "title": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness .",
    "entities": [
      "computing semantic relatedness of concepts",
      "german counterpart of wordnet",
      "artificial conceptual glosses",
      "german dataset",
      "life sciences",
      "corpus analysis",
      "conceptual networks",
      "semantic re-latedness",
      "textual definitions",
      "semantic relatedness",
      "conceptual network",
      "germanet"
    ],
    "types": "<task> <material> <otherscientificterm> <material> <material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "german dataset -- EVALUATE-FOR -- germanet",
      "conceptual networks -- USED-FOR -- semantic relatedness",
      "conceptual network -- USED-FOR -- artificial conceptual glosses"
    ],
    "abstract": "we present a new method for <task_0> . the method relies solely on the structure of a <method_10> and eliminates the need for performing additional <task_5> . the <method_10> is employed to generate <otherscientificterm_2> . they replace <otherscientificterm_8> proper written by humans and are processed by a dictionary based metric of <otherscientificterm_9> -lsb- 1 -rsb- . we implemented the metric on the basis of <method_11> , the <material_1> , and evaluated the results on a <material_3> of 57 word pairs rated by human subjects for their <otherscientificterm_7> . our approach can be easily applied to compute <otherscientificterm_9> based on alternative <method_6> , e.g. in the domain of <material_4> .",
    "abstract_og": "we present a new method for computing semantic relatedness of concepts . the method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis . the conceptual network is employed to generate artificial conceptual glosses . they replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness -lsb- 1 -rsb- . we implemented the metric on the basis of germanet , the german counterpart of wordnet , and evaluated the results on a german dataset of 57 word pairs rated by human subjects for their semantic re-latedness . our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks , e.g. in the domain of life sciences ."
  },
  {
    "title": "Multiresolution eigenimages for texture classification .",
    "entities": [
      "fuzzy c-means algorithm",
      "gaussian properties of eigenimages",
      "classification of the texture",
      "r largest resulting coefficients",
      "fixed size \u03b4\u00b5\u03b4",
      "brodatz album",
      "multiresolution eigenimages",
      "texture classification",
      "hotelling coefficients",
      "classification process",
      "sub-images",
      "classification",
      "\u03b4",
      "hotelling"
    ],
    "types": "<method> <material> <task> <otherscientificterm> <otherscientificterm> <material> <material> <task> <otherscientificterm> <method> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "r largest resulting coefficients -- USED-FOR -- classification of the texture",
      "fuzzy c-means algorithm -- USED-FOR -- classification",
      "sub-images -- USED-FOR -- classification of the texture"
    ],
    "abstract": "following an idea from -lsb- 1 -rsb- , based on the <material_1> , this paper presents a new technique for <task_7> using <material_6> . the input image , composed of two textures from the <material_5> , is subdivided into n <otherscientificterm_10> of <otherscientificterm_4> , which are blurred with a gaussian and normalized . the application of <method_13> transform decomposes each sub-image into <method_12> 2 eigenimages . the <otherscientificterm_3> can be used for <task_2> present in the <otherscientificterm_10> . <task_11> is done using the <method_0> and the performance is measured with an appropriate quality factor . we discuss the successful application of this technique , as well as the influence of the different parameters of the <method_9> on several pairs of textures . moreover , combination of <otherscientificterm_8> obtained with different values of <method_12> is shown to improve the performance , based on the idea of analyzing the texture at different levels of resolution .",
    "abstract_og": "following an idea from -lsb- 1 -rsb- , based on the gaussian properties of eigenimages , this paper presents a new technique for texture classification using multiresolution eigenimages . the input image , composed of two textures from the brodatz album , is subdivided into n sub-images of fixed size \u03b4\u00b5\u03b4 , which are blurred with a gaussian and normalized . the application of hotelling transform decomposes each sub-image into \u03b4 2 eigenimages . the r largest resulting coefficients can be used for classification of the texture present in the sub-images . classification is done using the fuzzy c-means algorithm and the performance is measured with an appropriate quality factor . we discuss the successful application of this technique , as well as the influence of the different parameters of the classification process on several pairs of textures . moreover , combination of hotelling coefficients obtained with different values of \u03b4 is shown to improve the performance , based on the idea of analyzing the texture at different levels of resolution ."
  },
  {
    "title": "Unsupervised Modeling of Dialog Acts in Asynchronous Conversations .",
    "entities": [
      "lexical and structural similarity metrics",
      "mod-eling dialog acts",
      "graph-theoretic deter-ministic framework",
      "probabilistic conversation models",
      "modeling dialog acts",
      "graph-structural order",
      "hmm model",
      "temporal order",
      "asynchronous conversations",
      "unsupervised approaches",
      "conversation models",
      "hmm",
      "emails"
    ],
    "types": "<metric> <otherscientificterm> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <material> <method> <method> <method> <material>",
    "relations": [
      "asynchronous conversations -- FEATURE-OF -- modeling dialog acts",
      "temporal order -- USED-FOR -- conversation models",
      "conversation models -- COMPARE -- hmm model",
      "lexical and structural similarity metrics -- USED-FOR -- graph-theoretic deter-ministic framework",
      "unsupervised approaches -- USED-FOR -- modeling dialog acts"
    ],
    "abstract": "we present <method_9> to the problem of <task_4> in <material_8> ; i.e. , conversations where participants collaborate with each other at different times . in particular , we investigate a <method_2> and two <method_3> -lrb- i.e. , <method_11> and hmm+m ix -rrb- for <otherscientificterm_1> in <material_12> and forums . we train and test our <method_10> on -lrb- a -rrb- <otherscientificterm_7> and -lrb- b -rrb- <otherscientificterm_5> of the datasets . empirical evaluation suggests -lrb- i -rrb- the <method_2> that relies on <metric_0> is not the right model for this task , -lrb- ii -rrb- <method_10> perform better on the <otherscientificterm_5> than the <otherscientificterm_7> of the datasets and -lrb- iii -rrb- hmm+m ix is a better <method_10> than the simple <method_6> .",
    "abstract_og": "we present unsupervised approaches to the problem of modeling dialog acts in asynchronous conversations ; i.e. , conversations where participants collaborate with each other at different times . in particular , we investigate a graph-theoretic deter-ministic framework and two probabilistic conversation models -lrb- i.e. , hmm and hmm+m ix -rrb- for mod-eling dialog acts in emails and forums . we train and test our conversation models on -lrb- a -rrb- temporal order and -lrb- b -rrb- graph-structural order of the datasets . empirical evaluation suggests -lrb- i -rrb- the graph-theoretic deter-ministic framework that relies on lexical and structural similarity metrics is not the right model for this task , -lrb- ii -rrb- conversation models perform better on the graph-structural order than the temporal order of the datasets and -lrb- iii -rrb- hmm+m ix is a better conversation models than the simple hmm model ."
  },
  {
    "title": "Grounded Models as a Basis for Intuitive Reasoning .",
    "entities": [
      "generation of logical categories",
      "axiomatic theories",
      "intuitive reasoning",
      "autonomous agents",
      "language game",
      "spatial reasoning",
      "grounded models",
      "grounded model",
      "disjunction",
      "implication",
      "conjunction",
      "negation"
    ],
    "types": "<task> <otherscientificterm> <task> <method> <method> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "negation -- CONJUNCTION -- conjunction",
      "negation -- CONJUNCTION -- disjunction",
      "disjunction -- CONJUNCTION -- implication",
      "implication -- HYPONYM-OF -- generation of logical categories",
      "grounded model -- USED-FOR -- spatial reasoning",
      "language game -- USED-FOR -- generation of logical categories",
      "conjunction -- HYPONYM-OF -- generation of logical categories",
      "conjunction -- CONJUNCTION -- implication",
      "conjunction -- CONJUNCTION -- disjunction",
      "disjunction -- HYPONYM-OF -- generation of logical categories"
    ],
    "abstract": "grounded models -lrb- siena 2001b -rrb- differ from <otherscientificterm_1> in establishing explicit connections between language and reality that are learned through language games -lrb- wittgen-stein 1953 -rrb- . this paper describes how <method_6> are constructed by <method_3> as a side effect of their activity playing different types of language games -lrb- steels 1999 -rrb- , and explains how they can be used for <task_2> . it proposes a particular <method_4> which can be used for simulating the <task_0> -lrb- such as <otherscientificterm_11> , <otherscientificterm_10> , <otherscientificterm_8> , <otherscientificterm_9> or equivalence -rrb- , and describes some experiments in which a couple of visually grounded agents construct a <method_7> that can be used for <task_5> .",
    "abstract_og": "grounded models -lrb- siena 2001b -rrb- differ from axiomatic theories in establishing explicit connections between language and reality that are learned through language games -lrb- wittgen-stein 1953 -rrb- . this paper describes how grounded models are constructed by autonomous agents as a side effect of their activity playing different types of language games -lrb- steels 1999 -rrb- , and explains how they can be used for intuitive reasoning . it proposes a particular language game which can be used for simulating the generation of logical categories -lrb- such as negation , conjunction , disjunction , implication or equivalence -rrb- , and describes some experiments in which a couple of visually grounded agents construct a grounded model that can be used for spatial reasoning ."
  },
  {
    "title": "A Framework for Image Based Authentication .",
    "entities": [
      "compression and transmission techniques",
      "personal digital assistants",
      "image based authentication",
      "human difficulty",
      "user authentication",
      "heterogeneous networks",
      "image scalability",
      "mobile phones",
      "image tiling",
      "interactivity protocol",
      "embedded bitstream",
      "image exchange",
      "password",
      "images",
      "laptops",
      "images",
      "passwords"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <task> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <material> <material> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "images -- COMPARE -- passwords",
      "personal digital assistants -- CONJUNCTION -- laptops",
      "image tiling -- CONJUNCTION -- interactivity protocol",
      "mobile phones -- CONJUNCTION -- personal digital assistants",
      "compression and transmission techniques -- USED-FOR -- image exchange",
      "laptops -- CONJUNCTION -- heterogeneous networks",
      "image scalability -- CONJUNCTION -- embedded bitstream",
      "embedded bitstream -- CONJUNCTION -- image tiling",
      "image scalability -- CONJUNCTION -- image tiling"
    ],
    "abstract": "this paper presents an innovative framework for <task_4> based on <material_15> . common <task_4> based on <otherscientificterm_16> has the main drawback of the <otherscientificterm_3> in recalling them . <material_13> are instead easier to remember than <otherscientificterm_16> . moreover , modern <method_0> make <task_11> between different devices -lrb- e.g. <material_7> , <otherscientificterm_1> , <otherscientificterm_14> , and workstations -rrb- in <method_5> practically feasible . in the proposed approach , <material_15> are coded using the emerging jpeg2000 standard and taking advantage of many of its features -lrb- e.g. <otherscientificterm_6> , <otherscientificterm_10> , <otherscientificterm_8> , and <otherscientificterm_9> -rrb- . the described <task_2> is more secure than the common approach based on <material_12> .",
    "abstract_og": "this paper presents an innovative framework for user authentication based on images . common user authentication based on passwords has the main drawback of the human difficulty in recalling them . images are instead easier to remember than passwords . moreover , modern compression and transmission techniques make image exchange between different devices -lrb- e.g. mobile phones , personal digital assistants , laptops , and workstations -rrb- in heterogeneous networks practically feasible . in the proposed approach , images are coded using the emerging jpeg2000 standard and taking advantage of many of its features -lrb- e.g. image scalability , embedded bitstream , image tiling , and interactivity protocol -rrb- . the described image based authentication is more secure than the common approach based on password ."
  },
  {
    "title": "WordTopic-MultiRank : A New Method for Automatic Keyphrase Extraction .",
    "entities": [
      "supervised and unsupervised graph-based ranking methods",
      "automatic keyphrase extraction task",
      "multi-relational word network",
      "automatic keyphrase extraction",
      "data sets",
      "latent topics",
      "heterogeneous relations",
      "ranking algorithm",
      "keyphrase extraction",
      "wordtopic-multirank",
      "robustness"
    ],
    "types": "<method> <task> <method> <task> <material> <otherscientificterm> <otherscientificterm> <method> <task> <method> <metric>",
    "relations": [
      "wordtopic-multirank -- HYPONYM-OF -- ranking algorithm",
      "wordtopic-multirank -- USED-FOR -- keyphrase extraction"
    ],
    "abstract": "automatic <task_8> aims to pick out a set of terms as a representation of a document without manual assignment efforts . <method_0> have been studied for this task . however , previous methods usually computed importance scores of words under the assumption of single relation between words . in this work , we propose <method_9> as a new method for <task_8> , based on the idea that words relate with each other via multiple relations . first we treat various <otherscientificterm_5> in documents as <otherscientificterm_6> between words and construct a <method_2> . then , a novel <method_7> , named <method_9> , is applied to score the importance of words and topics simultaneously , as words and topics are considered to have mutual influence on each other . experimental results on two different <material_4> show the outstanding performance and <metric_10> of our proposed approach in <task_1> .",
    "abstract_og": "automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts . supervised and unsupervised graph-based ranking methods have been studied for this task . however , previous methods usually computed importance scores of words under the assumption of single relation between words . in this work , we propose wordtopic-multirank as a new method for keyphrase extraction , based on the idea that words relate with each other via multiple relations . first we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network . then , a novel ranking algorithm , named wordtopic-multirank , is applied to score the importance of words and topics simultaneously , as words and topics are considered to have mutual influence on each other . experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task ."
  },
  {
    "title": "Geospatial Correspondences for Multimodal Registration .",
    "entities": [
      "change detection problems",
      "markov random field",
      "mid-level sensor-invariant representation",
      "ground truth transfer",
      "land-cover changes",
      "aerial images",
      "locality priors",
      "image regions",
      "multi-sensor images",
      "geographical areas",
      "spatial distribution",
      "non-uniform errors",
      "nonlinear mis-registrations"
    ],
    "types": "<task> <method> <method> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "markov random field -- USED-FOR -- nonlinear mis-registrations",
      "mid-level sensor-invariant representation -- USED-FOR -- image regions"
    ],
    "abstract": "the growing availability of very high resolution -lrb- < 1 m/pixel -rrb- satellite and <material_5> has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world . to do so , images of the same <otherscientificterm_9> acquired at different times and , potentially , with different sensors must be efficiently parsed to update maps and detect <otherscientificterm_4> . however , a na \u00a8 \u0131ve transfer of ground truth labels from one location in the source image to the corresponding location in the target image is generally not feasible , as these images are often only loosely registered -lrb- with up to \u00b1 50m of <otherscientificterm_11> -rrb- . furthermore , <otherscientificterm_4> in an area over time must be taken into account for an accurate <task_3> . to tackle these challenges , we propose a <method_2> that encodes <otherscientificterm_7> in terms of the <otherscientificterm_10> of their spectral neighbors . we incorporate this <method_2> in a <method_1> to simultaneously account for <otherscientificterm_12> and enforce <otherscientificterm_6> to find matches between <material_8> . we show how our approach can be used to assist in several mul-timodal land-cover update and <task_0> .",
    "abstract_og": "the growing availability of very high resolution -lrb- < 1 m/pixel -rrb- satellite and aerial images has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world . to do so , images of the same geographical areas acquired at different times and , potentially , with different sensors must be efficiently parsed to update maps and detect land-cover changes . however , a na \u00a8 \u0131ve transfer of ground truth labels from one location in the source image to the corresponding location in the target image is generally not feasible , as these images are often only loosely registered -lrb- with up to \u00b1 50m of non-uniform errors -rrb- . furthermore , land-cover changes in an area over time must be taken into account for an accurate ground truth transfer . to tackle these challenges , we propose a mid-level sensor-invariant representation that encodes image regions in terms of the spatial distribution of their spectral neighbors . we incorporate this mid-level sensor-invariant representation in a markov random field to simultaneously account for nonlinear mis-registrations and enforce locality priors to find matches between multi-sensor images . we show how our approach can be used to assist in several mul-timodal land-cover update and change detection problems ."
  },
  {
    "title": "Approximating Posterior Distributions in Belief Networks Using Mixtures .",
    "entities": [
      "densely connected bayesian networks",
      "sigmoid belief networks",
      "mean field distributions",
      "mean-field approximating distribution",
      "mean field distribution",
      "mean field theory",
      "approximation schemes",
      "tractable algorithm",
      "approximating distributions",
      "mixture parameters",
      "log likelihood",
      "mixture components"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "mean-field approximating distribution -- USED-FOR -- log likelihood"
    ],
    "abstract": "exact inference in <method_0> is computation-ally intractable , and so there is considerable interest in developing effective <method_6> . one approach which has been adopted is to bound the <otherscientificterm_10> using a <method_3> . while this leads to a <method_7> , the <method_4> is assumed to be factorial and hence unimodal . in this paper we demonstrate the feasibility of using a richer class of <otherscientificterm_8> based on mixtures of <otherscientificterm_2> . we derive an efficient algorithm for updating the <otherscientificterm_9> and apply it to the problem of learning in <method_1> . our results demonstrate a systematic improvement over simple <method_5> as the number of <method_11> is increased .",
    "abstract_og": "exact inference in densely connected bayesian networks is computation-ally intractable , and so there is considerable interest in developing effective approximation schemes . one approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution . while this leads to a tractable algorithm , the mean field distribution is assumed to be factorial and hence unimodal . in this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions . we derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks . our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased ."
  },
  {
    "title": "Spectral analysis of discrete signals generated by multiplicative and additive iterative procedures .",
    "entities": [
      "mul-tiplicative and additive iterative procedures",
      "additive signals",
      "generating vectors",
      "power spectra",
      "generation level",
      "features",
      "fourier"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "generating vectors -- USED-FOR -- additive signals"
    ],
    "abstract": "the discrete <method_6> transform of signals constructed through <method_0> is determined and its specific <otherscientificterm_5> are considered . it is shown that -- in spite of the rather different structure of multiplicative and <otherscientificterm_1> -- the <method_6> transforms of both types of signals exhibit the property of self-affinity . the <otherscientificterm_3> of <otherscientificterm_1> produced by different <method_2> have similar forms and can be divided into similar branches . the number of branches depends on the <otherscientificterm_4> and the symmetry of the power spectrum of the generating vector .",
    "abstract_og": "the discrete fourier transform of signals constructed through mul-tiplicative and additive iterative procedures is determined and its specific features are considered . it is shown that -- in spite of the rather different structure of multiplicative and additive signals -- the fourier transforms of both types of signals exhibit the property of self-affinity . the power spectra of additive signals produced by different generating vectors have similar forms and can be divided into similar branches . the number of branches depends on the generation level and the symmetry of the power spectrum of the generating vector ."
  },
  {
    "title": "Reasoning about Occlusions during Hypothesis Verification .",
    "entities": [
      "object topology",
      "recognition hypotheses",
      "con-nectivity relationships",
      "topology reasoning",
      "verification strategies",
      "object recognition",
      "features",
      "occlusions",
      "verification"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "verification strategies -- USED-FOR -- object recognition"
    ],
    "abstract": "in this paper we study the limitations of current <method_4> in <task_5> and suggest how <method_4> may be enhanced . on the whole <otherscientificterm_0> is exploited little during <task_8> . in practice , understanding the <otherscientificterm_2> between <otherscientificterm_6> in the image , or on the object , can lead to significantly more accurate evaluations of <otherscientificterm_1> . we study how <method_3> allows us to hypothesize the presence of <otherscientificterm_7> in the image . analysis of these hypotheses provides information which turns out to be crucial to the quality of our overall <task_8> results .",
    "abstract_og": "in this paper we study the limitations of current verification strategies in object recognition and suggest how verification strategies may be enhanced . on the whole object topology is exploited little during verification . in practice , understanding the con-nectivity relationships between features in the image , or on the object , can lead to significantly more accurate evaluations of recognition hypotheses . we study how topology reasoning allows us to hypothesize the presence of occlusions in the image . analysis of these hypotheses provides information which turns out to be crucial to the quality of our overall verification results ."
  },
  {
    "title": "A Probabilistic Approach to Integrating Multiple Cues in Visual Tracking .",
    "entities": [
      "linked hidden markov models",
      "message passing scheme",
      "hidden markov model",
      "chain topol-ogy",
      "probabilistic approach",
      "hierarchical integration",
      "visual cues",
      "particle filters",
      "visual tracking",
      "belief propagation",
      "particle filter",
      "edges",
      "color",
      "robustness"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "message passing scheme -- USED-FOR -- particle filters",
      "particle filters -- CONJUNCTION -- belief propagation",
      "color -- CONJUNCTION -- edges",
      "edges -- HYPONYM-OF -- visual cues",
      "color -- HYPONYM-OF -- visual cues",
      "probabilistic approach -- USED-FOR -- visual tracking"
    ],
    "abstract": "this paper presents a novel <method_4> to integrating multiple cues in <task_8> . we perform <task_8> in different cues by interacting processes . each process is represented by a <method_2> , and these parallel processes are arranged in a <method_3> . the resulting <method_0> naturally allow the use of <method_7> and <task_9> in a unified framework . in particular , a target is tracked in each cue by a <method_10> , and the <method_7> in different cues interact via a <method_1> . the general framework of our <method_4> allows a customized combination of different cues in different situations , which is desirable from the implementation point of view . our examples selectively integrate four <otherscientificterm_6> including <otherscientificterm_12> , <otherscientificterm_11> , motion and contours . we demonstrate empirically that the ordering of the cues is nearly inconsequential , and that our <method_4> is superior to other approaches such as independent integration and <method_5> in terms of flexibility and <metric_13> .",
    "abstract_og": "this paper presents a novel probabilistic approach to integrating multiple cues in visual tracking . we perform visual tracking in different cues by interacting processes . each process is represented by a hidden markov model , and these parallel processes are arranged in a chain topol-ogy . the resulting linked hidden markov models naturally allow the use of particle filters and belief propagation in a unified framework . in particular , a target is tracked in each cue by a particle filter , and the particle filters in different cues interact via a message passing scheme . the general framework of our probabilistic approach allows a customized combination of different cues in different situations , which is desirable from the implementation point of view . our examples selectively integrate four visual cues including color , edges , motion and contours . we demonstrate empirically that the ordering of the cues is nearly inconsequential , and that our probabilistic approach is superior to other approaches such as independent integration and hierarchical integration in terms of flexibility and robustness ."
  },
  {
    "title": "Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method .",
    "entities": [
      "alternating direction multiplier method",
      "online proximal gradient descent type method",
      "on-line proximal gradient descent",
      "stochastic optimization techniques",
      "regular-ized dual averaging",
      "overlapped group lasso",
      "stochastic optimization methods",
      "composite function",
      "structured sparsity",
      "learning tasks",
      "online variants",
      "structured regularizations"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "alternating direction multiplier method -- USED-FOR -- composite function",
      "overlapped group lasso -- HYPONYM-OF -- structured sparsity",
      "on-line proximal gradient descent -- CONJUNCTION -- regular-ized dual averaging",
      "stochastic optimization methods -- USED-FOR -- structured regularizations",
      "alternating direction multiplier method -- PART-OF -- stochastic optimization methods",
      "structured sparsity -- EVALUATE-FOR -- stochastic optimization methods",
      "stochastic optimization techniques -- CONJUNCTION -- alternating direction multiplier method"
    ],
    "abstract": "we develop new <method_6> that are applicable to a wide range of <otherscientificterm_11> . basically our <method_6> are combinations of basic <method_3> and <method_0> . <method_0> is a general framework for optimizing a <otherscientificterm_7> , and has a wide range of applications . we propose two types of <method_10> of <method_0> , which correspond to <otherscientificterm_2> and <otherscientificterm_4> respectively . the proposed <method_6> are computationally efficient and easy to implement . our <method_6> yield o -lrb- 1 / \u221a t -rrb- convergence of the expected risk . moreover , the <method_1> yields o -lrb- log -lrb- t -rrb- / t -rrb- convergence for a strongly convex loss . numerical experiments show effectiveness of our <method_6> in <task_9> with <otherscientificterm_8> such as <otherscientificterm_5> .",
    "abstract_og": "we develop new stochastic optimization methods that are applicable to a wide range of structured regularizations . basically our stochastic optimization methods are combinations of basic stochastic optimization techniques and alternating direction multiplier method . alternating direction multiplier method is a general framework for optimizing a composite function , and has a wide range of applications . we propose two types of online variants of alternating direction multiplier method , which correspond to on-line proximal gradient descent and regular-ized dual averaging respectively . the proposed stochastic optimization methods are computationally efficient and easy to implement . our stochastic optimization methods yield o -lrb- 1 / \u221a t -rrb- convergence of the expected risk . moreover , the online proximal gradient descent type method yields o -lrb- log -lrb- t -rrb- / t -rrb- convergence for a strongly convex loss . numerical experiments show effectiveness of our stochastic optimization methods in learning tasks with structured sparsity such as overlapped group lasso ."
  },
  {
    "title": "Analysis of Privacy Loss in Distributed Constraint Optimization .",
    "entities": [
      "distributed constraint optimization",
      "distributed contraint reasoning design decisions",
      "agent privacy",
      "centralized approaches",
      "privacy-efficiency tradeoffs",
      "multiagent coordination",
      "constraint-graph topology",
      "dpop",
      "adopt"
    ],
    "types": "<method> <task> <task> <method> <otherscientificterm> <task> <method> <method> <method>",
    "relations": [
      "dpop -- CONJUNCTION -- adopt",
      "dpop -- HYPONYM-OF -- distributed constraint optimization",
      "agent privacy -- USED-FOR -- distributed constraint optimization",
      "distributed constraint optimization -- COMPARE -- centralized approaches",
      "distributed constraint optimization -- USED-FOR -- multiagent coordination",
      "adopt -- HYPONYM-OF -- distributed constraint optimization"
    ],
    "abstract": "distributed constraint optimization -lrb- dcop -rrb- is rapidly emerging as a prominent technique for <task_5> . however , despite <task_2> being a key motivation for applying <method_0> in many applications , rigorous quantitative evaluations of privacy loss in <method_0> have been lacking . recently , -lsb- maheswaran et al. 2005 -rsb- introduced a framework for quantitative evaluations of privacy in <method_0> , showing that some <method_0> lose more privacy than purely <method_3> and questioning the motivation for applying <method_0> . this paper addresses the question of whether state-of-the art <method_0> suffer from a similar shortcoming by investigating several of the most efficient <method_0> , including both <method_7> and <method_8> . furthermore , while previous work investigated the impact on efficiency of <task_1> -lrb- e.g. <method_6> , asynchrony , message-contents -rrb- , this paper examines the privacy aspect of such <task_1> , providing an improved understanding of <otherscientificterm_4> .",
    "abstract_og": "distributed constraint optimization -lrb- dcop -rrb- is rapidly emerging as a prominent technique for multiagent coordination . however , despite agent privacy being a key motivation for applying distributed constraint optimization in many applications , rigorous quantitative evaluations of privacy loss in distributed constraint optimization have been lacking . recently , -lsb- maheswaran et al. 2005 -rsb- introduced a framework for quantitative evaluations of privacy in distributed constraint optimization , showing that some distributed constraint optimization lose more privacy than purely centralized approaches and questioning the motivation for applying distributed constraint optimization . this paper addresses the question of whether state-of-the art distributed constraint optimization suffer from a similar shortcoming by investigating several of the most efficient distributed constraint optimization , including both dpop and adopt . furthermore , while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions -lrb- e.g. constraint-graph topology , asynchrony , message-contents -rrb- , this paper examines the privacy aspect of such distributed contraint reasoning design decisions , providing an improved understanding of privacy-efficiency tradeoffs ."
  },
  {
    "title": "Ensemble selection from libraries of models .",
    "entities": [
      "forward stepwise selection",
      "learning algorithms",
      "ensemble selection",
      "mean precision",
      "parameter settings",
      "cross entropy",
      "roc area",
      "ensemble selection",
      "accuracy"
    ],
    "types": "<method> <method> <task> <metric> <otherscientificterm> <otherscientificterm> <metric> <task> <metric>",
    "relations": [
      "cross entropy -- CONJUNCTION -- mean precision",
      "learning algorithms -- CONJUNCTION -- parameter settings",
      "mean precision -- CONJUNCTION -- roc area",
      "accuracy -- CONJUNCTION -- mean precision",
      "accuracy -- CONJUNCTION -- cross entropy"
    ],
    "abstract": "we present a method for constructing ensembles from libraries of thousands of models . model libraries are generated using different <method_1> and <otherscientificterm_4> . <method_0> is used to add to the ensemble the models that maximize its performance . <task_2> allows ensembles to be optimized to performance metric such as <metric_8> , <otherscientificterm_5> , <metric_3> , or <metric_6> . experiments with seven test problems and ten metrics demonstrate the benefit of <task_7> .",
    "abstract_og": "we present a method for constructing ensembles from libraries of thousands of models . model libraries are generated using different learning algorithms and parameter settings . forward stepwise selection is used to add to the ensemble the models that maximize its performance . ensemble selection allows ensembles to be optimized to performance metric such as accuracy , cross entropy , mean precision , or roc area . experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection ."
  },
  {
    "title": "A Siamese Long Short-Term Memory Architecture for Human Re-identification .",
    "entities": [
      "siamese long short-term memory architecture",
      "market-1501 , cuhk03 and viper datasets",
      "human re-identification",
      "local feature representation",
      "internal gating mechanism",
      "visual surveillance",
      "contextual information",
      "feedback connections",
      "feature extraction",
      "image regions",
      "spatial dependencies",
      "matching pedestrians",
      "lstm units"
    ],
    "types": "<method> <material> <task> <method> <method> <task> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "siamese long short-term memory architecture -- USED-FOR -- spatial dependencies",
      "siamese long short-term memory architecture -- USED-FOR -- image regions",
      "feedback connections -- CONJUNCTION -- siamese long short-term memory architecture",
      "feedback connections -- CONJUNCTION -- internal gating mechanism"
    ],
    "abstract": "matching pedestrians across multiple camera views known as <task_2> is a challenging problem in <task_5> . in the existing works concentrating on <task_8> , representations are formed locally and independent of other regions . we present a novel <method_0> that can process <otherscientificterm_9> sequentially and enhance the discriminative capability of <method_3> by leverag-ing <otherscientificterm_6> . the <material_7> and <method_4> of the <method_0> enable our <method_0> to memorize the <otherscientificterm_10> and selectively propagate relevant <otherscientificterm_6> through the network . we demonstrate improved performance compared to the baseline algorithm with no <otherscientificterm_12> and promising results compared to state-of-the-art methods on <material_1> . visualization of the internal mechanism of <method_0> shows meaningful patterns can be learned by our <method_0> .",
    "abstract_og": "matching pedestrians across multiple camera views known as human re-identification is a challenging problem in visual surveillance . in the existing works concentrating on feature extraction , representations are formed locally and independent of other regions . we present a novel siamese long short-term memory architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leverag-ing contextual information . the feedback connections and internal gating mechanism of the siamese long short-term memory architecture enable our siamese long short-term memory architecture to memorize the spatial dependencies and selectively propagate relevant contextual information through the network . we demonstrate improved performance compared to the baseline algorithm with no lstm units and promising results compared to state-of-the-art methods on market-1501 , cuhk03 and viper datasets . visualization of the internal mechanism of siamese long short-term memory architecture shows meaningful patterns can be learned by our siamese long short-term memory architecture ."
  },
  {
    "title": "Trace Quotient Problems Revisited .",
    "entities": [
      "optimal projection pursuing",
      "generalized eigenvalue decomposition approach",
      "computer vision problems",
      "final distance measurement",
      "trace difference problem",
      "quotient trace formulation",
      "trace quotient problem",
      "objective function value",
      "non-singular linear transformation",
      "quotient trace problem",
      "quotient trace",
      "orthogonal transformations",
      "face recognition",
      "classification capability",
      "trace quotient",
      "former formulation",
      "iterative procedure",
      "grassmann manifold"
    ],
    "types": "<method> <method> <task> <metric> <task> <method> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <metric> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "trace quotient problem -- CONJUNCTION -- trace difference problem",
      "face recognition -- EVALUATE-FOR -- optimal projection pursuing"
    ],
    "abstract": "the formulation of <otherscientificterm_14> is shared by many <task_2> ; however , it was conventionally approximated by an essentially different formulation of <otherscientificterm_10> , which can be solved with the <method_1> . in this paper , we present a direct solution to the <method_15> . first , considering that the feasible solutions are constrained on a <otherscientificterm_17> , we present a necessary condition for the optimal solution of the <task_6> , which then naturally elicits an <method_16> for pursuing the optimal solution . the proposed algorithm , referred to as <method_0> , has the following characteristics : 1 -rrb- <method_0> directly optimizes the <otherscientificterm_14> , and is theoretically optimal ; 2 -rrb- <method_0> does not suffer from the solution uncertainty issue existing in the <method_5> that the <otherscientificterm_7> is invariant under any <otherscientificterm_8> , and <method_0> is invariant only under <otherscientificterm_11> , which does not affect <metric_3> ; and 3 -rrb- <method_0> reveals the underlying equivalence between the <task_6> and the corresponding <task_4> . extensive experiments on <task_12> validate the superiority of <method_0> over the solution of the corresponding <task_9> in both <otherscientificterm_7> and <metric_13> .",
    "abstract_og": "the formulation of trace quotient is shared by many computer vision problems ; however , it was conventionally approximated by an essentially different formulation of quotient trace , which can be solved with the generalized eigenvalue decomposition approach . in this paper , we present a direct solution to the former formulation . first , considering that the feasible solutions are constrained on a grassmann manifold , we present a necessary condition for the optimal solution of the trace quotient problem , which then naturally elicits an iterative procedure for pursuing the optimal solution . the proposed algorithm , referred to as optimal projection pursuing , has the following characteristics : 1 -rrb- optimal projection pursuing directly optimizes the trace quotient , and is theoretically optimal ; 2 -rrb- optimal projection pursuing does not suffer from the solution uncertainty issue existing in the quotient trace formulation that the objective function value is invariant under any non-singular linear transformation , and optimal projection pursuing is invariant only under orthogonal transformations , which does not affect final distance measurement ; and 3 -rrb- optimal projection pursuing reveals the underlying equivalence between the trace quotient problem and the corresponding trace difference problem . extensive experiments on face recognition validate the superiority of optimal projection pursuing over the solution of the corresponding quotient trace problem in both objective function value and classification capability ."
  },
  {
    "title": "Towards optimal query design for relevance feedback in image retrieval .",
    "entities": [
      "greedy active learning based relevance feedback methods",
      "real image retrieval",
      "active learning approach",
      "image retrieval",
      "relevance feedbacks",
      "precession/recall rate"
    ],
    "types": "<method> <task> <method> <task> <otherscientificterm> <metric>",
    "relations": [
      "precession/recall rate -- EVALUATE-FOR -- active learning approach",
      "real image retrieval -- EVALUATE-FOR -- active learning approach",
      "relevance feedbacks -- EVALUATE-FOR -- active learning approach"
    ],
    "abstract": "we analyze the sub-optimality of traditional <method_0> in <task_3> , and propose a novel <method_2> to query labels of multiple images together , which minimize the needed round of feedbacks and achieve satisfactory result in a near optimal manner . our experiments on <task_1> demonstrate that our <method_2> can yield comparable <metric_5> by significantly less <otherscientificterm_4> .",
    "abstract_og": "we analyze the sub-optimality of traditional greedy active learning based relevance feedback methods in image retrieval , and propose a novel active learning approach to query labels of multiple images together , which minimize the needed round of feedbacks and achieve satisfactory result in a near optimal manner . our experiments on real image retrieval demonstrate that our active learning approach can yield comparable precession/recall rate by significantly less relevance feedbacks ."
  },
  {
    "title": "Fine-Grained Categorization by Alignments .",
    "entities": [
      "cu-2011 birds and stanford dogs fine-grained datasets",
      "matching oriented features",
      "fine-grained sub-categories",
      "human interaction",
      "fine-grained categorization",
      "fisher vectors",
      "localized information",
      "training images",
      "super-class shape",
      "classification-oriented encodings",
      "part annotations",
      "detectors",
      "hog"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "training images -- USED-FOR -- part annotations",
      "classification-oriented encodings -- USED-FOR -- localized information",
      "fisher vectors -- HYPONYM-OF -- classification-oriented encodings",
      "fisher vectors -- USED-FOR -- localized information",
      "fisher vectors -- HYPONYM-OF -- fine-grained sub-categories"
    ],
    "abstract": "the aim of this paper is <task_4> without <otherscientificterm_3> . different from prior work , which relies on <method_11> for specific object parts , we propose to localize distinctive details by roughly aligning the objects using just the overall shape , since implicit to <task_4> is the existence of a <otherscientificterm_8> shared among all classes . the alignments are then used to transfer <otherscientificterm_10> from <material_7> to test images -lrb- supervised alignment -rrb- , or to blindly yet consistently segment the object in a number of regions -lrb- unsupervised alignment -rrb- . we furthermore argue that in the distinction of <otherscientificterm_2> , <otherscientificterm_9> like <method_5> are better suited for describing <otherscientificterm_6> than popular <otherscientificterm_1> like <method_12> . we evaluate the method on the <material_0> , outperforming the state-of-the-art .",
    "abstract_og": "the aim of this paper is fine-grained categorization without human interaction . different from prior work , which relies on detectors for specific object parts , we propose to localize distinctive details by roughly aligning the objects using just the overall shape , since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes . the alignments are then used to transfer part annotations from training images to test images -lrb- supervised alignment -rrb- , or to blindly yet consistently segment the object in a number of regions -lrb- unsupervised alignment -rrb- . we furthermore argue that in the distinction of fine-grained sub-categories , classification-oriented encodings like fisher vectors are better suited for describing localized information than popular matching oriented features like hog . we evaluate the method on the cu-2011 birds and stanford dogs fine-grained datasets , outperforming the state-of-the-art ."
  },
  {
    "title": "Minimum Bayes-risk System Combination .",
    "entities": [
      "unified multi-system minimum bayes-risk technique",
      "minimum bayes-risk system combination",
      "minimum risk translation",
      "mbr decision rule",
      "system combination methods",
      "single-system-based mbr decoding",
      "consensus decoding",
      "linear combination",
      "probability distributions",
      "smt models",
      "mbr methods",
      "finite-length strings",
      "bleu score",
      "smt system",
      "system combination",
      "heterogeneous structure",
      "approximation",
      "bleu"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm> <method> <method> <otherscientificterm> <metric> <method> <method> <otherscientificterm> <method> <metric>",
    "relations": [
      "minimum bayes-risk system combination -- PART-OF -- smt models",
      "linear combination -- USED-FOR -- minimum bayes-risk system combination",
      "consensus decoding -- CONJUNCTION -- system combination",
      "system combination -- PART-OF -- unified multi-system minimum bayes-risk technique",
      "mbr methods -- USED-FOR -- smt system",
      "linear combination -- USED-FOR -- minimum risk translation"
    ],
    "abstract": "we present <method_1> , a method that integrates <method_6> and <method_14> into a <method_0> . unlike other <method_10> that re-rank translations of a single <method_13> , <method_1> uses the <otherscientificterm_3> and a <method_7> of the component systems ' <otherscientificterm_8> to search for the <otherscientificterm_2> among all the <otherscientificterm_11> over the output vocabulary . we introduce expected <metric_17> , an <method_16> to the <metric_12> that allows to efficiently apply <metric_17> in these conditions . <method_1> is a general method that is independent of specific <method_9> , enabling us to combine systems with <otherscientificterm_15> . experiments show that our approach bring significant improvements to <task_5> and achieves comparable results to different state-of-the-art <method_4> .",
    "abstract_og": "we present minimum bayes-risk system combination , a method that integrates consensus decoding and system combination into a unified multi-system minimum bayes-risk technique . unlike other mbr methods that re-rank translations of a single smt system , minimum bayes-risk system combination uses the mbr decision rule and a linear combination of the component systems ' probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary . we introduce expected bleu , an approximation to the bleu score that allows to efficiently apply bleu in these conditions . minimum bayes-risk system combination is a general method that is independent of specific smt models , enabling us to combine systems with heterogeneous structure . experiments show that our approach bring significant improvements to single-system-based mbr decoding and achieves comparable results to different state-of-the-art system combination methods ."
  },
  {
    "title": "Improved `` TEO '' feature-based automatic stress detection using physiological and acoustic speech sensors .",
    "entities": [
      "vibrations of the vocal tract",
      "stand-alone speech data collection devices",
      "relative average error rate reductions",
      "teo-cb-autoenv-based automatic stress recognition system",
      "acoustic and physiological microphone data",
      "alternative speech collection sensors",
      "weighted composite decision scheme",
      "acoustic and physiological sensors",
      "law enforcement training scenario",
      "automatic speech recognition systems",
      "stress level assessment",
      "speech production organs",
      "acoustic pressure microphone",
      "background noise",
      "gel-based device",
      "vocal folds",
      "feature relation",
      "physiological microphone",
      "speech data",
      "teo-cb-autoenv feature",
      "sensitivity"
    ],
    "types": "<otherscientificterm> <method> <metric> <method> <material> <method> <method> <method> <task> <task> <task> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <metric>",
    "relations": [
      "gel-based device -- USED-FOR -- vocal folds",
      "teo-cb-autoenv feature -- USED-FOR -- law enforcement training scenario",
      "gel-based device -- USED-FOR -- vibrations of the vocal tract",
      "acoustic and physiological sensors -- CONJUNCTION -- stand-alone speech data collection devices",
      "speech data -- USED-FOR -- automatic speech recognition systems",
      "alternative speech collection sensors -- USED-FOR -- automatic speech recognition systems",
      "acoustic and physiological microphone data -- USED-FOR -- weighted composite decision scheme",
      "feature relation -- USED-FOR -- stress level assessment",
      "acoustic pressure microphone -- USED-FOR -- speech data",
      "acoustic pressure microphone -- USED-FOR -- automatic speech recognition systems",
      "gel-based device -- USED-FOR -- background noise"
    ],
    "abstract": "the <method_12> has served as the primary instrument for collecting <material_18> for <task_9> . the <method_12> suffers from limitations , such as <metric_20> to <otherscientificterm_13> and relatively far proximity to <material_11> . <method_5> may serve to enhance the effectiveness of <task_9> . in this study , we first consider an experimental evaluation of the <otherscientificterm_19> in an actual <task_8> . we consider <otherscientificterm_16> to <task_10> over time . next , we explore the use of the <method_17> , a <method_14> placed next to the <otherscientificterm_15> on the outside of the throat used to measure <otherscientificterm_0> and minimize <otherscientificterm_13> , as we investigate the effectiveness of a <method_3> . we employ both <method_7> as <method_1> as well as consider both sensors concurrently . for the latter , we devise a <method_6> using both the <material_4> that yields <metric_2> of 32 % and 6 % versus sole employment of <material_4> , respectively , in a realistic stressful environment .",
    "abstract_og": "the acoustic pressure microphone has served as the primary instrument for collecting speech data for automatic speech recognition systems . the acoustic pressure microphone suffers from limitations , such as sensitivity to background noise and relatively far proximity to speech production organs . alternative speech collection sensors may serve to enhance the effectiveness of automatic speech recognition systems . in this study , we first consider an experimental evaluation of the teo-cb-autoenv feature in an actual law enforcement training scenario . we consider feature relation to stress level assessment over time . next , we explore the use of the physiological microphone , a gel-based device placed next to the vocal folds on the outside of the throat used to measure vibrations of the vocal tract and minimize background noise , as we investigate the effectiveness of a teo-cb-autoenv-based automatic stress recognition system . we employ both acoustic and physiological sensors as stand-alone speech data collection devices as well as consider both sensors concurrently . for the latter , we devise a weighted composite decision scheme using both the acoustic and physiological microphone data that yields relative average error rate reductions of 32 % and 6 % versus sole employment of acoustic and physiological microphone data , respectively , in a realistic stressful environment ."
  },
  {
    "title": "Balance between Complexity and Quality : Local Search for Minimum Vertex Cover in Massive Graphs .",
    "entities": [
      "state of the art local search algorithms",
      "minimum vertex cover",
      "real world massive graphs",
      "local search algorithms",
      "local search algorithm",
      "local search method",
      "high-complexity heuristics",
      "heuristic algorithms",
      "low-complexity heuristics",
      "np-hard problem",
      "graph"
    ],
    "types": "<method> <method> <material> <method> <method> <method> <otherscientificterm> <method> <method> <task> <otherscientificterm>",
    "relations": [
      "minimum vertex cover -- USED-FOR -- minimum vertex cover",
      "heuristic algorithms -- USED-FOR -- minimum vertex cover",
      "real world massive graphs -- EVALUATE-FOR -- minimum vertex cover",
      "local search algorithms -- USED-FOR -- minimum vertex cover",
      "local search method -- USED-FOR -- heuristic algorithms"
    ],
    "abstract": "the problem of finding a <method_1> in a <otherscientificterm_10> is a well known <task_9> with important applications . there has been much interest in developing <method_7> for finding a '' good '' vertex cover in graphs . in practice , most <method_7> for <method_1> are based on the <method_5> . previously , <method_3> for <method_1> have focused on solving academic benchmarks where the graphs are of relatively small size , and they are not suitable for solving massive graphs as they usually have <otherscientificterm_6> . in this paper , we propose a simple and fast <method_4> called <method_1> for solving <method_1> in massive graphs , which is based on two <method_8> . experimental results on a broad range of <material_2> show that <method_1> finds much better vertex covers -lrb- and thus also independent sets -rrb- than <method_0> for <method_1> .",
    "abstract_og": "the problem of finding a minimum vertex cover in a graph is a well known np-hard problem with important applications . there has been much interest in developing heuristic algorithms for finding a '' good '' vertex cover in graphs . in practice , most heuristic algorithms for minimum vertex cover are based on the local search method . previously , local search algorithms for minimum vertex cover have focused on solving academic benchmarks where the graphs are of relatively small size , and they are not suitable for solving massive graphs as they usually have high-complexity heuristics . in this paper , we propose a simple and fast local search algorithm called minimum vertex cover for solving minimum vertex cover in massive graphs , which is based on two low-complexity heuristics . experimental results on a broad range of real world massive graphs show that minimum vertex cover finds much better vertex covers -lrb- and thus also independent sets -rrb- than state of the art local search algorithms for minimum vertex cover ."
  },
  {
    "title": "A new pitch modeling approach for Mandarin speech .",
    "entities": [
      "mean and shape of syllable pitch contour",
      "closed and open tests",
      "model syllable pitch contour",
      "pitch mean model",
      "prosodic phrase boundaries",
      "pitch modeling approach",
      "mandarin speech",
      "em algorithm",
      "inter-syllable locations",
      "reconstructed pitch",
      "prosodic states",
      "linguistic knowledge",
      "rmses"
    ],
    "types": "<otherscientificterm> <material> <task> <method> <otherscientificterm> <method> <material> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "reconstructed pitch -- USED-FOR -- closed and open tests",
      "rmses -- USED-FOR -- closed and open tests",
      "model syllable pitch contour -- USED-FOR -- mandarin speech"
    ],
    "abstract": "in this paper , a new approach to <task_2> for <material_6> is proposed . it takes the <otherscientificterm_0> as two basic modeling units and considers several affecting factors that contribute to their variations . parameters of the two models are automatically estimated by the <method_7> . experimental results showed that <metric_12> of 0.551 ms and 0.614 ms in the <material_9> were obtained for the <material_1> , respectively . all inferred values of those affecting factors agreed well with our prior <otherscientificterm_11> . besides , the <otherscientificterm_10> automatically labeled by the <method_3> provided useful cues to determine the <otherscientificterm_4> occurred at <otherscientificterm_8> without punctuation marks . so it is a promising <method_5> .",
    "abstract_og": "in this paper , a new approach to model syllable pitch contour for mandarin speech is proposed . it takes the mean and shape of syllable pitch contour as two basic modeling units and considers several affecting factors that contribute to their variations . parameters of the two models are automatically estimated by the em algorithm . experimental results showed that rmses of 0.551 ms and 0.614 ms in the reconstructed pitch were obtained for the closed and open tests , respectively . all inferred values of those affecting factors agreed well with our prior linguistic knowledge . besides , the prosodic states automatically labeled by the pitch mean model provided useful cues to determine the prosodic phrase boundaries occurred at inter-syllable locations without punctuation marks . so it is a promising pitch modeling approach ."
  },
  {
    "title": "Durational characteristics of hindi stop consonants .",
    "entities": [
      "annotated and time-aligned hindi speech database",
      "durational characteristics of hindi stop consonants",
      "durations of closure",
      "spoken sentences",
      "post-release duration",
      "continuous speech",
      "closure duration",
      "gemination",
      "vowel",
      "aspiration"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "durations of closure -- CONJUNCTION -- gemination"
    ],
    "abstract": "a study of the <otherscientificterm_1> in <material_3> was carried out . an <material_0> was used in the experiment . the influences of <otherscientificterm_9> , voicing and <otherscientificterm_7> on the <otherscientificterm_2> and post-release segments of plosives as well as the duration of the preceding <otherscientificterm_8> were studied . it was observed that the <otherscientificterm_4> of a plosive changes systematically with manner of articulation . however , due to its large variation in <material_5> , the <otherscientificterm_4> alone is not sufficient to identify the manner of articulation of hindi stops as hypothesised in earlier studies . a low value of the ratio of the duration of a <otherscientificterm_8> to the <otherscientificterm_6> of the following plosive is a reliable indicator of <otherscientificterm_7> in hindi stop consonants in <material_5> .",
    "abstract_og": "a study of the durational characteristics of hindi stop consonants in spoken sentences was carried out . an annotated and time-aligned hindi speech database was used in the experiment . the influences of aspiration , voicing and gemination on the durations of closure and post-release segments of plosives as well as the duration of the preceding vowel were studied . it was observed that the post-release duration of a plosive changes systematically with manner of articulation . however , due to its large variation in continuous speech , the post-release duration alone is not sufficient to identify the manner of articulation of hindi stops as hypothesised in earlier studies . a low value of the ratio of the duration of a vowel to the closure duration of the following plosive is a reliable indicator of gemination in hindi stop consonants in continuous speech ."
  },
  {
    "title": "Nonlinear average consensus based on weight morphing .",
    "entities": [
      "unknown noise levels",
      "signal-adaptive morphing coefficients",
      "convex-optimization weights",
      "metropolis-hastings weights",
      "weight design",
      "average consensus",
      "abrupt changes",
      "dynamic scenarios"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "metropolis-hastings weights -- CONJUNCTION -- convex-optimization weights",
      "weight design -- USED-FOR -- average consensus",
      "abrupt changes -- CONJUNCTION -- unknown noise levels",
      "weight design -- USED-FOR -- dynamic scenarios",
      "metropolis-hastings weights -- CONJUNCTION -- signal-adaptive morphing coefficients"
    ],
    "abstract": "we present a novel <method_4> for <otherscientificterm_5> that improves its transient and stead-state performance . the idea is to blend <method_3> and <otherscientificterm_2> via <otherscientificterm_1> . the resulting <method_4> is shown to be particularly useful in <task_7> where the measurements feature <otherscientificterm_6> or <otherscientificterm_0> .",
    "abstract_og": "we present a novel weight design for average consensus that improves its transient and stead-state performance . the idea is to blend metropolis-hastings weights and convex-optimization weights via signal-adaptive morphing coefficients . the resulting weight design is shown to be particularly useful in dynamic scenarios where the measurements feature abrupt changes or unknown noise levels ."
  },
  {
    "title": "Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person .",
    "entities": [
      "sparse variation dictionary learning method",
      "large-scale cmu multi-pie , frgc and lfw databases",
      "face recognition",
      "sparse representation based classification",
      "sparse representation based classification",
      "sparse variation dictionary",
      "query sample representation",
      "query sample",
      "gallery set",
      "face images",
      "projection",
      "pose",
      "expression",
      "illumination",
      "occlusion"
    ],
    "types": "<method> <material> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "occlusion -- HYPONYM-OF -- face images",
      "sparse representation based classification -- USED-FOR -- sparse variation dictionary",
      "face recognition -- USED-FOR -- query sample representation",
      "illumination -- CONJUNCTION -- expression",
      "occlusion -- CONJUNCTION -- pose",
      "illumination -- HYPONYM-OF -- face images",
      "sparse variation dictionary learning method -- USED-FOR -- gallery set",
      "expression -- HYPONYM-OF -- face images",
      "sparse variation dictionary -- USED-FOR -- query sample representation",
      "pose -- HYPONYM-OF -- face images",
      "expression -- CONJUNCTION -- occlusion",
      "face recognition -- USED-FOR -- sparse variation dictionary"
    ],
    "abstract": "face recognition -lrb- <task_2> -rrb- with a single training sample per person -lrb- <task_2> -rrb- is a very challenging problem due to the lack of information to predict the variations in the <otherscientificterm_7> . <method_4> has shown interesting results in robust <task_2> ; however , its performance will deteriorate much for <task_2> with <task_2> . to address this issue , in this paper we learn a <otherscientificterm_5> from a generic training set to improve the <method_6> by <task_2> . instead of learning from the generic training set independently w.r.t. the <material_8> , the proposed <method_0> is adaptive to the <material_8> by jointly learning a <otherscientificterm_10> to connect the generic training set with the <material_8> . the learnt <otherscientificterm_5> can be easily integrated into the framework of <method_3> so that various variations in <otherscientificterm_9> , including <otherscientificterm_13> , <otherscientificterm_12> , <otherscientificterm_14> , <otherscientificterm_11> , etc. , can be better handled . experiments on the <material_1> demonstrate the promising performance of <task_2> on <task_2> with <task_2> .",
    "abstract_og": "face recognition -lrb- face recognition -rrb- with a single training sample per person -lrb- face recognition -rrb- is a very challenging problem due to the lack of information to predict the variations in the query sample . sparse representation based classification has shown interesting results in robust face recognition ; however , its performance will deteriorate much for face recognition with face recognition . to address this issue , in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by face recognition . instead of learning from the generic training set independently w.r.t. the gallery set , the proposed sparse variation dictionary learning method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set . the learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images , including illumination , expression , occlusion , pose , etc. , can be better handled . experiments on the large-scale cmu multi-pie , frgc and lfw databases demonstrate the promising performance of face recognition on face recognition with face recognition ."
  },
  {
    "title": "Articulatory features from deep neural networks and their role in speech recognition .",
    "entities": [
      "wall street journal corpus",
      "deep neural network",
      "corpus of synthetic english words",
      "continuous speech recognition task",
      "vocal tract variables",
      "deep neural networks",
      "automatic speech recognition",
      "articulatory trajectory estimation",
      "articulatory trajectories",
      "input speech",
      "articulatory features",
      "cepstral features",
      "task-dynamic model",
      "training data",
      "hidden variables",
      "speech recognition",
      "articulatory information",
      "word recognition",
      "speech signal",
      "index terms",
      "feature",
      "accuracy",
      "aurora-4"
    ],
    "types": "<material> <method> <material> <task> <otherscientificterm> <method> <task> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <task> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <metric> <material>",
    "relations": [
      "speech recognition -- EVALUATE-FOR -- word recognition",
      "vocal tract variables -- HYPONYM-OF -- index terms",
      "automatic speech recognition -- CONJUNCTION -- vocal tract variables",
      "deep neural network -- USED-FOR -- articulatory trajectories",
      "articulatory trajectories -- HYPONYM-OF -- index terms",
      "deep neural network -- USED-FOR -- speech recognition",
      "automatic speech recognition -- HYPONYM-OF -- index terms",
      "hidden variables -- USED-FOR -- deep neural network",
      "articulatory trajectories -- CONJUNCTION -- deep neural networks",
      "articulatory features -- USED-FOR -- speech recognition",
      "vocal tract variables -- CONJUNCTION -- deep neural networks",
      "articulatory trajectories -- CONJUNCTION -- vocal tract variables",
      "feature -- USED-FOR -- deep neural network",
      "articulatory features -- FEATURE-OF -- word recognition",
      "automatic speech recognition -- CONJUNCTION -- articulatory trajectories",
      "deep neural network -- USED-FOR -- articulatory information"
    ],
    "abstract": "this paper presents a <method_1> to extract <otherscientificterm_16> from the <material_18> and explores different ways to use such information in a <task_3> . the <method_1> was trained to estimate <otherscientificterm_8> from <material_9> , where the <material_13> is a <material_2> generated by the haskins laboratories ' <method_12> of speech production . speech parameterized as <otherscientificterm_11> were used to train the <method_1> , where we explored different <otherscientificterm_11> to observe their role in the <metric_21> of <task_7> . the best <otherscientificterm_20> was used to train the final <method_1> , where the <method_1> was used to predict <otherscientificterm_8> for the training and testing set of <material_22> , the noisy <material_0> . this study also explored the use of <otherscientificterm_14> in the <method_1> as a potential acoustic <otherscientificterm_20> candidate for <task_15> and the results were encouraging . <task_17> results from <material_22> indicate that the <otherscientificterm_10> from the <method_1> provide improvement in <task_15> performance when fused with other standard <otherscientificterm_11> ; however when tried by themselves , they failed to match the baseline performance . <otherscientificterm_19> -- <task_6> , <otherscientificterm_8> , <otherscientificterm_4> , <method_5> .",
    "abstract_og": "this paper presents a deep neural network to extract articulatory information from the speech signal and explores different ways to use such information in a continuous speech recognition task . the deep neural network was trained to estimate articulatory trajectories from input speech , where the training data is a corpus of synthetic english words generated by the haskins laboratories ' task-dynamic model of speech production . speech parameterized as cepstral features were used to train the deep neural network , where we explored different cepstral features to observe their role in the accuracy of articulatory trajectory estimation . the best feature was used to train the final deep neural network , where the deep neural network was used to predict articulatory trajectories for the training and testing set of aurora-4 , the noisy wall street journal corpus . this study also explored the use of hidden variables in the deep neural network as a potential acoustic feature candidate for speech recognition and the results were encouraging . word recognition results from aurora-4 indicate that the articulatory features from the deep neural network provide improvement in speech recognition performance when fused with other standard cepstral features ; however when tried by themselves , they failed to match the baseline performance . index terms -- automatic speech recognition , articulatory trajectories , vocal tract variables , deep neural networks ."
  },
  {
    "title": "Approximation of complex-valued 2-D frequency response specifications by separable-denominator digital filters .",
    "entities": [
      "real-valued 2-d iir sd digital lter",
      "real-valued 2-d fir digital lter",
      "fir-to-iir approximation problem",
      "approximation problem",
      "frequency response",
      "complex-valued spec-ication",
      "theorem"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "frequency response -- USED-FOR -- complex-valued spec-ication"
    ],
    "abstract": "in this paper the approximation of a <otherscientificterm_5> by the <method_4> of a 2-d iir separable denominator -lrb- sd -rrb- digital lter is considered . the <task_3> is transformed into an equivalent one , where a <otherscientificterm_0> with some additional characteristics has to be determined that approximates a given <otherscientificterm_1> . a <otherscientificterm_6> is presented that helps to reduce the number of parameters in the <task_2> and a procedure to solve the problem numerically is given .",
    "abstract_og": "in this paper the approximation of a complex-valued spec-ication by the frequency response of a 2-d iir separable denominator -lrb- sd -rrb- digital lter is considered . the approximation problem is transformed into an equivalent one , where a real-valued 2-d iir sd digital lter with some additional characteristics has to be determined that approximates a given real-valued 2-d fir digital lter . a theorem is presented that helps to reduce the number of parameters in the fir-to-iir approximation problem and a procedure to solve the problem numerically is given ."
  },
  {
    "title": "Optimum channel shortening for discrete multitone transceivers .",
    "entities": [
      "maximum shortening snr method",
      "optimum channel shortening method",
      "discrete multitone dmt transceivers",
      "input energy distribution",
      "bit rate",
      "dmt symbol",
      "isi paths",
      "snr",
      "noise",
      "subchannel"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "optimum channel shortening method -- COMPARE -- optimum channel shortening method",
      "noise -- CONJUNCTION -- isi paths",
      "optimum channel shortening method -- USED-FOR -- discrete multitone dmt transceivers"
    ],
    "abstract": "we propose an <method_1> for <method_2> . the proposed <method_1> shortens a given channel to a desired length while maximizing the number of bits transmitted on a <otherscientificterm_5> . the key to the optimum solution is the deenition of the <otherscientificterm_7> in a <otherscientificterm_9> using the equivalent signal , <otherscientificterm_8> , and <otherscientificterm_6> in the system . our simulation results show that the proposed <method_1> outperforms the best existing <method_1> with a 18 increase in the <metric_4> . we show that the <method_0> is a special case of the proposed <method_1> and both methods are nearly equivalent when the <otherscientificterm_3> is constant o v er all subchannels .",
    "abstract_og": "we propose an optimum channel shortening method for discrete multitone dmt transceivers . the proposed optimum channel shortening method shortens a given channel to a desired length while maximizing the number of bits transmitted on a dmt symbol . the key to the optimum solution is the deenition of the snr in a subchannel using the equivalent signal , noise , and isi paths in the system . our simulation results show that the proposed optimum channel shortening method outperforms the best existing optimum channel shortening method with a 18 increase in the bit rate . we show that the maximum shortening snr method is a special case of the proposed optimum channel shortening method and both methods are nearly equivalent when the input energy distribution is constant o v er all subchannels ."
  },
  {
    "title": "Jacobian adaptation based on the frequency-filtered spectral energies .",
    "entities": [
      "mel-frequency cepstral coefficients",
      "frequency-filtered spectral energies",
      "jacobian adaptation",
      "jacobian linear transformation",
      "vocal tract length",
      "robust speech recognition",
      "adaptation technique",
      "jacobian adaptation",
      "channel distortion",
      "degrading factors",
      "ja technique",
      "acoustic models",
      "ja",
      "features",
      "recognition"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <task> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task>",
    "relations": [
      "mel-frequency cepstral coefficients -- FEATURE-OF -- jacobian adaptation",
      "channel distortion -- HYPONYM-OF -- degrading factors",
      "recognition -- EVALUATE-FOR -- ja technique",
      "ja technique -- USED-FOR -- features",
      "vocal tract length -- HYPONYM-OF -- degrading factors",
      "acoustic models -- USED-FOR -- robust speech recognition",
      "adaptation technique -- USED-FOR -- robust speech recognition",
      "channel distortion -- CONJUNCTION -- vocal tract length"
    ],
    "abstract": "\u2020 <method_2> of the <method_11> is an efficient <method_6> for <task_5> . several improvements for the <method_12> have been proposed in the last years , either to generalize the <method_3> for the case of large noise mismatch between training and testing or to extend the adaptation to other <otherscientificterm_9> , like <otherscientificterm_8> and <otherscientificterm_4> . however , the <method_10> has only been used so far with the conventional <method_0> . in this paper , the <method_10> is applied to an alternative type of <otherscientificterm_13> , the <otherscientificterm_1> , resulting in a more computationally efficient <method_10> . furthermore , in experimental tests with the database aurora1 , this new <method_10> has shown an improved <task_14> performance with respect to the <task_7> with <method_0> .",
    "abstract_og": "\u2020 jacobian adaptation of the acoustic models is an efficient adaptation technique for robust speech recognition . several improvements for the ja have been proposed in the last years , either to generalize the jacobian linear transformation for the case of large noise mismatch between training and testing or to extend the adaptation to other degrading factors , like channel distortion and vocal tract length . however , the ja technique has only been used so far with the conventional mel-frequency cepstral coefficients . in this paper , the ja technique is applied to an alternative type of features , the frequency-filtered spectral energies , resulting in a more computationally efficient ja technique . furthermore , in experimental tests with the database aurora1 , this new ja technique has shown an improved recognition performance with respect to the jacobian adaptation with mel-frequency cepstral coefficients ."
  },
  {
    "title": "Branch-and-price global optimization for multi-view multi-target tracking .",
    "entities": [
      "3d human pose tracking",
      "configuration of the cameras",
      "temporal correlations between objects",
      "combinato-rial optimization problem",
      "reconstruction and tracking",
      "graph structure",
      "assignment problem",
      "min-cost problem",
      "multi-view images",
      "global optimization",
      "spatial correlations",
      "dantzig-wolfe decomposition",
      "branching"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <task> <task> <material> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "dantzig-wolfe decomposition -- CONJUNCTION -- branching",
      "min-cost problem -- USED-FOR -- assignment problem"
    ],
    "abstract": "we present a new algorithm to jointly track multiple objects in <material_8> . while this has been typically addressed separately in the past , we tackle the problem as a single <task_9> . we formulate this <task_6> as a <task_7> by defining a <otherscientificterm_5> that captures both <otherscientificterm_2> as well as <otherscientificterm_10> enforced by the <otherscientificterm_1> . this leads to a complex <task_3> that we solve using <method_11> and <otherscientificterm_12> . our formulation allows us to solve the problem of <task_4> in a single step by taking all available evidence into account . in several experiments on multiple people tracking and <task_0> , we show our method outperforms state-of-the-art approaches .",
    "abstract_og": "we present a new algorithm to jointly track multiple objects in multi-view images . while this has been typically addressed separately in the past , we tackle the problem as a single global optimization . we formulate this assignment problem as a min-cost problem by defining a graph structure that captures both temporal correlations between objects as well as spatial correlations enforced by the configuration of the cameras . this leads to a complex combinato-rial optimization problem that we solve using dantzig-wolfe decomposition and branching . our formulation allows us to solve the problem of reconstruction and tracking in a single step by taking all available evidence into account . in several experiments on multiple people tracking and 3d human pose tracking , we show our method outperforms state-of-the-art approaches ."
  },
  {
    "title": "Reducing multiclass to binary by coupling probability estimates .",
    "entities": [
      "pairwise coupling of probability estimates",
      "calibrated class membership probability estimates",
      "obtaining class membership probability estimates",
      "arbitrary code matrices",
      "multiclass classification problems",
      "probability estimates",
      "loss-based decoding",
      "classification accuracy",
      "binary classifiers"
    ],
    "types": "<task> <task> <task> <method> <task> <method> <method> <metric> <method>",
    "relations": [
      "binary classifiers -- USED-FOR -- probability estimates"
    ],
    "abstract": "this paper presents a method for <task_2> for <task_4> by coupling the <method_5> produced by <method_8> . this is an extension for <method_3> of a method due to hastie and tibshirani for <task_0> . experimental results with boosted naive bayes show that our method produces <task_1> , while having similar <metric_7> as <method_6> , a method for obtaining the most likely class that does not generate <method_5> .",
    "abstract_og": "this paper presents a method for obtaining class membership probability estimates for multiclass classification problems by coupling the probability estimates produced by binary classifiers . this is an extension for arbitrary code matrices of a method due to hastie and tibshirani for pairwise coupling of probability estimates . experimental results with boosted naive bayes show that our method produces calibrated class membership probability estimates , while having similar classification accuracy as loss-based decoding , a method for obtaining the most likely class that does not generate probability estimates ."
  },
  {
    "title": "Continuous listening for unconstrained spoken dialog .",
    "entities": [
      "domain-independent , multi-modal computational architecture",
      "runtime behavior of presenter",
      "powerpoint slide shows",
      "spoken dialog systems",
      "spoken requests",
      "prototype system",
      "bayesian models",
      "recorded lecture",
      "continuous listening",
      "push-to-talk device",
      "spoken dialog",
      "decision-theoretic approach",
      "presenter"
    ],
    "types": "<method> <method> <material> <method> <material> <method> <method> <material> <method> <method> <material> <method> <method>",
    "relations": [
      "presenter -- HYPONYM-OF -- prototype system",
      "decision-theoretic approach -- USED-FOR -- spoken dialog systems"
    ],
    "abstract": "a major hindrance to rendering <method_3> capable of ongoing , <method_8> without requiring a <method_9> is the <method_3> of distinguishing speech which is intended for the system from that which is overheard . we present a <method_11> to this <method_3> that exploits <method_6> of <material_10> at four levels of analysis within a <method_0> called quartet . we applied quartet to the task of navigating <material_2> during a spoken presentation in a <method_5> called <method_12> . we describe the <method_1> as well as the results of an experimental study comparing the performance of <method_12> to human subjects in discriminating arbitrarily formed <material_4> for <material_2> during a <material_7> .",
    "abstract_og": "a major hindrance to rendering spoken dialog systems capable of ongoing , continuous listening without requiring a push-to-talk device is the spoken dialog systems of distinguishing speech which is intended for the system from that which is overheard . we present a decision-theoretic approach to this spoken dialog systems that exploits bayesian models of spoken dialog at four levels of analysis within a domain-independent , multi-modal computational architecture called quartet . we applied quartet to the task of navigating powerpoint slide shows during a spoken presentation in a prototype system called presenter . we describe the runtime behavior of presenter as well as the results of an experimental study comparing the performance of presenter to human subjects in discriminating arbitrarily formed spoken requests for powerpoint slide shows during a recorded lecture ."
  },
  {
    "title": "Low-complexity sinusoidal component selection using loudness patterns .",
    "entities": [
      "perceptual sinusoidal component selection strategies",
      "low perceptual sinusoidal synthesis error",
      "hybrid loudness estimation scheme",
      "quantitative or perceptual criterion",
      "sinusoidal modeling of audio",
      "sinusoidal selection algorithm",
      "multi-tone signal",
      "loudness patterns",
      "real-time applications",
      "computational complexity"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <task> <metric>",
    "relations": [
      "hybrid loudness estimation scheme -- USED-FOR -- multi-tone signal",
      "hybrid loudness estimation scheme -- USED-FOR -- sinusoidal selection algorithm",
      "perceptual sinusoidal component selection strategies -- USED-FOR -- real-time applications"
    ],
    "abstract": "sinusoidal modeling of audio at low-bit rates involves selecting a limited number of parameters according to a <otherscientificterm_3> . most <method_0> are computationally intensive and not suitable for <task_8> . in this paper , a computationally efficient <method_5> based on a novel <method_2> is presented . the <method_2> first estimates efficiently the loudness of a <otherscientificterm_6> from the <otherscientificterm_7> of its constituent sinusoidal components . then it refines this estimate by performing a full evaluation of loudness but only in select critical bands . experimental results show that the proposed technique maintains a <otherscientificterm_1> at a much lower <metric_9> .",
    "abstract_og": "sinusoidal modeling of audio at low-bit rates involves selecting a limited number of parameters according to a quantitative or perceptual criterion . most perceptual sinusoidal component selection strategies are computationally intensive and not suitable for real-time applications . in this paper , a computationally efficient sinusoidal selection algorithm based on a novel hybrid loudness estimation scheme is presented . the hybrid loudness estimation scheme first estimates efficiently the loudness of a multi-tone signal from the loudness patterns of its constituent sinusoidal components . then it refines this estimate by performing a full evaluation of loudness but only in select critical bands . experimental results show that the proposed technique maintains a low perceptual sinusoidal synthesis error at a much lower computational complexity ."
  },
  {
    "title": "When Does Schwartz Conjecture Hold ? .",
    "entities": [
      "schwartz 's conjecture",
      "polynomial time",
      "\u03c4"
    ],
    "types": "<method> <otherscientificterm> <method>",
    "relations": [
      "polynomial time -- FEATURE-OF -- \u03c4"
    ],
    "abstract": "in 1990 , thomas schwartz proposed the conjecture that every nonempty tournament has a unique minimal \u03c4-retentive set -lrb- <method_2> stands for tournament equilibrium set -rrb- . a weak variant of <method_0> was recently proposed by felix brandt . however , both conjectures were disproved very recently by two counterexamples . in this paper , we prove sufficient conditions for infinite classes of tournaments that satisfy <method_0> and brandt 's conjecture . moreover , we prove that <method_2> can be calculated in <otherscientificterm_1> in several infinite classes of tournaments . furthermore , our results reveal some structures that are forbidden in every counterexample to <method_0> .",
    "abstract_og": "in 1990 , thomas schwartz proposed the conjecture that every nonempty tournament has a unique minimal \u03c4-retentive set -lrb- \u03c4 stands for tournament equilibrium set -rrb- . a weak variant of schwartz 's conjecture was recently proposed by felix brandt . however , both conjectures were disproved very recently by two counterexamples . in this paper , we prove sufficient conditions for infinite classes of tournaments that satisfy schwartz 's conjecture and brandt 's conjecture . moreover , we prove that \u03c4 can be calculated in polynomial time in several infinite classes of tournaments . furthermore , our results reveal some structures that are forbidden in every counterexample to schwartz 's conjecture ."
  },
  {
    "title": "Monocular 3D Object Detection for Autonomous Driving .",
    "entities": [
      "candidate class-specific object proposals",
      "object proposal generation approach",
      "3d object detection",
      "high-quality object detections",
      "published monocular competitors",
      "cnn pipeline",
      "contextual information",
      "autonomous driving",
      "semantic seg-mentation",
      "monocular approaches",
      "proposal generation",
      "object shape",
      "kitti benchmark",
      "image plane",
      "monocular image",
      "ground-plane",
      "detection"
    ],
    "types": "<otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <method> <task> <otherscientificterm> <metric> <otherscientificterm> <material> <otherscientificterm> <task>",
    "relations": [
      "cnn pipeline -- USED-FOR -- high-quality object detections",
      "object proposal generation approach -- COMPARE -- monocular approaches",
      "cnn pipeline -- USED-FOR -- candidate class-specific object proposals",
      "monocular image -- USED-FOR -- 3d object detection",
      "autonomous driving -- USED-FOR -- 3d object detection",
      "semantic seg-mentation -- CONJUNCTION -- contextual information",
      "object proposal generation approach -- USED-FOR -- detection"
    ],
    "abstract": "the goal of this paper is to perform <task_2> from a single <material_14> in the domain of <task_7> . our method first aims to generate a set of <otherscientificterm_0> , which are then run through a standard <method_5> to obtain <otherscientificterm_3> . the focus of this paper is on <task_10> . in particular , we propose an <method_1> that places object candidates in 3d using the fact that objects should be on the <otherscientificterm_15> . we then score each candidate box projected to the <otherscientificterm_13> via several intuitive potentials encoding <otherscientificterm_8> , <otherscientificterm_6> , size and location priors and typical <otherscientificterm_11> . our experimental evaluation demonstrates that our <method_1> significantly outperforms all <method_9> , and achieves the best <task_16> performance on the challenging <metric_12> , among <otherscientificterm_4> .",
    "abstract_og": "the goal of this paper is to perform 3d object detection from a single monocular image in the domain of autonomous driving . our method first aims to generate a set of candidate class-specific object proposals , which are then run through a standard cnn pipeline to obtain high-quality object detections . the focus of this paper is on proposal generation . in particular , we propose an object proposal generation approach that places object candidates in 3d using the fact that objects should be on the ground-plane . we then score each candidate box projected to the image plane via several intuitive potentials encoding semantic seg-mentation , contextual information , size and location priors and typical object shape . our experimental evaluation demonstrates that our object proposal generation approach significantly outperforms all monocular approaches , and achieves the best detection performance on the challenging kitti benchmark , among published monocular competitors ."
  },
  {
    "title": "Pipeline Iteration .",
    "entities": [
      "union of un-constrained parses",
      "finite-state shallow parser",
      "pcfg parsing pipeline",
      "pipeline iteration",
      "base-phrase constraints",
      "parsing pipeline",
      "heavily-constrained parses"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "base-phrase constraints -- USED-FOR -- pcfg parsing pipeline"
    ],
    "abstract": "this paper presents <task_3> , an approach that uses output from later stages of a <task_3> to constrain earlier stages of the same <task_3> . we demonstrate significant improvements in a state-of-the-art <method_2> using <otherscientificterm_4> , derived either from later stages of the <method_5> or from a <method_1> . the best performance is achieved by reranking the <method_0> and relatively <otherscientificterm_6> .",
    "abstract_og": "this paper presents pipeline iteration , an approach that uses output from later stages of a pipeline iteration to constrain earlier stages of the same pipeline iteration . we demonstrate significant improvements in a state-of-the-art pcfg parsing pipeline using base-phrase constraints , derived either from later stages of the parsing pipeline or from a finite-state shallow parser . the best performance is achieved by reranking the union of un-constrained parses and relatively heavily-constrained parses ."
  },
  {
    "title": "An efficient top-down parsing algorithm for understanding speech by using stochastic syntactic and semantic models .",
    "entities": [
      "syntactic and semantic knowledge",
      "speech understanding system",
      "pronunciation layer",
      "consistency problems",
      "high efficiency",
      "top-down strategy",
      "parse tree",
      "semantic content",
      "acoustic-phonetic knowledge",
      "chart-parsing technique",
      "understanding speech",
      "incremental algorithm",
      "integrated search",
      "probabilistic models",
      "word hypotheses",
      "structure-sharing",
      "grammar",
      "recognizer"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <method> <otherscientificterm> <material> <otherscientificterm> <method> <task> <method> <method> <method> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "structure-sharing -- FEATURE-OF -- chart-parsing technique",
      "integrated search -- USED-FOR -- semantic content"
    ],
    "abstract": "the paper is concerning an approach for <task_10> using a new form of <method_13> to represent <otherscientificterm_0> of a restricted domain . one important feature of our <method_16> is that the <otherscientificterm_6> directly represents the <material_7> of the utterance . since we determine that <material_7> by an <method_12> , we avoid <otherscientificterm_3> at the interface between the <method_17> and the language understanding part of the <method_1> . we succeeded in designing such an <method_11> , which integrates semantic , syntactic , and <otherscientificterm_8> in a seamless , consistent way . <metric_4> is achieved by using a <method_9> with <method_15> and a strict <method_5> for opening new <otherscientificterm_14> in the <otherscientificterm_2> .",
    "abstract_og": "the paper is concerning an approach for understanding speech using a new form of probabilistic models to represent syntactic and semantic knowledge of a restricted domain . one important feature of our grammar is that the parse tree directly represents the semantic content of the utterance . since we determine that semantic content by an integrated search , we avoid consistency problems at the interface between the recognizer and the language understanding part of the speech understanding system . we succeeded in designing such an incremental algorithm , which integrates semantic , syntactic , and acoustic-phonetic knowledge in a seamless , consistent way . high efficiency is achieved by using a chart-parsing technique with structure-sharing and a strict top-down strategy for opening new word hypotheses in the pronunciation layer ."
  },
  {
    "title": "On Region Merging : The Statistical Soundness of Fast Sorting , with Applications .",
    "entities": [
      "occlu-sions and/or hard noise levels",
      "grey-level and color images",
      "fast segmentation algorithm",
      "numerical feature spaces",
      "image segmentation",
      "computer vision",
      "processing images",
      "c-code"
    ],
    "types": "<otherscientificterm> <material> <method> <otherscientificterm> <task> <task> <task> <method>",
    "relations": [
      "numerical feature spaces -- USED-FOR -- fast segmentation algorithm",
      "fast segmentation algorithm -- USED-FOR -- processing images"
    ],
    "abstract": "this work explores a statistical basis for a process often described in <task_5> : <task_4> by region merging following a particular order in the choice of regions . we exhibit a particular blend of algorithmics and statistics whose error is , as we formally show , close to the best possible . this approach can be approximated in a very <method_2> for <task_6> described using most common <otherscientificterm_3> . simple modifications of the algorithm allow to cope with <otherscientificterm_0> . experiments on <material_1> , obtained with a short <method_7> , display the quality of the segmentations obtained .",
    "abstract_og": "this work explores a statistical basis for a process often described in computer vision : image segmentation by region merging following a particular order in the choice of regions . we exhibit a particular blend of algorithmics and statistics whose error is , as we formally show , close to the best possible . this approach can be approximated in a very fast segmentation algorithm for processing images described using most common numerical feature spaces . simple modifications of the algorithm allow to cope with occlu-sions and/or hard noise levels . experiments on grey-level and color images , obtained with a short c-code , display the quality of the segmentations obtained ."
  },
  {
    "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition .",
    "entities": [
      "light-field dataset of materials",
      "view-dependent reflectance effects",
      "2d image classification",
      "multiple sub-aperture views",
      "light-field images",
      "image segmentation",
      "recognition networks",
      "material recognition",
      "deep learning",
      "4d images",
      "object detection",
      "4d light-field",
      "mid-size dataset",
      "lytro illum",
      "cnn architectures",
      "view interpolation",
      "light-field applications",
      "light-fields"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <material> <task> <method> <task> <method> <material> <task> <otherscientificterm> <material> <method> <method> <task> <task> <otherscientificterm>",
    "relations": [
      "image segmentation -- HYPONYM-OF -- light-fields",
      "material recognition -- USED-FOR -- 4d light-field",
      "object detection -- HYPONYM-OF -- light-fields",
      "view interpolation -- HYPONYM-OF -- light-fields",
      "image segmentation -- CONJUNCTION -- view interpolation",
      "object detection -- CONJUNCTION -- image segmentation",
      "multiple sub-aperture views -- CONJUNCTION -- view-dependent reflectance effects",
      "mid-size dataset -- USED-FOR -- light-field images",
      "cnn architectures -- USED-FOR -- light-field applications",
      "deep learning -- USED-FOR -- material recognition"
    ],
    "abstract": "we introduce a new <method_0> , and take advantage of the recent success of <method_8> to perform <task_7> on the <otherscientificterm_11> . our <method_0> contains 12 material categories , each with 100 images taken with a <method_13> , from which we extract about 30,000 patches in total . to the best of our knowledge , <method_0> is the first <material_12> for <material_4> . our main goal is to investigate whether the additional information in a light-field -lrb- such as <otherscientificterm_3> and <otherscientificterm_1> -rrb- can aid <task_7> . since <method_6> have not been trained on <material_9> before , we propose and compare several novel <method_14> to train on <material_4> . in our experiments , the best performing <method_14> achieves a 7 % boost compared with <method_2> -lrb- 70 % \u2192 77 % -rrb- . these results constitute important baselines that can spur further research in the use of <method_14> for <task_16> . upon publication , our <method_0> also enables other novel applications of <otherscientificterm_17> , including <task_10> , <task_5> and <task_15> .",
    "abstract_og": "we introduce a new light-field dataset of materials , and take advantage of the recent success of deep learning to perform material recognition on the 4d light-field . our light-field dataset of materials contains 12 material categories , each with 100 images taken with a lytro illum , from which we extract about 30,000 patches in total . to the best of our knowledge , light-field dataset of materials is the first mid-size dataset for light-field images . our main goal is to investigate whether the additional information in a light-field -lrb- such as multiple sub-aperture views and view-dependent reflectance effects -rrb- can aid material recognition . since recognition networks have not been trained on 4d images before , we propose and compare several novel cnn architectures to train on light-field images . in our experiments , the best performing cnn architectures achieves a 7 % boost compared with 2d image classification -lrb- 70 % \u2192 77 % -rrb- . these results constitute important baselines that can spur further research in the use of cnn architectures for light-field applications . upon publication , our light-field dataset of materials also enables other novel applications of light-fields , including object detection , image segmentation and view interpolation ."
  },
  {
    "title": "Address Block Location with a Neural Net System .",
    "entities": [
      "sp arc2 processor board",
      "net32k neural net chips",
      "digital signal processors",
      "modular hardware platform",
      "mail pieces",
      "address blocks",
      "cleaner images",
      "skew angle",
      "noisy images",
      "layout analysis",
      "writing style",
      "board",
      "images"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <material> <otherscientificterm> <material> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "net32k neural net chips -- PART-OF -- board",
      "digital signal processors -- PART-OF -- board",
      "sp arc2 processor board -- CONJUNCTION -- board",
      "sp arc2 processor board -- PART-OF -- board",
      "net32k neural net chips -- CONJUNCTION -- sp arc2 processor board"
    ],
    "abstract": "eric cosatto we developed a system for finding <otherscientificterm_5> on <material_4> that can process four <material_12> per second . besides locating the <otherscientificterm_5> , our system also determines the <otherscientificterm_10> , handwritten or machine printed , and moreover , it measures the <otherscientificterm_7> of the text lines and cleans <material_8> . a <method_9> of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address . a speed of more than four <material_12> per second is obtained on a <method_3> , containing a <otherscientificterm_11> with two of the <method_1> , a <otherscientificterm_0> , and a <otherscientificterm_11> with 2 <method_2> . the system has been tested with more than 100,000 <material_12> . its performance depends on the quality of the <material_12> , and lies between 85 % correct location in very <material_8> to over 98 % in <material_6> .",
    "abstract_og": "eric cosatto we developed a system for finding address blocks on mail pieces that can process four images per second . besides locating the address blocks , our system also determines the writing style , handwritten or machine printed , and moreover , it measures the skew angle of the text lines and cleans noisy images . a layout analysis of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address . a speed of more than four images per second is obtained on a modular hardware platform , containing a board with two of the net32k neural net chips , a sp arc2 processor board , and a board with 2 digital signal processors . the system has been tested with more than 100,000 images . its performance depends on the quality of the images , and lies between 85 % correct location in very noisy images to over 98 % in cleaner images ."
  },
  {
    "title": "Learning for Deep Language Understanding .",
    "entities": [
      "tractable learning algorithms",
      "algorithm complexity",
      "syntactic-semantic grammar",
      "hypothesis space",
      "a-priori knowledge",
      "grammars"
    ],
    "types": "<method> <metric> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "a-priori knowledge -- CONJUNCTION -- hypothesis space",
      "hypothesis space -- CONJUNCTION -- algorithm complexity"
    ],
    "abstract": "the paper addresses the problem of learning to parse sentences to logical representations of their underlying meaning , by inducing a <method_2> . the approach uses a class of <method_5> which has been proven to be learnable from representative examples . in this paper , we introduce <method_0> for learning this class of <method_5> , comparing them in terms of <otherscientificterm_4> needed by the learner , <otherscientificterm_3> and <metric_1> . we present experimental results on learning tense , aspect , modality and negation of verbal constructions .",
    "abstract_og": "the paper addresses the problem of learning to parse sentences to logical representations of their underlying meaning , by inducing a syntactic-semantic grammar . the approach uses a class of grammars which has been proven to be learnable from representative examples . in this paper , we introduce tractable learning algorithms for learning this class of grammars , comparing them in terms of a-priori knowledge needed by the learner , hypothesis space and algorithm complexity . we present experimental results on learning tense , aspect , modality and negation of verbal constructions ."
  },
  {
    "title": "Gaussian Mixture Gain Priors for Regularized Nonnegative Matrix Factorization in Single-Channel Source Separation .",
    "entities": [
      "single-channel source separation applications",
      "nonnegative matrix factorization",
      "log-normalized gain prior model",
      "weight combination patterns",
      "trained basis vectors",
      "nmf decomposition weights",
      "weighted linear combination",
      "nmf based scss",
      "statistical prior information",
      "nmf reconstruction error",
      "statistical priors",
      "nmf solution",
      "nmf solutions",
      "log-likelihood",
      "spectra"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "nmf solutions -- USED-FOR -- log-likelihood",
      "nonnegative matrix factorization -- USED-FOR -- single-channel source separation applications",
      "log-normalized gain prior model -- USED-FOR -- nmf solution"
    ],
    "abstract": "we propose a new method to incorporate <method_10> on the solution of the <method_1> for <task_0> . the gaussian mixture model -lrb- gmm -rrb- is used as a <method_2> for the <method_11> . the normalization makes the prior <method_2> energy independent . in <method_7> , nmf is used to decompose the <otherscientificterm_14> of the observed mixed signal as a <method_6> of a set of <otherscientificterm_4> . in this work , the <otherscientificterm_5> are enforced to consider <otherscientificterm_8> on the <otherscientificterm_3> that the <otherscientificterm_4> can jointly receive for each source in the observed mixed signal . the <method_12> for the weights are encouraged to increase the <otherscientificterm_13> with the trained gain prior gmms while reducing the <otherscientificterm_9> at the same time .",
    "abstract_og": "we propose a new method to incorporate statistical priors on the solution of the nonnegative matrix factorization for single-channel source separation applications . the gaussian mixture model -lrb- gmm -rrb- is used as a log-normalized gain prior model for the nmf solution . the normalization makes the prior log-normalized gain prior model energy independent . in nmf based scss , nmf is used to decompose the spectra of the observed mixed signal as a weighted linear combination of a set of trained basis vectors . in this work , the nmf decomposition weights are enforced to consider statistical prior information on the weight combination patterns that the trained basis vectors can jointly receive for each source in the observed mixed signal . the nmf solutions for the weights are encouraged to increase the log-likelihood with the trained gain prior gmms while reducing the nmf reconstruction error at the same time ."
  },
  {
    "title": "Stochastic Inference for Scalable Probabilistic Modeling of Binary Matrices .",
    "entities": [
      "proba-bilistic matrix factorization methods",
      "fully observed binary matrices",
      "data subsampling strategies",
      "stochastic inference algorithm",
      "batch algorithms",
      "convergence rates",
      "data matrix",
      "parameter updates",
      "uniform subsampling"
    ],
    "types": "<method> <material> <method> <method> <method> <metric> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "batch algorithms -- USED-FOR -- proba-bilistic matrix factorization methods",
      "convergence rates -- EVALUATE-FOR -- batch algorithms",
      "data subsampling strategies -- COMPARE -- uniform subsampling",
      "stochastic inference algorithm -- COMPARE -- batch algorithms",
      "data subsampling strategies -- PART-OF -- stochastic inference algorithm",
      "convergence rates -- EVALUATE-FOR -- stochastic inference algorithm",
      "stochastic inference algorithm -- USED-FOR -- proba-bilistic matrix factorization methods"
    ],
    "abstract": "fully observed large binary matrices appear in a wide variety of contexts . to model them , <method_0> are an attractive solution . however , current <method_4> for <method_0> can be inefficient because <method_4> need to analyze the entire <otherscientificterm_6> before producing any <otherscientificterm_7> . we derive an efficient <method_3> for <method_0> of <material_1> . our <method_3> exhibits faster <metric_5> than more expensive <method_4> and has better predictive performance than scalable alternatives . the proposed <method_3> includes new <method_2> which produce large gains over standard <method_8> . we also address the task of automatically selecting the size of the minibatches of data used by our <method_3> . for this , we derive an <method_3> that adjusts this hyper-parameter online .",
    "abstract_og": "fully observed large binary matrices appear in a wide variety of contexts . to model them , proba-bilistic matrix factorization methods are an attractive solution . however , current batch algorithms for proba-bilistic matrix factorization methods can be inefficient because batch algorithms need to analyze the entire data matrix before producing any parameter updates . we derive an efficient stochastic inference algorithm for proba-bilistic matrix factorization methods of fully observed binary matrices . our stochastic inference algorithm exhibits faster convergence rates than more expensive batch algorithms and has better predictive performance than scalable alternatives . the proposed stochastic inference algorithm includes new data subsampling strategies which produce large gains over standard uniform subsampling . we also address the task of automatically selecting the size of the minibatches of data used by our stochastic inference algorithm . for this , we derive an stochastic inference algorithm that adjusts this hyper-parameter online ."
  },
  {
    "title": "Efficient vector quantisation of line spectral frequencies using the switched split vector quantiser .",
    "entities": [
      "coding linear predictive coding parameters",
      "switched split vector quantiser",
      "computational complexity",
      "bit-rate and distortion",
      "line spectral frequencies",
      "split vector quantiser",
      "split vq",
      "spectral distortion",
      "timit database",
      "memory requirements",
      "lpc parameters"
    ],
    "types": "<task> <method> <metric> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "switched split vector quantiser -- COMPARE -- split vq",
      "switched split vector quantiser -- USED-FOR -- coding linear predictive coding parameters",
      "switched split vector quantiser -- USED-FOR -- lpc parameters",
      "computational complexity -- EVALUATE-FOR -- switched split vector quantiser"
    ],
    "abstract": "in this paper , we investigate the use of a <method_1> for <task_0> . the <method_1> is applied to quantise the <otherscientificterm_10> in terms of <otherscientificterm_4> from the <material_8> and its performance is compared with the <method_5> . experimental results show that the <method_1> provides a better trade-off between <otherscientificterm_3> performance than the <method_6> . in addition , the <method_1> has a lower <metric_2> than the <method_6> , though this is attained at the expense of an increase in <otherscientificterm_9> . in order to achieve a <otherscientificterm_7> of 1 db , the <method_1> with an 8 directional switch requires 23 bits/frame , 4.41 kflops/frame of computations and 8272 floats of memory , while the corresponding values for a traditional three-part <method_6> are 25 bits/frame , 13.3 kflops/frame and 3328 floats , respectively .",
    "abstract_og": "in this paper , we investigate the use of a switched split vector quantiser for coding linear predictive coding parameters . the switched split vector quantiser is applied to quantise the lpc parameters in terms of line spectral frequencies from the timit database and its performance is compared with the split vector quantiser . experimental results show that the switched split vector quantiser provides a better trade-off between bit-rate and distortion performance than the split vq . in addition , the switched split vector quantiser has a lower computational complexity than the split vq , though this is attained at the expense of an increase in memory requirements . in order to achieve a spectral distortion of 1 db , the switched split vector quantiser with an 8 directional switch requires 23 bits/frame , 4.41 kflops/frame of computations and 8272 floats of memory , while the corresponding values for a traditional three-part split vq are 25 bits/frame , 13.3 kflops/frame and 3328 floats , respectively ."
  },
  {
    "title": "A Linguistic Feature Vector for the Visual Interpretation of Sign Language .",
    "entities": [
      "temporal transitions of individual signs",
      "classifier bank of markov chains",
      "2 stage classification procedure",
      "sign language recognition",
      "independent component analysis",
      "minimal training data",
      "classification rates",
      "sign linguistics",
      "temporal activities",
      "classification stage",
      "conceptual level",
      "classification"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <task> <method> <material> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "classification -- USED-FOR -- temporal transitions of individual signs",
      "classifier bank of markov chains -- USED-FOR -- temporal transitions of individual signs"
    ],
    "abstract": "this paper presents a novel approach to <task_3> that provides extremely high <metric_6> on <material_5> . key to this approach is a <method_2> where an initial <otherscientificterm_9> extracts a high level description of hand shape and motion . this high level description is based upon <otherscientificterm_7> and describes actions at a <otherscientificterm_10> easily understood by humans . moreover , such a description broadly generalises <otherscientificterm_8> naturally overcoming variability of people and environments . a second stage of <task_11> is then used to model the <otherscientificterm_0> using a <otherscientificterm_1> combined with <method_4> . we demonstrate <metric_6> as high as 97.67 % for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required .",
    "abstract_og": "this paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data . key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion . this high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans . moreover , such a description broadly generalises temporal activities naturally overcoming variability of people and environments . a second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of markov chains combined with independent component analysis . we demonstrate classification rates as high as 97.67 % for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required ."
  },
  {
    "title": "Graph Traversal Methods for Reasoning in Large Knowledge-Based Systems .",
    "entities": [
      "cyc 's predicate-type hierarchy",
      "q/a and script construction",
      "heuristic graph traversal methods",
      "connection graph-based techniques",
      "plausible inference chains",
      "commonsense reasoning",
      "cognitive systems",
      "script-like structures",
      "accuracy"
    ],
    "types": "<method> <task> <method> <method> <otherscientificterm> <method> <method> <otherscientificterm> <metric>",
    "relations": [
      "connection graph-based techniques -- USED-FOR -- script-like structures",
      "heuristic graph traversal methods -- USED-FOR -- plausible inference chains",
      "accuracy -- EVALUATE-FOR -- heuristic graph traversal methods"
    ],
    "abstract": "commonsense reasoning at scale is a core problem for <method_6> . in this paper , we discuss two ways in which <method_2> can be used to generate <otherscientificterm_4> . first , we discuss how <method_0> can be used to get reasonable answers to queries . second , we explain how <method_3> can be used to identify <otherscientificterm_7> . finally , we demonstrate through experiments that these <method_2> lead to significant improvement in <metric_8> for both <task_1> .",
    "abstract_og": "commonsense reasoning at scale is a core problem for cognitive systems . in this paper , we discuss two ways in which heuristic graph traversal methods can be used to generate plausible inference chains . first , we discuss how cyc 's predicate-type hierarchy can be used to get reasonable answers to queries . second , we explain how connection graph-based techniques can be used to identify script-like structures . finally , we demonstrate through experiments that these heuristic graph traversal methods lead to significant improvement in accuracy for both q/a and script construction ."
  },
  {
    "title": "Knowledge-Based Support Vector Machine Classifiers .",
    "entities": [
      "data-based linear support vector machine classifiers",
      "linear support vector machine classifier",
      "test set accuracy",
      "breast cancer prognosis",
      "real world examples",
      "prior knowledge rules",
      "dna sequencing",
      "linear program",
      "prior knowledge",
      "prior knowledge",
      "linear classifier"
    ],
    "types": "<method> <method> <metric> <task> <material> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "dna sequencing -- CONJUNCTION -- breast cancer prognosis",
      "test set accuracy -- EVALUATE-FOR -- data-based linear support vector machine classifiers",
      "breast cancer prognosis -- HYPONYM-OF -- real world examples",
      "prior knowledge -- PART-OF -- data-based linear support vector machine classifiers",
      "prior knowledge -- USED-FOR -- linear classifier",
      "dna sequencing -- HYPONYM-OF -- real world examples"
    ],
    "abstract": "prior knowledge in the form of multiple polyhedral sets , each belonging to one of two categories , is introduced into a reformulation of a <method_1> . the resulting formulation leads to a <method_7> that can be solved efficiently . <material_4> , from <task_6> and <task_3> , demonstrate the effectiveness of the proposed method . numerical results show improvement in <metric_2> after the incorporation of <otherscientificterm_8> into ordinary , <method_0> . one experiment also shows that a <method_10> , based solely on <otherscientificterm_8> , far outperforms the direct application of <otherscientificterm_5> to classify data .",
    "abstract_og": "prior knowledge in the form of multiple polyhedral sets , each belonging to one of two categories , is introduced into a reformulation of a linear support vector machine classifier . the resulting formulation leads to a linear program that can be solved efficiently . real world examples , from dna sequencing and breast cancer prognosis , demonstrate the effectiveness of the proposed method . numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary , data-based linear support vector machine classifiers . one experiment also shows that a linear classifier , based solely on prior knowledge , far outperforms the direct application of prior knowledge rules to classify data ."
  },
  {
    "title": "A new approach to the temporal evolution of a family of curves .",
    "entities": [
      "theory of spline curves",
      "family of curves",
      "curve evolution modeling",
      "oceanic satellite data",
      "motion tracking",
      "curve evolution",
      "modeling tool",
      "linear structures",
      "spatial relationships",
      "spatial modeling",
      "vector field",
      "image processing"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <material> <task> <task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "modeling tool -- USED-FOR -- spatial modeling",
      "spatial modeling -- USED-FOR -- motion tracking",
      "oceanic satellite data -- EVALUATE-FOR -- modeling tool",
      "modeling tool -- USED-FOR -- motion tracking"
    ],
    "abstract": "in this study the problem of modeling a <otherscientificterm_1> is addressed . the need of such modeling appears frequently in many aspects of <task_11> where many <otherscientificterm_7> keep <otherscientificterm_8> during their evolution . we come up with a <method_6> well suited to the <method_9> of a <otherscientificterm_1> , and which can be very useful for <task_4> and <task_5> as well . the <otherscientificterm_1> is represented as the line paths -lrb- orbits -rrb- of a '' spline <otherscientificterm_10> '' , i.e. a <otherscientificterm_10> interpolating data using a framework similar to the <otherscientificterm_0> . the <method_6> is exemplified with <material_3> . its usefullness for <task_2> is also presented .",
    "abstract_og": "in this study the problem of modeling a family of curves is addressed . the need of such modeling appears frequently in many aspects of image processing where many linear structures keep spatial relationships during their evolution . we come up with a modeling tool well suited to the spatial modeling of a family of curves , and which can be very useful for motion tracking and curve evolution as well . the family of curves is represented as the line paths -lrb- orbits -rrb- of a '' spline vector field '' , i.e. a vector field interpolating data using a framework similar to the theory of spline curves . the modeling tool is exemplified with oceanic satellite data . its usefullness for curve evolution modeling is also presented ."
  },
  {
    "title": "Exploiting N-best Hypotheses for SMT Self-Enhancement .",
    "entities": [
      "statistical machine translation",
      "word and n-gram posterior probabilities",
      "target language n-grams",
      "nist chinese-to-english task",
      "rescoring framework",
      "posterior knowledge",
      "re-decoding framework",
      "posterior probabilities",
      "n-best hypotheses",
      "bleu score",
      "nist-2005 set",
      "translation phrase-pairs"
    ],
    "types": "<task> <otherscientificterm> <method> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <material> <task>",
    "relations": [
      "n-best hypotheses -- FEATURE-OF -- word and n-gram posterior probabilities",
      "n-best hypotheses -- USED-FOR -- translation phrase-pairs",
      "posterior probabilities -- USED-FOR -- translation phrase-pairs",
      "posterior knowledge -- USED-FOR -- statistical machine translation",
      "word and n-gram posterior probabilities -- USED-FOR -- statistical machine translation",
      "n-best hypotheses -- PART-OF -- re-decoding framework",
      "n-best hypotheses -- USED-FOR -- posterior knowledge"
    ],
    "abstract": "word and n-gram <otherscientificterm_7> estimated on <otherscientificterm_8> have been used to improve the performance of <task_0> in a <method_4> . in this paper , we extend the idea to estimate the <otherscientificterm_7> on <otherscientificterm_8> for <task_11> , <method_2> , and source word re-orderings . the <task_0> is self-enhanced with the <otherscientificterm_5> learned from <otherscientificterm_8> in a <method_6> . experiments on <material_3> show performance improvements for all the strategies . moreover , the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 <metric_9> on nist-2003 set , and 0.64 on <material_10> , respectively .",
    "abstract_og": "word and n-gram posterior probabilities estimated on n-best hypotheses have been used to improve the performance of statistical machine translation in a rescoring framework . in this paper , we extend the idea to estimate the posterior probabilities on n-best hypotheses for translation phrase-pairs , target language n-grams , and source word re-orderings . the statistical machine translation is self-enhanced with the posterior knowledge learned from n-best hypotheses in a re-decoding framework . experiments on nist chinese-to-english task show performance improvements for all the strategies . moreover , the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 bleu score on nist-2003 set , and 0.64 on nist-2005 set , respectively ."
  },
  {
    "title": "A prosody-based approach to end-of-utterance detection that does not require speech recognition .",
    "entities": [
      "prosodic and/or language models",
      "word and alignment",
      "prosodic end-of-utterance detector",
      "alignment information",
      "end-of-utterance detection",
      "pause-length thresholding",
      "dialog systems",
      "utterance endpoints",
      "prosodic knowledge",
      "speech recognizer",
      "endpointing",
      "recog-nizer",
      "latency"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <metric>",
    "relations": [
      "end-of-utterance detection -- USED-FOR -- dialog systems",
      "word and alignment -- USED-FOR -- prosodic and/or language models",
      "prosodic and/or language models -- USED-FOR -- utterance endpoints",
      "prosodic and/or language models -- USED-FOR -- dialog systems"
    ],
    "abstract": "in previous work we showed that state-of-the-art <task_4> -lrb- as used , for example , in <method_6> -rrb- can be improved significantly by making use of <method_0> that predict <otherscientificterm_7> , based on <otherscientificterm_1> output from a <method_9> . however , using a <method_11> in <task_10> might not be practical in certain applications . in this paper we demonstrate that the improvements due to the <otherscientificterm_8> can be realized largely without <otherscientificterm_3> , i.e. , without requiring a <method_9> . a <method_2> using only speech/nonspeech detection output is still considerably more accurate and has lower <metric_12> than a baseline system based on <otherscientificterm_5> .",
    "abstract_og": "in previous work we showed that state-of-the-art end-of-utterance detection -lrb- as used , for example , in dialog systems -rrb- can be improved significantly by making use of prosodic and/or language models that predict utterance endpoints , based on word and alignment output from a speech recognizer . however , using a recog-nizer in endpointing might not be practical in certain applications . in this paper we demonstrate that the improvements due to the prosodic knowledge can be realized largely without alignment information , i.e. , without requiring a speech recognizer . a prosodic end-of-utterance detector using only speech/nonspeech detection output is still considerably more accurate and has lower latency than a baseline system based on pause-length thresholding ."
  },
  {
    "title": "The Impact of Balancing on Problem Hardness in a Highly Structured Domain .",
    "entities": [
      "quasigroup completion problem",
      "structured problem domain",
      "random problem distributions",
      "generalized sudoku instances",
      "random problem distributions",
      "square shape",
      "qcp/qwh instances",
      "balancing strategies",
      "constraint satisfaction",
      "problem hardness",
      "block regions",
      "sudoku puzzle",
      "parameter settings",
      "worst-case complexity",
      "rectangular shape",
      "computational hardness",
      "boolean satisfiability",
      "backbone variables",
      "real-world applications"
    ],
    "types": "<method> <material> <task> <material> <task> <otherscientificterm> <material> <method> <task> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "generalized sudoku instances -- USED-FOR -- parameter settings",
      "constraint satisfaction -- CONJUNCTION -- boolean satisfiability",
      "balancing strategies -- USED-FOR -- problem hardness",
      "structured problem domain -- USED-FOR -- random problem distributions",
      "computational hardness -- EVALUATE-FOR -- generalized sudoku instances"
    ],
    "abstract": "random problem distributions have played a key role in the study and design of algorithms for <task_8> and <otherscientificterm_16> , as well as in our understanding of <metric_9> , beyond standard <metric_13> . we consider <task_4> from a highly <material_1> that generalizes the <method_0> and quasigroup with holes -lrb- qwh -rrb- , a widely used domain that captures the structure underlying a range of <task_18> . our problem domain is also a generalization of the well-known <otherscientificterm_11> : we consider sudoku instances of arbitrary order , with the additional generalization that the <otherscientificterm_10> can have <otherscientificterm_14> , in addition to the standard <otherscientificterm_5> . we evaluate the <metric_15> of <material_3> , for different <otherscientificterm_12> . our experimental hardness results show that we can generate instances that are considerably harder than <material_6> of the same size . more interestingly , we show the impact of different <method_7> on <metric_9> . we also provide insights into <otherscientificterm_17> in <material_3> and how they correlate to <metric_9> .",
    "abstract_og": "random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and boolean satisfiability , as well as in our understanding of problem hardness , beyond standard worst-case complexity . we consider random problem distributions from a highly structured problem domain that generalizes the quasigroup completion problem and quasigroup with holes -lrb- qwh -rrb- , a widely used domain that captures the structure underlying a range of real-world applications . our problem domain is also a generalization of the well-known sudoku puzzle : we consider sudoku instances of arbitrary order , with the additional generalization that the block regions can have rectangular shape , in addition to the standard square shape . we evaluate the computational hardness of generalized sudoku instances , for different parameter settings . our experimental hardness results show that we can generate instances that are considerably harder than qcp/qwh instances of the same size . more interestingly , we show the impact of different balancing strategies on problem hardness . we also provide insights into backbone variables in generalized sudoku instances and how they correlate to problem hardness ."
  },
  {
    "title": "Physical modeling of drums by transfer function methods .",
    "entities": [
      "multidimensional physical systems",
      "transfer function models",
      "partial differential equations",
      "physics based digital sound synthesis algorithms",
      "initial and boundary conditions",
      "digital signal processors",
      "two-dimensional drum models",
      "excitation functions",
      "spatial dimension",
      "one-dimensional systems"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "transfer function models -- USED-FOR -- physics based digital sound synthesis algorithms",
      "partial differential equations -- USED-FOR -- multidimensional physical systems",
      "initial and boundary conditions -- CONJUNCTION -- excitation functions"
    ],
    "abstract": "multidimensional -lrb- md -rrb- physical systems are usually given in terms of <otherscientificterm_2> . similar to <method_9> , they can also be described by <method_1> . in addition to including <otherscientificterm_4> as well as <otherscientificterm_7> exactly , the <method_1> can also be discretized in a simple way . this leads to suitable implementations for <task_5> . therefore it is possible to implement <method_3> derived from <method_1> in real-time . this paper extends the recently presented solution for vibrating strings with one <otherscientificterm_8> to <method_6> .",
    "abstract_og": "multidimensional -lrb- md -rrb- physical systems are usually given in terms of partial differential equations . similar to one-dimensional systems , they can also be described by transfer function models . in addition to including initial and boundary conditions as well as excitation functions exactly , the transfer function models can also be discretized in a simple way . this leads to suitable implementations for digital signal processors . therefore it is possible to implement physics based digital sound synthesis algorithms derived from transfer function models in real-time . this paper extends the recently presented solution for vibrating strings with one spatial dimension to two-dimensional drum models ."
  },
  {
    "title": "A new acoustic measure for aspiration noise detection .",
    "entities": [
      "detecting aspiration noise in vowels",
      "automatic detection of aspiration noise",
      "vocal tract responses",
      "spectral slope measures",
      "frequency bands",
      "acoustic measure",
      "aspiration noise",
      "glottal excitation"
    ],
    "types": "<task> <task> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "spectral slope measures -- USED-FOR -- acoustic measure",
      "spectral slope measures -- USED-FOR -- automatic detection of aspiration noise",
      "acoustic measure -- USED-FOR -- detecting aspiration noise in vowels",
      "vocal tract responses -- USED-FOR -- glottal excitation",
      "acoustic measure -- USED-FOR -- automatic detection of aspiration noise"
    ],
    "abstract": "in this paper , we propose a new <method_5> for <task_0> . the <method_5> is an index of synchronization between <otherscientificterm_4> around the first and third formants . the <method_5> is based on the principle that the <otherscientificterm_2> to the <otherscientificterm_7> are synchronized between these <otherscientificterm_4> when <otherscientificterm_6> is absent , and uncorrelated otherwise . evaluation results show that the proposed <method_5> can be used together with <method_3> for <task_1> .",
    "abstract_og": "in this paper , we propose a new acoustic measure for detecting aspiration noise in vowels . the acoustic measure is an index of synchronization between frequency bands around the first and third formants . the acoustic measure is based on the principle that the vocal tract responses to the glottal excitation are synchronized between these frequency bands when aspiration noise is absent , and uncorrelated otherwise . evaluation results show that the proposed acoustic measure can be used together with spectral slope measures for automatic detection of aspiration noise ."
  },
  {
    "title": "Efficient anisotropic wavelet packet basis selection in JPEG2000 .",
    "entities": [
      "top-down anisotropic wavelet packet selection scheme",
      "isotropic wavelet packet basis selection algorithms",
      "anisotropic wavelet basis selection",
      "jpeg2000 part 2",
      "quality assessment tools",
      "psnr evaluations",
      "coding framework",
      "jpeg2000"
    ],
    "types": "<method> <method> <task> <method> <method> <task> <method> <method>",
    "relations": [
      "quality assessment tools -- USED-FOR -- psnr evaluations"
    ],
    "abstract": "jpeg2000 part 2 allows the application of arbitrary wavelet decomposition structures -lrb- wavelet packet bases -rrb- . efficient anisotropic wavelet packet basis selection for the <method_6> of <method_7> has been developed and evaluated . previous work focused on <method_1> for <method_7> , which serves as basis for the performance of the assessment of <task_2> . several cost functions are applied in a <method_0> . our evaluations employ state-of-the-art <method_4> supplementary to <task_5> .",
    "abstract_og": "jpeg2000 part 2 allows the application of arbitrary wavelet decomposition structures -lrb- wavelet packet bases -rrb- . efficient anisotropic wavelet packet basis selection for the coding framework of jpeg2000 has been developed and evaluated . previous work focused on isotropic wavelet packet basis selection algorithms for jpeg2000 , which serves as basis for the performance of the assessment of anisotropic wavelet basis selection . several cost functions are applied in a top-down anisotropic wavelet packet selection scheme . our evaluations employ state-of-the-art quality assessment tools supplementary to psnr evaluations ."
  },
  {
    "title": "Fast and memory efficient JBIG2 encoder .",
    "entities": [
      "text image compression",
      "multi-page document images",
      "singleton exclusion dictionary",
      "jbig2 standard",
      "physical memory",
      "horizontal stripes",
      "multi-page document",
      "single-page documents",
      "encoding time",
      "updating processes",
      "modified-class dictionary",
      "encoding strategy",
      "compression"
    ],
    "types": "<task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <metric> <method> <material> <method> <task>",
    "relations": [
      "encoding strategy -- USED-FOR -- text image compression",
      "updating processes -- USED-FOR -- multi-page document images",
      "updating processes -- USED-FOR -- singleton exclusion dictionary",
      "updating processes -- USED-FOR -- compression",
      "jbig2 standard -- USED-FOR -- encoding strategy",
      "encoding time -- CONJUNCTION -- physical memory"
    ],
    "abstract": "in this paper we propose a fast and memory efficient <method_11> for <task_0> with the <otherscientificterm_3> . the encoder splits up the input image into <otherscientificterm_5> and encodes one stripe at a time . construction of the current dictionary is based on updating dictionaries from previous stripes . we describe separate <method_9> for the <otherscientificterm_2> and for the <material_10> . experiments show that , for both dictionaries , splitting the page into two stripes can save 30 % of <metric_8> and 40 % of <otherscientificterm_4> with a small loss of about 1.5 % in <task_12> . further gains can be obtained by using more stripes but with diminishing returns . the same <method_9> are also applied to compressing <material_1> and shown to improve <task_12> by 8-10 % over coding a <material_6> as a collection of <material_7> .",
    "abstract_og": "in this paper we propose a fast and memory efficient encoding strategy for text image compression with the jbig2 standard . the encoder splits up the input image into horizontal stripes and encodes one stripe at a time . construction of the current dictionary is based on updating dictionaries from previous stripes . we describe separate updating processes for the singleton exclusion dictionary and for the modified-class dictionary . experiments show that , for both dictionaries , splitting the page into two stripes can save 30 % of encoding time and 40 % of physical memory with a small loss of about 1.5 % in compression . further gains can be obtained by using more stripes but with diminishing returns . the same updating processes are also applied to compressing multi-page document images and shown to improve compression by 8-10 % over coding a multi-page document as a collection of single-page documents ."
  },
  {
    "title": "A fast and simple algorithm for training neural probabilistic language models .",
    "entities": [
      "neural probabilistic language models",
      "microsoft research sentence completion challenge dataset",
      "estimating unnormalized continuous distributions",
      "penn treebank corpus",
      "neural language models",
      "log-likelihood gradients",
      "n-gram models",
      "noise-contrastive estimation",
      "training times",
      "80k-word vocabulary",
      "training nplms",
      "47m-word corpus",
      "importance sampling",
      "moderately-sized datasets"
    ],
    "types": "<method> <material> <task> <material> <method> <otherscientificterm> <method> <method> <metric> <otherscientificterm> <method> <material> <method> <material>",
    "relations": [
      "neural probabilistic language models -- COMPARE -- n-gram models",
      "microsoft research sentence completion challenge dataset -- EVALUATE-FOR -- neural language models",
      "neural probabilistic language models -- USED-FOR -- estimating unnormalized continuous distributions",
      "80k-word vocabulary -- USED-FOR -- neural language models",
      "47m-word corpus -- EVALUATE-FOR -- neural language models"
    ],
    "abstract": "in spite of their superior performance , <method_0> remain far less widely used than <method_6> due to their notoriously long <metric_8> , which are measured in weeks even for <material_13> . <method_10> is computationally expensive because <method_10> are explicitly normalized , which leads to having to consider all words in the vocabulary when computing the <otherscientificterm_5> . we propose a fast and simple algorithm for training <method_0> based on <method_7> , a newly introduced procedure for <task_2> . we investigate the behaviour of the algorithm on the <material_3> and show that it reduces the <metric_8> by more than an order of magnitude without affecting the quality of the resulting models . the algorithm is also more efficient and much more stable than <method_12> because it requires far fewer noise samples to perform well . we demonstrate the scalability of the proposed approach by training several <method_4> on a <material_11> with a <otherscientificterm_9> , obtaining state-of-the-art results on the <material_1> .",
    "abstract_og": "in spite of their superior performance , neural probabilistic language models remain far less widely used than n-gram models due to their notoriously long training times , which are measured in weeks even for moderately-sized datasets . training nplms is computationally expensive because training nplms are explicitly normalized , which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients . we propose a fast and simple algorithm for training neural probabilistic language models based on noise-contrastive estimation , a newly introduced procedure for estimating unnormalized continuous distributions . we investigate the behaviour of the algorithm on the penn treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models . the algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well . we demonstrate the scalability of the proposed approach by training several neural language models on a 47m-word corpus with a 80k-word vocabulary , obtaining state-of-the-art results on the microsoft research sentence completion challenge dataset ."
  },
  {
    "title": "Recycling Randomness with Structure for Sublinear time Kernel Expansions .",
    "entities": [
      "recycling gaussian random vectors",
      "random feature maps",
      "kernel methods",
      "statistical variance",
      "sublin-ear time",
      "random embeddings",
      "approximation quality",
      "low-displacement matrices",
      "random features",
      "kernel functions",
      "low-displacement rank",
      "structured matrices",
      "coherence",
      "randomness",
      "structure"
    ],
    "types": "<task> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "structured matrices -- USED-FOR -- kernel functions",
      "structured matrices -- USED-FOR -- kernel methods",
      "sublin-ear time -- FEATURE-OF -- kernel functions"
    ],
    "abstract": "we propose a scheme for <task_0> into <otherscientificterm_11> to approximate various <otherscientificterm_9> in <otherscientificterm_4> via <otherscientificterm_5> . our framework includes the fastfood construction of le et al. -lrb- 2013 -rrb- as a special case , but also extends to circulant , toeplitz and hankel matrices , and the broader family of <otherscientificterm_11> that are characterized by the concept of <otherscientificterm_10> . we introduce notions of <otherscientificterm_12> and graph-theoretic structural constants that control the <metric_6> , and prove unbiasedness and low-variance properties of <otherscientificterm_1> that arise within our framework . for the case of <method_7> , we show how the degree of <otherscientificterm_14> and <otherscientificterm_13> can be controlled to reduce <metric_3> at the cost of increased computation and storage requirements . empirical results strongly support our theory and justify the use of a broader family of <otherscientificterm_11> for scaling up <method_2> using <otherscientificterm_8> .",
    "abstract_og": "we propose a scheme for recycling gaussian random vectors into structured matrices to approximate various kernel functions in sublin-ear time via random embeddings . our framework includes the fastfood construction of le et al. -lrb- 2013 -rrb- as a special case , but also extends to circulant , toeplitz and hankel matrices , and the broader family of structured matrices that are characterized by the concept of low-displacement rank . we introduce notions of coherence and graph-theoretic structural constants that control the approximation quality , and prove unbiasedness and low-variance properties of random feature maps that arise within our framework . for the case of low-displacement matrices , we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements . empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features ."
  },
  {
    "title": "Model Learning and Real-Time Tracking Using Multi-Resolution Surfel Maps .",
    "entities": [
      "multi-resolution 3d shape and texture representations",
      "iterative closest points algorithm",
      "full-view models of objects",
      "probabilistic optimization framework",
      "models of objects",
      "mobile manipulation task",
      "registering maps",
      "sensor view",
      "model learning",
      "rgb-d benchmarks",
      "rgb-d images",
      "real-time tracking",
      "efficiency",
      "cpu",
      "accuracy",
      "livestreams",
      "robustness"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <otherscientificterm> <task> <task> <otherscientificterm> <task> <material> <material> <task> <metric> <otherscientificterm> <metric> <otherscientificterm> <metric>",
    "relations": [
      "rgb-d images -- USED-FOR -- multi-resolution 3d shape and texture representations",
      "efficiency -- CONJUNCTION -- real-time tracking",
      "iterative closest points algorithm -- USED-FOR -- registering maps",
      "model learning -- CONJUNCTION -- real-time tracking",
      "efficiency -- CONJUNCTION -- model learning",
      "efficiency -- CONJUNCTION -- robustness"
    ],
    "abstract": "for interaction with its environment , a robot is required to learn <otherscientificterm_4> and to perceive these models in the <otherscientificterm_15> from its sensors . in this paper , we propose a novel approach to <task_8> and <task_11> . we extract <task_0> from <material_10> at high frame-rates . an efficient variant of the <method_1> allows for <task_6> in real-time on a <otherscientificterm_13> . our approach learns <otherscientificterm_2> in a <method_3> in which we find the best alignment between multiple views . finally , we track the pose of the camera with respect to the learned model by registering the current <otherscientificterm_7> to the model . we evaluate our approach on <material_9> and demonstrate its <metric_14> , <metric_12> , and <metric_16> in <task_8> and <task_11> . we also report on the successful public demonstration of our approach in a <task_5> .",
    "abstract_og": "for interaction with its environment , a robot is required to learn models of objects and to perceive these models in the livestreams from its sensors . in this paper , we propose a novel approach to model learning and real-time tracking . we extract multi-resolution 3d shape and texture representations from rgb-d images at high frame-rates . an efficient variant of the iterative closest points algorithm allows for registering maps in real-time on a cpu . our approach learns full-view models of objects in a probabilistic optimization framework in which we find the best alignment between multiple views . finally , we track the pose of the camera with respect to the learned model by registering the current sensor view to the model . we evaluate our approach on rgb-d benchmarks and demonstrate its accuracy , efficiency , and robustness in model learning and real-time tracking . we also report on the successful public demonstration of our approach in a mobile manipulation task ."
  },
  {
    "title": "Active learning for misspecified generalized linear models .",
    "entities": [
      "un-biased estimators of generalization",
      "convex optimization problem",
      "sequential active learning",
      "generalized linear models",
      "model misspecifica-tion",
      "non-linear settings",
      "algorithmic frameworks",
      "active learning",
      "multi-class classification",
      "learning method",
      "binary classification",
      "sampling distributions",
      "mercer kernels",
      "generalization error",
      "regression"
    ],
    "types": "<method> <task> <task> <method> <method> <otherscientificterm> <method> <method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "multi-class classification -- CONJUNCTION -- regression",
      "regression -- HYPONYM-OF -- generalized linear models",
      "convex optimization problem -- USED-FOR -- sampling distributions",
      "multi-class classification -- HYPONYM-OF -- generalized linear models",
      "active learning -- USED-FOR -- generalized linear models",
      "binary classification -- HYPONYM-OF -- generalized linear models",
      "binary classification -- CONJUNCTION -- multi-class classification"
    ],
    "abstract": "active learning refers to <method_6> aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a <method_9> . in this paper , we present an asymptotic analysis of <method_7> for <method_3> . our analysis holds under the common practical situation of <method_4> , and is based on realistic assumptions regarding the nature of the <otherscientificterm_11> , which are usually neither independent nor identical . we derive <method_0> performance , as well as estimators of expected reduction in <otherscientificterm_13> after adding a new training data point , that allow us to optimize its <otherscientificterm_11> through a <task_1> . our analysis naturally leads to an algorithm for <task_2> which is applicable for all tasks supported by <method_3> -lrb- e.g. , <method_10> , <method_8> , <method_14> -rrb- and can be applied in <otherscientificterm_5> through the use of <method_12> .",
    "abstract_og": "active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method . in this paper , we present an asymptotic analysis of active learning for generalized linear models . our analysis holds under the common practical situation of model misspecifica-tion , and is based on realistic assumptions regarding the nature of the sampling distributions , which are usually neither independent nor identical . we derive un-biased estimators of generalization performance , as well as estimators of expected reduction in generalization error after adding a new training data point , that allow us to optimize its sampling distributions through a convex optimization problem . our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models -lrb- e.g. , binary classification , multi-class classification , regression -rrb- and can be applied in non-linear settings through the use of mercer kernels ."
  },
  {
    "title": "DARTs : Efficient scale-space extraction of DAISY keypoints .",
    "entities": [
      "viewpoint and illumination invariant keypoints",
      "computer vision applications",
      "daisy-like layout",
      "daisy descriptor",
      "computational cost",
      "precision",
      "recall",
      "sift",
      "surf"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <method> <metric> <metric> <metric> <method> <method>",
    "relations": [
      "precision -- CONJUNCTION -- recall",
      "sift -- CONJUNCTION -- surf"
    ],
    "abstract": "winder et al. -lsb- 15 , 14 -rsb- have recently shown the superiority of the <method_3> -lsb- 12 -rsb- in comparison to other widely extended descriptors such as <method_7> -lsb- 8 -rsb- and <method_8> -lsb- 1 -rsb- . motivated by those results , we present a novel algorithm that extracts <otherscientificterm_0> and describes them with a particular implementation of a <otherscientificterm_2> . we demonstrate how to efficiently compute the scale-space and re-use this information for the descriptor . comparison to similar approaches such as <method_7> and <method_8> show higher <metric_5> vs <metric_6> performance of the proposed method . moreover , we dramatically reduce the <metric_4> by a factor of 6x and 3x , respectively . we also prove the use of the proposed method for <task_1> .",
    "abstract_og": "winder et al. -lsb- 15 , 14 -rsb- have recently shown the superiority of the daisy descriptor -lsb- 12 -rsb- in comparison to other widely extended descriptors such as sift -lsb- 8 -rsb- and surf -lsb- 1 -rsb- . motivated by those results , we present a novel algorithm that extracts viewpoint and illumination invariant keypoints and describes them with a particular implementation of a daisy-like layout . we demonstrate how to efficiently compute the scale-space and re-use this information for the descriptor . comparison to similar approaches such as sift and surf show higher precision vs recall performance of the proposed method . moreover , we dramatically reduce the computational cost by a factor of 6x and 3x , respectively . we also prove the use of the proposed method for computer vision applications ."
  },
  {
    "title": "Multiple View Region Matching as a Lagrangian Optimization Problem .",
    "entities": [
      "rate-distortion budget-constrained allocation problem",
      "lagrangian optimization techniques",
      "matching regions",
      "constrained optimization",
      "color similarity"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "constrained optimization -- USED-FOR -- matching regions"
    ],
    "abstract": "a method to establish correspondences between regions belonging to independent segmentations of multiple views of a scene is presented . the trade-off between <otherscientificterm_4> and projective similarity of the <otherscientificterm_2> is formulated in terms of a <method_3> , analogous to a <task_0> , and solved using <method_1> .",
    "abstract_og": "a method to establish correspondences between regions belonging to independent segmentations of multiple views of a scene is presented . the trade-off between color similarity and projective similarity of the matching regions is formulated in terms of a constrained optimization , analogous to a rate-distortion budget-constrained allocation problem , and solved using lagrangian optimization techniques ."
  },
  {
    "title": "Exemplar-based pitch accent categorisation using the generalized context model .",
    "entities": [
      "psychologically motivated exemplar-theoretic model of categorisation",
      "pitch accent categorisation simulation",
      "pitch accents",
      "categorisation process",
      "tonal features",
      "exemplar-based comparison"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "exemplar-based comparison -- USED-FOR -- pitch accents"
    ],
    "abstract": "this paper presents the results of a <method_1> which attempts to classify l * h and h * l <otherscientificterm_2> using a <method_0> . <otherscientificterm_2> are represented in terms of six linguistically meaningful parameters describing their shape . no additional information is employed in the <method_3> . the results indicate that these <otherscientificterm_2> can be successfully categorised , via <method_5> , using a limited number of purely <otherscientificterm_4> .",
    "abstract_og": "this paper presents the results of a pitch accent categorisation simulation which attempts to classify l * h and h * l pitch accents using a psychologically motivated exemplar-theoretic model of categorisation . pitch accents are represented in terms of six linguistically meaningful parameters describing their shape . no additional information is employed in the categorisation process . the results indicate that these pitch accents can be successfully categorised , via exemplar-based comparison , using a limited number of purely tonal features ."
  },
  {
    "title": "Learning to Adapt to Unknown Users : Referring Expression Generation in Spoken Dialogue Systems .",
    "entities": [
      "user-adaptive referring expression generation policies",
      "user 's domain knowledge",
      "reinforcement learning framework",
      "user 's domain expertise",
      "adaptive hand-coded baseline policies",
      "statistical user simulation",
      "spoken dialogue systems",
      "supervised learning methods",
      "non-adaptive human-machine interaction",
      "hand-coded policy",
      "technical domains",
      "adaptation accuracy",
      "referring expressions",
      "referring expressions",
      "reg policies",
      "adaptive behaviour",
      "adaptive policies",
      "rl framework",
      "data-driven approach",
      "dialogue time"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <metric>",
    "relations": [
      "data-driven approach -- USED-FOR -- spoken dialogue systems",
      "data-driven approach -- USED-FOR -- user-adaptive referring expression generation policies",
      "data-driven approach -- USED-FOR -- reg policies",
      "rl framework -- USED-FOR -- non-adaptive human-machine interaction",
      "rl framework -- CONJUNCTION -- statistical user simulation",
      "data-driven approach -- USED-FOR -- user 's domain knowledge",
      "non-adaptive human-machine interaction -- USED-FOR -- adaptive policies",
      "user-adaptive referring expression generation policies -- USED-FOR -- spoken dialogue systems"
    ],
    "abstract": "we present a <method_18> to learn <method_0> for <method_6> . <otherscientificterm_12> can be difficult to understand in <otherscientificterm_10> where users may not know the technical ` jargon ' names of the domain entities . in such cases , <method_18> must be able to model the <otherscientificterm_1> and use appropriate <otherscientificterm_13> . we present a <method_2> in which the <method_18> learns <method_14> which can adapt to unknown users online . furthermore , unlike <method_7> which require a large corpus of expert <otherscientificterm_15> to train on , we show that effective <method_16> can be learned from a small dialogue corpus of <otherscientificterm_8> , by using a <method_17> and a <method_5> . we show that in comparison to <method_4> , the learned policy performs significantly better , with an 18.6 % average increase in <metric_11> . the best learned policy also takes less <metric_19> -lrb- average 1.07 min less -rrb- than the best <otherscientificterm_9> . this is because the learned policies can adapt online to changing evidence about the <otherscientificterm_3> .",
    "abstract_og": "we present a data-driven approach to learn user-adaptive referring expression generation policies for spoken dialogue systems . referring expressions can be difficult to understand in technical domains where users may not know the technical ` jargon ' names of the domain entities . in such cases , data-driven approach must be able to model the user 's domain knowledge and use appropriate referring expressions . we present a reinforcement learning framework in which the data-driven approach learns reg policies which can adapt to unknown users online . furthermore , unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on , we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction , by using a rl framework and a statistical user simulation . we show that in comparison to adaptive hand-coded baseline policies , the learned policy performs significantly better , with an 18.6 % average increase in adaptation accuracy . the best learned policy also takes less dialogue time -lrb- average 1.07 min less -rrb- than the best hand-coded policy . this is because the learned policies can adapt online to changing evidence about the user 's domain expertise ."
  },
  {
    "title": "Pronunciation variant-based multi-path HMMs for syllables .",
    "entities": [
      "37-hour corpus of dutch read speech",
      "initialisation of the parallel paths",
      "syllable-length acoustic models",
      "multi-path model topologies",
      "data-driven solution",
      "multi-path models",
      "triphone recogniser",
      "syllable level",
      "pronunciation variation",
      "context-dependent phones",
      "phonetic knowledge",
      "complexity",
      "recognition"
    ],
    "types": "<material> <task> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <metric> <task>",
    "relations": [
      "syllable-length acoustic models -- COMPARE -- context-dependent phones",
      "phonetic knowledge -- USED-FOR -- initialisation of the parallel paths"
    ],
    "abstract": "recent research suggests that it is more appropriate to model <task_8> with <method_2> than with <method_9> . due to the large number of factors contributing to <task_8> at the <otherscientificterm_7> , the creation of <otherscientificterm_3> appears necessary . in this paper , we propose a novel approach for constructing <method_5> for frequent syllables . the suggested approach uses <otherscientificterm_10> for the <task_1> , and a <method_4> for their re-estimation . when applied to 94 frequent syllables in a <material_0> , it leads to improved <task_12> performance when compared with a <method_6> of similar <metric_11> .",
    "abstract_og": "recent research suggests that it is more appropriate to model pronunciation variation with syllable-length acoustic models than with context-dependent phones . due to the large number of factors contributing to pronunciation variation at the syllable level , the creation of multi-path model topologies appears necessary . in this paper , we propose a novel approach for constructing multi-path models for frequent syllables . the suggested approach uses phonetic knowledge for the initialisation of the parallel paths , and a data-driven solution for their re-estimation . when applied to 94 frequent syllables in a 37-hour corpus of dutch read speech , it leads to improved recognition performance when compared with a triphone recogniser of similar complexity ."
  },
  {
    "title": "Detection of digital transmission systems for voice quality measurements .",
    "entities": [
      "digital transmission systems",
      "active speech level",
      "echo delay",
      "recognition rate",
      "echo attenuation",
      "active speech",
      "noise level",
      "transient failures",
      "telephone link",
      "perceived quality",
      "telephone call",
      "frame losses",
      "quality-defining criteria",
      "speech signal",
      "quality",
      "features"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <metric> <material> <metric> <otherscientificterm>",
    "relations": [
      "echo attenuation -- CONJUNCTION -- active speech level",
      "speech signal -- USED-FOR -- features",
      "active speech level -- HYPONYM-OF -- quality-defining criteria",
      "echo delay -- HYPONYM-OF -- quality-defining criteria",
      "echo attenuation -- HYPONYM-OF -- quality-defining criteria",
      "echo delay -- CONJUNCTION -- noise level",
      "echo attenuation -- CONJUNCTION -- noise level",
      "noise level -- HYPONYM-OF -- quality-defining criteria",
      "echo delay -- CONJUNCTION -- active speech level",
      "noise level -- CONJUNCTION -- transient failures",
      "echo attenuation -- CONJUNCTION -- echo delay",
      "frame losses -- CONJUNCTION -- transient failures",
      "frame losses -- HYPONYM-OF -- quality-defining criteria",
      "echo delay -- CONJUNCTION -- frame losses",
      "transient failures -- HYPONYM-OF -- quality-defining criteria",
      "noise level -- CONJUNCTION -- frame losses",
      "active speech level -- CONJUNCTION -- noise level",
      "active speech level -- CONJUNCTION -- frame losses"
    ],
    "abstract": "in-service , non-intrusive measurement devices -lrb- inmd -rrb- estimate the <metric_9> of the <method_8> by extracting <metric_12> like <otherscientificterm_4> , <otherscientificterm_2> , <otherscientificterm_1> , <otherscientificterm_6> , <otherscientificterm_11> and <otherscientificterm_7> from a <otherscientificterm_10> . in addition , the <metric_14> depends on the used <method_0> -lrb- codec systems -rrb- . this paper proposes a method to distinguish between two codec classes . with the help of <otherscientificterm_15> determined from the <material_13> , a classi-fier decides about the class affiliation of the signal . the <metric_3> for signals with 16 seconds of <material_5> is about 97 % .",
    "abstract_og": "in-service , non-intrusive measurement devices -lrb- inmd -rrb- estimate the perceived quality of the telephone link by extracting quality-defining criteria like echo attenuation , echo delay , active speech level , noise level , frame losses and transient failures from a telephone call . in addition , the quality depends on the used digital transmission systems -lrb- codec systems -rrb- . this paper proposes a method to distinguish between two codec classes . with the help of features determined from the speech signal , a classi-fier decides about the class affiliation of the signal . the recognition rate for signals with 16 seconds of active speech is about 97 % ."
  },
  {
    "title": "A phone-dependent confidence measure for utterance rejection .",
    "entities": [
      "database of spoken company names",
      "acceptanceerejection of recognition hypotheses",
      "word sequence conndence",
      "average phone conn-dence",
      "continuous speech utterances",
      "acoustic conndence measure",
      "acoustic observation",
      "speech dissuencies",
      "phone-based approach",
      "global threshold",
      "out-of-vocabulary words",
      "hypothesis rejection",
      "computational expense",
      "ller-model approach",
      "phone duration",
      "phone conn-dence"
    ],
    "types": "<material> <task> <task> <metric> <material> <metric> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <task> <metric> <method> <otherscientificterm> <task>",
    "relations": [
      "global threshold -- USED-FOR -- hypothesis rejection",
      "acoustic conndence measure -- USED-FOR -- continuous speech utterances",
      "acoustic conndence measure -- USED-FOR -- acceptanceerejection of recognition hypotheses",
      "average phone conn-dence -- USED-FOR -- word sequence conndence",
      "acceptanceerejection of recognition hypotheses -- USED-FOR -- continuous speech utterances",
      "database of spoken company names -- USED-FOR -- acoustic conndence measure",
      "out-of-vocabulary words -- CONJUNCTION -- speech dissuencies"
    ],
    "abstract": "an <metric_5> for <task_1> for <material_4> is proposed . this <metric_5> is useful for rejecting utterances that are out of domain , or contain <otherscientificterm_10> or <material_7> . a <method_8> is implemented so that a single <otherscientificterm_9> can be applied to <task_11> for any w ord sequence . <task_15> is computed for each frame of speech as the posterior phone probability g i v en the <otherscientificterm_6> . <task_2> is evaluated as the <metric_3> , either by w eighting all frames equally or by normalizing by <otherscientificterm_14> . the <metric_5> is tested on a <material_0> . when normalized by <otherscientificterm_14> , <metric_5> achieves , in some cases with less <metric_12> , rejection performance comparable to a baseline system implementing a common <method_13> . when all frames are equally weighted , performance is substantially poorer .",
    "abstract_og": "an acoustic conndence measure for acceptanceerejection of recognition hypotheses for continuous speech utterances is proposed . this acoustic conndence measure is useful for rejecting utterances that are out of domain , or contain out-of-vocabulary words or speech dissuencies . a phone-based approach is implemented so that a single global threshold can be applied to hypothesis rejection for any w ord sequence . phone conn-dence is computed for each frame of speech as the posterior phone probability g i v en the acoustic observation . word sequence conndence is evaluated as the average phone conn-dence , either by w eighting all frames equally or by normalizing by phone duration . the acoustic conndence measure is tested on a database of spoken company names . when normalized by phone duration , acoustic conndence measure achieves , in some cases with less computational expense , rejection performance comparable to a baseline system implementing a common ller-model approach . when all frames are equally weighted , performance is substantially poorer ."
  },
  {
    "title": "Issues in measuring the benefits of multimodal interfaces .",
    "entities": [
      "sensory dimensions of sight",
      "collabo-rative distributed c omputing",
      "intellectual abilities of humans",
      "distributed information resources",
      "collective decision making",
      "human/machine/human communication",
      "networked system",
      "multimodal interfaces",
      "machine mediation",
      "multimedia interfaces",
      "hu-man/machine communication",
      "digital networking",
      "human abilities"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <material> <method> <task> <method> <otherscientificterm> <task> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "networked system -- USED-FOR -- collabo-rative distributed c omputing",
      "multimodal interfaces -- PART-OF -- networked system",
      "multimedia interfaces -- USED-FOR -- hu-man/machine communication",
      "machine mediation -- COMPARE -- human abilities",
      "distributed information resources -- CONJUNCTION -- collective decision making"
    ],
    "abstract": "multimedia <otherscientificterm_9> are r apidly evolving to facilitate <task_10> . most of the technologies on which they are b ased a r e , as yet , imperfect . but , the <otherscientificterm_9> do begin to allow information exchange in ways familiar and comfortable to the human | principally through natural actions in the <otherscientificterm_0> , sound and touch . further , as <method_11> becomes ubiquitous , the opportunity grows for collaborative work through confer-enced c omputing . in this context the machine takes on the role of mediator in <task_5> | the ideal being to extend the <otherscientificterm_2> through access to <material_3> and <method_4> . the challenge is how to design <task_8> so that <task_8> extends , not impedes , <otherscientificterm_12> . this report describes evolving work to incorporate <otherscientificterm_7> into a <method_6> for <task_1> . <task_8> also addresses strategies for quantifying the synergies that may be gained .",
    "abstract_og": "multimedia multimedia interfaces are r apidly evolving to facilitate hu-man/machine communication . most of the technologies on which they are b ased a r e , as yet , imperfect . but , the multimedia interfaces do begin to allow information exchange in ways familiar and comfortable to the human | principally through natural actions in the sensory dimensions of sight , sound and touch . further , as digital networking becomes ubiquitous , the opportunity grows for collaborative work through confer-enced c omputing . in this context the machine takes on the role of mediator in human/machine/human communication | the ideal being to extend the intellectual abilities of humans through access to distributed information resources and collective decision making . the challenge is how to design machine mediation so that machine mediation extends , not impedes , human abilities . this report describes evolving work to incorporate multimodal interfaces into a networked system for collabo-rative distributed c omputing . machine mediation also addresses strategies for quantifying the synergies that may be gained ."
  },
  {
    "title": "Web-Scale N-gram Models for Lexical Disambiguation .",
    "entities": [
      "supervised and unsupervised systems",
      "context-sensitive spelling correction",
      "lexical disambiguation",
      "preposition selection",
      "web counts",
      "language research",
      "disambiguation error",
      "web-scale data"
    ],
    "types": "<method> <task> <task> <task> <material> <task> <otherscientificterm> <material>",
    "relations": [
      "web-scale data -- USED-FOR -- language research",
      "web counts -- USED-FOR -- lexical disambiguation",
      "preposition selection -- CONJUNCTION -- context-sensitive spelling correction"
    ],
    "abstract": "web-scale data has been used in a diverse range of <task_5> . most of this research has used <material_4> for only short , fixed spans of context . we present a unified view of using <material_4> for <task_2> . unlike previous approaches , our <method_0> combine information from multiple and overlapping segments of context . on the tasks of <task_3> and <task_1> , the <method_0> reduces <otherscientificterm_6> by 20-24 % over the current state-of-the-art .",
    "abstract_og": "web-scale data has been used in a diverse range of language research . most of this research has used web counts for only short , fixed spans of context . we present a unified view of using web counts for lexical disambiguation . unlike previous approaches , our supervised and unsupervised systems combine information from multiple and overlapping segments of context . on the tasks of preposition selection and context-sensitive spelling correction , the supervised and unsupervised systems reduces disambiguation error by 20-24 % over the current state-of-the-art ."
  },
  {
    "title": "Predicting Opinion Dependency Relations for Opinion Analysis .",
    "entities": [
      "opinion dependency parser",
      "annotated syntactic structures",
      "opinion syntactic structures",
      "supervised learning methods",
      "opinion dependency relations",
      "human friendly materials",
      "language resources",
      "dependency trees",
      "parsing trees",
      "chinese treebank",
      "syntactic structures",
      "syntactic labels",
      "opinion analysis",
      "annotation processing",
      "dependency relations",
      "syntactic structures",
      "parsing tree",
      "features",
      "annotators"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <material> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method>",
    "relations": [
      "parsing trees -- CONJUNCTION -- syntactic structures",
      "parsing trees -- USED-FOR -- annotators",
      "syntactic structures -- USED-FOR -- opinion analysis",
      "parsing tree -- USED-FOR -- opinion syntactic structures",
      "parsing trees -- CONJUNCTION -- dependency trees",
      "supervised learning methods -- USED-FOR -- features",
      "parsing trees -- USED-FOR -- annotation processing",
      "parsing trees -- COMPARE -- dependency trees"
    ],
    "abstract": "syntactic structures have been good <otherscientificterm_17> for <task_12> , but it is not easy to use them . to find these <otherscientificterm_17> by <method_3> , correct <otherscientificterm_11> are indispensible . two possible sources to acquire <otherscientificterm_15> are <task_8> and <otherscientificterm_7> . for the <task_13> , <task_8> are more readable for <method_18> , while <otherscientificterm_7> are easier to use by programs . to use <otherscientificterm_15> as <otherscientificterm_17> , this paper tried to annotate on <material_5> and transform these annotations to the corresponding machine friendly materials . we annotated the gold answers of <otherscientificterm_2> on the <task_16> from <material_9> , and then proposed methods to find their corresponding <otherscientificterm_14> on the <otherscientificterm_7> generated from the same sentence . with these relations , we could train a model to annotate <otherscientificterm_4> automatically to provide an <method_0> , which is language independent if <material_6> are incorporated . experiment results show that the <otherscientificterm_1> and their corresponding <otherscientificterm_14> improve at least 8 % of the performance of <task_12> .",
    "abstract_og": "syntactic structures have been good features for opinion analysis , but it is not easy to use them . to find these features by supervised learning methods , correct syntactic labels are indispensible . two possible sources to acquire syntactic structures are parsing trees and dependency trees . for the annotation processing , parsing trees are more readable for annotators , while dependency trees are easier to use by programs . to use syntactic structures as features , this paper tried to annotate on human friendly materials and transform these annotations to the corresponding machine friendly materials . we annotated the gold answers of opinion syntactic structures on the parsing tree from chinese treebank , and then proposed methods to find their corresponding dependency relations on the dependency trees generated from the same sentence . with these relations , we could train a model to annotate opinion dependency relations automatically to provide an opinion dependency parser , which is language independent if language resources are incorporated . experiment results show that the annotated syntactic structures and their corresponding dependency relations improve at least 8 % of the performance of opinion analysis ."
  },
  {
    "title": "Minimizing Human Effort in Interactive Tracking by Incremental Learning of Model Parameters .",
    "entities": [
      "infant-mother interaction dataset",
      "sequence-specific model parameters",
      "maximum margin framework",
      "virat dataset",
      "interactive tracking",
      "model parameters",
      "precision"
    ],
    "types": "<material> <otherscientificterm> <method> <material> <task> <otherscientificterm> <metric>",
    "relations": [
      "virat dataset -- CONJUNCTION -- infant-mother interaction dataset",
      "sequence-specific model parameters -- USED-FOR -- interactive tracking"
    ],
    "abstract": "we address the problem of minimizing human effort in <task_4> by learning <otherscientificterm_1> . determining the optimal <otherscientificterm_5> for each sequence is a critical problem in <task_4> . we demonstrate that by using the optimal <otherscientificterm_5> for each sequence we can achieve high precision <task_4> results with significantly less effort . we leverage the sequential nature of <task_4> to formulate an efficient method for learning <otherscientificterm_5> through a <method_2> . by using our method we are able to save \u223c 60 \u2212 90 % of human effort to achieve high <metric_6> on two datasets : the <material_3> and an <material_0> .",
    "abstract_og": "we address the problem of minimizing human effort in interactive tracking by learning sequence-specific model parameters . determining the optimal model parameters for each sequence is a critical problem in interactive tracking . we demonstrate that by using the optimal model parameters for each sequence we can achieve high precision interactive tracking results with significantly less effort . we leverage the sequential nature of interactive tracking to formulate an efficient method for learning model parameters through a maximum margin framework . by using our method we are able to save \u223c 60 \u2212 90 % of human effort to achieve high precision on two datasets : the virat dataset and an infant-mother interaction dataset ."
  },
  {
    "title": "Close the loop : Joint blind image restoration and recognition with sparse representation prior .",
    "entities": [
      "joint blind image restoration and recognition method",
      "ill-posed blind image restoration",
      "treating restoration and recognition",
      "simultaneous restoration and recognition",
      "image restoration task",
      "sparse representation prior",
      "degraded input image",
      "visual recognition systems",
      "blind image restoration",
      "face recognition",
      "sparest representation",
      "real-world degradations",
      "unknown degradations",
      "face datasets",
      "low resolution",
      "motion blur",
      "degradation model",
      "out-of-focus blur",
      "low-quality images",
      "recognition task",
      "classifier"
    ],
    "types": "<method> <method> <task> <task> <task> <otherscientificterm> <otherscientificterm> <method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <task> <method>",
    "relations": [
      "real-world degradations -- USED-FOR -- visual recognition systems",
      "out-of-focus blur -- HYPONYM-OF -- real-world degradations",
      "motion blur -- HYPONYM-OF -- real-world degradations",
      "image restoration task -- CONJUNCTION -- recognition task",
      "face datasets -- EVALUATE-FOR -- joint blind image restoration and recognition method",
      "low resolution -- HYPONYM-OF -- real-world degradations",
      "low resolution -- CONJUNCTION -- motion blur",
      "low-quality images -- USED-FOR -- face recognition",
      "sparse representation prior -- USED-FOR -- joint blind image restoration and recognition method",
      "blind image restoration -- USED-FOR -- face recognition",
      "degradation model -- USED-FOR -- face recognition",
      "sparest representation -- USED-FOR -- blind image restoration",
      "sparest representation -- USED-FOR -- face recognition",
      "motion blur -- CONJUNCTION -- out-of-focus blur"
    ],
    "abstract": "most previous <method_7> simply assume ideal inputs without <otherscientificterm_11> , such as <otherscientificterm_14> , <otherscientificterm_15> and <otherscientificterm_17> . in presence of such <otherscientificterm_12> , the conventional approach first resorts to <task_8> and then feeds the restored image into a <method_20> . <task_2> separately , such a straightforward approach , however , suffers greatly from the defective output of the <method_1> . in this paper , we present a <method_0> based on the <otherscientificterm_5> to handle the challenging problem of <task_9> from <material_18> , where the <method_16> is realistic and totally unknown . the <otherscientificterm_5> states that the <otherscientificterm_6> , if correctly restored , will have a good sparse representation in terms of the training set , which indicates the identity of the test image . the proposed <method_0> achieves <task_3> by iteratively solving the <task_8> in pursuit of the <method_10> for <task_9> . based on such a <otherscientificterm_5> , we demonstrate that the <task_4> and the <task_19> can benefit greatly from each other . extensive experiments on <material_13> under various degradations are carried out and the results of our <method_0> shows significant improvements over conventional methods of treating the two tasks independently .",
    "abstract_og": "most previous visual recognition systems simply assume ideal inputs without real-world degradations , such as low resolution , motion blur and out-of-focus blur . in presence of such unknown degradations , the conventional approach first resorts to blind image restoration and then feeds the restored image into a classifier . treating restoration and recognition separately , such a straightforward approach , however , suffers greatly from the defective output of the ill-posed blind image restoration . in this paper , we present a joint blind image restoration and recognition method based on the sparse representation prior to handle the challenging problem of face recognition from low-quality images , where the degradation model is realistic and totally unknown . the sparse representation prior states that the degraded input image , if correctly restored , will have a good sparse representation in terms of the training set , which indicates the identity of the test image . the proposed joint blind image restoration and recognition method achieves simultaneous restoration and recognition by iteratively solving the blind image restoration in pursuit of the sparest representation for face recognition . based on such a sparse representation prior , we demonstrate that the image restoration task and the recognition task can benefit greatly from each other . extensive experiments on face datasets under various degradations are carried out and the results of our joint blind image restoration and recognition method shows significant improvements over conventional methods of treating the two tasks independently ."
  },
  {
    "title": "Combined acoustic and linguistic look-ahead for one-pass time-synchronous decoding .",
    "entities": [
      "one-pass trigram decoding of broadcast news",
      "crossword hmms and m-gram language models",
      "large vocabulary speech recognition",
      "active search space",
      "time-synchronous beam search",
      "base error rate",
      "pruning technique",
      "speed-up decoding",
      "hub4 evaluation",
      "phonetic arc",
      "lexical tree",
      "real-time decoding",
      "decoding pass",
      "decoder",
      "pruning",
      "accuracy"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <method> <metric> <method> <task> <metric> <otherscientificterm> <otherscientificterm> <task> <method> <method> <task> <metric>",
    "relations": [
      "pruning technique -- USED-FOR -- speed-up decoding",
      "lexical tree -- USED-FOR -- decoder",
      "time-synchronous beam search -- USED-FOR -- decoder",
      "pruning technique -- USED-FOR -- large vocabulary speech recognition",
      "hub4 evaluation -- EVALUATE-FOR -- real-time decoding",
      "pruning technique -- USED-FOR -- one-pass trigram decoding of broadcast news",
      "lexical tree -- CONJUNCTION -- time-synchronous beam search"
    ],
    "abstract": "this paper describes an enhanced <method_6> aimed at a further reduction of the <otherscientificterm_3> in <task_2> , to <task_7> while maintaining the <metric_15> . the <method_6> is based on anticipating both the linguistic and acoustic contribution of a <otherscientificterm_9> , before expanding that arc in the search . the <method_13> is based on a <method_4> and a <otherscientificterm_10> . <method_1> are integrated in a single <method_12> . the new <method_6> has been evaluated for <task_0> . with respect to the baseline , the search eeort can be halved at almost no degradation . when <task_14> more aggressively to get a speed-up of 10 , <task_11> is achieved on <metric_8> , however , with an increase of the <metric_5> by one third .",
    "abstract_og": "this paper describes an enhanced pruning technique aimed at a further reduction of the active search space in large vocabulary speech recognition , to speed-up decoding while maintaining the accuracy . the pruning technique is based on anticipating both the linguistic and acoustic contribution of a phonetic arc , before expanding that arc in the search . the decoder is based on a time-synchronous beam search and a lexical tree . crossword hmms and m-gram language models are integrated in a single decoding pass . the new pruning technique has been evaluated for one-pass trigram decoding of broadcast news . with respect to the baseline , the search eeort can be halved at almost no degradation . when pruning more aggressively to get a speed-up of 10 , real-time decoding is achieved on hub4 evaluation , however , with an increase of the base error rate by one third ."
  },
  {
    "title": "Discovery of Term Variation in Japanese Web Search Queries .",
    "entities": [
      "english spelling correction of web queries",
      "string and semantic similarity features",
      "japa-nese web search queries",
      "term variation identification task",
      "web search queries",
      "semantic similarity features",
      "click-through logs",
      "term variations",
      "spelling mistakes",
      "mart algorithm",
      "model features",
      "term variants",
      "writing system",
      "statistical model",
      "error rate",
      "precision",
      "transliterations",
      "recall"
    ],
    "types": "<task> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <metric> <metric> <otherscientificterm> <metric>",
    "relations": [
      "semantic similarity features -- USED-FOR -- web search queries",
      "mart algorithm -- USED-FOR -- statistical model",
      "spelling mistakes -- HYPONYM-OF -- term variants",
      "precision -- EVALUATE-FOR -- term variation identification task"
    ],
    "abstract": "in this paper we address the problem of identifying a broad range of <otherscientificterm_7> in <task_2> , where these variations pose a particularly thorny problem due to the multiple character types employed in its <method_12> . our method extends the techniques proposed for <task_0> to handle a wider range of <otherscientificterm_11> including <otherscientificterm_8> , valid alternative spellings using multiple character types , <otherscientificterm_16> and abbreviations . the core of our method is a <method_13> built on the <method_9> -lrb- friedman , 2001 -rrb- . we show that both <otherscientificterm_1> contribute to identifying term variation in <otherscientificterm_4> ; specifically , the <otherscientificterm_5> used in our <method_13> are learned by mining user session and <otherscientificterm_6> , and are useful not only as <otherscientificterm_10> but also in generating term variation candidates efficiently . the proposed method achieves 70 % <metric_15> on the <task_3> with the <metric_17> slightly higher than 60 % , reducing the <metric_14> of a na\u00efve baseline by 38 % .",
    "abstract_og": "in this paper we address the problem of identifying a broad range of term variations in japa-nese web search queries , where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system . our method extends the techniques proposed for english spelling correction of web queries to handle a wider range of term variants including spelling mistakes , valid alternative spellings using multiple character types , transliterations and abbreviations . the core of our method is a statistical model built on the mart algorithm -lrb- friedman , 2001 -rrb- . we show that both string and semantic similarity features contribute to identifying term variation in web search queries ; specifically , the semantic similarity features used in our statistical model are learned by mining user session and click-through logs , and are useful not only as model features but also in generating term variation candidates efficiently . the proposed method achieves 70 % precision on the term variation identification task with the recall slightly higher than 60 % , reducing the error rate of a na\u00efve baseline by 38 % ."
  },
  {
    "title": "Pitch estimation using phase locked loops .",
    "entities": [
      "harmonic decomposition of the speech signal",
      "pitch estimation algorithms",
      "band-pass filter bank",
      "pitch contour",
      "laryngograph-labeled speech",
      "pitch estimation",
      "phase locked-loop",
      "phase-locked-loop devices",
      "phase-locked-loops",
      "harmonic"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "band-pass filter bank -- USED-FOR -- harmonic decomposition of the speech signal",
      "band-pass filter bank -- CONJUNCTION -- phase-locked-loops"
    ],
    "abstract": "in this paper we present a new method for <task_5> using a system based on <otherscientificterm_7> . three main blocks define our system . the aim of the first one is to make an <otherscientificterm_0> . this stage is implemented using a <otherscientificterm_2> and <otherscientificterm_8> cascaded to the output of each filter . a second block enhances the <otherscientificterm_9> corresponding to the fundamental frequency and attenuates all other harmonics . finally a third stage re-synthesizes a new signal with high energy at the fundamental frequency and extracts <otherscientificterm_3> from that signal using another <otherscientificterm_6> . performance is evaluated over two databases of <material_4> and compared to various well known <method_1> .",
    "abstract_og": "in this paper we present a new method for pitch estimation using a system based on phase-locked-loop devices . three main blocks define our system . the aim of the first one is to make an harmonic decomposition of the speech signal . this stage is implemented using a band-pass filter bank and phase-locked-loops cascaded to the output of each filter . a second block enhances the harmonic corresponding to the fundamental frequency and attenuates all other harmonics . finally a third stage re-synthesizes a new signal with high energy at the fundamental frequency and extracts pitch contour from that signal using another phase locked-loop . performance is evaluated over two databases of laryngograph-labeled speech and compared to various well known pitch estimation algorithms ."
  },
  {
    "title": "Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model .",
    "entities": [
      "connected digit recognition task",
      "supervised speech technology",
      "unlabelled speech data",
      "categorical linguistic structure",
      "word error rates",
      "unlabelled input speech",
      "gibbs sampler",
      "fixed-dimensional space",
      "unsupervised transcription",
      "transcribed speech",
      "hmm-based system",
      "bayesian model",
      "word-like units",
      "unsupervised methods",
      "unsupervised output",
      "pronunciation dictionaries",
      "seg-mentation",
      "audio"
    ],
    "types": "<task> <method> <material> <otherscientificterm> <metric> <material> <method> <otherscientificterm> <material> <material> <method> <method> <otherscientificterm> <method> <material> <material> <otherscientificterm> <material>",
    "relations": [
      "unsupervised methods -- USED-FOR -- categorical linguistic structure",
      "transcribed speech -- USED-FOR -- supervised speech technology",
      "unsupervised output -- USED-FOR -- connected digit recognition task",
      "transcribed speech -- CONJUNCTION -- pronunciation dictionaries",
      "pronunciation dictionaries -- USED-FOR -- supervised speech technology",
      "bayesian model -- USED-FOR -- word-like units"
    ],
    "abstract": "current <method_1> relies heavily on <material_9> and <material_15> . in settings where <material_2> alone is available , <method_13> are required to discover <otherscientificterm_3> directly from the <material_17> . we present a novel <method_11> which segments <material_5> into <otherscientificterm_12> , resulting in a complete <material_8> of the speech in terms of discovered word types . in our <method_11> , a potential word segment -lrb- of arbitrary length -rrb- is embedded in a <otherscientificterm_7> ; the model -lrb- implemented as a <method_6> -rrb- then builds a whole-word acoustic model in this space while jointly doing <otherscientificterm_16> . we report <metric_4> in a <task_0> by mapping the <material_14> to ground truth transcriptions . our model outperforms a previously developed <method_10> , even when the model is not constrained to discover only the 11 word types present in the data .",
    "abstract_og": "current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries . in settings where unlabelled speech data alone is available , unsupervised methods are required to discover categorical linguistic structure directly from the audio . we present a novel bayesian model which segments unlabelled input speech into word-like units , resulting in a complete unsupervised transcription of the speech in terms of discovered word types . in our bayesian model , a potential word segment -lrb- of arbitrary length -rrb- is embedded in a fixed-dimensional space ; the model -lrb- implemented as a gibbs sampler -rrb- then builds a whole-word acoustic model in this space while jointly doing seg-mentation . we report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions . our model outperforms a previously developed hmm-based system , even when the model is not constrained to discover only the 11 word types present in the data ."
  },
  {
    "title": "Word-level invariant representations from acoustic waveforms .",
    "entities": [
      "unsupervised learning of invariant sensory representations",
      "sound waveform of speech units",
      "extracting discriminant , transformation-invariant features",
      "acoustic , temporal domain",
      "spectral properties of speech",
      "speech recognition systems",
      "raw audio signals",
      "frame-level acoustic modeling",
      "word level",
      "raw waveform",
      "one-dimensional distributions",
      "mfcc-based representation",
      "speaker variability",
      "speech recognition",
      "word classification",
      "spectral encoding",
      "accent",
      "gender",
      "preprocess-ing",
      "signature",
      "dialect",
      "features"
    ],
    "types": "<task> <otherscientificterm> <task> <material> <material> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "dialect -- CONJUNCTION -- gender",
      "unsupervised learning of invariant sensory representations -- USED-FOR -- signature",
      "frame-level acoustic modeling -- USED-FOR -- spectral properties of speech",
      "accent -- CONJUNCTION -- gender",
      "frame-level acoustic modeling -- USED-FOR -- speech recognition systems",
      "raw audio signals -- USED-FOR -- extracting discriminant , transformation-invariant features",
      "acoustic , temporal domain -- USED-FOR -- features",
      "accent -- CONJUNCTION -- dialect",
      "spectral encoding -- CONJUNCTION -- preprocess-ing"
    ],
    "abstract": "extracting discriminant , transformation-invariant <otherscientificterm_21> from <material_6> remains a serious challenge for <task_13> . the issue of <otherscientificterm_12> is central to this problem , as changes in <otherscientificterm_16> , <otherscientificterm_20> , <otherscientificterm_17> , and age alter the <otherscientificterm_1> at multiple levels -lrb- phonemes , words , or phrases -rrb- . approaches for dealing with this variability have typically focused on analyzing the <material_4> at the level of frames , on par with <method_7> usually applied to <method_5> . in this paper , we propose a framework for representing speech at the <otherscientificterm_8> and extracting <otherscientificterm_21> from the <material_3> , without the need for <task_15> or <task_18> . leveraging recent work on <task_0> , we extract a <otherscientificterm_19> for a word by first projecting its <otherscientificterm_9> onto a set of templates and their transformations , and then forming empirical estimates of the resulting <otherscientificterm_10> via histograms . the representation and relevant parameters are evaluated for <task_14> on a series of datasets with increasing speaker-mismatch difficulty , and the results are compared to those of an <method_11> .",
    "abstract_og": "extracting discriminant , transformation-invariant features from raw audio signals remains a serious challenge for speech recognition . the issue of speaker variability is central to this problem , as changes in accent , dialect , gender , and age alter the sound waveform of speech units at multiple levels -lrb- phonemes , words , or phrases -rrb- . approaches for dealing with this variability have typically focused on analyzing the spectral properties of speech at the level of frames , on par with frame-level acoustic modeling usually applied to speech recognition systems . in this paper , we propose a framework for representing speech at the word level and extracting features from the acoustic , temporal domain , without the need for spectral encoding or preprocess-ing . leveraging recent work on unsupervised learning of invariant sensory representations , we extract a signature for a word by first projecting its raw waveform onto a set of templates and their transformations , and then forming empirical estimates of the resulting one-dimensional distributions via histograms . the representation and relevant parameters are evaluated for word classification on a series of datasets with increasing speaker-mismatch difficulty , and the results are compared to those of an mfcc-based representation ."
  },
  {
    "title": "Detecting audio events for semantic video search .",
    "entities": [
      "290-hour corpus of sound effects",
      "european project vidivideo",
      "audio event detection",
      "overlapping audio events",
      "real-life videos",
      "svm classifiers",
      "semantic concepts",
      "features",
      "detectors",
      "f-measure"
    ],
    "types": "<material> <material> <task> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <method> <metric>",
    "relations": [
      "overlapping audio events -- FEATURE-OF -- real-life videos",
      "detectors -- USED-FOR -- semantic concepts"
    ],
    "abstract": "this paper describes our work on <task_2> , one of our tasks in the <material_1> . preliminary experiments with a small corpus of sound effects have shown the potential of this type of corpus for training purposes . this paper describes our experiments with <method_5> , and different <otherscientificterm_7> , using a <material_0> , which allowed us to build <method_8> for almost 50 <otherscientificterm_6> . although the performance of these <method_8> on the development set is quite good -lrb- achieving an average <metric_9> of 0.87 -rrb- , preliminary experiments on documentaries and films showed that the task is much harder in <material_4> , which so often include <otherscientificterm_3> .",
    "abstract_og": "this paper describes our work on audio event detection , one of our tasks in the european project vidivideo . preliminary experiments with a small corpus of sound effects have shown the potential of this type of corpus for training purposes . this paper describes our experiments with svm classifiers , and different features , using a 290-hour corpus of sound effects , which allowed us to build detectors for almost 50 semantic concepts . although the performance of these detectors on the development set is quite good -lrb- achieving an average f-measure of 0.87 -rrb- , preliminary experiments on documentaries and films showed that the task is much harder in real-life videos , which so often include overlapping audio events ."
  },
  {
    "title": "Explaining the Ineffable : AI on the Topics of Intuition , Insight and Inspiration .",
    "entities": [
      "intelligent computer systems",
      "`` ineffable '' phenomena",
      "artificial intelligence methods",
      "human simulation",
      "human intelligence",
      "intuition"
    ],
    "types": "<task> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "artificial intelligence methods -- USED-FOR -- human intelligence",
      "artificial intelligence methods -- USED-FOR -- intelligent computer systems",
      "intuition -- HYPONYM-OF -- `` ineffable '' phenomena"
    ],
    "abstract": "artificial intelligence methods may be used to model <otherscientificterm_4> or to build <task_0> . al has already reached the stage of <task_3> where it can model such <otherscientificterm_1> as <otherscientificterm_5> , insight and inspiration . this paper reviews the empirical evidence for these capabilities .",
    "abstract_og": "artificial intelligence methods may be used to model human intelligence or to build intelligent computer systems . al has already reached the stage of human simulation where it can model such `` ineffable '' phenomena as intuition , insight and inspiration . this paper reviews the empirical evidence for these capabilities ."
  },
  {
    "title": "Exploratory Interaction with a Bayesian Argumentation System .",
    "entities": [
      "activation of concepts",
      "exploratory responses",
      "argument grammar",
      "probabilistic patterns",
      "interactive behaviour",
      "bayesian networks",
      "interactive system",
      "attentional mechanism"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "probabilistic patterns -- CONJUNCTION -- argument grammar",
      "attentional mechanism -- USED-FOR -- interactive behaviour",
      "bayesian networks -- USED-FOR -- interactive system"
    ],
    "abstract": "we describe an <method_6> which supports the exploration of arguments generated from <method_5> . in particular , we consider key features which support <otherscientificterm_4> : -lrb- 1 -rrb- an <method_7> which updates the <otherscientificterm_0> as the interaction progresses ; -lrb- 2 -rrb- a set of <otherscientificterm_1> ; and -lrb- 3 -rrb- a set of <otherscientificterm_3> and an <method_2> which support the generation of natural language arguments from <method_5> . a preliminary evaluation assesses the effect of our <otherscientificterm_1> on users ' beliefs .",
    "abstract_og": "we describe an interactive system which supports the exploration of arguments generated from bayesian networks . in particular , we consider key features which support interactive behaviour : -lrb- 1 -rrb- an attentional mechanism which updates the activation of concepts as the interaction progresses ; -lrb- 2 -rrb- a set of exploratory responses ; and -lrb- 3 -rrb- a set of probabilistic patterns and an argument grammar which support the generation of natural language arguments from bayesian networks . a preliminary evaluation assesses the effect of our exploratory responses on users ' beliefs ."
  },
  {
    "title": "A V1 Model of Pop Out and Asymmetty in Visual Search .",
    "entities": [
      "roles of gure and ground",
      "unique features of targets",
      "visual search",
      "intracortical interactions",
      "target detection",
      "visual search",
      "features",
      "v1",
      "asymmetry",
      "segmentation"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "asymmetry -- FEATURE-OF -- visual search"
    ],
    "abstract": "visual search is the task of nding a target in an image against a background of distractors . <otherscientificterm_1> enable them to pop out against the background , while targets deened by lacks of <otherscientificterm_6> or conjunctions of <otherscientificterm_6> are more diicult to spot . it is known that the ease of <task_4> can change when the <otherscientificterm_0> are switched . the mechanisms underlying the ease of pop out and <otherscientificterm_8> in <task_5> have been elusive . this paper shows that a model of <otherscientificterm_9> in <method_7> based on <otherscientificterm_3> can explain many of the qualitative aspects of <task_5> .",
    "abstract_og": "visual search is the task of nding a target in an image against a background of distractors . unique features of targets enable them to pop out against the background , while targets deened by lacks of features or conjunctions of features are more diicult to spot . it is known that the ease of target detection can change when the roles of gure and ground are switched . the mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive . this paper shows that a model of segmentation in v1 based on intracortical interactions can explain many of the qualitative aspects of visual search ."
  },
  {
    "title": "Information Retrieval Oriented Word Segmentation based on Character Association Strength Ranking .",
    "entities": [
      "chinese information retrieval",
      "ranking-style word segmentation approach",
      "internal associative strength",
      "word segmentation methods",
      "training corpus",
      "dictionary information",
      "query items",
      "overlapping ambiguity",
      "seg-mentation decision",
      "mutual information",
      "ranking model",
      "statistical features",
      "rsvm-seg",
      "ranking",
      "t-test",
      "frequency",
      "granular-ity"
    ],
    "types": "<task> <method> <metric> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "ranking-style word segmentation approach -- USED-FOR -- overlapping ambiguity",
      "mutual information -- CONJUNCTION -- t-test",
      "ranking -- USED-FOR -- seg-mentation decision",
      "frequency -- HYPONYM-OF -- statistical features",
      "t-test -- HYPONYM-OF -- statistical features",
      "dictionary information -- HYPONYM-OF -- statistical features",
      "t-test -- CONJUNCTION -- frequency",
      "ranking-style word segmentation approach -- COMPARE -- word segmentation methods",
      "ranking-style word segmentation approach -- USED-FOR -- seg-mentation decision",
      "rsvm-seg -- HYPONYM-OF -- ranking-style word segmentation approach",
      "ranking-style word segmentation approach -- USED-FOR -- chinese information retrieval",
      "frequency -- CONJUNCTION -- dictionary information",
      "mutual information -- HYPONYM-OF -- statistical features"
    ],
    "abstract": "this paper presents a novel , <method_1> , called <method_12> , which is well tailored to <task_0> . this <method_1> makes <task_8> based on the <otherscientificterm_13> of the <metric_2> between each pair of adjacent characters of the sentence . on the <material_4> composed of <otherscientificterm_6> , a <method_10> is learned by a widely-used tool ranking svm , with some useful <otherscientificterm_11> , such as <otherscientificterm_9> , difference of <otherscientificterm_14> , <otherscientificterm_15> and <otherscientificterm_5> . experimental results show that , this <method_1> is able to eliminate <otherscientificterm_7> much more effectively , compared to the current <method_3> . furthermore , as this <method_1> naturally generates segmentation results with different <otherscientificterm_16> , the performance of <task_0> is improved and achieves the state of the art .",
    "abstract_og": "this paper presents a novel , ranking-style word segmentation approach , called rsvm-seg , which is well tailored to chinese information retrieval . this ranking-style word segmentation approach makes seg-mentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence . on the training corpus composed of query items , a ranking model is learned by a widely-used tool ranking svm , with some useful statistical features , such as mutual information , difference of t-test , frequency and dictionary information . experimental results show that , this ranking-style word segmentation approach is able to eliminate overlapping ambiguity much more effectively , compared to the current word segmentation methods . furthermore , as this ranking-style word segmentation approach naturally generates segmentation results with different granular-ity , the performance of chinese information retrieval is improved and achieves the state of the art ."
  },
  {
    "title": "Temporal Milestones in HTNs .",
    "entities": [
      "task abstraction boundaries",
      "hierarchical task networks",
      "end time",
      "start time",
      "temporal milestone",
      "landmark variables",
      "structural properties",
      "temporal milestones"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "start time -- CONJUNCTION -- end time",
      "temporal milestones -- USED-FOR -- hierarchical task networks"
    ],
    "abstract": "we present <otherscientificterm_7> for <method_1> to enable the complex synchronization of tasks . a <otherscientificterm_4> of a task is an intermediate event that occurs during the execution of a complex task , e.g. , the <otherscientificterm_3> , the <otherscientificterm_2> or a milestone of any of its subtasks . unlike <otherscientificterm_5> , introduced in existing work , <otherscientificterm_7> respect the <otherscientificterm_0> and preserve <otherscientificterm_6> enabling much more efficient reasoning . furthermore , <otherscientificterm_7> are as expressive as <otherscientificterm_5> . we provide analytical and empirical evidence to support these claims .",
    "abstract_og": "we present temporal milestones for hierarchical task networks to enable the complex synchronization of tasks . a temporal milestone of a task is an intermediate event that occurs during the execution of a complex task , e.g. , the start time , the end time or a milestone of any of its subtasks . unlike landmark variables , introduced in existing work , temporal milestones respect the task abstraction boundaries and preserve structural properties enabling much more efficient reasoning . furthermore , temporal milestones are as expressive as landmark variables . we provide analytical and empirical evidence to support these claims ."
  },
  {
    "title": "Application of Machine Learning To Epileptic Seizure Detection .",
    "entities": [
      "brain 's electrical activity",
      "median false detection rate",
      "machine learning framework",
      "median detection delay",
      "machine learning approach",
      "patient-specific classifiers",
      "scalp eeg",
      "brain activity",
      "continuous eeg",
      "epileptic seizure",
      "non-invasive measure",
      "chb-mit database",
      "false detections",
      "features"
    ],
    "types": "<otherscientificterm> <metric> <method> <metric> <method> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <metric> <material> <metric> <otherscientificterm>",
    "relations": [
      "median false detection rate -- EVALUATE-FOR -- machine learning approach",
      "median detection delay -- EVALUATE-FOR -- machine learning approach",
      "machine learning approach -- USED-FOR -- patient-specific classifiers",
      "continuous eeg -- EVALUATE-FOR -- machine learning approach",
      "median detection delay -- CONJUNCTION -- median false detection rate"
    ],
    "abstract": "we present and evaluate a <method_4> to constructing <method_5> that detect the onset of an <otherscientificterm_9> through analysis of the <otherscientificterm_6> , a <metric_10> of the <otherscientificterm_0> . this problem is challenging because the <otherscientificterm_0> is composed of numerous classes with overlapping characteristics . the key steps involved in realizing a high performance <method_4> included shaping the problem into an appropriate <method_2> , and identifying the <otherscientificterm_13> critical to separating seizure from other types of <otherscientificterm_7> . when trained on 2 or more seizures per patient and tested on 916 hours of <material_8> from 24 patients , our <method_4> detected 96 % of 173 test seizures with a <metric_3> of 3 seconds and a <metric_1> of 2 <metric_12> per 24 hour period . we also provide information about how to download the <material_11> , which contains the data used in this study .",
    "abstract_og": "we present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp eeg , a non-invasive measure of the brain 's electrical activity . this problem is challenging because the brain 's electrical activity is composed of numerous classes with overlapping characteristics . the key steps involved in realizing a high performance machine learning approach included shaping the problem into an appropriate machine learning framework , and identifying the features critical to separating seizure from other types of brain activity . when trained on 2 or more seizures per patient and tested on 916 hours of continuous eeg from 24 patients , our machine learning approach detected 96 % of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period . we also provide information about how to download the chb-mit database , which contains the data used in this study ."
  },
  {
    "title": "Scalable Metric Learning via Weighted Approximate Rank Component Analysis .",
    "entities": [
      "weighted approximate rank component analysis",
      "large scale market-1501 and cuhk03 datasets",
      "stochastic gradient descent algorithm",
      "non-linear extension of warca",
      "metric learning formulation",
      "kernel space embedding",
      "low-rank matrix optimization",
      "matrix rank degeneration",
      "person re-identification datasets",
      "weighted rank loss",
      "inarbitrary distance measures",
      "kernel trick",
      "mahalanobis distance",
      "person re-identification",
      "non-isolated minima",
      "data dimension",
      "computer vision",
      "precision",
      "ranks",
      "features",
      "regularizer"
    ],
    "types": "<method> <material> <method> <method> <method> <method> <method> <task> <material> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <metric> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "non-isolated minima -- USED-FOR -- low-rank matrix optimization",
      "kernel trick -- USED-FOR -- weighted approximate rank component analysis",
      "weighted approximate rank component analysis -- HYPONYM-OF -- metric learning formulation",
      "person re-identification -- HYPONYM-OF -- computer vision",
      "matrix rank degeneration -- FEATURE-OF -- low-rank matrix optimization",
      "regularizer -- USED-FOR -- low-rank matrix optimization"
    ],
    "abstract": "our goal is to learn a <otherscientificterm_12> by minimizing a loss defined on the weighted sum of the <metric_17> at different <otherscientificterm_18> . our core motivation is that minimizing a <otherscientificterm_9> is a natural criterion for many problems in <task_16> such as <task_13> . we propose a novel <method_4> called <method_0> . we then derive a scalable <method_2> for the resulting <task_16> . we also derive an efficient <method_3> by using the <method_11> . <method_5> decouples the training and prediction costs from the <otherscientificterm_15> and enables us to plug <method_10> which are more natural for the <otherscientificterm_19> . we also address a more general problem of <task_7> & <otherscientificterm_14> in the <method_6> by using new type of <method_20> which approximately enforces the or-thonormality of the learned matrix very efficiently . we validate this new method on nine standard <material_8> including two <material_1> and show that we improve upon the current state-of-the-art methods on all of them .",
    "abstract_og": "our goal is to learn a mahalanobis distance by minimizing a loss defined on the weighted sum of the precision at different ranks . our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in computer vision such as person re-identification . we propose a novel metric learning formulation called weighted approximate rank component analysis . we then derive a scalable stochastic gradient descent algorithm for the resulting computer vision . we also derive an efficient non-linear extension of warca by using the kernel trick . kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features . we also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently . we validate this new method on nine standard person re-identification datasets including two large scale market-1501 and cuhk03 datasets and show that we improve upon the current state-of-the-art methods on all of them ."
  },
  {
    "title": "Hidden information detection based on quantized Laplacian distribution .",
    "entities": [
      "discrete cosine transform coefficients",
      "detection of hidden information",
      "hidden information detection",
      "jpeg image compression",
      "quantified laplacian distribution",
      "false alarm",
      "jpeg image",
      "dct coefficients",
      "statistical decision",
      "hidden bits",
      "statistical test"
    ],
    "types": "<otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <method>",
    "relations": [
      "jpeg image compression -- USED-FOR -- dct coefficients",
      "discrete cosine transform coefficients -- USED-FOR -- statistical test"
    ],
    "abstract": "the goal of this paper is to propose the optimal <method_10> based on the modeling of <otherscientificterm_0> with a <otherscientificterm_4> . this paper focuses on the <task_1> embedded in bits of the <otherscientificterm_7> of a <material_6> . this <task_1> is difficult , in terms of <task_8> , for two main reasons : first , the number of <otherscientificterm_7> used to conceal the <otherscientificterm_9> is random ; second , the <method_3> induces a strong quantization of <otherscientificterm_7> . the proposed test explicitly takes into account the randomness of the number of <otherscientificterm_7> used . it maximizes the probability of <task_2> by ensuring a prescribed level of <otherscientificterm_5> .",
    "abstract_og": "the goal of this paper is to propose the optimal statistical test based on the modeling of discrete cosine transform coefficients with a quantified laplacian distribution . this paper focuses on the detection of hidden information embedded in bits of the dct coefficients of a jpeg image . this detection of hidden information is difficult , in terms of statistical decision , for two main reasons : first , the number of dct coefficients used to conceal the hidden bits is random ; second , the jpeg image compression induces a strong quantization of dct coefficients . the proposed test explicitly takes into account the randomness of the number of dct coefficients used . it maximizes the probability of hidden information detection by ensuring a prescribed level of false alarm ."
  },
  {
    "title": "Optimal graph laplacian regularization for natural image denoising .",
    "entities": [
      "non-local means",
      "per-patch optimal metric space g",
      "graph 's edge weights",
      "graph laplacian regu-larizer",
      "graph laplacian regularizer",
      "local graph-based filtering",
      "image denoising algorithm",
      "graph laplacian reg-ularizer",
      "graph-signal h d",
      "non-local pixel patches",
      "continuous functional s\u03c9",
      "image denoising",
      "pixel patch",
      "gradient estimates",
      "edge weights",
      "graph-signal domain",
      "image priors",
      "laplacian l",
      "g",
      "\u03c9",
      "filtering"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "per-patch optimal metric space g -- EVALUATE-FOR -- non-local means",
      "laplacian l -- USED-FOR -- filtering",
      "per-patch optimal metric space g -- USED-FOR -- image denoising algorithm",
      "g -- USED-FOR -- laplacian l",
      "graph 's edge weights -- USED-FOR -- image priors"
    ],
    "abstract": "image denoising is an under-determined problem , and hence it is important to define appropriate <otherscientificterm_16> for regularization . one recent popular prior is the <method_4> , where a given <otherscientificterm_12> is assumed to be smooth in the <material_15> . the strength and direction of the resulting <otherscientificterm_16> are computed from the <otherscientificterm_2> . in this paper , we derive the optimal <otherscientificterm_14> for <task_5> using <method_13> from <otherscientificterm_9> that are self-similar . to analyze the effects of the <method_13> on the <otherscientificterm_7> , we first show theoretically that , given <method_8> is a set of discrete samples on continuous function h -lrb- x , y -rrb- in a closed region <otherscientificterm_19> , <method_4> -lrb- h d -rrb- t lh d converges to a <method_10> integrating gradient norm of h in metric space <method_18> -- i.e. , -lrb- \u2207 h -rrb- t <method_18> \u2212 1 -lrb- \u2207 h -rrb- -- over <otherscientificterm_19> . we then derive the optimal metric space <method_18> : one that leads to a <method_3> that is discriminant when the <method_13> are accurate , and robust when the <method_13> are noisy . finally , having derived <method_18> we compute the corresponding <otherscientificterm_14> to define the <otherscientificterm_17> used for <task_20> . experimental results show that our <method_6> using the <method_1> outperforms <otherscientificterm_0> by up to 1.5 db in <otherscientificterm_0> .",
    "abstract_og": "image denoising is an under-determined problem , and hence it is important to define appropriate image priors for regularization . one recent popular prior is the graph laplacian regularizer , where a given pixel patch is assumed to be smooth in the graph-signal domain . the strength and direction of the resulting image priors are computed from the graph 's edge weights . in this paper , we derive the optimal edge weights for local graph-based filtering using gradient estimates from non-local pixel patches that are self-similar . to analyze the effects of the gradient estimates on the graph laplacian reg-ularizer , we first show theoretically that , given graph-signal h d is a set of discrete samples on continuous function h -lrb- x , y -rrb- in a closed region \u03c9 , graph laplacian regularizer -lrb- h d -rrb- t lh d converges to a continuous functional s\u03c9 integrating gradient norm of h in metric space g -- i.e. , -lrb- \u2207 h -rrb- t g \u2212 1 -lrb- \u2207 h -rrb- -- over \u03c9 . we then derive the optimal metric space g : one that leads to a graph laplacian regu-larizer that is discriminant when the gradient estimates are accurate , and robust when the gradient estimates are noisy . finally , having derived g we compute the corresponding edge weights to define the laplacian l used for filtering . experimental results show that our image denoising algorithm using the per-patch optimal metric space g outperforms non-local means by up to 1.5 db in non-local means ."
  },
  {
    "title": "Neural Responding Machine for Short-Text Conversation .",
    "entities": [
      "neural responding machine",
      "recurrent neural networks",
      "latent representation of the input text",
      "neural network-based response generator",
      "retrieval-based and smt-based models",
      "generation of response",
      "one-round conversation data",
      "microblogging service",
      "decoding process",
      "encoder-decoder framework",
      "short-text conversation",
      "decoding",
      "encoding"
    ],
    "types": "<method> <method> <material> <method> <method> <task> <material> <material> <method> <method> <task> <method> <otherscientificterm>",
    "relations": [
      "neural responding machine -- HYPONYM-OF -- neural network-based response generator",
      "microblogging service -- USED-FOR -- one-round conversation data",
      "neural network-based response generator -- USED-FOR -- short-text conversation",
      "latent representation of the input text -- USED-FOR -- decoding process",
      "one-round conversation data -- USED-FOR -- neural responding machine"
    ],
    "abstract": "we propose <method_0> , a <method_3> for <task_10> . <method_0> takes the general <method_9> : <method_0> formalizes the <task_5> as a <method_8> based on the <material_2> , while both <otherscientificterm_12> and <method_11> are realized with <method_1> . the <method_0> is trained with a large amount of <material_6> collected from a <material_7> . empirical study shows that <method_0> can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including <method_4> .",
    "abstract_og": "we propose neural responding machine , a neural network-based response generator for short-text conversation . neural responding machine takes the general encoder-decoder framework : neural responding machine formalizes the generation of response as a decoding process based on the latent representation of the input text , while both encoding and decoding are realized with recurrent neural networks . the neural responding machine is trained with a large amount of one-round conversation data collected from a microblogging service . empirical study shows that neural responding machine can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including retrieval-based and smt-based models ."
  },
  {
    "title": "Ranking via Robust Binary Classification .",
    "entities": [
      "explicit feature vectors",
      "large scale problems",
      "robust classification",
      "evaluation metrics",
      "wall-clock time",
      "competitor algorithm",
      "ranking algorithm",
      "learning",
      "computation",
      "robirank"
    ],
    "types": "<otherscientificterm> <task> <task> <metric> <otherscientificterm> <method> <method> <task> <task> <method>",
    "relations": [
      "wall-clock time -- FEATURE-OF -- computation",
      "wall-clock time -- FEATURE-OF -- competitor algorithm",
      "ranking algorithm -- USED-FOR -- learning",
      "robirank -- HYPONYM-OF -- ranking algorithm"
    ],
    "abstract": "we propose <method_9> , a <method_6> that is motivated by observing a close connection between <metric_3> for <task_7> to rank and loss functions for <task_2> . <method_9> shows competitive performance on standard benchmark datasets against a number of other representative algorithms in the literature . we also discuss extensions of <method_9> to <task_1> where <otherscientificterm_0> and scores are not given . we show that <method_9> can be efficiently parallelized across a large number of machines ; for a task that requires 386 , 133 \u00d7 49 , 824 , 519 pairwise interactions between items to be ranked , robi-rank finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art <method_5> , given the same amount of <otherscientificterm_4> for <task_8> .",
    "abstract_og": "we propose robirank , a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification . robirank shows competitive performance on standard benchmark datasets against a number of other representative algorithms in the literature . we also discuss extensions of robirank to large scale problems where explicit feature vectors and scores are not given . we show that robirank can be efficiently parallelized across a large number of machines ; for a task that requires 386 , 133 \u00d7 49 , 824 , 519 pairwise interactions between items to be ranked , robi-rank finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm , given the same amount of wall-clock time for computation ."
  },
  {
    "title": "Understanding Performance Tradeoffs in Algorithms for Solving Oversubscribed Scheduling .",
    "entities": [
      "squeaky wheel optimization",
      "taskswap",
      "schedule-space repair search procedure",
      "oversubscribed scheduling problems",
      "real-world scheduling problem",
      "permutation-space scheduling procedure",
      "permutation-based search methods",
      "schedule-space search methods",
      "resource over-subscription",
      "schedule builder",
      "schedule-space methods",
      "permutation-based methods",
      "mapping"
    ],
    "types": "<method> <method> <method> <task> <task> <method> <method> <method> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "taskswap -- COMPARE -- squeaky wheel optimization",
      "squeaky wheel optimization -- HYPONYM-OF -- permutation-space scheduling procedure",
      "schedule-space methods -- CONJUNCTION -- permutation-based methods",
      "permutation-based search methods -- COMPARE -- schedule-space search methods"
    ],
    "abstract": "in recent years , planning and scheduling research has paid increasing attention to problems that involve <otherscientificterm_8> , where cumulative demand for resources out-strips their availability and some subset of goals or tasks must be excluded . two basic classes of techniques to solve <task_3> have emerged : searching directly in the space of possible schedules and searching in an alternative space of task permutations -lrb- by relying on a <method_9> to provide a <method_12> to schedule space -rrb- . in some problem contexts , <method_6> have been shown to outperform <method_7> , while in others the opposite has been shown to be the case . we consider two techniques for which this behavior has been observed : <method_1> , a <method_2> , and <method_0> , a <method_5> . we analyze the circumstances under which one can be expected to dominate the other . starting from a <task_4> where <method_0> has been shown to outperform <method_1> , we construct a series of problem instances that increasingly incorporate characteristics of a second <task_4> , where <method_1> has been found to outperform <method_0> . experimental results provide insights into when <method_10> and <method_11> may be most appropriate .",
    "abstract_og": "in recent years , planning and scheduling research has paid increasing attention to problems that involve resource over-subscription , where cumulative demand for resources out-strips their availability and some subset of goals or tasks must be excluded . two basic classes of techniques to solve oversubscribed scheduling problems have emerged : searching directly in the space of possible schedules and searching in an alternative space of task permutations -lrb- by relying on a schedule builder to provide a mapping to schedule space -rrb- . in some problem contexts , permutation-based search methods have been shown to outperform schedule-space search methods , while in others the opposite has been shown to be the case . we consider two techniques for which this behavior has been observed : taskswap , a schedule-space repair search procedure , and squeaky wheel optimization , a permutation-space scheduling procedure . we analyze the circumstances under which one can be expected to dominate the other . starting from a real-world scheduling problem where squeaky wheel optimization has been shown to outperform taskswap , we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem , where taskswap has been found to outperform squeaky wheel optimization . experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate ."
  },
  {
    "title": "Cross-domain robust acoustic training .",
    "entities": [
      "ibm viavoice product engine",
      "8khz closing talking microphone",
      "weighted multi-style training",
      "language model tasks",
      "reduced phone sets",
      "hybrid acoustic models",
      "decision tree size",
      "phone set design",
      "cross-domain acoustic training",
      "flat grammars",
      "hybrid system",
      "general dictation",
      "spanish",
      "atis",
      "accuracy"
    ],
    "types": "<method> <task> <method> <task> <material> <method> <metric> <method> <task> <method> <method> <task> <material> <method> <metric>",
    "relations": [
      "hybrid system -- USED-FOR -- general dictation",
      "atis -- CONJUNCTION -- general dictation",
      "hybrid system -- USED-FOR -- language model tasks",
      "atis -- HYPONYM-OF -- language model tasks",
      "hybrid system -- USED-FOR -- 8khz closing talking microphone",
      "flat grammars -- EVALUATE-FOR -- hybrid system"
    ],
    "abstract": "this paper describes our efforts towards <task_8> for large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems . we used <method_2> by pooling insufficient telephony landline and cellular data with down sampled wide band clean data to develop better <method_5> . we explored the effects on <metric_6> to <metric_14> by approximately 10 % . the results show that by fixing number of parameters , system with smaller number of context dependent hmm states yields better <metric_14> . it leads to a smaller <method_7> . we then investigated the performance degradation on two <material_4> for <material_12> . based on these studies , we are able to develop a <method_10> for <task_1> , telephony landline and cellular phone environments . the <method_10> is evaluated on both <method_9> , digit and name at department , and <task_3> , <method_13> and <task_11> , using the <method_0> .",
    "abstract_og": "this paper describes our efforts towards cross-domain acoustic training for large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems . we used weighted multi-style training by pooling insufficient telephony landline and cellular data with down sampled wide band clean data to develop better hybrid acoustic models . we explored the effects on decision tree size to accuracy by approximately 10 % . the results show that by fixing number of parameters , system with smaller number of context dependent hmm states yields better accuracy . it leads to a smaller phone set design . we then investigated the performance degradation on two reduced phone sets for spanish . based on these studies , we are able to develop a hybrid system for 8khz closing talking microphone , telephony landline and cellular phone environments . the hybrid system is evaluated on both flat grammars , digit and name at department , and language model tasks , atis and general dictation , using the ibm viavoice product engine ."
  },
  {
    "title": "Bypassing Combinatorial Protections : Polynomial-Time Algorithms for Single-Peaked Electorates .",
    "entities": [
      "combinatorial covering challenges",
      "weighted coalition manipulation",
      "combinatorially rich structures",
      "central political-science model",
      "np-hard bribery problems",
      "single-peaked preferences",
      "scoring protocols",
      "single-peaked electorates",
      "election systems",
      "polynomial time",
      "hardness protections",
      "partitions",
      "complexity",
      "covers",
      "constructions"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <task> <otherscientificterm> <task> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <task>",
    "relations": [
      "polynomial time -- USED-FOR -- single-peaked electorates",
      "single-peaked preferences -- USED-FOR -- combinatorial covering challenges",
      "weighted coalition manipulation -- USED-FOR -- scoring protocols",
      "covers -- HYPONYM-OF -- combinatorially rich structures",
      "partitions -- HYPONYM-OF -- combinatorially rich structures",
      "partitions -- CONJUNCTION -- covers"
    ],
    "abstract": "for many <method_8> , bribery -lrb- and related -rrb- attacks have been shown np-hard using <task_14> on <otherscientificterm_2> such as <otherscientificterm_11> and <otherscientificterm_13> . this paper shows that for voters who follow the most <method_3> of electorates -- <otherscientificterm_5> -- those <otherscientificterm_10> vanish . by using <otherscientificterm_5> to simplify <task_0> , we for the first time show that <task_4> -- including those for kemeny and llull elections -- fall to <otherscientificterm_9> for <material_7> . by using <otherscientificterm_5> to simplify <task_0> , we for the first time show that <task_4> fall to <otherscientificterm_9> for <material_7> . we show that for <material_7> , the winner problems for dodgson and kemeny elections , though \u03b8 p 2-complete in the general case , fall to <otherscientificterm_9> . and we completely classify the <metric_12> of <method_1> for <task_6> in <material_7> .",
    "abstract_og": "for many election systems , bribery -lrb- and related -rrb- attacks have been shown np-hard using constructions on combinatorially rich structures such as partitions and covers . this paper shows that for voters who follow the most central political-science model of electorates -- single-peaked preferences -- those hardness protections vanish . by using single-peaked preferences to simplify combinatorial covering challenges , we for the first time show that np-hard bribery problems -- including those for kemeny and llull elections -- fall to polynomial time for single-peaked electorates . by using single-peaked preferences to simplify combinatorial covering challenges , we for the first time show that np-hard bribery problems fall to polynomial time for single-peaked electorates . we show that for single-peaked electorates , the winner problems for dodgson and kemeny elections , though \u03b8 p 2-complete in the general case , fall to polynomial time . and we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates ."
  },
  {
    "title": "DFW-based spectral smoothing for concatenative speech synthesis .",
    "entities": [
      "derivative logarithmic magnitude spectra",
      "euclidean spectral distance measurements",
      "phonetic point of view",
      "spectral smoothing technique",
      "autoregressive filter coefficients",
      "smoothed frequency responses",
      "interpolated formant trajectories",
      "weighted linear interpolation",
      "dynamic programming",
      "lsp interpolation",
      "dp backtracking",
      "autocorrelation coefficients",
      "spectral representations"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "dynamic programming -- USED-FOR -- spectral representations",
      "weighted linear interpolation -- USED-FOR -- smoothed frequency responses",
      "spectral smoothing technique -- USED-FOR -- derivative logarithmic magnitude spectra"
    ],
    "abstract": "this paper proposes and evaluates a new <method_3> whose performance is comparable with <method_9> in terms of <otherscientificterm_1> but whose <otherscientificterm_6> are more reasonable from a <otherscientificterm_2> . the <method_3> firstly estimates <otherscientificterm_0> from both the source and the target frame represented by <otherscientificterm_4> . then , <method_8> yields the best alignment between these two <method_12> . <otherscientificterm_5> are achieved by <method_7> between the corresponding source and target spectral lines whose alignment was found by <method_10> . finally , the spectrum is converted to <otherscientificterm_4> with the intermediate stage of <otherscientificterm_11> .",
    "abstract_og": "this paper proposes and evaluates a new spectral smoothing technique whose performance is comparable with lsp interpolation in terms of euclidean spectral distance measurements but whose interpolated formant trajectories are more reasonable from a phonetic point of view . the spectral smoothing technique firstly estimates derivative logarithmic magnitude spectra from both the source and the target frame represented by autoregressive filter coefficients . then , dynamic programming yields the best alignment between these two spectral representations . smoothed frequency responses are achieved by weighted linear interpolation between the corresponding source and target spectral lines whose alignment was found by dp backtracking . finally , the spectrum is converted to autoregressive filter coefficients with the intermediate stage of autocorrelation coefficients ."
  },
  {
    "title": "TextonBoost : Joint Appearance , Shape and Context Modeling for Multi-class Object Recognition and Segmentation .",
    "entities": [
      "high classification and segmentation accuracy",
      "semantic segmentation of photographs",
      "7-class corel subset",
      "general lighting conditions",
      "conditional random field",
      "random feature selection",
      "7-class sowerby database",
      "21-object class database",
      "automatic visual recognition",
      "piecewise training methods",
      "discrimi-native model",
      "image segmentation",
      "unary classification",
      "shared boosting",
      "feature selection",
      "object classes",
      "features",
      "classifier"
    ],
    "types": "<metric> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <material> <task> <method> <method> <task> <task> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "shared boosting -- USED-FOR -- feature selection",
      "7-class corel subset -- CONJUNCTION -- 7-class sowerby database",
      "shared boosting -- USED-FOR -- classifier",
      "classifier -- USED-FOR -- image segmentation",
      "random feature selection -- CONJUNCTION -- piecewise training methods",
      "conditional random field -- USED-FOR -- image segmentation",
      "automatic visual recognition -- CONJUNCTION -- semantic segmentation of photographs",
      "unary classification -- CONJUNCTION -- feature selection"
    ],
    "abstract": "this paper proposes a new approach to learning a <method_10> of <otherscientificterm_15> , incorporating appearance , shape and context information efficiently . the learned model is used for <task_8> and <task_1> . our dis-criminative model exploits novel <otherscientificterm_16> , based on textons , which jointly model shape and texture . <task_12> and <method_14> is achieved using <method_13> to give an efficient <method_17> which can be applied to a large number of classes . accurate <task_11> is achieved by incorporating these <method_17> in a <otherscientificterm_4> . efficient training of the model on very large datasets is achieved by exploiting both <method_5> and <method_9> . <metric_0> are demonstrated on three different databases : i -rrb- our own <material_7> of photographs of real objects viewed under <otherscientificterm_3> , poses and viewpoints , ii -rrb- the <otherscientificterm_2> and iii -rrb- the <material_6> used in -lsb- 1 -rsb- . the proposed algorithm gives competitive results both for highly textured -lrb- e.g.",
    "abstract_og": "this paper proposes a new approach to learning a discrimi-native model of object classes , incorporating appearance , shape and context information efficiently . the learned model is used for automatic visual recognition and semantic segmentation of photographs . our dis-criminative model exploits novel features , based on textons , which jointly model shape and texture . unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes . accurate image segmentation is achieved by incorporating these classifier in a conditional random field . efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods . high classification and segmentation accuracy are demonstrated on three different databases : i -rrb- our own 21-object class database of photographs of real objects viewed under general lighting conditions , poses and viewpoints , ii -rrb- the 7-class corel subset and iii -rrb- the 7-class sowerby database used in -lsb- 1 -rsb- . the proposed algorithm gives competitive results both for highly textured -lrb- e.g."
  },
  {
    "title": "Efficient filter flow for space-variant multiframe blind deconvolution .",
    "entities": [
      "noisy magnetic resonance images",
      "space-variant blind deconvolution",
      "space-variant filters",
      "atmospheric turbulences",
      "astronomical imaging",
      "linear transformations",
      "matrix-vector-multiplications"
    ],
    "types": "<material> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "atmospheric turbulences -- USED-FOR -- astronomical imaging"
    ],
    "abstract": "ultimately being motivated by facilitating <otherscientificterm_1> , we present a class of <otherscientificterm_5> , that are expressive enough for <method_2> , but at the same time especially designed for efficient <otherscientificterm_6> . successful results on <task_4> through <otherscientificterm_3> and on <material_0> of constantly moving objects demonstrate the practical significance of our approach .",
    "abstract_og": "ultimately being motivated by facilitating space-variant blind deconvolution , we present a class of linear transformations , that are expressive enough for space-variant filters , but at the same time especially designed for efficient matrix-vector-multiplications . successful results on astronomical imaging through atmospheric turbulences and on noisy magnetic resonance images of constantly moving objects demonstrate the practical significance of our approach ."
  },
  {
    "title": "Parsing speech into articulatory events .",
    "entities": [
      "mel frequency cepstral coefficient representation",
      "recurrent neural network classifiers",
      "speech production process state",
      "place and manner features",
      "frame and segment levels",
      "posterior probabilities of classes",
      "variable depth lattice generator",
      "language and duration constraints",
      "variable depth lattices",
      "categorical articulatory features",
      "articulatory feature detection",
      "viterbi decoder",
      "articulatory feature",
      "product lattices",
      "viterbi algorithm",
      "timit data",
      "feature",
      "classifiers"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method>",
    "relations": [
      "categorical articulatory features -- USED-FOR -- speech production process state",
      "variable depth lattice generator -- CONJUNCTION -- viterbi decoder",
      "classifiers -- USED-FOR -- articulatory feature detection",
      "variable depth lattices -- USED-FOR -- feature"
    ],
    "abstract": "in this paper , the <method_2> is defined by a number of <otherscientificterm_9> . we describe a detector that outputs a stream -lrb- sequence of classes -rrb- for each <otherscientificterm_12> given the <method_0> of the input speech . the detector consists of a bank of <method_1> , a <method_6> and <method_11> . a bank of <method_17> has been previously used for <task_10> by many researchers . however , we extend their work first by creating <otherscientificterm_8> for each <otherscientificterm_16> and then by combining <otherscientificterm_8> into <otherscientificterm_13> for rescoring using the <method_14> . during the rescor-ing we incorporate <otherscientificterm_7> along with the <otherscientificterm_5> provided by the <method_0> . we present our results for <otherscientificterm_3> using <material_15> , and compare the results to a baseline system . we report performance improvements both at the <otherscientificterm_4> .",
    "abstract_og": "in this paper , the speech production process state is defined by a number of categorical articulatory features . we describe a detector that outputs a stream -lrb- sequence of classes -rrb- for each articulatory feature given the mel frequency cepstral coefficient representation of the input speech . the detector consists of a bank of recurrent neural network classifiers , a variable depth lattice generator and viterbi decoder . a bank of classifiers has been previously used for articulatory feature detection by many researchers . however , we extend their work first by creating variable depth lattices for each feature and then by combining variable depth lattices into product lattices for rescoring using the viterbi algorithm . during the rescor-ing we incorporate language and duration constraints along with the posterior probabilities of classes provided by the mel frequency cepstral coefficient representation . we present our results for place and manner features using timit data , and compare the results to a baseline system . we report performance improvements both at the frame and segment levels ."
  },
  {
    "title": "Spatio-temporal resolution enhancement of video sequence based on super-resolution reconstruction .",
    "entities": [
      "high-resolution and high frame rate video sequences",
      "spatio-temporal resolution enhancement method",
      "feature point correspondence",
      "motion estimation function",
      "super-resolution reconstruction",
      "observation model",
      "video sequences",
      "warping matrix",
      "optimization formula",
      "constraint introduction",
      "constraint"
    ],
    "types": "<material> <method> <otherscientificterm> <otherscientificterm> <method> <method> <material> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "motion estimation function -- USED-FOR -- observation model",
      "motion estimation function -- PART-OF -- warping matrix",
      "feature point correspondence -- USED-FOR -- observation model",
      "constraint -- USED-FOR -- optimization formula",
      "optimization formula -- USED-FOR -- observation model"
    ],
    "abstract": "this paper proposes a new <method_1> of <material_6> based on <method_4> . the proposed method derives a new <method_5> based on <otherscientificterm_2> between successive frames . the <method_5> is defined by including the <otherscientificterm_3> in computing the <otherscientificterm_7> . also , a new <otherscientificterm_10> is introduced to the <method_8> for estimating the parameters of the <method_5> , in order to achieve effective resolution-enhancement . by the newly obtained matrix and the new <method_9> make accurate <material_0> . simulation results are shown to confirm the high performance of the proposed method .",
    "abstract_og": "this paper proposes a new spatio-temporal resolution enhancement method of video sequences based on super-resolution reconstruction . the proposed method derives a new observation model based on feature point correspondence between successive frames . the observation model is defined by including the motion estimation function in computing the warping matrix . also , a new constraint is introduced to the optimization formula for estimating the parameters of the observation model , in order to achieve effective resolution-enhancement . by the newly obtained matrix and the new constraint introduction make accurate high-resolution and high frame rate video sequences . simulation results are shown to confirm the high performance of the proposed method ."
  },
  {
    "title": "On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems .",
    "entities": [
      "recurrent neural network encoder-decoder",
      "continuous space dialogue representation",
      "on-line learning framework",
      "noisy user feedback",
      "dialogue policy learning",
      "gaussian process model",
      "explicit user feedback",
      "data annotation costs",
      "dialogue policy",
      "reinforcement learning",
      "unsupervised fashion",
      "reward signal",
      "reward function",
      "real-world applications",
      "reward model",
      "active learning"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <metric> <task> <method> <method> <otherscientificterm> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "continuous space dialogue representation -- USED-FOR -- on-line learning framework",
      "reinforcement learning -- USED-FOR -- dialogue policy",
      "reward function -- USED-FOR -- dialogue policy",
      "active learning -- USED-FOR -- on-line learning framework",
      "data annotation costs -- EVALUATE-FOR -- on-line learning framework",
      "gaussian process model -- USED-FOR -- reward model",
      "unsupervised fashion -- USED-FOR -- on-line learning framework",
      "recurrent neural network encoder-decoder -- USED-FOR -- unsupervised fashion",
      "active learning -- USED-FOR -- reward model",
      "active learning -- USED-FOR -- dialogue policy",
      "on-line learning framework -- USED-FOR -- noisy user feedback",
      "recurrent neural network encoder-decoder -- USED-FOR -- continuous space dialogue representation"
    ],
    "abstract": "the ability to compute an accurate <otherscientificterm_12> is essential for optimising a <task_8> via <method_9> . in <task_13> , using <otherscientificterm_6> as the <otherscientificterm_11> is often unreliable and costly to collect . this problem can be mitigated if the user 's intent is known in advance or data is available to pre-train a task success predictor off-line . in practice neither of these apply for most real world applications . here we propose an <method_2> whereby the <task_8> is jointly trained alongside the <method_14> via <method_15> with a <method_5> . this <method_2> operates on a <method_1> generated in an <method_10> using a <method_0> . the experimental results demonstrate that the proposed <method_2> is able to significantly reduce <metric_7> and mitigate <otherscientificterm_3> in <task_4> .",
    "abstract_og": "the ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning . in real-world applications , using explicit user feedback as the reward signal is often unreliable and costly to collect . this problem can be mitigated if the user 's intent is known in advance or data is available to pre-train a task success predictor off-line . in practice neither of these apply for most real world applications . here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a gaussian process model . this on-line learning framework operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder . the experimental results demonstrate that the proposed on-line learning framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning ."
  },
  {
    "title": "Improved entropic gain for speech signals analysis/synthesis based on an adaptive time-frequency segmentation scheme .",
    "entities": [
      "wavelet packet decomposition",
      "adaptive representation of speech signals",
      "entropy of the base",
      "lack of temporal segmentation",
      "low cost extended tree",
      "basis search algorithm",
      "experimental speech signals",
      "local entropic criterion",
      "frequency adaptation skills",
      "entropic test",
      "dyadic structure",
      "eecient tool",
      "temporal segmentation",
      "classical wpd",
      "global analysis",
      "reconstruction"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <material> <metric> <method> <otherscientificterm> <method> <method> <material> <method> <task>",
    "relations": [
      "local entropic criterion -- USED-FOR -- temporal segmentation",
      "low cost extended tree -- USED-FOR -- wavelet packet decomposition"
    ],
    "abstract": "in the search for <task_1> , the <method_0> has been proved to be a <method_11> because of its <metric_8> through the best <method_5> . the en-tropic minimization of this algorithm is bounded by two artifacts : the <otherscientificterm_10> of the decomposition and the <otherscientificterm_3> . we propose here a <otherscientificterm_4> in the <method_0> which improves the best basis search by reducing the <otherscientificterm_2> and which is still compatible with the <material_13> . the decomposition also allows perfect <task_15> . the <method_9> is updated to take into account the new basis . the preliminary use of a <method_12> , based on the <material_7> highly improves the entropic gain of the <method_14> . the results are shown on <material_6> comparing the gain of our scheme versus a usual <method_0> .",
    "abstract_og": "in the search for adaptive representation of speech signals , the wavelet packet decomposition has been proved to be a eecient tool because of its frequency adaptation skills through the best basis search algorithm . the en-tropic minimization of this algorithm is bounded by two artifacts : the dyadic structure of the decomposition and the lack of temporal segmentation . we propose here a low cost extended tree in the wavelet packet decomposition which improves the best basis search by reducing the entropy of the base and which is still compatible with the classical wpd . the decomposition also allows perfect reconstruction . the entropic test is updated to take into account the new basis . the preliminary use of a temporal segmentation , based on the local entropic criterion highly improves the entropic gain of the global analysis . the results are shown on experimental speech signals comparing the gain of our scheme versus a usual wavelet packet decomposition ."
  },
  {
    "title": "Speech Utterance Classification Model Training without Manual Transcriptions .",
    "entities": [
      "manually transcribed and annotated training data",
      "classification model training approach",
      "speech utterance classification systems",
      "unsupervised language model adaptation",
      "spoken language understanding tasks",
      "data-driven statistical learning approach",
      "speech utterance classification",
      "training speech utterances",
      "command and control",
      "classification accuracy",
      "classification destinations",
      "manual transcriptions",
      "modeling training",
      "dialog systems",
      "manual transcription",
      "wave files",
      "call routing"
    ],
    "types": "<material> <method> <method> <method> <task> <method> <task> <material> <otherscientificterm> <metric> <otherscientificterm> <material> <task> <task> <material> <material> <task>",
    "relations": [
      "dialog systems -- HYPONYM-OF -- spoken language understanding tasks",
      "speech utterance classification -- USED-FOR -- spoken language understanding tasks",
      "classification accuracy -- EVALUATE-FOR -- classification model training approach",
      "call routing -- CONJUNCTION -- dialog systems",
      "unsupervised language model adaptation -- USED-FOR -- classification model training approach",
      "command and control -- HYPONYM-OF -- spoken language understanding tasks",
      "call routing -- HYPONYM-OF -- spoken language understanding tasks",
      "dialog systems -- CONJUNCTION -- command and control",
      "data-driven statistical learning approach -- USED-FOR -- speech utterance classification systems"
    ],
    "abstract": "speech utterance classification has been widely applied to a variety of <task_4> , including <task_16> , <task_13> , and <otherscientificterm_8> . most <method_2> adopt a <method_5> , which requires <material_0> . in this paper we introduce a novel <method_1> based on <method_3> . <method_1> only requires <material_15> of the <material_7> and their corresponding <otherscientificterm_10> for <task_12> . no <material_14> of the utterances is necessary . experimental results show that this <method_1> , which is much cheaper to implement , has achieved <metric_9> at the same level as the model trained with <material_11> .",
    "abstract_og": "speech utterance classification has been widely applied to a variety of spoken language understanding tasks , including call routing , dialog systems , and command and control . most speech utterance classification systems adopt a data-driven statistical learning approach , which requires manually transcribed and annotated training data . in this paper we introduce a novel classification model training approach based on unsupervised language model adaptation . classification model training approach only requires wave files of the training speech utterances and their corresponding classification destinations for modeling training . no manual transcription of the utterances is necessary . experimental results show that this classification model training approach , which is much cheaper to implement , has achieved classification accuracy at the same level as the model trained with manual transcriptions ."
  },
  {
    "title": "Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization .",
    "entities": [
      "unsupervised and projected parsing systems",
      "google universal dependency treebanks",
      "unsupervised dependency parsers",
      "labeled training data",
      "probabilistic parsing models",
      "entropy reg-ularization",
      "cross-lingual knowledge",
      "data sets",
      "human translations",
      "resource-rich language",
      "resource-poor languages",
      "treebanks",
      "languages"
    ],
    "types": "<method> <material> <method> <material> <method> <method> <otherscientificterm> <material> <otherscientificterm> <material> <material> <material> <material>",
    "relations": [
      "google universal dependency treebanks -- CONJUNCTION -- treebanks",
      "entropy reg-ularization -- FEATURE-OF -- cross-lingual knowledge",
      "probabilistic parsing models -- USED-FOR -- resource-poor languages",
      "google universal dependency treebanks -- HYPONYM-OF -- data sets"
    ],
    "abstract": "we present a novel approach for inducing <method_2> for <material_12> that have no <material_3> , but have translated text in a <material_9> . we train <method_4> for <material_10> by transferring <otherscientificterm_6> from <material_9> with <method_5> . our method can be used as a purely monolingual dependency parser , requiring no <otherscientificterm_8> for the test data , thus making <method_4> applicable to a wide range of <material_10> . we perform experiments on three <material_7> -- version 1.0 and version 2.0 of <material_1> and <material_11> from conll shared-tasks , across ten <material_12> . we obtain state-of-the art performance of all the three data sets when compared with previously studied <method_0> .",
    "abstract_og": "we present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language . we train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy reg-ularization . our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making probabilistic parsing models applicable to a wide range of resource-poor languages . we perform experiments on three data sets -- version 1.0 and version 2.0 of google universal dependency treebanks and treebanks from conll shared-tasks , across ten languages . we obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems ."
  },
  {
    "title": "Classification of emotional content of sighs in dyadic human interactions .",
    "entities": [
      "acoustic and gestural features",
      "spontaneous affective dialogs",
      "emotionally valenced sighs",
      "negative emotion",
      "valence axis",
      "emotional sighs",
      "human communication",
      "multimodal characteristics",
      "unweighted accuracy",
      "sighs",
      "sighs",
      "cries",
      "laughter",
      "classification"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "valence axis -- FEATURE-OF -- emotional sighs",
      "negative emotion -- FEATURE-OF -- sighs",
      "cries -- CONJUNCTION -- sighs",
      "laughter -- CONJUNCTION -- cries"
    ],
    "abstract": "emotions are an important part of <otherscientificterm_6> and are expressed both verbally and non-verbally . common non-verbal vocalizations such as <otherscientificterm_12> , <otherscientificterm_11> and <otherscientificterm_9> carry important emotional content in conversations . <otherscientificterm_10> often are associated with <otherscientificterm_3> . in this work , we show that <otherscientificterm_5> exist along both ends of the <otherscientificterm_4> -lrb- positive-emotion vs. negative-emotion <otherscientificterm_9> -rrb- in <otherscientificterm_1> and that they have certain distinct <otherscientificterm_7> . <task_13> results show that it is possible to differentiate between the two types of <material_2> , using a combination of <otherscientificterm_0> with an overall <metric_8> of 58.26 % .",
    "abstract_og": "emotions are an important part of human communication and are expressed both verbally and non-verbally . common non-verbal vocalizations such as laughter , cries and sighs carry important emotional content in conversations . sighs often are associated with negative emotion . in this work , we show that emotional sighs exist along both ends of the valence axis -lrb- positive-emotion vs. negative-emotion sighs -rrb- in spontaneous affective dialogs and that they have certain distinct multimodal characteristics . classification results show that it is possible to differentiate between the two types of emotionally valenced sighs , using a combination of acoustic and gestural features with an overall unweighted accuracy of 58.26 % ."
  },
  {
    "title": "Sparse Kernel Orthonormalized PLS for feature extraction in large data sets .",
    "entities": [
      "analysis of integrated short-time music features",
      "uci data sets",
      "multivariate analysis method",
      "kernel pls",
      "genre prediction",
      "feature extraction",
      "expressive power",
      "labelled data",
      "sparsity constrains",
      "features"
    ],
    "types": "<task> <material> <method> <method> <task> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "multivariate analysis method -- COMPARE -- kernel pls"
    ],
    "abstract": "in this paper we are presenting a novel <method_2> . our <method_2> is based on a novel kernel orthonormalized partial least squares -lrb- pls -rrb- variant for <task_5> , imposing <otherscientificterm_8> in the solution to improve scalabil-ity . the <method_2> is tested on a benchmark of <material_1> , and on the <task_0> for <task_4> . the upshot is that the <method_2> has strong <otherscientificterm_6> even with rather few <otherscientificterm_9> , is clearly outperforming the ordinary <method_3> , and therefore is an appealing <method_2> for <task_5> of <material_7> .",
    "abstract_og": "in this paper we are presenting a novel multivariate analysis method . our multivariate analysis method is based on a novel kernel orthonormalized partial least squares -lrb- pls -rrb- variant for feature extraction , imposing sparsity constrains in the solution to improve scalabil-ity . the multivariate analysis method is tested on a benchmark of uci data sets , and on the analysis of integrated short-time music features for genre prediction . the upshot is that the multivariate analysis method has strong expressive power even with rather few features , is clearly outperforming the ordinary kernel pls , and therefore is an appealing multivariate analysis method for feature extraction of labelled data ."
  },
  {
    "title": "Sparse representations for multiple measurement vectors -LRB- MMV -RRB- in an over-complete dictionary .",
    "entities": [
      "single measurement vector",
      "multiple measurement vector",
      "sparse representations",
      "efficient methods",
      "0-norm minimization",
      "sparse representation",
      "1-norm minimization",
      "theoretical analysis",
      "over-complete dictionary"
    ],
    "types": "<method> <task> <method> <method> <task> <method> <task> <method> <material>",
    "relations": [
      "efficient methods -- USED-FOR -- sparse representations",
      "multiple measurement vector -- HYPONYM-OF -- sparse representation",
      "multiple measurement vector -- USED-FOR -- multiple measurement vector"
    ],
    "abstract": "multiple measurement vector -lrb- mmv -rrb- is a newly emerged problem in <method_5> in an <material_8> ; it poses new challenges . <method_3> have been designed to search for <method_2> -lsb- 1 , 2 -rsb- ; however , we have not seen substantial development in the <method_7> , considering what has been done in a simpler case -- <method_0> -- in which many theoretical results are known , e.g. , -lsb- 9 , 3 , 4 , 5 , 6 -rsb- . this paper extends the known results of <task_1> to mmv . our theoretical results show the fundamental limitation on when a <method_5> is unique . moreover , the relation between the solutions of <task_4> and the solutions of <task_6> indicates a compu-tationally efficient approach to find a <method_5> . interestingly , simulations show that the predictions made by these theorems tend to be conservative .",
    "abstract_og": "multiple measurement vector -lrb- mmv -rrb- is a newly emerged problem in sparse representation in an over-complete dictionary ; it poses new challenges . efficient methods have been designed to search for sparse representations -lsb- 1 , 2 -rsb- ; however , we have not seen substantial development in the theoretical analysis , considering what has been done in a simpler case -- single measurement vector -- in which many theoretical results are known , e.g. , -lsb- 9 , 3 , 4 , 5 , 6 -rsb- . this paper extends the known results of multiple measurement vector to mmv . our theoretical results show the fundamental limitation on when a sparse representation is unique . moreover , the relation between the solutions of 0-norm minimization and the solutions of 1-norm minimization indicates a compu-tationally efficient approach to find a sparse representation . interestingly , simulations show that the predictions made by these theorems tend to be conservative ."
  },
  {
    "title": "Open-set speaker identification using adapted Gaussian mixture models .",
    "entities": [
      "open-set , text-independent speaker identification",
      "nist sre2003 1-speaker detection task",
      "unconstrained co-hort normalisation",
      "gaussian mixture models",
      "score normalisation methods",
      "osti-si framework",
      "speaker verification",
      "fast-scoring method",
      "t-norm",
      "tz-norm"
    ],
    "types": "<task> <material> <method> <method> <method> <method> <task> <method> <method> <method>",
    "relations": [
      "t-norm -- CONJUNCTION -- tz-norm",
      "unconstrained co-hort normalisation -- CONJUNCTION -- t-norm",
      "gaussian mixture models -- USED-FOR -- open-set , text-independent speaker identification",
      "fast-scoring method -- USED-FOR -- speaker verification"
    ],
    "abstract": "this paper presents an investigation into the use of adapted <method_3> in the context of <task_0> . the study includes a scheme for using the <method_7> which has been proposed for <task_6> . furthermore , it provides an evaluation of various <method_4> in the proposed <method_5> . the dataset used for the experimental investigation is based on <material_1> . it is shown that significant improvements can be achieved if only a single mixture is used in the fast-scoring technique . furthermore , it is experimentally observed that comparable performance is obtained using <method_2> , <method_8> and <method_9> . the paper provides a detailed description of the experimental set up , and presents an analysis of the results obtained .",
    "abstract_og": "this paper presents an investigation into the use of adapted gaussian mixture models in the context of open-set , text-independent speaker identification . the study includes a scheme for using the fast-scoring method which has been proposed for speaker verification . furthermore , it provides an evaluation of various score normalisation methods in the proposed osti-si framework . the dataset used for the experimental investigation is based on nist sre2003 1-speaker detection task . it is shown that significant improvements can be achieved if only a single mixture is used in the fast-scoring technique . furthermore , it is experimentally observed that comparable performance is obtained using unconstrained co-hort normalisation , t-norm and tz-norm . the paper provides a detailed description of the experimental set up , and presents an analysis of the results obtained ."
  },
  {
    "title": "Hill-climbing feature selection for multi-stream ASR .",
    "entities": [
      "clean and noisy data sets",
      "ensemble -rrb- automatic speech recognition",
      "hill-climbing algorithm",
      "multi-layer-perceptron-based numbers asr system",
      "opitz 's scoring formula",
      "single-classifier word recognition accuracy",
      "automated feature selection",
      "ogi numbers corpus",
      "hc process",
      "ensemble accuracy",
      "hc scripts",
      "performance score",
      "ensemble diversity",
      "noise types",
      "feature"
    ],
    "types": "<material> <task> <method> <method> <method> <metric> <task> <material> <method> <metric> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "automated feature selection -- USED-FOR -- ensemble -rrb- automatic speech recognition",
      "hc scripts -- USED-FOR -- clean and noisy data sets",
      "ensemble diversity -- FEATURE-OF -- opitz 's scoring formula",
      "ogi numbers corpus -- USED-FOR -- clean and noisy data sets",
      "single-classifier word recognition accuracy -- CONJUNCTION -- ensemble diversity",
      "single-classifier word recognition accuracy -- EVALUATE-FOR -- opitz 's scoring formula",
      "ogi numbers corpus -- PART-OF -- multi-layer-perceptron-based numbers asr system",
      "single-classifier word recognition accuracy -- CONJUNCTION -- ensemble accuracy",
      "hill-climbing algorithm -- USED-FOR -- ensemble -rrb- automatic speech recognition"
    ],
    "abstract": "we performed <task_6> for multi-stream -lrb- i.e. , <task_1> , using a <method_2> that changes one <otherscientificterm_14> at a time if the change improves a <metric_11> . for both <material_0> -lrb- using the <material_7> -rrb- , <method_10> usually improved performance on held out data compared to the initial system it started with , even for <otherscientificterm_13> that were not seen during the <method_8> . overall , we found that using <method_4> , which blends <metric_5> and <otherscientificterm_12> , worked better than <metric_9> as a <metric_11> for guiding <method_10> in cases of extreme mismatch between the snr of training and test sets . our noisy version of the <material_7> , our <method_3> , and our <method_10> are available online .",
    "abstract_og": "we performed automated feature selection for multi-stream -lrb- i.e. , ensemble -rrb- automatic speech recognition , using a hill-climbing algorithm that changes one feature at a time if the change improves a performance score . for both clean and noisy data sets -lrb- using the ogi numbers corpus -rrb- , hc scripts usually improved performance on held out data compared to the initial system it started with , even for noise types that were not seen during the hc process . overall , we found that using opitz 's scoring formula , which blends single-classifier word recognition accuracy and ensemble diversity , worked better than ensemble accuracy as a performance score for guiding hc scripts in cases of extreme mismatch between the snr of training and test sets . our noisy version of the ogi numbers corpus , our multi-layer-perceptron-based numbers asr system , and our hc scripts are available online ."
  },
  {
    "title": "The RoboCup Synthetic Agent Challenge 97 .",
    "entities": [
      "dynamic , real-time , multi-agent domain",
      "multi-agent team planning",
      "intelligent agent researchers",
      "robocup challenge oers",
      "machine learning"
    ],
    "types": "<material> <method> <task> <method> <task>",
    "relations": [
      "robocup challenge oers -- USED-FOR -- machine learning"
    ],
    "abstract": "robocup challenge oers a set of challenges for <task_2> using a friendly competition in a <material_0> . while <method_3> in general envisions longer range challenges over the next few decades , <method_3> presents three specic challenges for the next two years : -lrb- i -rrb- learning of individual agents and teams ; -lrb- ii -rrb- <method_1> and plan-execution in service of teamwork ; and -lrb- iii -rrb- opponent mod-eling . <method_3> provides a novel opportunity for <task_4> , planning , and multi-agent researchers | <method_3> not only supplies a concrete domain to evalute their techniques , but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain : real-time , uncertainty , and teamwork .",
    "abstract_og": "robocup challenge oers a set of challenges for intelligent agent researchers using a friendly competition in a dynamic , real-time , multi-agent domain . while robocup challenge oers in general envisions longer range challenges over the next few decades , robocup challenge oers presents three specic challenges for the next two years : -lrb- i -rrb- learning of individual agents and teams ; -lrb- ii -rrb- multi-agent team planning and plan-execution in service of teamwork ; and -lrb- iii -rrb- opponent mod-eling . robocup challenge oers provides a novel opportunity for machine learning , planning , and multi-agent researchers | robocup challenge oers not only supplies a concrete domain to evalute their techniques , but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain : real-time , uncertainty , and teamwork ."
  },
  {
    "title": "Speaker normalization based on test to reference speaker mapping .",
    "entities": [
      "vocal tract envelope and excitation model",
      "formant based nonlinear frequency warping approach",
      "estimated frequency warping function",
      "teaching and training system",
      "vocal tract envelope",
      "reference speaker excitation",
      "speaker normalization technique",
      "time-frequency speech representation",
      "vocal tract shape",
      "vocal tract normalization",
      "spectral distance",
      "excitation characteristics",
      "speech decomposition",
      "inter-speaker variability",
      "synthesis framework",
      "normalization",
      "variance",
      "analysis"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "synthesis framework -- USED-FOR -- speaker normalization technique",
      "formant based nonlinear frequency warping approach -- USED-FOR -- vocal tract normalization",
      "estimated frequency warping function -- USED-FOR -- vocal tract envelope",
      "speech decomposition -- PART-OF -- vocal tract envelope and excitation model"
    ],
    "abstract": "the paper presents the <method_6> we implemented in a <method_3> for hearing handicapped children with the goal to reduce <otherscientificterm_13> in <task_7> . in an effort to reduce <otherscientificterm_16> caused by variation in <otherscientificterm_8> among speakers , a <method_1> to <task_9> is investigated . the proposed <method_6> can be efficiently realized in an <task_17> by <method_14> . after the <task_12> into the <method_0> , the <otherscientificterm_4> is warped by the <otherscientificterm_2> , while the <otherscientificterm_11> are mapped to the <otherscientificterm_5> . the results have shown significant <otherscientificterm_10> decrease for correctly pronounced words between test and the reference speaker after the <otherscientificterm_15> has been applied , while for poor pronunciation by the test speaker the <otherscientificterm_10> remains relatively high .",
    "abstract_og": "the paper presents the speaker normalization technique we implemented in a teaching and training system for hearing handicapped children with the goal to reduce inter-speaker variability in time-frequency speech representation . in an effort to reduce variance caused by variation in vocal tract shape among speakers , a formant based nonlinear frequency warping approach to vocal tract normalization is investigated . the proposed speaker normalization technique can be efficiently realized in an analysis by synthesis framework . after the speech decomposition into the vocal tract envelope and excitation model , the vocal tract envelope is warped by the estimated frequency warping function , while the excitation characteristics are mapped to the reference speaker excitation . the results have shown significant spectral distance decrease for correctly pronounced words between test and the reference speaker after the normalization has been applied , while for poor pronunciation by the test speaker the spectral distance remains relatively high ."
  },
  {
    "title": "Solving POMDPs : RTDP-Bel vs. Point-based Algorithms .",
    "entities": [
      "parallel value iteration",
      "discretization function",
      "approximate methods",
      "tabular representation",
      "goal pomdps",
      "representational gap",
      "discounted pomdps",
      "value function",
      "point-based algorithms",
      "point-based algorithms",
      "rtdp-bel"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <method> <method> <material>",
    "relations": [
      "rtdp-bel -- USED-FOR -- rtdp-bel",
      "discounted pomdps -- USED-FOR -- goal pomdps",
      "rtdp-bel -- COMPARE -- discounted pomdps",
      "rtdp-bel -- COMPARE -- point-based algorithms",
      "point-based algorithms -- USED-FOR -- rtdp-bel",
      "tabular representation -- CONJUNCTION -- discretization function",
      "point-based algorithms -- COMPARE -- discounted pomdps"
    ],
    "abstract": "point-based algorithms and <material_10> are <method_2> for solving <material_10> that replace the full updates of <task_0> by faster and more effective updates at selected beliefs . an important difference between the two methods is that the former adopt sondik 's representation of the <otherscientificterm_7> , while the latter uses a <method_3> and a <otherscientificterm_1> . the algorithms , however , have not been compared up to now , because they target different <material_10> : <material_6> on the one hand , and <method_4> on the other . in this paper , we bridge this <otherscientificterm_5> , showing how to transform <material_6> into <method_4> , and use the transformation to compare <material_10> with <method_9> over the existing <material_6> . the results appear to contradict the conventional wisdom in the area showing that <material_10> is competitive , and sometimes superior to <method_9> in both quality and time .",
    "abstract_og": "point-based algorithms and rtdp-bel are approximate methods for solving rtdp-bel that replace the full updates of parallel value iteration by faster and more effective updates at selected beliefs . an important difference between the two methods is that the former adopt sondik 's representation of the value function , while the latter uses a tabular representation and a discretization function . the algorithms , however , have not been compared up to now , because they target different rtdp-bel : discounted pomdps on the one hand , and goal pomdps on the other . in this paper , we bridge this representational gap , showing how to transform discounted pomdps into goal pomdps , and use the transformation to compare rtdp-bel with point-based algorithms over the existing discounted pomdps . the results appear to contradict the conventional wisdom in the area showing that rtdp-bel is competitive , and sometimes superior to point-based algorithms in both quality and time ."
  },
  {
    "title": "NOKIA Research Center Beijing Chinese Word Segmentation System for the SIGHAN Bakeoff 2007 .",
    "entities": [
      "state language commission of p.r.c.",
      "chinese word segmentation system",
      "open and closed track",
      "post processing strategies",
      "n-gram model",
      "preprocessing module",
      "out-of-vocabulary words",
      "segmentation",
      "sighan"
    ],
    "types": "<task> <method> <material> <method> <method> <method> <otherscientificterm> <task> <method>",
    "relations": [
      "preprocessing module -- USED-FOR -- segmentation",
      "preprocessing module -- USED-FOR -- out-of-vocabulary words",
      "n-gram model -- USED-FOR -- post processing strategies",
      "n-gram model -- USED-FOR -- segmentation"
    ],
    "abstract": "this paper presents the <method_1> developed by nokia research center -lrb- nrc -rrb- , which was evaluated in the fourth international chinese language processing bakeoff and the first cips chinese language processing evaluation organized by <method_8> . in our <method_1> , a <method_5> was used to discover the <otherscientificterm_6> which occur repeatedly in the text , then an improved <method_4> was used for <task_7> and some <method_3> are adopted in <method_1> to recognize the organization names and new words . we took part in three tracks , which are called the <material_2> on corpora <task_0> , and closed track on corpora shanxi university -lrb- sxu -rrb- . our <method_1> achieved good performance , especially in the open track on <task_0> , our <method_1> ranks 1 st among 11 systems .",
    "abstract_og": "this paper presents the chinese word segmentation system developed by nokia research center -lrb- nrc -rrb- , which was evaluated in the fourth international chinese language processing bakeoff and the first cips chinese language processing evaluation organized by sighan . in our chinese word segmentation system , a preprocessing module was used to discover the out-of-vocabulary words which occur repeatedly in the text , then an improved n-gram model was used for segmentation and some post processing strategies are adopted in chinese word segmentation system to recognize the organization names and new words . we took part in three tracks , which are called the open and closed track on corpora state language commission of p.r.c. , and closed track on corpora shanxi university -lrb- sxu -rrb- . our chinese word segmentation system achieved good performance , especially in the open track on state language commission of p.r.c. , our chinese word segmentation system ranks 1 st among 11 systems ."
  },
  {
    "title": "Linear stereo matching .",
    "entities": [
      "o methods",
      "o approaches",
      "local stereo matching algorithms",
      "stereo matching algorithms",
      "disparity refinement pipeline",
      "cost aggregation step",
      "window size",
      "colour images",
      "integral his-tograms",
      "global approaches",
      "adaptive-weight strategy",
      "complexity",
      "aggregation",
      "accuracy"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <material> <method> <method> <method> <metric> <method> <metric>",
    "relations": [
      "local stereo matching algorithms -- COMPARE -- global approaches",
      "cost aggregation step -- COMPARE -- stereo matching algorithms",
      "cost aggregation step -- USED-FOR -- disparity refinement pipeline",
      "colour images -- USED-FOR -- cost aggregation step",
      "adaptive-weight strategy -- USED-FOR -- local stereo matching algorithms",
      "complexity -- FEATURE-OF -- cost aggregation step",
      "adaptive-weight strategy -- USED-FOR -- global approaches"
    ],
    "abstract": "recent <method_2> based on an <method_10> achieve <metric_13> similar to <method_9> . one of the major problems of these <method_2> is that they are computationally expensive and this <metric_11> increases proportionally to the <otherscientificterm_6> . this paper proposes a novel <method_5> with <metric_11> independent of the <otherscientificterm_6> -lrb- i.e. o -lrb- 1 -rrb- -rrb- that outperforms state-of-the-art <method_0> . moreover , compared to other <method_1> , our <method_5> does not rely on <method_8> enabling <method_12> using <material_7> instead of grayscale ones . finally , to improve the results of the proposed <method_5> a <method_4> is also proposed . the overall <method_5> produces results comparable to those of state-of-the-art <method_3> .",
    "abstract_og": "recent local stereo matching algorithms based on an adaptive-weight strategy achieve accuracy similar to global approaches . one of the major problems of these local stereo matching algorithms is that they are computationally expensive and this complexity increases proportionally to the window size . this paper proposes a novel cost aggregation step with complexity independent of the window size -lrb- i.e. o -lrb- 1 -rrb- -rrb- that outperforms state-of-the-art o methods . moreover , compared to other o approaches , our cost aggregation step does not rely on integral his-tograms enabling aggregation using colour images instead of grayscale ones . finally , to improve the results of the proposed cost aggregation step a disparity refinement pipeline is also proposed . the overall cost aggregation step produces results comparable to those of state-of-the-art stereo matching algorithms ."
  },
  {
    "title": "Semantic Frames to Predict Stock Price Movement .",
    "entities": [
      "semantic frame parsing",
      "binary classification tasks",
      "general nlp problems",
      "support vector machines",
      "semantic frame parsers",
      "polarity task",
      "linguistic resource",
      "semantic frames",
      "tree kernels",
      "financial news",
      "predic-tive models",
      "tree representation",
      "text representations",
      "features"
    ],
    "types": "<method> <task> <task> <method> <method> <task> <material> <method> <otherscientificterm> <material> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "polarity task -- EVALUATE-FOR -- semantic frame parsing",
      "polarity task -- EVALUATE-FOR -- features",
      "support vector machines -- USED-FOR -- tree representation",
      "semantic frame parsers -- USED-FOR -- general nlp problems",
      "binary classification tasks -- FEATURE-OF -- text representations",
      "semantic frame parsing -- USED-FOR -- features",
      "support vector machines -- USED-FOR -- tree kernels"
    ],
    "abstract": "semantic frames are a rich <material_6> . there has been much work on <method_4> , but less that applies them to <task_2> . we address a task to predict change in stock price from <material_9> . <method_7> help to generalize from specific sentences to scenarios , and to detect the -lrb- positive or negative -rrb- roles of specific companies . we introduce a novel <method_11> , and use <method_11> to train <method_10> with <otherscientificterm_8> using <method_3> . our experiments test multiple <method_12> on two <task_1> , change of price and polarity . experiments show that <otherscientificterm_13> derived from <method_0> have significantly better performance across years on the <task_5> .",
    "abstract_og": "semantic frames are a rich linguistic resource . there has been much work on semantic frame parsers , but less that applies them to general nlp problems . we address a task to predict change in stock price from financial news . semantic frames help to generalize from specific sentences to scenarios , and to detect the -lrb- positive or negative -rrb- roles of specific companies . we introduce a novel tree representation , and use tree representation to train predic-tive models with tree kernels using support vector machines . our experiments test multiple text representations on two binary classification tasks , change of price and polarity . experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task ."
  },
  {
    "title": "Phase recovery in NMF for audio source separation : an insightful benchmark .",
    "entities": [
      "nonnegative matrix factorization",
      "time-frequency domain",
      "nmf-based source separation techniques",
      "supervised model learning",
      "phase recovery",
      "audio signals",
      "estimation methods",
      "source separation",
      "prior information",
      "audible artifacts",
      "expressive power",
      "blind separation",
      "oracle separation"
    ],
    "types": "<method> <material> <method> <method> <task> <material> <method> <task> <otherscientificterm> <material> <otherscientificterm> <task> <task>",
    "relations": [
      "prior information -- CONJUNCTION -- supervised model learning",
      "prior information -- CONJUNCTION -- oracle separation",
      "supervised model learning -- CONJUNCTION -- oracle separation",
      "phase recovery -- USED-FOR -- nmf-based source separation techniques"
    ],
    "abstract": "nonnegative matrix factorization -lrb- nmf -rrb- is a powerful tool for decomposing mixtures of <material_5> in the <material_1> . in applications such as <task_7> , the <task_4> for each extracted component is a major issue since it often leads to <material_9> . in this paper , we present a methodology for evaluating various <method_2> involving <task_4> . for each model considered , a comparison between two approaches -lrb- <task_11> without <otherscientificterm_8> and <task_12> with <method_3> -rrb- is performed , in order to inquire about the room for improvement for the <method_6> . experimental results show that the high resolution nmf -lrb- hrnmf -rrb- model is particularly promising , because it is able to take phases and correlations over time into account with a great <otherscientificterm_10> .",
    "abstract_og": "nonnegative matrix factorization -lrb- nmf -rrb- is a powerful tool for decomposing mixtures of audio signals in the time-frequency domain . in applications such as source separation , the phase recovery for each extracted component is a major issue since it often leads to audible artifacts . in this paper , we present a methodology for evaluating various nmf-based source separation techniques involving phase recovery . for each model considered , a comparison between two approaches -lrb- blind separation without prior information and oracle separation with supervised model learning -rrb- is performed , in order to inquire about the room for improvement for the estimation methods . experimental results show that the high resolution nmf -lrb- hrnmf -rrb- model is particularly promising , because it is able to take phases and correlations over time into account with a great expressive power ."
  },
  {
    "title": "A wavelet based noise reduction algorithm for speech signal corrupted by coloured noise .",
    "entities": [
      "node dependent wavelet threshold-ing approach",
      "voiced or unvoiced nature",
      "common level dependent method",
      "infinitely smooth soft threshold",
      "decomposition tree",
      "noise power",
      "recursive method",
      "coloured noises",
      "speech signals",
      "japanese database",
      "node"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <material> <material> <otherscientificterm>",
    "relations": [
      "recursive method -- USED-FOR -- noise power",
      "japanese database -- EVALUATE-FOR -- node dependent wavelet threshold-ing approach"
    ],
    "abstract": "in this paper , we present a <method_0> in order to remove strongly <material_7> from <material_8> . the <otherscientificterm_5> in each <otherscientificterm_10> is first estimated using a <method_6> . given the <otherscientificterm_1> of the frame , the signal is expanded onto a predefined best basis . then a <otherscientificterm_3> is applied depending on each <otherscientificterm_10> of the <otherscientificterm_4> . finally the estimated clean signal is reconstructed . experimental results on a <material_9> , for various <material_7> , demonstrate the effectiveness of the proposed <method_0> , even at low snr . compared with the <method_2> , this <method_0> provides better denoising results .",
    "abstract_og": "in this paper , we present a node dependent wavelet threshold-ing approach in order to remove strongly coloured noises from speech signals . the noise power in each node is first estimated using a recursive method . given the voiced or unvoiced nature of the frame , the signal is expanded onto a predefined best basis . then a infinitely smooth soft threshold is applied depending on each node of the decomposition tree . finally the estimated clean signal is reconstructed . experimental results on a japanese database , for various coloured noises , demonstrate the effectiveness of the proposed node dependent wavelet threshold-ing approach , even at low snr . compared with the common level dependent method , this node dependent wavelet threshold-ing approach provides better denoising results ."
  },
  {
    "title": "Non-Gaussian Component Analysis : a Semi-parametric Framework for Linear Dimension Reduction .",
    "entities": [
      "ngca -lrb- non-gaussian component analysis -rrb-",
      "data analysis process",
      "high dimensional data",
      "data visualization",
      "semi-parametric framework",
      "non-gaussian components",
      "projection methods",
      "non-gaussian subspace",
      "dimension reduction",
      "paramet-ric rate",
      "ngca components",
      "denoising",
      "clustering",
      "classification"
    ],
    "types": "<method> <task> <material> <task> <method> <otherscientificterm> <method> <otherscientificterm> <task> <metric> <method> <otherscientificterm> <task> <task>",
    "relations": [
      "clustering -- CONJUNCTION -- denoising",
      "data visualization -- CONJUNCTION -- clustering",
      "data visualization -- HYPONYM-OF -- data analysis process",
      "clustering -- HYPONYM-OF -- data analysis process",
      "semi-parametric framework -- USED-FOR -- ngca -lrb- non-gaussian component analysis -rrb-",
      "ngca -lrb- non-gaussian component analysis -rrb- -- USED-FOR -- dimension reduction",
      "ngca -lrb- non-gaussian component analysis -rrb- -- USED-FOR -- non-gaussian components",
      "denoising -- HYPONYM-OF -- data analysis process",
      "classification -- HYPONYM-OF -- data analysis process",
      "denoising -- CONJUNCTION -- classification"
    ],
    "abstract": "we propose a new <method_0> for <task_8> to identify <otherscientificterm_5> in <material_2> . our <method_0> , <method_0> , uses a very general <method_4> . in contrast to existing <method_6> we define what is uninteresting -lrb- gaussian -rrb- : by projecting out uninterestingness , we can estimate the relevant <otherscientificterm_7> . we show that the estimation error of finding the <otherscientificterm_5> tends to zero at a <metric_9> . once <method_10> are identified and extracted , various tasks can be applied in the <task_1> , like <task_3> , <task_12> , <otherscientificterm_11> or <task_13> . a numerical study demonstrates the usefulness of our <method_0> .",
    "abstract_og": "we propose a new ngca -lrb- non-gaussian component analysis -rrb- for dimension reduction to identify non-gaussian components in high dimensional data . our ngca -lrb- non-gaussian component analysis -rrb- , ngca -lrb- non-gaussian component analysis -rrb- , uses a very general semi-parametric framework . in contrast to existing projection methods we define what is uninteresting -lrb- gaussian -rrb- : by projecting out uninterestingness , we can estimate the relevant non-gaussian subspace . we show that the estimation error of finding the non-gaussian components tends to zero at a paramet-ric rate . once ngca components are identified and extracted , various tasks can be applied in the data analysis process , like data visualization , clustering , denoising or classification . a numerical study demonstrates the usefulness of our ngca -lrb- non-gaussian component analysis -rrb- ."
  },
  {
    "title": "High dynamic range image tone mapping by maximizing a structural fidelity measure .",
    "entities": [
      "tone mapping operators",
      "visually appealing images",
      "tone mapping approach",
      "gradient ascent direction",
      "computational structure",
      "objective measure"
    ],
    "types": "<method> <material> <method> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "computational structure -- USED-FOR -- tone mapping operators",
      "tone mapping operators -- USED-FOR -- visually appealing images"
    ],
    "abstract": "tone mapping operators -lrb- <method_0> -rrb- that convert high dynamic range -lrb- hdr -rrb- images to standard low dynamic range -lrb- ldr -rrb- images are highly desirable for the visualization of these images on standard displays . although many existing <method_0> produce <material_1> , it is until recently validated objective measures that can assess their quality have been proposed . without such objective measures , the design of traditional <method_0> can only be based on intuitive ideas , lacking clear goals for further improvement . in this paper , we propose a substantially different <method_2> , where instead of explicitly designing a new <otherscientificterm_4> for <method_0> , we search in the space of images to find better quality images in terms of a recent <metric_5> that can assess the structural fidelity between two images of different dynamic ranges . specifically , starting from any initial image , the proposed algorithm moves the image along the <otherscientificterm_3> and stops until it converges to a maximal point . our experiments show that the proposed algorithm reliably produces better quality images upon a number of state-of-the-art <method_0> .",
    "abstract_og": "tone mapping operators -lrb- tone mapping operators -rrb- that convert high dynamic range -lrb- hdr -rrb- images to standard low dynamic range -lrb- ldr -rrb- images are highly desirable for the visualization of these images on standard displays . although many existing tone mapping operators produce visually appealing images , it is until recently validated objective measures that can assess their quality have been proposed . without such objective measures , the design of traditional tone mapping operators can only be based on intuitive ideas , lacking clear goals for further improvement . in this paper , we propose a substantially different tone mapping approach , where instead of explicitly designing a new computational structure for tone mapping operators , we search in the space of images to find better quality images in terms of a recent objective measure that can assess the structural fidelity between two images of different dynamic ranges . specifically , starting from any initial image , the proposed algorithm moves the image along the gradient ascent direction and stops until it converges to a maximal point . our experiments show that the proposed algorithm reliably produces better quality images upon a number of state-of-the-art tone mapping operators ."
  },
  {
    "title": "A Class-Based Agreement Model for Generating Accurately Inflected Translations .",
    "entities": [
      "target-side , class-based agreement model",
      "bitext or phrase table annotations",
      "weakly inflected source language",
      "richer grammatical features",
      "fine-grained morpho-syntactic classes",
      "morpho-syntactic agreement errors",
      "translation hypothesis",
      "english-to-arabic translation",
      "phrase-based decoders",
      "decoding",
      "feature",
      "english",
      "agreement"
    ],
    "types": "<method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <task> <otherscientificterm> <material> <method>",
    "relations": [
      "fine-grained morpho-syntactic classes -- USED-FOR -- decoding",
      "decoding -- USED-FOR -- translation hypothesis",
      "english -- HYPONYM-OF -- weakly inflected source language"
    ],
    "abstract": "when automatically translating from a <material_2> like <material_11> to a target language with <otherscientificterm_3> such as gender and dual number , the output commonly contains <otherscientificterm_5> . to address this issue , we present a <method_0> . <method_12> is promoted by scoring a sequence of <otherscientificterm_4> that are predicted during <task_9> for each <otherscientificterm_6> . for <task_7> , our <method_0> yields a +1.04 bleu average improvement over a state-of-the-art baseline . the <method_0> does not require <otherscientificterm_1> and can be easily implemented as a <otherscientificterm_10> in many <method_8> .",
    "abstract_og": "when automatically translating from a weakly inflected source language like english to a target language with richer grammatical features such as gender and dual number , the output commonly contains morpho-syntactic agreement errors . to address this issue , we present a target-side , class-based agreement model . agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis . for english-to-arabic translation , our target-side , class-based agreement model yields a +1.04 bleu average improvement over a state-of-the-art baseline . the target-side , class-based agreement model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders ."
  },
  {
    "title": "Articulated and Restricted Motion Subspaces and Their Signatures .",
    "entities": [
      "extraction of the corresponding motion parameters",
      "articulated or otherwise restricted motion",
      "allowable translation directions",
      "static 3d reconstruction",
      "real data sets",
      "dynamic semantics",
      "algebraic constraints",
      "rotating blackboard",
      "low-rank constraints",
      "fixed axes",
      "articulated objects",
      "motion-restricted parts",
      "motion parameters",
      "matrix manipulations",
      "rigid parts",
      "linear algebra",
      "relative transformations",
      "linear subspaces",
      "restricted motion",
      "rotation axes",
      "automatic detection",
      "signature"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "rotation axes -- HYPONYM-OF -- dynamic semantics",
      "dynamic semantics -- FEATURE-OF -- static 3d reconstruction",
      "automatic detection -- USED-FOR -- static 3d reconstruction",
      "articulated or otherwise restricted motion -- USED-FOR -- automatic detection",
      "rotating blackboard -- HYPONYM-OF -- real data sets",
      "matrix manipulations -- FEATURE-OF -- motion parameters",
      "low-rank constraints -- HYPONYM-OF -- algebraic constraints",
      "signature -- USED-FOR -- relative transformations",
      "allowable translation directions -- HYPONYM-OF -- dynamic semantics",
      "rotation axes -- CONJUNCTION -- allowable translation directions"
    ],
    "abstract": "articulated objects represent an important class of objects in our everyday environment . <task_20> of the type of <otherscientificterm_1> and <otherscientificterm_0> are therefore of high value , e.g. in order to augment an otherwise <task_3> with <otherscientificterm_5> , such as <otherscientificterm_19> and <otherscientificterm_2> for certain <otherscientificterm_14> or objects . hence , in this paper , a novel theory to analyse <otherscientificterm_16> between two <otherscientificterm_11> will be presented . the analysis is based on <otherscientificterm_17> spanned by <otherscientificterm_16> . moreover , a <otherscientificterm_21> for <otherscientificterm_16> will be introduced which uniquely specifies the type of <otherscientificterm_18> encoded in these <otherscientificterm_16> . this theoretic framework enables the derivation of novel <otherscientificterm_6> , such as <otherscientificterm_8> for subsequent rotations around two <otherscientificterm_9> for example . lastly , given the type of <otherscientificterm_18> as predicted by the <otherscientificterm_21> , the paper shows how to extract all the <otherscientificterm_12> with <otherscientificterm_13> from <otherscientificterm_15> . our theory is verified on several <material_4> , such as a <otherscientificterm_7> or a wheel rolling on the floor amongst others .",
    "abstract_og": "articulated objects represent an important class of objects in our everyday environment . automatic detection of the type of articulated or otherwise restricted motion and extraction of the corresponding motion parameters are therefore of high value , e.g. in order to augment an otherwise static 3d reconstruction with dynamic semantics , such as rotation axes and allowable translation directions for certain rigid parts or objects . hence , in this paper , a novel theory to analyse relative transformations between two motion-restricted parts will be presented . the analysis is based on linear subspaces spanned by relative transformations . moreover , a signature for relative transformations will be introduced which uniquely specifies the type of restricted motion encoded in these relative transformations . this theoretic framework enables the derivation of novel algebraic constraints , such as low-rank constraints for subsequent rotations around two fixed axes for example . lastly , given the type of restricted motion as predicted by the signature , the paper shows how to extract all the motion parameters with matrix manipulations from linear algebra . our theory is verified on several real data sets , such as a rotating blackboard or a wheel rolling on the floor amongst others ."
  },
  {
    "title": "Comparing features for forming music streams in automatic music transcription .",
    "entities": [
      "formating temporal sequences of notes",
      "timbre of musical instruments",
      "pitch relation consistency",
      "music stream formation",
      "polyphonic music",
      "direction proximity",
      "pitch transition",
      "timbre similarity",
      "frequency components",
      "power-related features",
      "timber extraction",
      "features"
    ],
    "types": "<task> <otherscientificterm> <metric> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "direction proximity -- CONJUNCTION -- pitch transition",
      "timbre similarity -- EVALUATE-FOR -- music stream formation",
      "features -- CONJUNCTION -- direction proximity",
      "pitch transition -- CONJUNCTION -- pitch relation consistency",
      "features -- CONJUNCTION -- timbre similarity",
      "timbre similarity -- CONJUNCTION -- pitch transition",
      "power-related features -- USED-FOR -- timber extraction",
      "timbre similarity -- CONJUNCTION -- direction proximity"
    ],
    "abstract": "in <task_0> played by the same instrument -lrb- referred to as music streams -rrb- , <otherscientificterm_1> may be a predominant feature . in <material_4> , the performance of <task_10> based on <otherscientificterm_9> deteriorates , because such <otherscientificterm_11> are blurred when two or more <method_8> are superimposed in the same frequency . to cope with this problem , we integrated <otherscientificterm_7> and <otherscientificterm_5> with success , but left using other <otherscientificterm_11> as future work . in this paper , we investigate four <otherscientificterm_11> , <otherscientificterm_7> , <otherscientificterm_5> , <otherscientificterm_6> and <metric_2> to clarify the precedence among them in <task_3> . experimental results with quartet music show that <otherscientificterm_5> is the most dominant feature , and <otherscientificterm_6> is the secondary . in addition , the performance of <task_3> was improved from 63.3 % by only <otherscientificterm_7> to 84.9 % by integrating four <otherscientificterm_11> .",
    "abstract_og": "in formating temporal sequences of notes played by the same instrument -lrb- referred to as music streams -rrb- , timbre of musical instruments may be a predominant feature . in polyphonic music , the performance of timber extraction based on power-related features deteriorates , because such features are blurred when two or more frequency components are superimposed in the same frequency . to cope with this problem , we integrated timbre similarity and direction proximity with success , but left using other features as future work . in this paper , we investigate four features , timbre similarity , direction proximity , pitch transition and pitch relation consistency to clarify the precedence among them in music stream formation . experimental results with quartet music show that direction proximity is the most dominant feature , and pitch transition is the secondary . in addition , the performance of music stream formation was improved from 63.3 % by only timbre similarity to 84.9 % by integrating four features ."
  },
  {
    "title": "Set-Theoretic Alignment for Comparable Corpora .",
    "entities": [
      "jaccard similarity coefficient",
      "parallel sentences",
      "comparable corpora",
      "noisiest datasets",
      "stacc",
      "board"
    ],
    "types": "<otherscientificterm> <material> <material> <material> <method> <otherscientificterm>",
    "relations": [
      "noisiest datasets -- EVALUATE-FOR -- stacc",
      "jaccard similarity coefficient -- USED-FOR -- stacc"
    ],
    "abstract": "we describe and evaluate a simple method to extract <material_1> from <material_2> . the approach , termed <method_4> , is based on expanded lexical sets and the <otherscientificterm_0> . we evaluate our <method_4> against state-of-the-art methods on a large range of datasets in different domains , for ten language pairs , showing that <method_4> either matches or outper-forms current methods across the <otherscientificterm_5> and gives significantly better results on the <material_3> . <method_4> is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of <material_1> in <material_2> .",
    "abstract_og": "we describe and evaluate a simple method to extract parallel sentences from comparable corpora . the approach , termed stacc , is based on expanded lexical sets and the jaccard similarity coefficient . we evaluate our stacc against state-of-the-art methods on a large range of datasets in different domains , for ten language pairs , showing that stacc either matches or outper-forms current methods across the board and gives significantly better results on the noisiest datasets . stacc is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora ."
  },
  {
    "title": "Comparisons of sequence labeling algorithms and extensions .",
    "entities": [
      "hidden markov model",
      "conditional random fields",
      "averaged perceptron",
      "search and learning algorithm",
      "max margin markov networks",
      "structured learning models",
      "structured learning problems",
      "state-of-art models"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <task> <method>",
    "relations": [
      "hidden markov model -- CONJUNCTION -- conditional random fields",
      "hidden markov model -- CONJUNCTION -- averaged perceptron",
      "state-of-art models -- USED-FOR -- structured learning problems"
    ],
    "abstract": "in this paper , we survey the current <method_7> for <task_6> , including <method_0> , <method_1> , <method_2> , structured svms -lrb- <i> svm <sup> struct </sup> -rrb- </i> , <method_4> -lrb- m <sup> 3 </sup> n -rrb- , and an integration of <method_3> -lrb- searn -rrb- . with all due tuning efforts of various parameters of each <method_7> , on the data sets we have applied the <method_7> to , we found that svm <i> <sup> struct </sup> </i> enjoys better performance compared with the others . in addition , we also propose a new method which we call the structured learning ensemble -lrb- sle -rrb- to combine these <method_5> . empirical results show that our sle algorithm provides more accurate solutions compared with the best results of the individual <method_7> .",
    "abstract_og": "in this paper , we survey the current state-of-art models for structured learning problems , including hidden markov model , conditional random fields , averaged perceptron , structured svms -lrb- <i> svm <sup> struct </sup> -rrb- </i> , max margin markov networks -lrb- m <sup> 3 </sup> n -rrb- , and an integration of search and learning algorithm -lrb- searn -rrb- . with all due tuning efforts of various parameters of each state-of-art models , on the data sets we have applied the state-of-art models to , we found that svm <i> <sup> struct </sup> </i> enjoys better performance compared with the others . in addition , we also propose a new method which we call the structured learning ensemble -lrb- sle -rrb- to combine these structured learning models . empirical results show that our sle algorithm provides more accurate solutions compared with the best results of the individual state-of-art models ."
  },
  {
    "title": "Dramatis : A Computational Model of Suspense .",
    "entities": [
      "computational model of suspense",
      "dramatis components",
      "suspense phenomenon",
      "suspense level",
      "suspense ratings",
      "dramatis"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "dramatis -- HYPONYM-OF -- computational model of suspense"
    ],
    "abstract": "we introduce <method_5> , a <method_0> based on a reformulation of a psychological definition of the <otherscientificterm_2> . in this reformulation , suspense is correlated with the audience 's ability to generate a plan for the protagonist to avoid an impending negative outcome . <method_5> measures the <otherscientificterm_3> by generating such a plan and determining its perceived likelihood of success . we report on three evaluations of <method_5> , including a comparison of <method_5> output to the suspense reported by human readers , as well as ablative tests of <method_1> . in these studies , we found that <method_5> output corresponded to the <otherscientificterm_4> given by human readers for stories in three separate domains .",
    "abstract_og": "we introduce dramatis , a computational model of suspense based on a reformulation of a psychological definition of the suspense phenomenon . in this reformulation , suspense is correlated with the audience 's ability to generate a plan for the protagonist to avoid an impending negative outcome . dramatis measures the suspense level by generating such a plan and determining its perceived likelihood of success . we report on three evaluations of dramatis , including a comparison of dramatis output to the suspense reported by human readers , as well as ablative tests of dramatis components . in these studies , we found that dramatis output corresponded to the suspense ratings given by human readers for stories in three separate domains ."
  },
  {
    "title": "Estimation , Optimization , and Parallelism when Data is Sparse .",
    "entities": [
      "large-scale learning tasks",
      "high-dimensional statistical learning",
      "stochastic optimization problems",
      "sample complexity",
      "sparse data",
      "minimax rate",
      "learning",
      "sparsity",
      "optimization",
      "parallelism"
    ],
    "types": "<task> <task> <task> <metric> <material> <metric> <task> <method> <task> <otherscientificterm>",
    "relations": [
      "minimax rate -- EVALUATE-FOR -- optimization",
      "optimization -- CONJUNCTION -- learning"
    ],
    "abstract": "we study <task_2> when the data is sparse , which is in a sense dual to current perspectives on <task_1> and <task_8> . we highlight both the difficulties -- in terms of increased <metric_3> that <material_4> necessitates -- and the potential benefits , in terms of allowing <otherscientificterm_9> and asynchrony in the design of algorithms . concretely , we derive matching upper and lower bounds on the <metric_5> for <task_8> and <task_6> with <material_4> , and we exhibit algorithms achieving these rates . we also show how leveraging <method_7> leads to -lrb- still minimax optimal -rrb- parallel and asynchronous algorithms , providing experimental evidence complementing our theoretical results on several medium to <task_0> .",
    "abstract_og": "we study stochastic optimization problems when the data is sparse , which is in a sense dual to current perspectives on high-dimensional statistical learning and optimization . we highlight both the difficulties -- in terms of increased sample complexity that sparse data necessitates -- and the potential benefits , in terms of allowing parallelism and asynchrony in the design of algorithms . concretely , we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data , and we exhibit algorithms achieving these rates . we also show how leveraging sparsity leads to -lrb- still minimax optimal -rrb- parallel and asynchronous algorithms , providing experimental evidence complementing our theoretical results on several medium to large-scale learning tasks ."
  },
  {
    "title": "Flexible Modeling of Latent Task Structures in Multitask Learning .",
    "entities": [
      "flexible , nonparametric bayesian model",
      "low-rank or linear/non-linear subspace assumption",
      "regression and classification problems",
      "priori known latent structure",
      "synthetic and real-world datasets",
      "vari-ational inference algorithm",
      "latent task structures",
      "multitask learning problem",
      "factor analyzers structure",
      "latent task structure",
      "multitask learning algorithms",
      "data-driven manner",
      "nonparametric aspect",
      "mean-regularized tasks",
      "clustered tasks"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <material> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "mean-regularized tasks -- CONJUNCTION -- clustered tasks",
      "latent task structure -- USED-FOR -- multitask learning problem",
      "regression and classification problems -- EVALUATE-FOR -- vari-ational inference algorithm",
      "vari-ational inference algorithm -- USED-FOR -- flexible , nonparametric bayesian model",
      "synthetic and real-world datasets -- EVALUATE-FOR -- vari-ational inference algorithm",
      "nonparametric aspect -- USED-FOR -- flexible , nonparametric bayesian model"
    ],
    "abstract": "multitask learning algorithms are typically designed assuming some fixed , a <otherscientificterm_3> shared by all the tasks . however , <method_0> is usually unclear what type of <otherscientificterm_9> is the most appropriate for a given <task_7> . ideally , the '' right '' <otherscientificterm_9> should be learned in a <method_11> . we present a <method_0> that posits a mixture of <otherscientificterm_8> on the tasks . the <method_12> makes the <method_0> expressive enough to subsume many existing <method_0> of <otherscientificterm_6> -lrb- e.g , <otherscientificterm_13> , <otherscientificterm_14> , <otherscientificterm_1> on tasks , etc. -rrb- . moreover , <method_0> can also learn more general task structures , addressing the shortcomings of such <method_0> . we present a <method_5> for our <method_0> . experimental results on <material_4> , on both <task_2> , demonstrate the effectiveness of the proposed <method_5> .",
    "abstract_og": "multitask learning algorithms are typically designed assuming some fixed , a priori known latent structure shared by all the tasks . however , flexible , nonparametric bayesian model is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem . ideally , the '' right '' latent task structure should be learned in a data-driven manner . we present a flexible , nonparametric bayesian model that posits a mixture of factor analyzers structure on the tasks . the nonparametric aspect makes the flexible , nonparametric bayesian model expressive enough to subsume many existing flexible , nonparametric bayesian model of latent task structures -lrb- e.g , mean-regularized tasks , clustered tasks , low-rank or linear/non-linear subspace assumption on tasks , etc. -rrb- . moreover , flexible , nonparametric bayesian model can also learn more general task structures , addressing the shortcomings of such flexible , nonparametric bayesian model . we present a vari-ational inference algorithm for our flexible , nonparametric bayesian model . experimental results on synthetic and real-world datasets , on both regression and classification problems , demonstrate the effectiveness of the proposed vari-ational inference algorithm ."
  },
  {
    "title": "State based sub-band Wiener filters for speech enhancement in car environments .",
    "entities": [
      "bmw and volvo car noise databases",
      "temporal-spectral composition of speech",
      "timit continuous speech database",
      "estimation of parameters",
      "subband wiener filters",
      "wiener filter structure",
      "parallel model combination",
      "correlation values",
      "speech processes",
      "power spectra",
      "bayesian method",
      "wiener filters",
      "parameter estimates",
      "noisy speech",
      "model decomposition",
      "noise",
      "accuracy",
      "quality"
    ],
    "types": "<material> <material> <material> <otherscientificterm> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <method> <method> <method> <material> <task> <otherscientificterm> <metric> <metric>",
    "relations": [
      "subband wiener filters -- USED-FOR -- temporal-spectral composition of speech",
      "timit continuous speech database -- USED-FOR -- parallel model combination",
      "estimation of parameters -- USED-FOR -- subband wiener filters",
      "model decomposition -- USED-FOR -- estimation of parameters",
      "subband wiener filters -- USED-FOR -- parameter estimates",
      "bmw and volvo car noise databases -- FEATURE-OF -- timit continuous speech database"
    ],
    "abstract": "the performance of <method_11> in restoring the <metric_17> and intelligibility of <material_13> depends on : -lrb- i -rrb- the <metric_16> of the estimates of the <otherscientificterm_9> or the <metric_7> of the <otherscientificterm_15> and the <otherscientificterm_8> , and -lrb- ii -rrb- on the <otherscientificterm_5> . in this paper a <method_10> is proposed where model combination and <task_14> are employed for the <otherscientificterm_3> required to implement <method_4> . the use of <method_4> provides advantages in terms of improved <method_12> and also in restoring the <material_1> . the <method_10> is evaluated , and compared with the <method_6> , using the <material_2> with <material_0> .",
    "abstract_og": "the performance of wiener filters in restoring the quality and intelligibility of noisy speech depends on : -lrb- i -rrb- the accuracy of the estimates of the power spectra or the correlation values of the noise and the speech processes , and -lrb- ii -rrb- on the wiener filter structure . in this paper a bayesian method is proposed where model combination and model decomposition are employed for the estimation of parameters required to implement subband wiener filters . the use of subband wiener filters provides advantages in terms of improved parameter estimates and also in restoring the temporal-spectral composition of speech . the bayesian method is evaluated , and compared with the parallel model combination , using the timit continuous speech database with bmw and volvo car noise databases ."
  },
  {
    "title": "Effect of prosodic changes on speech intelligibility .",
    "entities": [
      "prosody-related intelligibility gain",
      "word identification rate",
      "explicit instruction",
      "speech modifications",
      "durational modifications",
      "acous-tic/prosodic parameters",
      "energetic masking",
      "relative intelligibility",
      "plain speech",
      "energetic masking",
      "environmental noise",
      "speech styles",
      "prosodic changes",
      "unfilled pauses",
      "durational increases",
      "intelligibility",
      "elongations"
    ],
    "types": "<otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "environmental noise -- CONJUNCTION -- explicit instruction",
      "plain speech -- COMPARE -- durational modifications",
      "speech modifications -- CONJUNCTION -- intelligibility"
    ],
    "abstract": "talkers adopt different <otherscientificterm_11> in response to factors such as the perceived needs of the interlocutor , <otherscientificterm_10> and <otherscientificterm_2> . some styles have been shown to be beneficial for listeners but many aspects of the relationship between <otherscientificterm_3> and <otherscientificterm_15> remain unclear , particularly for <otherscientificterm_12> . the current study measures the <otherscientificterm_7> in noise of speech spoken in 5 <otherscientificterm_11> -- plain , infant - , computer-and foreigner-directed , and shouted -- and relates listener scores to <otherscientificterm_5> and quantitative estimates of <task_6> . intelligibility changes over <material_8> correlated well with <method_4> , which included <otherscientificterm_16> of all segments as well as increases in the number of <otherscientificterm_13> . both mean fundamental frequency and its range displayed great variation across styles but with no clear <otherscientificterm_15> benefits . <task_9> per unit time was similar in each style but the total amount of speech which escaped masking was a good predictor of <metric_1> . these findings suggest that much of the <otherscientificterm_0> is derived from <otherscientificterm_14> .",
    "abstract_og": "talkers adopt different speech styles in response to factors such as the perceived needs of the interlocutor , environmental noise and explicit instruction . some styles have been shown to be beneficial for listeners but many aspects of the relationship between speech modifications and intelligibility remain unclear , particularly for prosodic changes . the current study measures the relative intelligibility in noise of speech spoken in 5 speech styles -- plain , infant - , computer-and foreigner-directed , and shouted -- and relates listener scores to acous-tic/prosodic parameters and quantitative estimates of energetic masking . intelligibility changes over plain speech correlated well with durational modifications , which included elongations of all segments as well as increases in the number of unfilled pauses . both mean fundamental frequency and its range displayed great variation across styles but with no clear intelligibility benefits . energetic masking per unit time was similar in each style but the total amount of speech which escaped masking was a good predictor of word identification rate . these findings suggest that much of the prosody-related intelligibility gain is derived from durational increases ."
  },
  {
    "title": "Wide-angle micro sensors for vision on a tight budget .",
    "entities": [
      "power and mass constraints",
      "computer vision tasks",
      "template-based optical convolution",
      "miniature vision sensors",
      "computer vision",
      "micro-scale devices",
      "analytic tools",
      "optical design",
      "detecting faces",
      "tracking targets",
      "milli-scale prototypes",
      "power requirements",
      "design space",
      "convolution",
      "edges",
      "volume"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <method> <task> <task> <method> <method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "milli-scale prototypes -- USED-FOR -- computer vision tasks",
      "volume -- CONJUNCTION -- miniature vision sensors",
      "detecting faces -- HYPONYM-OF -- computer vision tasks",
      "micro-scale devices -- USED-FOR -- computer vision",
      "tracking targets -- CONJUNCTION -- detecting faces",
      "miniature vision sensors -- USED-FOR -- power requirements",
      "template-based optical convolution -- USED-FOR -- miniature vision sensors",
      "analytic tools -- USED-FOR -- design space",
      "tracking targets -- HYPONYM-OF -- computer vision tasks"
    ],
    "abstract": "achieving <task_4> on <task_5> is a challenge . on these <task_4> , the <otherscientificterm_0> are severe enough for even the most common computations -lrb- matrix manipulations , <otherscientificterm_13> , etc. -rrb- to be difficult . this paper proposes and analyzes a class of <method_3> that can help overcome these constraints . these <method_3> reduce <otherscientificterm_11> through <otherscientificterm_2> , and <method_3> enable a wide field-of-view within a small form through a novel <method_7> . we describe the trade-offs between the field of view , <otherscientificterm_15> , and mass of these <method_3> and we provide <method_6> to navigate the <otherscientificterm_12> . we also demonstrate <method_10> for <task_1> such as locating <otherscientificterm_14> , <task_9> , and <task_8> .",
    "abstract_og": "achieving computer vision on micro-scale devices is a challenge . on these computer vision , the power and mass constraints are severe enough for even the most common computations -lrb- matrix manipulations , convolution , etc. -rrb- to be difficult . this paper proposes and analyzes a class of miniature vision sensors that can help overcome these constraints . these miniature vision sensors reduce power requirements through template-based optical convolution , and miniature vision sensors enable a wide field-of-view within a small form through a novel optical design . we describe the trade-offs between the field of view , volume , and mass of these miniature vision sensors and we provide analytic tools to navigate the design space . we also demonstrate milli-scale prototypes for computer vision tasks such as locating edges , tracking targets , and detecting faces ."
  },
  {
    "title": "Duality and Geometry in SVM Classifiers .",
    "entities": [
      "support vector machine",
      "linearly separable and inseparable data",
      "reduced convex hull formulation",
      "2-norm and 1-norm svm",
      "inseparable svm",
      "point algorithms",
      "maximum margin",
      "convex hulls",
      "classification"
    ],
    "types": "<method> <material> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "reduced convex hull formulation -- USED-FOR -- inseparable svm",
      "support vector machine -- USED-FOR -- classification"
    ],
    "abstract": "we develop an intuitive geometric interpretation of the standard <method_0> for <task_8> of both <material_1> and provide a rigorous derivation of the concepts behind the geometry . for the separable case finding the <otherscientificterm_6> between the two sets is equivalent to finding the closest points in the smallest convex sets that contain each class -lrb- the <otherscientificterm_7> -rrb- . we now extend this argument to the inseparable case by using a <otherscientificterm_2> reduced away from out-liers . we prove that solving the <otherscientificterm_2> is exactly equivalent to solving the standard <method_4> for appropriate choices of parameters . some additional advantages of the new formulation are that the effect of the choice of parameters becomes geometrically clear and that the formulation may be solved by fast nearest <method_5> . by changing norms these arguments hold for both the standard <method_3> .",
    "abstract_og": "we develop an intuitive geometric interpretation of the standard support vector machine for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concepts behind the geometry . for the separable case finding the maximum margin between the two sets is equivalent to finding the closest points in the smallest convex sets that contain each class -lrb- the convex hulls -rrb- . we now extend this argument to the inseparable case by using a reduced convex hull formulation reduced away from out-liers . we prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable svm for appropriate choices of parameters . some additional advantages of the new formulation are that the effect of the choice of parameters becomes geometrically clear and that the formulation may be solved by fast nearest point algorithms . by changing norms these arguments hold for both the standard 2-norm and 1-norm svm ."
  },
  {
    "title": "Rapidly building domain-specific entity-centric language models using semantic web knowledge sources .",
    "entities": [
      "domain-specific speech recognition tasks",
      "statistical language model component",
      "language modeling techniques",
      "speech recognition engines",
      "query click logs",
      "statistical language models",
      "web language model",
      "semantic web sources",
      "recognition system",
      "language model",
      "knowledge graphs",
      "movies domain",
      "n-gram models",
      "text data",
      "domain-specific knowledge",
      "voice queries",
      "crowd sourcing",
      "natural language",
      "first-pass decoding"
    ],
    "types": "<task> <method> <method> <method> <otherscientificterm> <method> <method> <material> <method> <method> <otherscientificterm> <material> <method> <material> <otherscientificterm> <material> <method> <material> <method>",
    "relations": [
      "text data -- USED-FOR -- statistical language model component",
      "query click logs -- CONJUNCTION -- knowledge graphs",
      "knowledge graphs -- HYPONYM-OF -- domain-specific knowledge",
      "query click logs -- HYPONYM-OF -- domain-specific knowledge",
      "knowledge graphs -- HYPONYM-OF -- semantic web sources",
      "statistical language models -- USED-FOR -- domain-specific speech recognition tasks",
      "statistical language model component -- USED-FOR -- domain-specific speech recognition tasks",
      "n-gram models -- HYPONYM-OF -- first-pass decoding",
      "speech recognition engines -- USED-FOR -- first-pass decoding",
      "n-gram models -- HYPONYM-OF -- speech recognition engines",
      "domain-specific knowledge -- USED-FOR -- semantic web sources"
    ],
    "abstract": "for <task_0> , it is best if the <method_1> is trained with <material_13> that is content-wise and style-wise similar to the targeted domain for which the application is built . for state-of-the-art <method_2> that can be used in real-time within <method_3> during <method_18> -lrb- e.g. , <method_12> -rrb- , the above constraints have to be fulfilled in the training data . however collecting such data , even through <method_16> , is expensive and time consuming , and can still be not representative of how a much larger user population would interact with the <method_8> . in this paper , we address this problem by employing several <material_7> that already contain the <otherscientificterm_14> , such as <otherscientificterm_4> and <otherscientificterm_10> . we build <method_5> that meet the requirements listed above for <task_0> where <material_17> is used and the user queries are about name entities in a specific domain . as a case study , in the <material_11> where users ' <material_15> are movie related , compared to a generic <method_6> , a <method_9> trained with the above resources not only yields significant perplexity and word-error-rate improvements , but also presents an approach where such <method_9> can be rapidly developed for other domains .",
    "abstract_og": "for domain-specific speech recognition tasks , it is best if the statistical language model component is trained with text data that is content-wise and style-wise similar to the targeted domain for which the application is built . for state-of-the-art language modeling techniques that can be used in real-time within speech recognition engines during first-pass decoding -lrb- e.g. , n-gram models -rrb- , the above constraints have to be fulfilled in the training data . however collecting such data , even through crowd sourcing , is expensive and time consuming , and can still be not representative of how a much larger user population would interact with the recognition system . in this paper , we address this problem by employing several semantic web sources that already contain the domain-specific knowledge , such as query click logs and knowledge graphs . we build statistical language models that meet the requirements listed above for domain-specific speech recognition tasks where natural language is used and the user queries are about name entities in a specific domain . as a case study , in the movies domain where users ' voice queries are movie related , compared to a generic web language model , a language model trained with the above resources not only yields significant perplexity and word-error-rate improvements , but also presents an approach where such language model can be rapidly developed for other domains ."
  },
  {
    "title": "A Consensus-Based Framework for Distributed Bundle Adjustment .",
    "entities": [
      "consensus based optimization methods",
      "bundle adjustment formulation",
      "bundle adjustment problem",
      "large-scale optimization problems",
      "real image datasets",
      "multi-view geometry",
      "structure-from-motion pipeline",
      "proximal splitting",
      "problem size",
      "concise derivation",
      "bundle adjustment",
      "complexity"
    ],
    "types": "<method> <method> <task> <task> <material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <metric>",
    "relations": [
      "multi-view geometry -- FEATURE-OF -- large-scale optimization problems",
      "proximal splitting -- USED-FOR -- concise derivation"
    ],
    "abstract": "in this paper we study <task_3> in <task_5> , in particular the <task_2> . in its conventional formulation , the <metric_11> of existing solvers scale poorly with <otherscientificterm_8> , hence this component of the <method_6> can quickly become a bottleneck . here we present a novel formulation for solving <task_10> in a truly distributed manner using <method_0> . our algorithm is presented with a <otherscientificterm_9> based on <otherscientificterm_7> , along with a theoretical proof of convergence and brief discussions on <metric_11> and implementation . experiments on a number of <material_4> convincingly demonstrates the potential of the proposed method by outperforming the conventional <method_1> by orders of magnitude .",
    "abstract_og": "in this paper we study large-scale optimization problems in multi-view geometry , in particular the bundle adjustment problem . in its conventional formulation , the complexity of existing solvers scale poorly with problem size , hence this component of the structure-from-motion pipeline can quickly become a bottleneck . here we present a novel formulation for solving bundle adjustment in a truly distributed manner using consensus based optimization methods . our algorithm is presented with a concise derivation based on proximal splitting , along with a theoretical proof of convergence and brief discussions on complexity and implementation . experiments on a number of real image datasets convincingly demonstrates the potential of the proposed method by outperforming the conventional bundle adjustment formulation by orders of magnitude ."
  },
  {
    "title": "Context as Supervisory Signal : Discovering Objects with Predictable Context .",
    "entities": [
      "iterative region prediction and context alignment approach",
      "visually consistent object clusters",
      "leave-one-out context prediction task",
      "weakly-supervised object discovery approaches",
      "statistical hypothesis testing",
      "visual object cluster",
      "stuff appearance models",
      "un-supervised object discovery",
      "weakly-supervised approaches",
      "context prediction",
      "keypoint annotations",
      "unsupervised clustering",
      "supervisory signal",
      "object patch",
      "fine-grained correspondences",
      "segmentation mask"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <method> <task> <method> <task> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "supervisory signal -- USED-FOR -- visually consistent object clusters",
      "segmentation mask -- CONJUNCTION -- fine-grained correspondences",
      "weakly-supervised approaches -- USED-FOR -- un-supervised object discovery",
      "iterative region prediction and context alignment approach -- USED-FOR -- visual object cluster",
      "statistical hypothesis testing -- USED-FOR -- context prediction"
    ],
    "abstract": "this paper addresses the well-established problem of <task_7> with a novel method inspired by <method_8> . in particular , the ability of an <method_13> to predict the rest of the object -lrb- its context -rrb- is used as <otherscientificterm_12> to help discover <otherscientificterm_1> . the main contributions of this work are : 1 -rrb- framing <method_11> as a <task_2> ; 2 -rrb- evaluating the quality of <task_9> by <method_4> between thing and <method_6> ; and 3 -rrb- an <method_0> that gradually discovers a <otherscientificterm_5> together with a <method_15> and <otherscientificterm_14> . the proposed method outperforms previous unsupervised as well as <method_3> , and is shown to provide correspondences detailed enough to transfer <task_10> .",
    "abstract_og": "this paper addresses the well-established problem of un-supervised object discovery with a novel method inspired by weakly-supervised approaches . in particular , the ability of an object patch to predict the rest of the object -lrb- its context -rrb- is used as supervisory signal to help discover visually consistent object clusters . the main contributions of this work are : 1 -rrb- framing unsupervised clustering as a leave-one-out context prediction task ; 2 -rrb- evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models ; and 3 -rrb- an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences . the proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches , and is shown to provide correspondences detailed enough to transfer keypoint annotations ."
  },
  {
    "title": "Lattice decoding and rescoring with long-Span neural network language models .",
    "entities": [
      "long-span neural network language models",
      "babel assamese keyword search",
      "lattice-based speech recognition techniques",
      "word error rate improvements",
      "refined pruning techniques",
      "full lattice rescoring",
      "lattice decoding",
      "search effort",
      "speech recognition",
      "search space",
      "lstms",
      "lattices"
    ],
    "types": "<method> <task> <method> <metric> <method> <task> <task> <metric> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "lstms -- USED-FOR -- babel assamese keyword search",
      "refined pruning techniques -- USED-FOR -- search effort"
    ],
    "abstract": "with <method_0> , considerable improvements have been obtained in <task_8> . however , it is difficult to apply these models if the underlying <otherscientificterm_9> is large . in this paper , we combine previous work on <task_6> with long short-term memory -lrb- lstm -rrb- neural network language models . by adding <method_4> , we are able to reduce the <metric_7> by a factor of three . furthermore , we introduce two novel approximations for <task_5> , which opens the potential of <method_2> . compared to 1000-best lists , we find that we can increase the <metric_3> obtained with <method_10> from 8.2 % to 10.7 % relative over a state-of-the-art baseline , while the resulting <otherscientificterm_11> are even considerably smaller . in addition , we investigate the use of <method_10> for <task_1> , obtaining significant improvements of 2.5 % relative .",
    "abstract_og": "with long-span neural network language models , considerable improvements have been obtained in speech recognition . however , it is difficult to apply these models if the underlying search space is large . in this paper , we combine previous work on lattice decoding with long short-term memory -lrb- lstm -rrb- neural network language models . by adding refined pruning techniques , we are able to reduce the search effort by a factor of three . furthermore , we introduce two novel approximations for full lattice rescoring , which opens the potential of lattice-based speech recognition techniques . compared to 1000-best lists , we find that we can increase the word error rate improvements obtained with lstms from 8.2 % to 10.7 % relative over a state-of-the-art baseline , while the resulting lattices are even considerably smaller . in addition , we investigate the use of lstms for babel assamese keyword search , obtaining significant improvements of 2.5 % relative ."
  },
  {
    "title": "Learning to Classify Observed Motor Behavior .",
    "entities": [
      "abstract movement concepts",
      "unsupervised learning system",
      "temporal structure",
      "representational format",
      "oxbow"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "unsupervised learning system -- USED-FOR -- abstract movement concepts",
      "oxbow -- HYPONYM-OF -- unsupervised learning system"
    ],
    "abstract": "we present a <method_3> for observed movements the representation has a <otherscientificterm_2> relating components of a single complex movement . we also present <method_4> , an <method_1> , which constructs classes of these movements . empirical results indicate that the <method_1> builds <otherscientificterm_0> with appropriate component structure allowing <method_1> to predict the latter portions of a partially observed movement .",
    "abstract_og": "we present a representational format for observed movements the representation has a temporal structure relating components of a single complex movement . we also present oxbow , an unsupervised learning system , which constructs classes of these movements . empirical results indicate that the unsupervised learning system builds abstract movement concepts with appropriate component structure allowing unsupervised learning system to predict the latter portions of a partially observed movement ."
  },
  {
    "title": "Backdoors into Heterogeneous Classes of SAT and CSP .",
    "entities": [
      "detecting strong and weak backdoor sets",
      "heterogeneous base classes",
      "heterogeneous base class",
      "backdoor sets",
      "complexity landscape",
      "backdoor variables",
      "csp"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "backdoor sets -- USED-FOR -- heterogeneous base classes",
      "complexity landscape -- USED-FOR -- detecting strong and weak backdoor sets"
    ],
    "abstract": "in this paper we extend the classical notion of strong and weak backdoor sets by allowing that different instantiations of the <otherscientificterm_5> result in instances that belong to different base classes ; the union of the base classes forms a <otherscientificterm_2> . <material_3> to <otherscientificterm_1> can be much smaller than backdoor sets to homogeneous ones , hence they are much more desirable but possibly harder to find . we draw a detailed <otherscientificterm_4> for the problem of <task_0> into <otherscientificterm_1> for sat and <material_6> .",
    "abstract_og": "in this paper we extend the classical notion of strong and weak backdoor sets by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes ; the union of the base classes forms a heterogeneous base class . backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones , hence they are much more desirable but possibly harder to find . we draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for sat and csp ."
  },
  {
    "title": "Recovery of Reflectances and Varying Illuminants from Multiple Views .",
    "entities": [
      "geometry of the scene",
      "linear and non-linear implementations",
      "geometric reconstruction techniques",
      "photometric stereo approaches",
      "surface albedoes",
      "real images",
      "unknown illuminants",
      "lambertian case",
      "radiometric reconstruction",
      "images"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <material> <task> <material>",
    "relations": [
      "images -- USED-FOR -- radiometric reconstruction"
    ],
    "abstract": "we introduce a new methodology for <task_8> from multiple <material_9> . it opens new possibilities because it allows simultaneous recovery of varying <otherscientificterm_6> -lrb- one per image -rrb- , <otherscientificterm_4> , and cameras ' radiometric responses . designed to complement <method_2> , it only requires as input the <otherscientificterm_0> and of the cameras . unlike <method_3> , it is not restricted to <material_9> taken from a single viewpoint . <method_1> in the <material_7> are proposed ; simulation results are discussed and compared to related work to demonstrate the gain in stability ; and results on <material_5> are shown .",
    "abstract_og": "we introduce a new methodology for radiometric reconstruction from multiple images . it opens new possibilities because it allows simultaneous recovery of varying unknown illuminants -lrb- one per image -rrb- , surface albedoes , and cameras ' radiometric responses . designed to complement geometric reconstruction techniques , it only requires as input the geometry of the scene and of the cameras . unlike photometric stereo approaches , it is not restricted to images taken from a single viewpoint . linear and non-linear implementations in the lambertian case are proposed ; simulation results are discussed and compared to related work to demonstrate the gain in stability ; and results on real images are shown ."
  },
  {
    "title": "Room impulse response shortening with infinity-norm optimization .",
    "entities": [
      "room impulse response shortening",
      "equiripple filter design method",
      "shortened global impulse response",
      "unwanted temporal range",
      "d50 measure",
      "shortening filter",
      "shortening filters",
      "design errors"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <metric> <method> <method> <otherscientificterm>",
    "relations": [
      "d50 measure -- USED-FOR -- shortening filter"
    ],
    "abstract": "the purpose of <method_0> is to improve the intelligibility of the received signal by pre-filtering the source signal before it is played with a loudspeaker in a closed room . in this paper , we propose to use the infinity-norm as optimization criterion for the design of <method_6> of <method_0> . similar to the <method_1> , <otherscientificterm_7> will be uniformly distributed over the <otherscientificterm_3> of the <otherscientificterm_2> . the <metric_4> is exploited during the design of the <method_5> , which makes it possible to significantly reduce the length of the prefilter without affecting the perceived performance .",
    "abstract_og": "the purpose of room impulse response shortening is to improve the intelligibility of the received signal by pre-filtering the source signal before it is played with a loudspeaker in a closed room . in this paper , we propose to use the infinity-norm as optimization criterion for the design of shortening filters of room impulse response shortening . similar to the equiripple filter design method , design errors will be uniformly distributed over the unwanted temporal range of the shortened global impulse response . the d50 measure is exploited during the design of the shortening filter , which makes it possible to significantly reduce the length of the prefilter without affecting the perceived performance ."
  },
  {
    "title": "Using Unlabeled Data for Supervised Learning .",
    "entities": [
      "asymptotic accuracy level",
      "supervised training framework",
      "unlabeled examples",
      "distribution information",
      "classification problems",
      "class label",
      "supervised learner",
      "accuracy"
    ],
    "types": "<metric> <method> <material> <otherscientificterm> <task> <otherscientificterm> <method> <metric>",
    "relations": [
      "accuracy -- EVALUATE-FOR -- supervised learner",
      "unlabeled examples -- USED-FOR -- distribution information"
    ],
    "abstract": "many <task_4> have the property that the only costly part of obtaining examples is the <otherscientificterm_5> . this paper suggests a simple method for using <otherscientificterm_3> contained in <material_2> to augment labeled examples in a <method_1> . empirical tests show that the technique described in this paper can significantly improve the <metric_7> of a <method_6> when the learner is well below its <metric_0> .",
    "abstract_og": "many classification problems have the property that the only costly part of obtaining examples is the class label . this paper suggests a simple method for using distribution information contained in unlabeled examples to augment labeled examples in a supervised training framework . empirical tests show that the technique described in this paper can significantly improve the accuracy of a supervised learner when the learner is well below its asymptotic accuracy level ."
  },
  {
    "title": "Discriminative Gaussian Mixture Models : A Comparison with Kernel Classifiers .",
    "entities": [
      "relevance vector machines",
      "support vector machines",
      "gaussian mixture models",
      "discrimi-native gmm classifier",
      "genera-tive gmm classifiers",
      "training procedure",
      "baum-welch algorithm",
      "kernel classifiers",
      "speech recognition",
      "classifier",
      "sparsity",
      "accuracy"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <method> <task> <method> <metric> <metric>",
    "relations": [
      "accuracy -- EVALUATE-FOR -- discrimi-native gmm classifier",
      "sparsity -- FEATURE-OF -- discrimi-native gmm classifier",
      "gaussian mixture models -- USED-FOR -- classifier",
      "genera-tive gmm classifiers -- CONJUNCTION -- discrimi-native gmm classifier",
      "relevance vector machines -- HYPONYM-OF -- kernel classifiers",
      "accuracy -- EVALUATE-FOR -- classifier",
      "baum-welch algorithm -- USED-FOR -- training procedure",
      "baum-welch algorithm -- USED-FOR -- speech recognition",
      "genera-tive gmm classifiers -- CONJUNCTION -- kernel classifiers",
      "sparsity -- EVALUATE-FOR -- genera-tive gmm classifiers",
      "support vector machines -- HYPONYM-OF -- kernel classifiers",
      "support vector machines -- CONJUNCTION -- relevance vector machines",
      "accuracy -- EVALUATE-FOR -- genera-tive gmm classifiers",
      "speech recognition -- USED-FOR -- training procedure"
    ],
    "abstract": "we show that a <method_9> based on <method_2> can be trained dis-criminatively to improve <metric_11> . we describe a <method_5> based on the extended <method_6> used in <task_8> . we also compare the <metric_11> and degree of <metric_10> of the new <method_3> with those of <method_4> , and of <method_7> , such as <method_1> and <method_0> .",
    "abstract_og": "we show that a classifier based on gaussian mixture models can be trained dis-criminatively to improve accuracy . we describe a training procedure based on the extended baum-welch algorithm used in speech recognition . we also compare the accuracy and degree of sparsity of the new discrimi-native gmm classifier with those of genera-tive gmm classifiers , and of kernel classifiers , such as support vector machines and relevance vector machines ."
  },
  {
    "title": "Deep neural network context embeddings for model selection in rich-context HMM synthesis .",
    "entities": [
      "statistical parametric speech synthesis",
      "rich context approach",
      "rich-context hmm-based synthesiser",
      "decision tree-tied models",
      "deep neural network",
      "linguistic contexts",
      "neural network",
      "tied model",
      "parametric synthesis",
      "context embeddings",
      "rich-context synthesis",
      "bottleneck layer",
      "gaussian distributions",
      "synthesis"
    ],
    "types": "<task> <method> <method> <method> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "bottleneck layer -- USED-FOR -- context embeddings",
      "context embeddings -- USED-FOR -- parametric synthesis",
      "bottleneck layer -- USED-FOR -- parametric synthesis",
      "neural network -- USED-FOR -- context embeddings",
      "deep neural network -- USED-FOR -- parametric synthesis"
    ],
    "abstract": "this paper introduces a novel form of <method_8> that uses <otherscientificterm_9> produced by the <otherscientificterm_11> of a <method_4> to guide the selection of models in a <method_2> . <task_10> -- in which <otherscientificterm_12> estimated from single <otherscientificterm_5> seen in the training data are used for <task_13> , rather than more conventional <method_3> -- was originally proposed to address over-smoothing due to averaging across contexts . our previous investigations have confirmed experimentally that averaging across different contexts is indeed one of the largest factors contributing to the limited quality of <task_0> . however , a possible weakness of the <method_1> as previously formulated is that a conventional <method_7> is still used to guide selection of gaussians at <task_13> time . our proposed <method_7> replaces <method_8> with <otherscientificterm_9> derived from a <method_6> .",
    "abstract_og": "this paper introduces a novel form of parametric synthesis that uses context embeddings produced by the bottleneck layer of a deep neural network to guide the selection of models in a rich-context hmm-based synthesiser . rich-context synthesis -- in which gaussian distributions estimated from single linguistic contexts seen in the training data are used for synthesis , rather than more conventional decision tree-tied models -- was originally proposed to address over-smoothing due to averaging across contexts . our previous investigations have confirmed experimentally that averaging across different contexts is indeed one of the largest factors contributing to the limited quality of statistical parametric speech synthesis . however , a possible weakness of the rich context approach as previously formulated is that a conventional tied model is still used to guide selection of gaussians at synthesis time . our proposed tied model replaces parametric synthesis with context embeddings derived from a neural network ."
  },
  {
    "title": "U-invariant sampling and stable reconstruction in atomic spaces .",
    "entities": [
      "arbitrary hilbert space h",
      "u-invariant sampling scheme",
      "atomic subspaces",
      "linear filter",
      "signal recovery"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <task>",
    "relations": [
      "linear filter -- USED-FOR -- signal recovery",
      "arbitrary hilbert space h -- USED-FOR -- u-invariant sampling scheme"
    ],
    "abstract": "given a <method_1> on an <otherscientificterm_0> . this paper characterizes <otherscientificterm_2> a of h such that every signal x \u2208 a can be reconstructed from its samples acquired with this <method_1> . if <task_4> is possible a <method_3> is derived which reconstructs the signal from the samples .",
    "abstract_og": "given a u-invariant sampling scheme on an arbitrary hilbert space h . this paper characterizes atomic subspaces a of h such that every signal x \u2208 a can be reconstructed from its samples acquired with this u-invariant sampling scheme . if signal recovery is possible a linear filter is derived which reconstructs the signal from the samples ."
  },
  {
    "title": "Analyzing Positions and Topics in Political Discussions of the German Bundestag .",
    "entities": [
      "transcriptions of political speeches",
      "supervised and unsupervised approaches",
      "topic modeling techniques",
      "textual data",
      "party manifestos"
    ],
    "types": "<material> <method> <method> <material> <material>",
    "relations": [
      "transcriptions of political speeches -- HYPONYM-OF -- textual data"
    ],
    "abstract": "we present ongoing doctoral work on automatically understanding the positions of politicians with respect to those of the party they belong to . to this end , we use <material_3> , namely <material_0> from meetings of the ger-man bundestag , and <material_4> , in order to automatically acquire the positions of political actors and parties , respectively . we discuss a variety of possible <method_1> to determine the topics of interest and compare positions , and propose to explore an approach based on <method_2> for these tasks .",
    "abstract_og": "we present ongoing doctoral work on automatically understanding the positions of politicians with respect to those of the party they belong to . to this end , we use textual data , namely transcriptions of political speeches from meetings of the ger-man bundestag , and party manifestos , in order to automatically acquire the positions of political actors and parties , respectively . we discuss a variety of possible supervised and unsupervised approaches to determine the topics of interest and compare positions , and propose to explore an approach based on topic modeling techniques for these tasks ."
  },
  {
    "title": "Combining Statistical and Knowledge-Based Spoken Language Understanding in Conditional Models .",
    "entities": [
      "conditional random fields",
      "spoken language understanding",
      "generative hmm/cfg composite model",
      "data-driven statistical learning framework",
      "extracting semantic meaning",
      "natural language processing",
      "easy-to-obtain domain knowledge",
      "conditional model framework",
      "slot error rate",
      "statistical learning approach",
      "data requirement",
      "new domain",
      "slu accuracy",
      "language engineering",
      "annotated data",
      "speech recognition",
      "statistical learning",
      "atis data",
      "model training",
      "knowledge-based approach",
      "perceptron learning",
      "hmm/cfg model"
    ],
    "types": "<method> <method> <method> <method> <task> <task> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <material> <metric> <task> <material> <task> <method> <material> <task> <method> <method> <method>",
    "relations": [
      "annotated data -- USED-FOR -- statistical learning approach",
      "statistical learning approach -- USED-FOR -- model training",
      "easy-to-obtain domain knowledge -- PART-OF -- data-driven statistical learning framework",
      "speech recognition -- CONJUNCTION -- language engineering",
      "perceptron learning -- CONJUNCTION -- conditional random fields",
      "slu accuracy -- EVALUATE-FOR -- hmm/cfg model",
      "language engineering -- USED-FOR -- new domain",
      "atis data -- EVALUATE-FOR -- hmm/cfg model",
      "annotated data -- USED-FOR -- model training",
      "spoken language understanding -- USED-FOR -- extracting semantic meaning",
      "slot error rate -- EVALUATE-FOR -- conditional model framework",
      "perceptron learning -- USED-FOR -- spoken language understanding",
      "natural language processing -- CONJUNCTION -- speech recognition",
      "slu accuracy -- EVALUATE-FOR -- conditional model framework"
    ],
    "abstract": "spoken language understanding -lrb- <method_1> -rrb- addresses the problem of <task_4> conveyed in an utterance . the traditional <method_19> to this problem is very expensive-it requires joint expertise in <task_5> and <task_15> , and best practices in <task_13> for every <material_11> . on the other hand , a <method_9> needs a large amount of <material_14> for <task_18> , which is seldom available in practical applications outside of large research labs . a <method_2> , which integrates <otherscientificterm_6> into a <method_3> , has previously been introduced to reduce <otherscientificterm_10> . the major contribution of this paper is the investigation of integrating prior knowledge and <method_16> in a <method_7> . we also study and compare <method_0> with <method_20> for <method_1> . experimental results show that the <method_7> achieve more than 20 % relative reduction in <metric_8> over the <method_21> , which had already achieved an <metric_12> at the same level as the best results reported on the <material_17> .",
    "abstract_og": "spoken language understanding -lrb- spoken language understanding -rrb- addresses the problem of extracting semantic meaning conveyed in an utterance . the traditional knowledge-based approach to this problem is very expensive-it requires joint expertise in natural language processing and speech recognition , and best practices in language engineering for every new domain . on the other hand , a statistical learning approach needs a large amount of annotated data for model training , which is seldom available in practical applications outside of large research labs . a generative hmm/cfg composite model , which integrates easy-to-obtain domain knowledge into a data-driven statistical learning framework , has previously been introduced to reduce data requirement . the major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework . we also study and compare conditional random fields with perceptron learning for spoken language understanding . experimental results show that the conditional model framework achieve more than 20 % relative reduction in slot error rate over the hmm/cfg model , which had already achieved an slu accuracy at the same level as the best results reported on the atis data ."
  },
  {
    "title": "From compressive to adaptive sampling of neural and ECG recordings .",
    "entities": [
      "brain machine interfaces",
      "compressive sensing framework",
      "integrate-and-fire",
      "shift invariant basis",
      "neural spike trains",
      "ambulatory cardiac monitoring",
      "local time structure",
      "adaptive sampling scheme",
      "neu-ral recordings",
      "data rates",
      "digital representations",
      "local characteristics",
      "global constraints",
      "miniaturization"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <material> <metric> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "integrate-and-fire -- USED-FOR -- brain machine interfaces",
      "adaptive sampling scheme -- USED-FOR -- ambulatory cardiac monitoring",
      "brain machine interfaces -- CONJUNCTION -- ambulatory cardiac monitoring",
      "adaptive sampling scheme -- USED-FOR -- brain machine interfaces"
    ],
    "abstract": "the <task_13> required for interfacing with the brain demands new methods of transforming neuron responses -lrb- spikes -rrb- into <method_10> . the sparse nature of <material_8> is evident when represented in a <otherscientificterm_3> . although a <method_1> may seem suitable in reducing the <metric_9> , we show that the time varying sparsity in the signals makes <method_7> difficult to apply . furthermore , we present an <method_7> which takes advantage of the <otherscientificterm_11> of the <method_4> and electrocardiograms -lrb- ecg -rrb- . in contrast to the <otherscientificterm_12> imposed in cs our <method_7> is sensitive to the <otherscientificterm_6> of the input . the simplicity in the design of the <method_2> make <method_7> a viable <method_7> in current <method_0> and <task_5> .",
    "abstract_og": "the miniaturization required for interfacing with the brain demands new methods of transforming neuron responses -lrb- spikes -rrb- into digital representations . the sparse nature of neu-ral recordings is evident when represented in a shift invariant basis . although a compressive sensing framework may seem suitable in reducing the data rates , we show that the time varying sparsity in the signals makes adaptive sampling scheme difficult to apply . furthermore , we present an adaptive sampling scheme which takes advantage of the local characteristics of the neural spike trains and electrocardiograms -lrb- ecg -rrb- . in contrast to the global constraints imposed in cs our adaptive sampling scheme is sensitive to the local time structure of the input . the simplicity in the design of the integrate-and-fire make adaptive sampling scheme a viable adaptive sampling scheme in current brain machine interfaces and ambulatory cardiac monitoring ."
  },
  {
    "title": "Multi-Agent Dynamic Coupling for Cooperative Vehicles Modeling .",
    "entities": [
      "cooperative intelligent transportation systems",
      "multi-model open-source vehicular-traffic simulator",
      "real traffic data",
      "multi-agent based modeling",
      "multi-agent mod-eling"
    ],
    "types": "<method> <method> <material> <method> <task>",
    "relations": [
      "cooperative intelligent transportation systems -- USED-FOR -- multi-agent mod-eling",
      "multi-agent based modeling -- USED-FOR -- cooperative intelligent transportation systems",
      "real traffic data -- USED-FOR -- multi-agent based modeling"
    ],
    "abstract": "cooperative intelligent transportation systems -lrb- <method_0> -rrb- are complex systems well-suited to a <task_4> . we propose a <method_3> of a <method_0> , that couples 3 dynamics -lrb- physical , informational and control dynamics -rrb- in order to ensure a smooth cooperation between non cooperative and cooperative vehicles , that communicate with each other -lrb- v2v communication -rrb- and the infrastructure -lrb- i2v and v2i communication -rrb- . we present our <method_3> , tested through simulations using <material_2> and integrated into our extension of the <method_1> .",
    "abstract_og": "cooperative intelligent transportation systems -lrb- cooperative intelligent transportation systems -rrb- are complex systems well-suited to a multi-agent mod-eling . we propose a multi-agent based modeling of a cooperative intelligent transportation systems , that couples 3 dynamics -lrb- physical , informational and control dynamics -rrb- in order to ensure a smooth cooperation between non cooperative and cooperative vehicles , that communicate with each other -lrb- v2v communication -rrb- and the infrastructure -lrb- i2v and v2i communication -rrb- . we present our multi-agent based modeling , tested through simulations using real traffic data and integrated into our extension of the multi-model open-source vehicular-traffic simulator ."
  },
  {
    "title": "An Algorithm for Generating Referential Descriptions with Flexible Interfaces .",
    "entities": [
      "perceptive and linguistics data",
      "object descriptors",
      "linguistic constraints",
      "lexical expression",
      "referring expression",
      "processing components",
      "core algorithm"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "object descriptors -- CONJUNCTION -- lexical expression"
    ],
    "abstract": "most algorithms dedicated to the generation of referential descriptions widely suffer from a fundamental problem : they make too strong assumptions about adjacent <method_5> , resulting in a limited coordination with their <material_0> , that is , the provider for <otherscientificterm_1> and the <otherscientificterm_3> by which the chosen descriptors is ultimately realized . motivated by this deficit , we present a new algorithm that -lrb- 1 -rrb- allows for a widely unconstrained , incremental , and goal-driven selection of descriptors , -lrb- 2 -rrb- integrates <otherscientificterm_2> to ensure the expressibility of the chosen descriptors , and -lrb- 3 -rrb- provides means to control the appearance of the created <otherscientificterm_4> . hence , the main achievement of our approach lies in providing a <method_6> that makes few assumptions about other <method_5> and improves the flow of control between modules .",
    "abstract_og": "most algorithms dedicated to the generation of referential descriptions widely suffer from a fundamental problem : they make too strong assumptions about adjacent processing components , resulting in a limited coordination with their perceptive and linguistics data , that is , the provider for object descriptors and the lexical expression by which the chosen descriptors is ultimately realized . motivated by this deficit , we present a new algorithm that -lrb- 1 -rrb- allows for a widely unconstrained , incremental , and goal-driven selection of descriptors , -lrb- 2 -rrb- integrates linguistic constraints to ensure the expressibility of the chosen descriptors , and -lrb- 3 -rrb- provides means to control the appearance of the created referring expression . hence , the main achievement of our approach lies in providing a core algorithm that makes few assumptions about other processing components and improves the flow of control between modules ."
  },
  {
    "title": "Blitz : A Principled Meta-Algorithm for Scaling Sparse Optimization .",
    "entities": [
      "stopping criteria",
      "set methods",
      "algorithmic parameters",
      "subproblem size",
      "1-regularized learning",
      "blitz",
      "iterations",
      "heuristics",
      "makeup",
      "sparsity",
      "optimization"
    ],
    "types": "<metric> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "makeup -- CONJUNCTION -- stopping criteria",
      "subproblem size -- CONJUNCTION -- makeup",
      "heuristics -- USED-FOR -- makeup"
    ],
    "abstract": "by reducing <task_10> to a sequence of small subproblems , working <method_1> achieve fast convergence times for many challenging problems . despite excellent performance , theoretical understanding of working sets is limited , and implementations often resort to <method_7> to determine <otherscientificterm_3> , <otherscientificterm_8> , and <metric_0> . we propose <method_5> , a fast working set algorithm accompanied by useful guarantees . making no assumptions on data , our theory relates <otherscientificterm_3> to progress toward convergence . this result motivates methods for optimizing <otherscientificterm_2> and discarding irrelevant variables as <otherscientificterm_6> progress . applied to <task_4> , <method_5> convincingly outperforms existing solvers in sequential , limited-memory , and distributed settings . <method_5> is not specific to <task_4> , making the algorithm relevant to many applications involving <otherscientificterm_9> or constraints .",
    "abstract_og": "by reducing optimization to a sequence of small subproblems , working set methods achieve fast convergence times for many challenging problems . despite excellent performance , theoretical understanding of working sets is limited , and implementations often resort to heuristics to determine subproblem size , makeup , and stopping criteria . we propose blitz , a fast working set algorithm accompanied by useful guarantees . making no assumptions on data , our theory relates subproblem size to progress toward convergence . this result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress . applied to 1-regularized learning , blitz convincingly outperforms existing solvers in sequential , limited-memory , and distributed settings . blitz is not specific to 1-regularized learning , making the algorithm relevant to many applications involving sparsity or constraints ."
  },
  {
    "title": "You are Here : Mimicking the Human Thinking Process in Reading Floor-Plans .",
    "entities": [
      "floor-plan-based localization methods",
      "human thinking process",
      "matching-localization algorithm",
      "human logic",
      "real-time applications",
      "floor-plan"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "human logic -- USED-FOR -- matching-localization algorithm"
    ],
    "abstract": "a human can easily find his or her way in an unfamiliar building , by walking around and reading the <otherscientificterm_5> . we try to mimic and automate this <task_1> . more precisely , we introduce a new and useful task of locating an user in the <otherscientificterm_5> , by using only a camera and a <otherscientificterm_5> without any other prior information . we address the problem with a novel <method_2> that is inspired by <otherscientificterm_3> . we demonstrate through experiments that our method outperforms state-of-the-art <method_0> by a large margin , while also being highly efficient for <task_4> .",
    "abstract_og": "a human can easily find his or her way in an unfamiliar building , by walking around and reading the floor-plan . we try to mimic and automate this human thinking process . more precisely , we introduce a new and useful task of locating an user in the floor-plan , by using only a camera and a floor-plan without any other prior information . we address the problem with a novel matching-localization algorithm that is inspired by human logic . we demonstrate through experiments that our method outperforms state-of-the-art floor-plan-based localization methods by a large margin , while also being highly efficient for real-time applications ."
  },
  {
    "title": "CL Research 's Knowledge Management System .",
    "entities": [
      "knowledge management system",
      "general and topic-based summarization",
      "web question answering",
      "document exploration functionality",
      "dynamic ontology creation",
      "information extraction",
      "document exploration",
      "user modeling",
      "text summarization"
    ],
    "types": "<method> <task> <task> <task> <method> <task> <task> <method> <task>",
    "relations": [
      "knowledge management system -- USED-FOR -- web question answering",
      "web question answering -- CONJUNCTION -- general and topic-based summarization",
      "general and topic-based summarization -- CONJUNCTION -- information extraction",
      "information extraction -- CONJUNCTION -- document exploration"
    ],
    "abstract": "cl research began experimenting with massive xml tagging of texts to answer questions in trec 2002 . in duc 2003 , the experiments were extended into <task_8> . based on these experiments , the <method_0> was developed to combine these two capabilities and to serve as a unified basis for other types of <task_6> . <method_0> has been extended to include <task_2> , both <task_1> , <task_5> , and <task_6> . the <task_3> includes identification of semantically similar concepts and <method_4> . as development of <method_0> has continued , <method_7> has become a key research issue : how will different users want to use the information they identify .",
    "abstract_og": "cl research began experimenting with massive xml tagging of texts to answer questions in trec 2002 . in duc 2003 , the experiments were extended into text summarization . based on these experiments , the knowledge management system was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration . knowledge management system has been extended to include web question answering , both general and topic-based summarization , information extraction , and document exploration . the document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation . as development of knowledge management system has continued , user modeling has become a key research issue : how will different users want to use the information they identify ."
  },
  {
    "title": "A predictive model of music preference using pairwise comparisons .",
    "entities": [
      "degree music preference",
      "collaborative filtering methods",
      "gaussian process priors",
      "personalized viewpoint",
      "music recommendation",
      "recommendation setting",
      "streaming services",
      "covariance function",
      "multi-media systems",
      "random predictions",
      "leave-one-out accuracy",
      "pairwise comparisons",
      "inference"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <task>",
    "relations": [
      "leave-one-out accuracy -- EVALUATE-FOR -- recommendation setting",
      "collaborative filtering methods -- USED-FOR -- music recommendation",
      "music recommendation -- HYPONYM-OF -- multi-media systems",
      "streaming services -- CONJUNCTION -- multi-media systems"
    ],
    "abstract": "music recommendation is an important aspect of many <method_6> and <method_8> , however , <task_4> is typically based on so-called <method_1> . in this paper we consider the recommendation task from a <otherscientificterm_3> and examine to which <otherscientificterm_0> can be elicited and predicted using simple and robust queries such as <method_11> . we propose to model-and in turn predict-the pairwise music preference using a very flexible model based on <method_2> for which we describe the required <task_12> . we further propose a specific <otherscientificterm_7> and evaluate the pre-dictive performance on a novel dataset . in a <method_5> we obtain a <metric_10> of 76 % compared to 50 % with <otherscientificterm_9> , showing potential for further refinement and evaluation .",
    "abstract_og": "music recommendation is an important aspect of many streaming services and multi-media systems , however , music recommendation is typically based on so-called collaborative filtering methods . in this paper we consider the recommendation task from a personalized viewpoint and examine to which degree music preference can be elicited and predicted using simple and robust queries such as pairwise comparisons . we propose to model-and in turn predict-the pairwise music preference using a very flexible model based on gaussian process priors for which we describe the required inference . we further propose a specific covariance function and evaluate the pre-dictive performance on a novel dataset . in a recommendation setting we obtain a leave-one-out accuracy of 76 % compared to 50 % with random predictions , showing potential for further refinement and evaluation ."
  },
  {
    "title": "Hierarchical-PEP model for real-world face recognition .",
    "entities": [
      "probabilistic elastic part model",
      "fine-grained structures of the face parts",
      "compact and invariant face representation",
      "image-based and video-based face verification",
      "uncon-strained face recognition problem",
      "face part representations layer-by-layer",
      "lfw and youtube faces",
      "real-world face recognition systems",
      "pose-invariant part-based face representations",
      "face part/face representations",
      "face part representations",
      "face recognition challenge",
      "deep hierarchical architecture",
      "visual tasks",
      "supervised information",
      "public benchmarks",
      "pose variation",
      "hierarchy",
      "accuracy",
      "dimensionality"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <task> <method> <material> <method> <method> <method> <method> <task> <method> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "supervised information -- USED-FOR -- face part/face representations",
      "probabilistic elastic part model -- USED-FOR -- uncon-strained face recognition problem",
      "supervised information -- USED-FOR -- probabilistic elastic part model",
      "lfw and youtube faces -- HYPONYM-OF -- public benchmarks",
      "face part representations layer-by-layer -- USED-FOR -- compact and invariant face representation",
      "public benchmarks -- EVALUATE-FOR -- probabilistic elastic part model",
      "probabilistic elastic part model -- USED-FOR -- pose-invariant part-based face representations",
      "deep hierarchical architecture -- USED-FOR -- visual tasks",
      "face recognition challenge -- HYPONYM-OF -- public benchmarks",
      "probabilistic elastic part model -- USED-FOR -- fine-grained structures of the face parts"
    ],
    "abstract": "pose variation remains one of the major factors adversely affect the <metric_18> of <method_7> . inspired by the recently proposed <method_0> and the success of the <method_12> in a number of <task_13> , we propose the <method_0> to approach the <task_4> . we apply the <method_0> hierarchically to decompose a face image into face parts at different levels of details to build <method_8> . following the <otherscientificterm_17> from bottom-up , we stack the <method_10> at each layer , discriminatively reduce its <otherscientificterm_19> , and hence aggregate the <method_5> to build a <method_2> . the <method_0> exploits the <otherscientificterm_1> at different levels of details to address the pose variations . <method_0> is also guided by <otherscientificterm_14> in constructing the <method_9> . we empirically verify the <method_0> on two <material_15> -lrb- i.e. , the <material_6> -rrb- and a <task_11> -lrb- i.e. , the pasc grand challenge -rrb- for <task_3> . the state-of-the-art performance demonstrates the potential of our <method_0> .",
    "abstract_og": "pose variation remains one of the major factors adversely affect the accuracy of real-world face recognition systems . inspired by the recently proposed probabilistic elastic part model and the success of the deep hierarchical architecture in a number of visual tasks , we propose the probabilistic elastic part model to approach the uncon-strained face recognition problem . we apply the probabilistic elastic part model hierarchically to decompose a face image into face parts at different levels of details to build pose-invariant part-based face representations . following the hierarchy from bottom-up , we stack the face part representations at each layer , discriminatively reduce its dimensionality , and hence aggregate the face part representations layer-by-layer to build a compact and invariant face representation . the probabilistic elastic part model exploits the fine-grained structures of the face parts at different levels of details to address the pose variations . probabilistic elastic part model is also guided by supervised information in constructing the face part/face representations . we empirically verify the probabilistic elastic part model on two public benchmarks -lrb- i.e. , the lfw and youtube faces -rrb- and a face recognition challenge -lrb- i.e. , the pasc grand challenge -rrb- for image-based and video-based face verification . the state-of-the-art performance demonstrates the potential of our probabilistic elastic part model ."
  },
  {
    "title": "Planning in the Presence of Cost Functions Controlled by an Adversary .",
    "entities": [
      "robot path planning problem",
      "zero-sum matrix game",
      "markov decision process",
      "linear programming formulation",
      "cost function",
      "cost vectors",
      "fast algorithms",
      "matrix games",
      "value iteration",
      "deterministic policies",
      "policy",
      "rows",
      "mdps"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "fast algorithms -- USED-FOR -- mdps",
      "zero-sum matrix game -- USED-FOR -- robot path planning problem"
    ],
    "abstract": "we investigate methods for planning in a <method_2> where the <otherscientificterm_4> is chosen by an adversary after we fix our <otherscientificterm_10> . as a running example , we consider a <task_0> where costs are influenced by sensors that an adversary places in the environment . we formulate the <task_0> as a <otherscientificterm_1> where <otherscientificterm_11> correspond to <otherscientificterm_9> for the planning player and columns correspond to <otherscientificterm_5> the adversary can select . for a fixed cost vector , <method_6> -lrb- such as <method_8> -rrb- are available for solving <method_12> . we develop efficient algorithms for <task_7> where such best response oracles exist . we show that for our <task_0> these algorithms are at least an order of magnitude faster than direct solution of the <method_3> .",
    "abstract_og": "we investigate methods for planning in a markov decision process where the cost function is chosen by an adversary after we fix our policy . as a running example , we consider a robot path planning problem where costs are influenced by sensors that an adversary places in the environment . we formulate the robot path planning problem as a zero-sum matrix game where rows correspond to deterministic policies for the planning player and columns correspond to cost vectors the adversary can select . for a fixed cost vector , fast algorithms -lrb- such as value iteration -rrb- are available for solving mdps . we develop efficient algorithms for matrix games where such best response oracles exist . we show that for our robot path planning problem these algorithms are at least an order of magnitude faster than direct solution of the linear programming formulation ."
  },
  {
    "title": "Generating Aspect-oriented Multi-Document Summarization with Event-aspect model .",
    "entities": [
      "automatic generation of aspect-oriented summaries",
      "event-aspect lda model",
      "integer linear programming",
      "random walk model",
      "sentence compression algorithm",
      "sentence selection",
      "parser tree",
      "dependency tree",
      "rouge metric",
      "lexrank algorithm",
      "sentence ranking",
      "cluster"
    ],
    "types": "<task> <method> <method> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "dependency tree -- USED-FOR -- sentence compression algorithm",
      "dependency tree -- CONJUNCTION -- parser tree",
      "parser tree -- USED-FOR -- sentence compression algorithm",
      "integer linear programming -- USED-FOR -- sentence selection"
    ],
    "abstract": "in this paper , we propose a novel approach to <task_0> from multiple documents . we first develop an <method_1> to <otherscientificterm_11> sentences into aspects . we then use extended <method_9> to rank the sentences in each <otherscientificterm_11> . we use <method_2> for <task_5> . key features of our method include automatic grouping of semantically related sentences and <otherscientificterm_10> based on extension of <method_3> . also , we implement a new <method_4> which use <otherscientificterm_7> instead of <otherscientificterm_6> . we compare our method with four baseline methods . quantitative evaluation based on <method_8> demonstrates the effectiveness and advantages of our method .",
    "abstract_og": "in this paper , we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents . we first develop an event-aspect lda model to cluster sentences into aspects . we then use extended lexrank algorithm to rank the sentences in each cluster . we use integer linear programming for sentence selection . key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model . also , we implement a new sentence compression algorithm which use dependency tree instead of parser tree . we compare our method with four baseline methods . quantitative evaluation based on rouge metric demonstrates the effectiveness and advantages of our method ."
  },
  {
    "title": "A Pot of Gold : Rainbows as a Calibration Cue .",
    "entities": [
      "semi-automatic and fully automatic methods",
      "estimate of camera location",
      "geometry of a rainbow",
      "calibrating outdoor imagery",
      "estimating camera calibration",
      "calibration cues",
      "capture time",
      "solar-refractive phenomena",
      "natural images",
      "calibration accuracy",
      "rainbow geometry",
      "rainbow appearance",
      "horizon line",
      "geometric properties",
      "rainbow images",
      "sun dogs",
      "rainbows",
      "parhelion",
      "rainbows"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "semi-automatic and fully automatic methods -- USED-FOR -- rainbow appearance",
      "rainbows -- USED-FOR -- calibrating outdoor imagery",
      "parhelion -- HYPONYM-OF -- solar-refractive phenomena",
      "sun dogs -- HYPONYM-OF -- solar-refractive phenomena"
    ],
    "abstract": "rainbows are a natural cue for <task_3> . while ephemeral , they provide unique <otherscientificterm_5> because they are centered exactly opposite the sun and have an outer radius of 42 degrees . in this work , we define the <otherscientificterm_2> and describe minimal sets of constraints that are sufficient for <task_4> . we present both <method_0> to calibrate a camera using an image of a rainbow . to demonstrate our <method_0> , we have collected a large database of <material_14> and use these to evaluate <metric_9> and to create an empirical <method_0> of <otherscientificterm_11> . we show how this <method_0> can be used to edit <otherscientificterm_11> in <material_8> and how <otherscientificterm_10> , in conjunction with a <otherscientificterm_12> and <otherscientificterm_6> , provides an <otherscientificterm_1> . while we focus on <otherscientificterm_18> , many of the <otherscientificterm_13> and algorithms we present also apply to other <otherscientificterm_7> , such as <otherscientificterm_17> , often called <otherscientificterm_15> , and the 22 degree solar halo .",
    "abstract_og": "rainbows are a natural cue for calibrating outdoor imagery . while ephemeral , they provide unique calibration cues because they are centered exactly opposite the sun and have an outer radius of 42 degrees . in this work , we define the geometry of a rainbow and describe minimal sets of constraints that are sufficient for estimating camera calibration . we present both semi-automatic and fully automatic methods to calibrate a camera using an image of a rainbow . to demonstrate our semi-automatic and fully automatic methods , we have collected a large database of rainbow images and use these to evaluate calibration accuracy and to create an empirical semi-automatic and fully automatic methods of rainbow appearance . we show how this semi-automatic and fully automatic methods can be used to edit rainbow appearance in natural images and how rainbow geometry , in conjunction with a horizon line and capture time , provides an estimate of camera location . while we focus on rainbows , many of the geometric properties and algorithms we present also apply to other solar-refractive phenomena , such as parhelion , often called sun dogs , and the 22 degree solar halo ."
  },
  {
    "title": "BUT 2014 Babel system : analysis of adaptation in NN based systems .",
    "entities": [
      "fundamental frequency estimates",
      "stacked bottleneck features",
      "hierarchy of neural networks",
      "full gmm-and dnn-based systems",
      "un-transcribed data",
      "bottleneck features",
      "nn structure",
      "com-pressive layers",
      "semi-supervised training",
      "babel languages",
      "lvcsr systems",
      "features"
    ],
    "types": "<method> <method> <method> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <task> <otherscientificterm>",
    "relations": [
      "un-transcribed data -- CONJUNCTION -- semi-supervised training",
      "bottleneck features -- PART-OF -- lvcsr systems",
      "com-pressive layers -- PART-OF -- hierarchy of neural networks",
      "hierarchy of neural networks -- USED-FOR -- features"
    ],
    "abstract": "features based on a <method_2> with <otherscientificterm_7> -- <method_1> -- were recently shown to provide excellent performance in <task_10> . this paper summarizes several techniques investigated in our work towards babel 2014 evaluations : -lrb- 1 -rrb- using several versions of <method_0> , -lrb- 2 -rrb- <method_8> on <material_4> and mainly -lrb- 3 -rrb- adapting the <otherscientificterm_6> at different levels . they are tested on three 2014 <material_9> with <method_3> . separately and in combination , they are shown to out-perform the baselines and confirm the usefulness of <otherscientificterm_5> in current <task_10> .",
    "abstract_og": "features based on a hierarchy of neural networks with com-pressive layers -- stacked bottleneck features -- were recently shown to provide excellent performance in lvcsr systems . this paper summarizes several techniques investigated in our work towards babel 2014 evaluations : -lrb- 1 -rrb- using several versions of fundamental frequency estimates , -lrb- 2 -rrb- semi-supervised training on un-transcribed data and mainly -lrb- 3 -rrb- adapting the nn structure at different levels . they are tested on three 2014 babel languages with full gmm-and dnn-based systems . separately and in combination , they are shown to out-perform the baselines and confirm the usefulness of bottleneck features in current lvcsr systems ."
  },
  {
    "title": "Large tagset labeling using Feed Forward Neural Networks . Case study on Romanian Language .",
    "entities": [
      "highly inflectional languages",
      "lexicon morpho-syntactic descriptions",
      "neural network architecture",
      "tiered tagging methodology",
      "linguistic knowledge",
      "local optimizations",
      "data sparseness",
      "standard methods",
      "morpho-syntactic features",
      "tiered tagging",
      "part-of-speech tagging",
      "neural networks",
      "recoverable features"
    ],
    "types": "<material> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm>",
    "relations": [
      "data sparseness -- USED-FOR -- standard methods",
      "neural networks -- CONJUNCTION -- local optimizations",
      "standard methods -- USED-FOR -- part-of-speech tagging"
    ],
    "abstract": "standard methods for <task_10> suffer from <otherscientificterm_6> when used on <material_0> -lrb- which require large lexical tagset inventories -rrb- . for this reason , a number of alternative methods have been proposed over the years . one of the most successful methods used for this task , fdoohg7lhuhg7djjlqj7xil , 1999 -rrb- , exploits a reduced set of tags derived by removing several <otherscientificterm_12> from the <otherscientificterm_1> . a second phase is aimed at recovering the full set of <otherscientificterm_8> . in this paper we present an alternative method to <method_9> , based on <method_5> with <method_11> and we show how , by properly encoding the input sequence in a general <method_2> , we achieve results similar to the <method_3> , significantly faster and without requiring extensive <otherscientificterm_4> as implied by the previously mentioned method .",
    "abstract_og": "standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages -lrb- which require large lexical tagset inventories -rrb- . for this reason , a number of alternative methods have been proposed over the years . one of the most successful methods used for this task , fdoohg7lhuhg7djjlqj7xil , 1999 -rrb- , exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions . a second phase is aimed at recovering the full set of morpho-syntactic features . in this paper we present an alternative method to tiered tagging , based on local optimizations with neural networks and we show how , by properly encoding the input sequence in a general neural network architecture , we achieve results similar to the tiered tagging methodology , significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method ."
  },
  {
    "title": "Introducing phonetically motivated information into ASR .",
    "entities": [
      "multi-band system speech recognition",
      "automatic speech recognition",
      "phonetically related phonemes",
      "numbers recognition task",
      "pho-netically motivated information",
      "phonetic expert",
      "dimensionality problem",
      "acoustic model",
      "discriminative information",
      "mlp",
      "fullband"
    ],
    "types": "<task> <task> <otherscientificterm> <task> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "pho-netically motivated information -- USED-FOR -- automatic speech recognition",
      "mlp -- USED-FOR -- phonetic expert",
      "fullband -- CONJUNCTION -- multi-band system speech recognition"
    ],
    "abstract": "in this paper we present an approach to introducing more <otherscientificterm_4> into <task_1> in the form of a phonetic ` expert ' . to avoid the curse of <task_6> , the expert information is introduced at the level of the <method_7> . two types of experts are used , each providing <otherscientificterm_8> regarding groups of <otherscientificterm_2> . the <method_5> is implemented using an <method_9> . experiments on a <task_3> show that , when using the expert in conjunction with both a <otherscientificterm_10> and a <task_0> performances are increased .",
    "abstract_og": "in this paper we present an approach to introducing more pho-netically motivated information into automatic speech recognition in the form of a phonetic ` expert ' . to avoid the curse of dimensionality problem , the expert information is introduced at the level of the acoustic model . two types of experts are used , each providing discriminative information regarding groups of phonetically related phonemes . the phonetic expert is implemented using an mlp . experiments on a numbers recognition task show that , when using the expert in conjunction with both a fullband and a multi-band system speech recognition performances are increased ."
  },
  {
    "title": "A General Expression of the Fundamental Matrix for Both Perspective and Affine Cameras .",
    "entities": [
      "recovery of structure and motion",
      "aane epipolar geometry",
      "uncalibrated images",
      "lens distortion",
      "full perspective",
      "projective reconstruction",
      "aane projection",
      "projection model",
      "aane images",
      "linear algebra",
      "aane reconstruction",
      "epipolar geometry",
      "triangulation"
    ],
    "types": "<task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <method> <method> <material> <otherscientificterm> <task> <task> <method>",
    "relations": [
      "epipolar geometry -- CONJUNCTION -- projective reconstruction",
      "full perspective -- CONJUNCTION -- aane projection",
      "aane images -- USED-FOR -- aane reconstruction",
      "full perspective -- HYPONYM-OF -- lens distortion",
      "projective reconstruction -- CONJUNCTION -- aane reconstruction"
    ],
    "abstract": "this paper addresses the <task_0> from two <material_2> of a scene under <otherscientificterm_4> or under <method_6> . <task_11> , <task_5> , and <task_10> are elaborated in a way such that everyone having knowledge of <otherscientificterm_9> can understand the discussion without diiculty . a general expression of the fundamental matrix is derived which is valid for any <method_7> without <otherscientificterm_3> -lrb- including <otherscientificterm_4> and aane camera -rrb- . a new technique for <task_10> from two <material_8> is developed , which consists in rst estimating the <otherscientificterm_1> and then performing a <method_12> for each point match with respect to an implicit common aane basis . this technique is very ef-cient .",
    "abstract_og": "this paper addresses the recovery of structure and motion from two uncalibrated images of a scene under full perspective or under aane projection . epipolar geometry , projective reconstruction , and aane reconstruction are elaborated in a way such that everyone having knowledge of linear algebra can understand the discussion without diiculty . a general expression of the fundamental matrix is derived which is valid for any projection model without lens distortion -lrb- including full perspective and aane camera -rrb- . a new technique for aane reconstruction from two aane images is developed , which consists in rst estimating the aane epipolar geometry and then performing a triangulation for each point match with respect to an implicit common aane basis . this technique is very ef-cient ."
  },
  {
    "title": "Retroflex and bunched English / r / with physical models of the human vocal tract .",
    "entities": [
      "retroflex and bunched / r / models",
      "acoustic analysis",
      "tongue position",
      "sliding blocks",
      "physical model",
      "narrow constriction",
      "lip rounding",
      "pronunciation"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "sliding blocks -- USED-FOR -- physical model"
    ],
    "abstract": "it is known that american english / r / can be produced as a retroflex or bunched / r / , but it can be challenging to teach students how to articulate both . we already developed a <method_4> for retroflex / r / and demonstrated that the <method_4> produces the / r / sound . however , almost no studies have reported a <method_4> for bunched / r / . we developed a new <method_4> using <otherscientificterm_3> for the lips and tongue to help teach students how to produce bunched / r / . we recorded several sets of sounds produced by the <method_4> , analyzed the output signals , and used <method_4> for perceptual experiments . <task_1> and perceptual experiments confirmed that the <method_0> produced clear american / r / sounds , and that the <otherscientificterm_5> placed between 5-7 cm from the lips seems to be the key in producing these sounds . furthermore , bunched / r / with <otherscientificterm_6> produced the most clear / r / sound . both <method_4> are helpful for practicing <task_7> because learners can readily see there are two ways to produce / r / , they can see and alter the <otherscientificterm_2> manually , and they can hear the output sounds .",
    "abstract_og": "it is known that american english / r / can be produced as a retroflex or bunched / r / , but it can be challenging to teach students how to articulate both . we already developed a physical model for retroflex / r / and demonstrated that the physical model produces the / r / sound . however , almost no studies have reported a physical model for bunched / r / . we developed a new physical model using sliding blocks for the lips and tongue to help teach students how to produce bunched / r / . we recorded several sets of sounds produced by the physical model , analyzed the output signals , and used physical model for perceptual experiments . acoustic analysis and perceptual experiments confirmed that the retroflex and bunched / r / models produced clear american / r / sounds , and that the narrow constriction placed between 5-7 cm from the lips seems to be the key in producing these sounds . furthermore , bunched / r / with lip rounding produced the most clear / r / sound . both physical model are helpful for practicing pronunciation because learners can readily see there are two ways to produce / r / , they can see and alter the tongue position manually , and they can hear the output sounds ."
  },
  {
    "title": "Blind predictive decision-feedback equalization via the constant modulus algorithm .",
    "entities": [
      "equalization of the coded modulation signals",
      "mean square algorithm",
      "nonblind linear mmse equalizer",
      "small residue intersymbol interference",
      "cm linear equalizer",
      "noise predictive structure",
      "cm cost function",
      "xed forward lter",
      "convergence rate",
      "forward lter",
      "feedback lter",
      "cm cost",
      "nonblind design",
      "closed form",
      "pcm-dfe",
      "dfe"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <metric> <method> <method> <metric> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "small residue intersymbol interference -- FEATURE-OF -- xed forward lter",
      "convergence rate -- EVALUATE-FOR -- feedback lter",
      "noise predictive structure -- USED-FOR -- equalization of the coded modulation signals",
      "cm linear equalizer -- USED-FOR -- pcm-dfe",
      "dfe -- USED-FOR -- equalization of the coded modulation signals",
      "cm cost function -- USED-FOR -- feedback lter"
    ],
    "abstract": "the <otherscientificterm_5> of <method_15> is attractive for the <task_0> . in this paper , a blind predictive constant modulus -lrb- cm -rrb- decision feedback equalizer -lrb- <method_14> -rrb- is presented and analyzed . the <method_14> employs the <method_4> as its <method_9> and a <method_10> that optimizes the <metric_11> of the decision variable . it is shown that for any <method_7> with reasonable <otherscientificterm_3> , the <otherscientificterm_6> for the <method_10> is approximately convex and its global minimum can be approximated in <otherscientificterm_13> . we demonstrate that the <metric_8> of the <method_10> is similar to the least <method_1> used in the <method_12> . we show that the <method_14> performs better than the <otherscientificterm_2> in simulations .",
    "abstract_og": "the noise predictive structure of dfe is attractive for the equalization of the coded modulation signals . in this paper , a blind predictive constant modulus -lrb- cm -rrb- decision feedback equalizer -lrb- pcm-dfe -rrb- is presented and analyzed . the pcm-dfe employs the cm linear equalizer as its forward lter and a feedback lter that optimizes the cm cost of the decision variable . it is shown that for any xed forward lter with reasonable small residue intersymbol interference , the cm cost function for the feedback lter is approximately convex and its global minimum can be approximated in closed form . we demonstrate that the convergence rate of the feedback lter is similar to the least mean square algorithm used in the nonblind design . we show that the pcm-dfe performs better than the nonblind linear mmse equalizer in simulations ."
  },
  {
    "title": "A Comprehensive Approach to On-Board Autonomy Verification and Validation .",
    "entities": [
      "on-board autonomous reasoning engine",
      "deep space missions",
      "plan generation",
      "space systems",
      "on-board autonomy",
      "reasoning capabilities",
      "formal model",
      "model-based reasoning",
      "run-time diagnosis",
      "controlled platform",
      "validation",
      "execution",
      "monitoring",
      "fdir"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <task> <method> <method> <method> <method> <otherscientificterm> <task> <task> <task> <method>",
    "relations": [
      "fdir -- HYPONYM-OF -- reasoning capabilities",
      "monitoring -- HYPONYM-OF -- reasoning capabilities",
      "validation -- CONJUNCTION -- execution",
      "execution -- CONJUNCTION -- monitoring",
      "execution -- HYPONYM-OF -- reasoning capabilities",
      "validation -- CONJUNCTION -- monitoring",
      "monitoring -- CONJUNCTION -- fdir",
      "plan generation -- CONJUNCTION -- execution",
      "fdir -- CONJUNCTION -- run-time diagnosis",
      "validation -- HYPONYM-OF -- reasoning capabilities",
      "run-time diagnosis -- HYPONYM-OF -- reasoning capabilities",
      "plan generation -- HYPONYM-OF -- reasoning capabilities",
      "plan generation -- CONJUNCTION -- validation",
      "execution -- CONJUNCTION -- fdir"
    ],
    "abstract": "deep space missions are characterized by severely constrained communication links . to meet the needs of future missions and increase their scientific return , future <method_3> will require an increased level of autonomy on-board . in this work , we propose a comprehensive approach to <task_4> relying on <method_7> , and encompassing many important <method_5> such as <task_2> , <task_10> , <task_11> and <task_12> , <method_13> , and <method_8> . the <otherscientificterm_9> is represented symbolically , and the <method_5> are seen as symbolic manipulation of such <method_6> . we have developed a prototype of our framework , implemented within an <method_0> . we have evaluated our approach on two case-studies inspired by real-world , ongoing projects , and characterized it in terms of reliability , availability and performance .",
    "abstract_og": "deep space missions are characterized by severely constrained communication links . to meet the needs of future missions and increase their scientific return , future space systems will require an increased level of autonomy on-board . in this work , we propose a comprehensive approach to on-board autonomy relying on model-based reasoning , and encompassing many important reasoning capabilities such as plan generation , validation , execution and monitoring , fdir , and run-time diagnosis . the controlled platform is represented symbolically , and the reasoning capabilities are seen as symbolic manipulation of such formal model . we have developed a prototype of our framework , implemented within an on-board autonomous reasoning engine . we have evaluated our approach on two case-studies inspired by real-world , ongoing projects , and characterized it in terms of reliability , availability and performance ."
  },
  {
    "title": "Revising Imprecise Probabilistic Beliefs in the Framework of Probabilistic Logic Programming .",
    "entities": [
      "probabilistic logic program",
      "instantiation of revision operators",
      "probabilistic conditional event",
      "imprecise probabilistic knowledge",
      "single probability distribution",
      "probabilistic logic programming",
      "jeffrey 's rule",
      "bayesian conditioning",
      "propositional formula",
      "conditional events",
      "probability intervals",
      "representation theorem",
      "knowledge base",
      "postulates"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "probability intervals -- FEATURE-OF -- knowledge base",
      "probability intervals -- FEATURE-OF -- conditional events",
      "representation theorem -- USED-FOR -- postulates",
      "jeffrey 's rule -- CONJUNCTION -- bayesian conditioning",
      "conditional events -- FEATURE-OF -- knowledge base"
    ],
    "abstract": "probabilistic logic programming is a powerful technique to represent and reason with <otherscientificterm_3> . a <method_0> is a <material_12> which contains a set of <otherscientificterm_9> with <otherscientificterm_10> . in this paper , we investigate the issue of revising such a <method_0> in light of receiving new information . we propose <otherscientificterm_13> for revising <method_0> when a new piece of evidence is also a <otherscientificterm_2> . our <otherscientificterm_13> lead to <otherscientificterm_6> and <method_7> when the original <method_0> defines a <otherscientificterm_4> . furthermore , we prove that our <otherscientificterm_13> are extensions to darwiche and pearl -lrb- dp -rrb- <otherscientificterm_13> when new evidence is a <method_8> . we also give the <method_11> for the <otherscientificterm_13> and provide an <method_1> satisfying the proposed <otherscientificterm_13> .",
    "abstract_og": "probabilistic logic programming is a powerful technique to represent and reason with imprecise probabilistic knowledge . a probabilistic logic program is a knowledge base which contains a set of conditional events with probability intervals . in this paper , we investigate the issue of revising such a probabilistic logic program in light of receiving new information . we propose postulates for revising probabilistic logic program when a new piece of evidence is also a probabilistic conditional event . our postulates lead to jeffrey 's rule and bayesian conditioning when the original probabilistic logic program defines a single probability distribution . furthermore , we prove that our postulates are extensions to darwiche and pearl -lrb- dp -rrb- postulates when new evidence is a propositional formula . we also give the representation theorem for the postulates and provide an instantiation of revision operators satisfying the proposed postulates ."
  },
  {
    "title": "A Parameterized Runtime Analysis of Evolutionary Algorithms for the Euclidean Traveling Salesperson Problem .",
    "entities": [
      "euclidean traveling salesperson problem -lrb- euclidean tsp -rrb-",
      "runtime of evolutionary algorithms",
      "randomized local search",
      "evolutionary algorithms",
      "euclidean tsp",
      "geometric constraints",
      "structural properties"
    ],
    "types": "<task> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "evolutionary algorithms -- USED-FOR -- euclidean tsp",
      "structural properties -- USED-FOR -- runtime of evolutionary algorithms",
      "structural properties -- USED-FOR -- evolutionary algorithms",
      "evolutionary algorithms -- USED-FOR -- euclidean traveling salesperson problem -lrb- euclidean tsp -rrb-"
    ],
    "abstract": "we contribute to the theoretical understanding of <method_3> and carry out a parameterized analysis of <method_3> for the <task_0> . we exploit <otherscientificterm_6> related to the optimization process of <method_3> for this problem and use <otherscientificterm_6> to bound the <method_1> . our analysis studies the runtime in dependence of the number of inner points k and shows that simple <method_3> solve the <method_4> in expected time o -lrb- n 4k -lrb- 2k \u2212 1 -rrb- ! -rrb- . moreover , we show that , under reasonable <otherscientificterm_5> , a locally optimal 2-opt tour can be found by <method_2> in expected time o -lrb- n 2k k ! -rrb- .",
    "abstract_og": "we contribute to the theoretical understanding of evolutionary algorithms and carry out a parameterized analysis of evolutionary algorithms for the euclidean traveling salesperson problem -lrb- euclidean tsp -rrb- . we exploit structural properties related to the optimization process of evolutionary algorithms for this problem and use structural properties to bound the runtime of evolutionary algorithms . our analysis studies the runtime in dependence of the number of inner points k and shows that simple evolutionary algorithms solve the euclidean tsp in expected time o -lrb- n 4k -lrb- 2k \u2212 1 -rrb- ! -rrb- . moreover , we show that , under reasonable geometric constraints , a locally optimal 2-opt tour can be found by randomized local search in expected time o -lrb- n 2k k ! -rrb- ."
  },
  {
    "title": "Multipath exploitation in sparse scene recovery using sensing-through-wall distributed radar sensor configurations .",
    "entities": [
      "distributed multistatic radar units",
      "distributed radar network configuration",
      "target and wall positions",
      "multipath signal model",
      "interior wall positions",
      "stationary target localization",
      "indoor scattering environment",
      "multipath exploitation",
      "wall locations",
      "joint optimization",
      "ghosts targets",
      "sparse reconstruction"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "multipath exploitation -- CONJUNCTION -- sparse reconstruction",
      "multipath signal model -- USED-FOR -- wall locations",
      "distributed radar network configuration -- USED-FOR -- wall locations",
      "multipath signal model -- USED-FOR -- distributed radar network configuration"
    ],
    "abstract": "in this paper , we consider <task_7> and <task_11> in a network of <method_0> for <task_5> behind walls . <task_7> leverages prior information of the <otherscientificterm_6> to eliminate <otherscientificterm_10> . however , uncertainties in <otherscientificterm_4> severely impair the effectiveness of <task_7> . we develop a <method_3> for the <method_1> , which parameterizes the <otherscientificterm_8> , and perform <method_9> for simultaneously recovering the <otherscientificterm_2> . supporting simulation results are provided , which validate the effectiveness of the proposed <method_3> .",
    "abstract_og": "in this paper , we consider multipath exploitation and sparse reconstruction in a network of distributed multistatic radar units for stationary target localization behind walls . multipath exploitation leverages prior information of the indoor scattering environment to eliminate ghosts targets . however , uncertainties in interior wall positions severely impair the effectiveness of multipath exploitation . we develop a multipath signal model for the distributed radar network configuration , which parameterizes the wall locations , and perform joint optimization for simultaneously recovering the target and wall positions . supporting simulation results are provided , which validate the effectiveness of the proposed multipath signal model ."
  },
  {
    "title": "Search Result Clustering Using Label Language Model .",
    "entities": [
      "language modeling approach",
      "search results clustering",
      "search result clustering",
      "label language model",
      "label selection",
      "dmoz"
    ],
    "types": "<method> <method> <task> <method> <task> <method>",
    "relations": [
      "dmoz -- USED-FOR -- label selection",
      "language modeling approach -- USED-FOR -- dmoz",
      "language modeling approach -- USED-FOR -- label selection"
    ],
    "abstract": "search results clustering helps users to browse the search results and locate what they are looking for . in the <task_2> , the <task_4> which annotates a meaningful phrase for each cluster becomes the most fundamental issue . in this paper , we present a new method of using the <method_0> over <method_5> for <task_4> , namely <method_3> . experimental results show that our method is helpful to obtain meaningful clustering labels of search results .",
    "abstract_og": "search results clustering helps users to browse the search results and locate what they are looking for . in the search result clustering , the label selection which annotates a meaningful phrase for each cluster becomes the most fundamental issue . in this paper , we present a new method of using the language modeling approach over dmoz for label selection , namely label language model . experimental results show that our method is helpful to obtain meaningful clustering labels of search results ."
  },
  {
    "title": "Combining packet loss compensation methods for robust distributed speech recognition .",
    "entities": [
      "distributed speech recognition",
      "aurora connected digits task",
      "wsjcam0 large vocabulary task",
      "packet loss compensation system",
      "packet loss conditions",
      "feature vector stream",
      "burst lengths",
      "decoding process",
      "terminal device",
      "recognition accuracy",
      "missing vectors",
      "interleaving",
      "recognition"
    ],
    "types": "<task> <material> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <metric> <otherscientificterm> <method> <task>",
    "relations": [
      "feature vector stream -- USED-FOR -- recognition",
      "aurora connected digits task -- CONJUNCTION -- wsjcam0 large vocabulary task",
      "missing vectors -- USED-FOR -- feature vector stream",
      "packet loss compensation system -- USED-FOR -- distributed speech recognition"
    ],
    "abstract": "this paper presents a combined <method_3> for <task_0> . compensation is applied at three stages within the <task_0> beginning with <method_11> on the <method_8> to reduce <otherscientificterm_6> in the received <otherscientificterm_5> . on the receiver side estimation of <otherscientificterm_10> is applied to reconstruct the <otherscientificterm_5> prior to <task_12> . finally , the <method_7> of the recogniser is modified to take into account the varying reliability of these estimated feature vectors . experiments performed on both the <material_1> and the <material_2> show substantial gains in <metric_9> across a range of <otherscientificterm_4> .",
    "abstract_og": "this paper presents a combined packet loss compensation system for distributed speech recognition . compensation is applied at three stages within the distributed speech recognition beginning with interleaving on the terminal device to reduce burst lengths in the received feature vector stream . on the receiver side estimation of missing vectors is applied to reconstruct the feature vector stream prior to recognition . finally , the decoding process of the recogniser is modified to take into account the varying reliability of these estimated feature vectors . experiments performed on both the aurora connected digits task and the wsjcam0 large vocabulary task show substantial gains in recognition accuracy across a range of packet loss conditions ."
  },
  {
    "title": "Utilizing Scatter for Pixel Subspace Selection .",
    "entities": [
      "eigenvector decomposition of large matrices",
      "low dimensional feature sub-space",
      "content based indexing",
      "matrix eigenvector decomposition",
      "selection of pixels",
      "statistical pattern recognition",
      "discrete optimization technique",
      "computational procedures",
      "scatter measures",
      "principal components",
      "linear time",
      "linear discriminants",
      "subspace size",
      "linear combinations",
      "features",
      "scatter",
      "clustering"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <task> <method> <method> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "linear time -- FEATURE-OF -- discrete optimization technique",
      "eigenvector decomposition of large matrices -- USED-FOR -- computational procedures",
      "clustering -- CONJUNCTION -- content based indexing",
      "discrete optimization technique -- USED-FOR -- selection of pixels",
      "principal components -- CONJUNCTION -- linear discriminants",
      "matrix eigenvector decomposition -- USED-FOR -- computational procedures",
      "discrete optimization technique -- USED-FOR -- scatter measures",
      "selection of pixels -- USED-FOR -- scatter measures"
    ],
    "abstract": "measures of <otherscientificterm_15> are used in <task_5> to identify and select important <otherscientificterm_14> , computed as <otherscientificterm_13> of the given <otherscientificterm_14> . examples include <method_9> and <otherscientificterm_11> . the classic <method_7> require <otherscientificterm_0> , and in the case of images they are only practical for identifying a <otherscientificterm_1> . we investigate the case in which the selected <otherscientificterm_14> are required to be a subset of the given <otherscientificterm_14> . it is shown that the same <metric_8> used in the general case can also be used in this discrete selection case , but the <method_7> no longer involves <method_3> . instead , the <otherscientificterm_4> that optimize <metric_8> can be accomplished by a very simple and efficient <method_6> that runs in <otherscientificterm_10> regardless of the <otherscientificterm_12> . applications to <method_16> and <task_2> are discussed .",
    "abstract_og": "measures of scatter are used in statistical pattern recognition to identify and select important features , computed as linear combinations of the given features . examples include principal components and linear discriminants . the classic computational procedures require eigenvector decomposition of large matrices , and in the case of images they are only practical for identifying a low dimensional feature sub-space . we investigate the case in which the selected features are required to be a subset of the given features . it is shown that the same scatter measures used in the general case can also be used in this discrete selection case , but the computational procedures no longer involves matrix eigenvector decomposition . instead , the selection of pixels that optimize scatter measures can be accomplished by a very simple and efficient discrete optimization technique that runs in linear time regardless of the subspace size . applications to clustering and content based indexing are discussed ."
  },
  {
    "title": "1-D continuous non-minimum phase retrieval using the wavelet transform .",
    "entities": [
      "1-d continuous , non-minimum phase retrieval algorithm",
      "noisy fourier magnitude information",
      "structured system of equations",
      "linear system of equations",
      "priori signal information",
      "continuous phase retrieval",
      "phase retrieval problem",
      "inverse fourier transform",
      "iterative algorithm",
      "wavelet expansions",
      "wavelet bases",
      "stagnation problems",
      "phase information",
      "non-iterative algorithms",
      "iterative algorithms"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <task> <task> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <method> <method>",
    "relations": [
      "iterative algorithms -- COMPARE -- non-iterative algorithms",
      "priori signal information -- USED-FOR -- wavelet bases",
      "iterative algorithms -- USED-FOR -- stagnation problems"
    ],
    "abstract": "the <task_6> arises when a signal must be reconstructed from only the magnitude of its fourier transform ; if the <otherscientificterm_12> were also available , the signal could simply be synthesized using the <otherscientificterm_7> . in <task_5> , most previous solutions rely on discretizing the problem and then employing an <method_8> . we a void this approximation by using <method_9> to transform this uncountably innnite problem into a <method_3> . the <otherscientificterm_10> permit a solution by incorporating a <otherscientificterm_4> and they provide a <method_2> which results in a fast algorithm . our solutions obviate the <task_11> associated with <method_14> , they are computationally simpler and more stable than previous <method_13> , and they can accommodate <otherscientificterm_1> . this paper develops our <method_0> and illustrates its eeectiveness with numerical examples .",
    "abstract_og": "the phase retrieval problem arises when a signal must be reconstructed from only the magnitude of its fourier transform ; if the phase information were also available , the signal could simply be synthesized using the inverse fourier transform . in continuous phase retrieval , most previous solutions rely on discretizing the problem and then employing an iterative algorithm . we a void this approximation by using wavelet expansions to transform this uncountably innnite problem into a linear system of equations . the wavelet bases permit a solution by incorporating a priori signal information and they provide a structured system of equations which results in a fast algorithm . our solutions obviate the stagnation problems associated with iterative algorithms , they are computationally simpler and more stable than previous non-iterative algorithms , and they can accommodate noisy fourier magnitude information . this paper develops our 1-d continuous , non-minimum phase retrieval algorithm and illustrates its eeectiveness with numerical examples ."
  },
  {
    "title": "Fingerprinting to Identify Repeated Sound Events in Long-Duration Personal Audio Recordings .",
    "entities": [
      "body-worn solid-state audio recorders",
      "audio fingerprinting technique",
      "electronic telephone rings",
      "garage door openings",
      "background noise levels",
      "recurrent sound events",
      "energy peaks",
      "structured sound",
      "music recordings",
      "framing issues",
      "noise",
      "time-frequency",
      "jingles"
    ],
    "types": "<method> <method> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "electronic telephone rings -- HYPONYM-OF -- structured sound",
      "energy peaks -- USED-FOR -- audio fingerprinting technique",
      "audio fingerprinting technique -- USED-FOR -- recurrent sound events",
      "time-frequency -- USED-FOR -- audio fingerprinting technique",
      "audio fingerprinting technique -- USED-FOR -- music recordings",
      "jingles -- HYPONYM-OF -- structured sound",
      "audio fingerprinting technique -- USED-FOR -- background noise levels",
      "jingles -- CONJUNCTION -- electronic telephone rings"
    ],
    "abstract": "body-worn solid-state audio recorders can easily and cheaply capture the bearer 's entire acoustic environment throughout the day ; we refer to such recordings as '' personal audio '' . extracting useful information , and providing access and navigation tools for this data is a challenge ; in this paper we investigate the use of an <method_1> , originally developed for identifying <material_8> corrupted by <otherscientificterm_10> , as a tool to rapidly identify <material_5> within long -lrb- multi-day -rrb- recordings . the <method_1> is based on <otherscientificterm_6> in <otherscientificterm_11> , largely removing <task_9> and making <method_1> intrinsically robust to <otherscientificterm_4> . we show that the <method_1> is very effective at identifying exact repetitions of <material_7> -lrb- such as <otherscientificterm_12> and <material_2> -rrb- but is unable to find repeats of more ` organic ' sound events such as <otherscientificterm_3> .",
    "abstract_og": "body-worn solid-state audio recorders can easily and cheaply capture the bearer 's entire acoustic environment throughout the day ; we refer to such recordings as '' personal audio '' . extracting useful information , and providing access and navigation tools for this data is a challenge ; in this paper we investigate the use of an audio fingerprinting technique , originally developed for identifying music recordings corrupted by noise , as a tool to rapidly identify recurrent sound events within long -lrb- multi-day -rrb- recordings . the audio fingerprinting technique is based on energy peaks in time-frequency , largely removing framing issues and making audio fingerprinting technique intrinsically robust to background noise levels . we show that the audio fingerprinting technique is very effective at identifying exact repetitions of structured sound -lrb- such as jingles and electronic telephone rings -rrb- but is unable to find repeats of more ` organic ' sound events such as garage door openings ."
  },
  {
    "title": "Dependency Between Error Variance of the a Priori Information and a Modified Channel Noise Variance in Turbo-Equalisation .",
    "entities": [
      "modification of the channel noise variance",
      "modified channel noise variance",
      "priori information statistics",
      "equaliser input",
      "turbo-equalisers"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "modified channel noise variance -- CONJUNCTION -- priori information statistics"
    ],
    "abstract": "in this paper , we propose a method based on the <otherscientificterm_0> at the <otherscientificterm_3> in order to improve the performance of a turbo-equaliser . we will show a relation between the <otherscientificterm_1> and the a <otherscientificterm_2> . the simulation are done for 2 types of <otherscientificterm_4> .",
    "abstract_og": "in this paper , we propose a method based on the modification of the channel noise variance at the equaliser input in order to improve the performance of a turbo-equaliser . we will show a relation between the modified channel noise variance and the a priori information statistics . the simulation are done for 2 types of turbo-equalisers ."
  },
  {
    "title": "Structured light 3D scanning in the presence of global illumination .",
    "entities": [
      "direct and global components of scene radiance",
      "structured light-based 3d scanning",
      "structured light-based shape recovery",
      "structured light patterns",
      "global illumination effects",
      "global illumination effects",
      "logical operations",
      "global illumination",
      "scanning systems",
      "combinatorial mathematics",
      "direct component",
      "capture time",
      "inter-reflections",
      "overhead",
      "diffusion"
    ],
    "types": "<otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "inter-reflections -- HYPONYM-OF -- global illumination effects",
      "structured light patterns -- USED-FOR -- global illumination effects",
      "diffusion -- HYPONYM-OF -- global illumination effects",
      "global illumination -- USED-FOR -- structured light-based shape recovery",
      "inter-reflections -- CONJUNCTION -- diffusion"
    ],
    "abstract": "global illumination effects such as <otherscientificterm_12> , <otherscientificterm_14> and sub-surface scattering severely degrade the performance of <task_1> . in this paper , we analyze the errors caused by <otherscientificterm_7> in <task_2> . based on this analysis , we design <otherscientificterm_3> that are resilient to individual <otherscientificterm_4> using simple <method_6> and tools from <method_9> . scenes exhibiting multiple phenomena are handled by combining results from a small ensemble of such <otherscientificterm_3> . this combination also allows us to detect any residual errors that are corrected by acquiring a few additional images . our techniques do not require explicit separation of the <otherscientificterm_0> and hence work even in scenarios where the separation fails or the <method_10> is too low . our methods can be readily incorporated into existing <method_8> without significant <otherscientificterm_13> in terms of <otherscientificterm_11> or hardware . we show results on a variety of scenes with complex shape and material properties and challenging <otherscientificterm_4> .",
    "abstract_og": "global illumination effects such as inter-reflections , diffusion and sub-surface scattering severely degrade the performance of structured light-based 3d scanning . in this paper , we analyze the errors caused by global illumination in structured light-based shape recovery . based on this analysis , we design structured light patterns that are resilient to individual global illumination effects using simple logical operations and tools from combinatorial mathematics . scenes exhibiting multiple phenomena are handled by combining results from a small ensemble of such structured light patterns . this combination also allows us to detect any residual errors that are corrected by acquiring a few additional images . our techniques do not require explicit separation of the direct and global components of scene radiance and hence work even in scenarios where the separation fails or the direct component is too low . our methods can be readily incorporated into existing scanning systems without significant overhead in terms of capture time or hardware . we show results on a variety of scenes with complex shape and material properties and challenging global illumination effects ."
  },
  {
    "title": "Intonation modelling for the synthesis of structured documents .",
    "entities": [
      "rnn -lrb- recurrent neural network -rrb- intonation model",
      "feature selection process",
      "read isolated sentences",
      "text type",
      "intonation models",
      "read documents",
      "intonation contours",
      "structured documents",
      "data-driven techniques",
      "intonation modelling",
      "text-level features",
      "isolated sentences",
      "prosody model",
      "text structure",
      "features",
      "type-setting"
    ],
    "types": "<method> <method> <material> <otherscientificterm> <method> <material> <otherscientificterm> <material> <method> <task> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "text structure -- CONJUNCTION -- type-setting",
      "text type -- CONJUNCTION -- text structure",
      "text structure -- HYPONYM-OF -- text-level features",
      "type-setting -- HYPONYM-OF -- text-level features",
      "text type -- HYPONYM-OF -- text-level features"
    ],
    "abstract": "human readings of <material_7> exhibit a much richer intonation than that observed in <material_2> . it is a challenge to capture this richness in an automatic way using <method_8> . in this paper , we extend our previous research on <task_9> for <material_11> in different respects : -lrb- i -rrb- the <method_0> is now trained and evaluated on <material_5> , -lrb- ii -rrb- the <method_0> is evaluated as part of the overall <method_12> , -lrb- iii -rrb- the <method_1> is completely automated , and -lrb- iv -rrb- the importance of <otherscientificterm_10> such as <otherscientificterm_3> , <otherscientificterm_13> and <otherscientificterm_15> are investigated . it is demonstrated that acceptable <method_4> can be constructed starting from a database that does not contain any explicit hand labelling of the <otherscientificterm_6> . it also appears that <otherscientificterm_3> and <otherscientificterm_13> are important <otherscientificterm_14> whereas <otherscientificterm_15> is not .",
    "abstract_og": "human readings of structured documents exhibit a much richer intonation than that observed in read isolated sentences . it is a challenge to capture this richness in an automatic way using data-driven techniques . in this paper , we extend our previous research on intonation modelling for isolated sentences in different respects : -lrb- i -rrb- the rnn -lrb- recurrent neural network -rrb- intonation model is now trained and evaluated on read documents , -lrb- ii -rrb- the rnn -lrb- recurrent neural network -rrb- intonation model is evaluated as part of the overall prosody model , -lrb- iii -rrb- the feature selection process is completely automated , and -lrb- iv -rrb- the importance of text-level features such as text type , text structure and type-setting are investigated . it is demonstrated that acceptable intonation models can be constructed starting from a database that does not contain any explicit hand labelling of the intonation contours . it also appears that text type and text structure are important features whereas type-setting is not ."
  },
  {
    "title": "A Blind Algorithm Based on Difference of Norms for Equalization of Biorthogonal Signals .",
    "entities": [
      "intersymbol interference",
      "biorthogonal modulation",
      "adaptive equalization of bom signals",
      "blind equalization of bom signals",
      "classical adaptive equalization techniques",
      "equalization of bom signals",
      "mmse and lms-based equalizers",
      "blind algorithm",
      "biorthogonal signaling",
      "nar-rowband systems",
      "numerical simulations",
      "bpsk",
      "mmse",
      "peculiarities"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <method> <task> <otherscientificterm> <method> <task> <method> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "classical adaptive equalization techniques -- USED-FOR -- equalization of bom signals",
      "mmse -- HYPONYM-OF -- blind algorithm",
      "blind algorithm -- USED-FOR -- biorthogonal signaling",
      "blind algorithm -- USED-FOR -- adaptive equalization of bom signals"
    ],
    "abstract": "motivated by increasing interest in energy efficient modulations , we investigate a <method_7> for <task_8> . while this modulation has historically been considered only for use in <method_9> without <task_0> , recent attention has been given to its use in <task_0> . due to the fact that <method_1> results in a source that is not i.i.d. , however , <method_4> can not be directly applied to <task_5> . we review the <otherscientificterm_6> , and then identify some <otherscientificterm_13> that arise in <otherscientificterm_3> when compared to more traditional modulations like <method_11> . next , we present a novel <method_7> , called <method_12> , for the <task_2> . we discuss the convergence properties of this <method_7> , and demonstrate its performance with <method_10> .",
    "abstract_og": "motivated by increasing interest in energy efficient modulations , we investigate a blind algorithm for biorthogonal signaling . while this modulation has historically been considered only for use in nar-rowband systems without intersymbol interference , recent attention has been given to its use in intersymbol interference . due to the fact that biorthogonal modulation results in a source that is not i.i.d. , however , classical adaptive equalization techniques can not be directly applied to equalization of bom signals . we review the mmse and lms-based equalizers , and then identify some peculiarities that arise in blind equalization of bom signals when compared to more traditional modulations like bpsk . next , we present a novel blind algorithm , called mmse , for the adaptive equalization of bom signals . we discuss the convergence properties of this blind algorithm , and demonstrate its performance with numerical simulations ."
  },
  {
    "title": "Active Music Listening Interfaces Based on Signal Processing .",
    "entities": [
      "active music listening interfaces",
      "timbre of instrument sounds",
      "active music listening",
      "active music listening",
      "song lyrics",
      "music-understanding technologies",
      "compact-disc recordings",
      "active interactions",
      "signal processing"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <material> <method> <material> <otherscientificterm> <method>",
    "relations": [
      "signal processing -- USED-FOR -- music-understanding technologies"
    ],
    "abstract": "this paper introduces our research aimed at building '' <method_0> '' . this research approach is intended to enrich end-users ' music listening experiences by applying <method_5> based on <method_8> . <method_2> is a way of listening to music through <otherscientificterm_7> . we have developed seven interfaces for <task_3> , such as interfaces for skipping sections of no interest within a musical piece while viewing a graphical overview of the entire song structure , for displaying virtual dancers or <material_4> synchronized with the music , for changing the <otherscientificterm_1> in <material_6> , and for browsing a large music collection to encounter interesting musical pieces or artists . these interfaces demonstrate the importance of <method_5> and the bene \u00bf t they offer to end users . our hope is that this work will help change music listening into a more active , immersive experience .",
    "abstract_og": "this paper introduces our research aimed at building '' active music listening interfaces '' . this research approach is intended to enrich end-users ' music listening experiences by applying music-understanding technologies based on signal processing . active music listening is a way of listening to music through active interactions . we have developed seven interfaces for active music listening , such as interfaces for skipping sections of no interest within a musical piece while viewing a graphical overview of the entire song structure , for displaying virtual dancers or song lyrics synchronized with the music , for changing the timbre of instrument sounds in compact-disc recordings , and for browsing a large music collection to encounter interesting musical pieces or artists . these interfaces demonstrate the importance of music-understanding technologies and the bene \u00bf t they offer to end users . our hope is that this work will help change music listening into a more active , immersive experience ."
  },
  {
    "title": "Learning Representation and Control in Continuous Markov Decision Processes .",
    "entities": [
      "spectral analysis of the state space manifold",
      "parametric radial basis function method",
      "least-squares policy iteration method",
      "continuous markov decision processes",
      "proto-value functions",
      "nystr\u00f6m extension",
      "control policy",
      "mountain car",
      "value function",
      "graph laplacian",
      "inverted pendulum",
      "subspace",
      "sensitivity"
    ],
    "types": "<otherscientificterm> <method> <method> <task> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "least-squares policy iteration method -- USED-FOR -- control policy",
      "inverted pendulum -- CONJUNCTION -- mountain car"
    ],
    "abstract": "this paper presents a novel framework for simultaneously learning representation and control in <task_3> . our approach builds on the framework of <otherscientificterm_4> , in which the underlying representation or basis functions are automatically derived from a <otherscientificterm_0> . the <otherscientificterm_4> correspond to the eigenfunctions of the <otherscientificterm_9> . we describe an approach to extend the eigenfunctions to novel states using the <method_5> . a <method_2> is used to learn the <method_6> , where the underlying <otherscientificterm_11> for approximating the <otherscientificterm_8> is spanned by the learned <otherscientificterm_4> . a detailed set of experiments is presented using classic benchmark tasks , including the <otherscientificterm_10> and the <method_7> , showing the <metric_12> in performance to various parameters , and including comparisons with a <method_1> .",
    "abstract_og": "this paper presents a novel framework for simultaneously learning representation and control in continuous markov decision processes . our approach builds on the framework of proto-value functions , in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold . the proto-value functions correspond to the eigenfunctions of the graph laplacian . we describe an approach to extend the eigenfunctions to novel states using the nystr\u00f6m extension . a least-squares policy iteration method is used to learn the control policy , where the underlying subspace for approximating the value function is spanned by the learned proto-value functions . a detailed set of experiments is presented using classic benchmark tasks , including the inverted pendulum and the mountain car , showing the sensitivity in performance to various parameters , and including comparisons with a parametric radial basis function method ."
  },
  {
    "title": "Parameter clustering and sharing in variable-parameter HMMs for noise robust speech recognition .",
    "entities": [
      "cubic-spline-based variable-parameter hidden markov model",
      "parameter clustering and sharing algorithm",
      "mean and variance parameters",
      "gaussian mixture components",
      "relative wer reduction",
      "cubic spline functions",
      "spline functions",
      "parameter sharing",
      "aurora-3 corpus",
      "environment-dependent parameters",
      "model parameters",
      "well-matched condition",
      "clustering algorithm",
      "noise robustness",
      "hmm"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric> <method>",
    "relations": [
      "cubic-spline-based variable-parameter hidden markov model -- USED-FOR -- noise robustness",
      "cubic-spline-based variable-parameter hidden markov model -- COMPARE -- hmm",
      "well-matched condition -- FEATURE-OF -- hmm",
      "aurora-3 corpus -- EVALUATE-FOR -- cubic-spline-based variable-parameter hidden markov model"
    ],
    "abstract": "recently we proposed a <method_0> whose <otherscientificterm_2> vary according to some <otherscientificterm_5> of additional <otherscientificterm_9> . we have shown good properties of the <method_0> and demonstrated on the <material_8> that <method_0> greatly outperforms the mce-trained conventional <method_14> at the cost of increased total number of <otherscientificterm_10> . in this paper , we propose to share <otherscientificterm_6> across different <method_3> to reduce the total number of <otherscientificterm_10> and develop a <method_12> to do so . we demonstrate the effectiveness of our <method_1> for the <method_0> on <material_8> and show that proper <method_7> can reduce the number of parameters from 4 times of that used in the conventional <method_14> to 1.13 times and still get 18 % <metric_4> over the mce trained conventional <method_14> under the <otherscientificterm_11> . effective <method_7> makes the <method_0> an attractive model for <metric_13> .",
    "abstract_og": "recently we proposed a cubic-spline-based variable-parameter hidden markov model whose mean and variance parameters vary according to some cubic spline functions of additional environment-dependent parameters . we have shown good properties of the cubic-spline-based variable-parameter hidden markov model and demonstrated on the aurora-3 corpus that cubic-spline-based variable-parameter hidden markov model greatly outperforms the mce-trained conventional hmm at the cost of increased total number of model parameters . in this paper , we propose to share spline functions across different gaussian mixture components to reduce the total number of model parameters and develop a clustering algorithm to do so . we demonstrate the effectiveness of our parameter clustering and sharing algorithm for the cubic-spline-based variable-parameter hidden markov model on aurora-3 corpus and show that proper parameter sharing can reduce the number of parameters from 4 times of that used in the conventional hmm to 1.13 times and still get 18 % relative wer reduction over the mce trained conventional hmm under the well-matched condition . effective parameter sharing makes the cubic-spline-based variable-parameter hidden markov model an attractive model for noise robustness ."
  },
  {
    "title": "Comparison of neural network natural and ordinary gradient algorithms for satellite down link identification .",
    "entities": [
      "identification of satellite communication channels",
      "classical multi-layer perceptron structure",
      "neural network architecture",
      "non-normalized power amplifier",
      "multi-layer perceptron family",
      "down link",
      "ordinary gradient",
      "natural gradient",
      "space telecommunications"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "ordinary gradient -- CONJUNCTION -- natural gradient",
      "neural network architecture -- USED-FOR -- multi-layer perceptron family",
      "identification of satellite communication channels -- PART-OF -- space telecommunications"
    ],
    "abstract": "in this paper , we present a <method_2> that belongs to the <method_4> , associated with two different algorithms : the <otherscientificterm_6> and the <otherscientificterm_7> , we compare performances of those algorithms . the identification of a <method_3> yielded to the introduction of an additional weight in the <otherscientificterm_1> . the application of this <method_2> is <method_8> : <task_0> , and especially the <otherscientificterm_5> . this link is made up with two elements . the first one is a high power amplifier -lrb- non-linearity -rrb- . the second one is a filter -lrb- memory -rrb- .",
    "abstract_og": "in this paper , we present a neural network architecture that belongs to the multi-layer perceptron family , associated with two different algorithms : the ordinary gradient and the natural gradient , we compare performances of those algorithms . the identification of a non-normalized power amplifier yielded to the introduction of an additional weight in the classical multi-layer perceptron structure . the application of this neural network architecture is space telecommunications : identification of satellite communication channels , and especially the down link . this link is made up with two elements . the first one is a high power amplifier -lrb- non-linearity -rrb- . the second one is a filter -lrb- memory -rrb- ."
  },
  {
    "title": "Bayes risk-based optimization of dialogue management for document retrieval system with speech interface .",
    "entities": [
      "minimization of bayes risk",
      "generating responses or confirmations",
      "average number of turns",
      "success rate of retrieval",
      "n-best candidates of asr",
      "document knowledge base",
      "information navigation system",
      "correct information presentation",
      "question-answering capability",
      "contextual information",
      "redundant turns",
      "dialogue management",
      "information access",
      "reward",
      "penalty"
    ],
    "types": "<task> <task> <metric> <metric> <otherscientificterm> <material> <task> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <metric>",
    "relations": [
      "success rate of retrieval -- CONJUNCTION -- average number of turns",
      "reward -- CONJUNCTION -- penalty",
      "information navigation system -- USED-FOR -- generating responses or confirmations",
      "average number of turns -- EVALUATE-FOR -- minimization of bayes risk",
      "reward -- USED-FOR -- minimization of bayes risk",
      "dialogue management -- USED-FOR -- information navigation system",
      "reward -- USED-FOR -- correct information presentation",
      "penalty -- USED-FOR -- minimization of bayes risk",
      "n-best candidates of asr -- CONJUNCTION -- contextual information",
      "contextual information -- USED-FOR -- information navigation system",
      "penalty -- USED-FOR -- redundant turns",
      "average number of turns -- USED-FOR -- information access",
      "n-best candidates of asr -- USED-FOR -- information navigation system",
      "success rate of retrieval -- EVALUATE-FOR -- minimization of bayes risk",
      "document knowledge base -- USED-FOR -- information navigation system"
    ],
    "abstract": "we propose an efficient <method_11> for an <task_6> based on a <material_5> . it is expected that incorporation of appropriate <otherscientificterm_4> and <otherscientificterm_9> will improve the <task_6> performance . the <task_6> also has several choices in <task_1> . in this paper , this selection is optimized as <task_0> based on <metric_13> for <metric_7> and <metric_14> for <otherscientificterm_10> . we have evaluated this <task_0> with our spoken dialogue <task_6> '' dialogue navigator for kyoto city '' , which also has <otherscientificterm_8> . effectiveness of the proposed <task_0> was confirmed in the <metric_3> and the <metric_2> for <otherscientificterm_12> .",
    "abstract_og": "we propose an efficient dialogue management for an information navigation system based on a document knowledge base . it is expected that incorporation of appropriate n-best candidates of asr and contextual information will improve the information navigation system performance . the information navigation system also has several choices in generating responses or confirmations . in this paper , this selection is optimized as minimization of bayes risk based on reward for correct information presentation and penalty for redundant turns . we have evaluated this minimization of bayes risk with our spoken dialogue information navigation system '' dialogue navigator for kyoto city '' , which also has question-answering capability . effectiveness of the proposed minimization of bayes risk was confirmed in the success rate of retrieval and the average number of turns for information access ."
  },
  {
    "title": "Joint source-channel coding for scalable video using models of rate-distortion functions .",
    "entities": [
      "unequal error protection",
      "universal rate-distortion characteristic plots",
      "residual bit error rate",
      "joint source-channel coding scheme",
      "snr scalable video coder",
      "end-to-end distortion",
      "source rate",
      "optimization algorithm",
      "scalable layers",
      "computational complexity",
      "total distortion",
      "channel coding",
      "scalable video",
      "error rate",
      "layer"
    ],
    "types": "<method> <otherscientificterm> <metric> <method> <method> <otherscientificterm> <metric> <method> <otherscientificterm> <metric> <otherscientificterm> <method> <material> <metric> <otherscientificterm>",
    "relations": [
      "joint source-channel coding scheme -- USED-FOR -- scalable video",
      "universal rate-distortion characteristic plots -- USED-FOR -- optimization algorithm",
      "unequal error protection -- USED-FOR -- snr scalable video coder",
      "computational complexity -- EVALUATE-FOR -- snr scalable video coder"
    ],
    "abstract": "a <method_3> for <material_12> is developed in this paper . an <method_4> is used and <method_0> is allowed for each scalable <otherscientificterm_14> . our problem is to allocate the available bit rate across <otherscientificterm_8> and , within each <otherscientificterm_14> , between source and <method_11> , while minimizing the <otherscientificterm_5> of the received video sequence . the resulting <method_7> we propose utilizes <otherscientificterm_1> . these plots show the contribution of each <otherscientificterm_14> to the <otherscientificterm_10> as a function of the <metric_6> of the <otherscientificterm_14> and the <metric_2> -lrb- the <metric_13> that remains after the use of <method_11> -rrb- . models for these plots are proposed in order to reduce the <metric_9> of the <method_4> . experimental results demonstrate the effectiveness of the proposed <method_3> .",
    "abstract_og": "a joint source-channel coding scheme for scalable video is developed in this paper . an snr scalable video coder is used and unequal error protection is allowed for each scalable layer . our problem is to allocate the available bit rate across scalable layers and , within each layer , between source and channel coding , while minimizing the end-to-end distortion of the received video sequence . the resulting optimization algorithm we propose utilizes universal rate-distortion characteristic plots . these plots show the contribution of each layer to the total distortion as a function of the source rate of the layer and the residual bit error rate -lrb- the error rate that remains after the use of channel coding -rrb- . models for these plots are proposed in order to reduce the computational complexity of the snr scalable video coder . experimental results demonstrate the effectiveness of the proposed joint source-channel coding scheme ."
  },
  {
    "title": "Training a Quantum Neural Network .",
    "entities": [
      "quantum learning algorithm",
      "quantum neural network",
      "machine intelligence",
      "quantum learning",
      "classical networks"
    ],
    "types": "<method> <method> <task> <method> <method>",
    "relations": [
      "quantum learning -- USED-FOR -- machine intelligence"
    ],
    "abstract": "quantum learning holds great promise for the field of <task_2> . the most studied <method_0> is the <method_1> . many such <method_0> have been proposed , yet none has become a standard . in addition , these <method_0> usually leave out many details , often excluding how they intend to train their networks . this paper discusses one approach to the problem and what advantages it would have over <method_4> .",
    "abstract_og": "quantum learning holds great promise for the field of machine intelligence . the most studied quantum learning algorithm is the quantum neural network . many such quantum learning algorithm have been proposed , yet none has become a standard . in addition , these quantum learning algorithm usually leave out many details , often excluding how they intend to train their networks . this paper discusses one approach to the problem and what advantages it would have over classical networks ."
  },
  {
    "title": "PLOW : A Collaborative Task Learning Agent .",
    "entities": [
      "knowledge representation and reasoning",
      "deep natural language understanding",
      "executable task models",
      "collaborative learning session",
      "ai technologies",
      "machine learning",
      "dialogue systems",
      "plan-ning/agent-based systems"
    ],
    "types": "<method> <task> <method> <method> <method> <task> <method> <method>",
    "relations": [
      "plan-ning/agent-based systems -- HYPONYM-OF -- ai technologies",
      "dialogue systems -- CONJUNCTION -- machine learning",
      "plan-ning/agent-based systems -- CONJUNCTION -- machine learning",
      "dialogue systems -- HYPONYM-OF -- ai technologies",
      "dialogue systems -- CONJUNCTION -- plan-ning/agent-based systems",
      "deep natural language understanding -- HYPONYM-OF -- ai technologies"
    ],
    "abstract": "to be effective , an agent that collaborates with humans needs to be able to learn new tasks from humans they work with . this paper describes a system that learns <method_2> from a single <method_3> consisting of demonstration , explanation and dialogue . to accomplish this , the system integrates a range of <method_4> : <task_1> , <method_0> , <method_6> , <method_7> and <task_5> . a formal evaluation shows the approach has great promise .",
    "abstract_og": "to be effective , an agent that collaborates with humans needs to be able to learn new tasks from humans they work with . this paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration , explanation and dialogue . to accomplish this , the system integrates a range of ai technologies : deep natural language understanding , knowledge representation and reasoning , dialogue systems , plan-ning/agent-based systems and machine learning . a formal evaluation shows the approach has great promise ."
  },
  {
    "title": "Periodic Motion Detection and Segmentation via Approximate Sequence Alignment .",
    "entities": [
      "detecting and segmenting periodic motion",
      "local motion and shape",
      "periodic motion detection",
      "dynamic geometric transformations",
      "time-linear matrix functions",
      "video sequences",
      "constant translation",
      "ransac procedure",
      "periodic views",
      "space-time points",
      "periodic motion",
      "fundamental matrices",
      "non-rigid backgrounds",
      "motion segmentation",
      "dynamic quantities",
      "image sequence",
      "sequence alignment",
      "periodicity",
      "homographies",
      "detection",
      "cue"
    ],
    "types": "<task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "sequence alignment -- USED-FOR -- periodic motion detection",
      "homographies -- USED-FOR -- time-linear matrix functions"
    ],
    "abstract": "a method for <task_0> is presented . we exploit <otherscientificterm_17> as a <otherscientificterm_20> and detect <otherscientificterm_10> in complex scenes where common methods for <task_13> are likely to fail . we note that <task_2> can be seen as an approximate case of <task_16> where an <material_15> is matched to itself over one or more periods of time . to use this observation , we first consider alignment of two <material_5> obtained by independently moving cameras . under assumption of <otherscientificterm_6> , the <otherscientificterm_11> and the <otherscientificterm_18> are shown to be <otherscientificterm_4> . these <otherscientificterm_14> can be estimated by matching corresponding <otherscientificterm_9> with similar <otherscientificterm_1> . for <otherscientificterm_10> , we match corresponding points across periods and develop a <method_7> to simultaneously estimate the period and the <otherscientificterm_3> between <otherscientificterm_8> . using this method , we demonstrate <task_19> and segmentation of human <otherscientificterm_10> in complex scenes with <otherscientificterm_12> , moving camera and motion parallax .",
    "abstract_og": "a method for detecting and segmenting periodic motion is presented . we exploit periodicity as a cue and detect periodic motion in complex scenes where common methods for motion segmentation are likely to fail . we note that periodic motion detection can be seen as an approximate case of sequence alignment where an image sequence is matched to itself over one or more periods of time . to use this observation , we first consider alignment of two video sequences obtained by independently moving cameras . under assumption of constant translation , the fundamental matrices and the homographies are shown to be time-linear matrix functions . these dynamic quantities can be estimated by matching corresponding space-time points with similar local motion and shape . for periodic motion , we match corresponding points across periods and develop a ransac procedure to simultaneously estimate the period and the dynamic geometric transformations between periodic views . using this method , we demonstrate detection and segmentation of human periodic motion in complex scenes with non-rigid backgrounds , moving camera and motion parallax ."
  },
  {
    "title": "Correction of B0 inhomogeneity distortion in magnetic resonance spectroscopic imaging .",
    "entities": [
      "echo-planar spectroscopic imaging sequence",
      "total variation regularization",
      "sparsity of the spectral data",
      "3-d mr spectroscopic data",
      "multi-slice proton imaging data",
      "0 b inhomogeneity artifact",
      "0 b field map",
      "uniform spectral peaks",
      "epsi one",
      "reconstruction",
      "shimming"
    ],
    "types": "<material> <method> <material> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method>",
    "relations": [
      "echo-planar spectroscopic imaging sequence -- USED-FOR -- 3-d mr spectroscopic data",
      "total variation regularization -- USED-FOR -- uniform spectral peaks",
      "shimming -- USED-FOR -- multi-slice proton imaging data",
      "0 b field map -- USED-FOR -- 0 b inhomogeneity artifact"
    ],
    "abstract": "in this paper we reconstruct the <material_3> acquired using an <material_0> . we propose to compensate for <otherscientificterm_5> by using the <otherscientificterm_6> and * 2 t decay estimated from a higher resolution , <material_4> scanned with similar <method_10> to the <otherscientificterm_8> . we employ <method_1> to obtain more <otherscientificterm_7> and lineshapes , and exploit the <material_2> by using the 1 a norm in the <task_9> .",
    "abstract_og": "in this paper we reconstruct the 3-d mr spectroscopic data acquired using an echo-planar spectroscopic imaging sequence . we propose to compensate for 0 b inhomogeneity artifact by using the 0 b field map and * 2 t decay estimated from a higher resolution , multi-slice proton imaging data scanned with similar shimming to the epsi one . we employ total variation regularization to obtain more uniform spectral peaks and lineshapes , and exploit the sparsity of the spectral data by using the 1 a norm in the reconstruction ."
  },
  {
    "title": "Active learning for spoken language understanding .",
    "entities": [
      "statistical call classification system",
      "at&t customer care",
      "active learning methods",
      "certainty-based active learning",
      "call classification system",
      "committee-based active learning",
      "active learning",
      "classifiers",
      "classifier"
    ],
    "types": "<method> <task> <method> <method> <method> <method> <method> <method> <method>",
    "relations": [
      "certainty-based active learning -- USED-FOR -- active learning methods",
      "call classification system -- USED-FOR -- at&t customer care",
      "committee-based active learning -- USED-FOR -- active learning methods",
      "call classification system -- USED-FOR -- active learning methods"
    ],
    "abstract": "in this paper , we describe <method_2> for reducing the labeling effort in a <method_0> . <method_6> aims to minimize the number of labeled utterances by automatically selecting for labeling the utterances that are likely to be most informative . the first <method_2> , inspired by <method_3> , selects the examples that the <method_8> is least confident about . the second <method_2> , inspired by <method_5> , selects the examples that multiple <method_7> do not agree on . we have evaluated these <method_2> using a <method_4> used for <task_1> . our results indicate that it is possible to reduce human labeling effort at least by a factor of two .",
    "abstract_og": "in this paper , we describe active learning methods for reducing the labeling effort in a statistical call classification system . active learning aims to minimize the number of labeled utterances by automatically selecting for labeling the utterances that are likely to be most informative . the first active learning methods , inspired by certainty-based active learning , selects the examples that the classifier is least confident about . the second active learning methods , inspired by committee-based active learning , selects the examples that multiple classifiers do not agree on . we have evaluated these active learning methods using a call classification system used for at&t customer care . our results indicate that it is possible to reduce human labeling effort at least by a factor of two ."
  },
  {
    "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features .",
    "entities": [
      "sun397 and mit indoor scenes classification datasets",
      "deep convolutional neural networks",
      "supervised or unsupervised recognition tasks",
      "inria holidays retrieval datasets",
      "global cnn activations",
      "orderless vlad pooling",
      "local patches",
      "generic feature",
      "image classification",
      "universal representation",
      "discriminative power",
      "cnn activations",
      "ilsvrc2012/2013 classification",
      "instance-level retrieval",
      "geometric invariance",
      "recognition",
      "robustness"
    ],
    "types": "<material> <method> <task> <material> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <task> <task> <otherscientificterm> <task> <metric>",
    "relations": [
      "geometric invariance -- USED-FOR -- global cnn activations",
      "instance-level retrieval -- HYPONYM-OF -- supervised or unsupervised recognition tasks",
      "ilsvrc2012/2013 classification -- CONJUNCTION -- inria holidays retrieval datasets"
    ],
    "abstract": "deep convolutional neural networks -lrb- cnn -rrb- have shown their promise as a <method_9> for <task_15> . however , <otherscientificterm_4> lack <otherscientificterm_14> , which limits their <metric_16> for <task_15> and matching of highly variable scenes . to improve the invariance of <method_11> without degrading their <otherscientificterm_10> , this paper presents a simple but effective scheme called multi-scale orderless pooling -lrb- mop-cnn -rrb- . this scheme extracts <method_11> for <otherscientificterm_6> at multiple scale levels , performs <method_5> of these <method_11> at each level separately , and concatenates the result . the resulting mop-cnn representation can be used as a <method_7> for either <task_2> , from <task_8> to <task_13> ; it consistently outperforms <otherscientificterm_4> without requiring any joint training of prediction layers for a particular target dataset . in absolute terms , it achieves state-of-the-art results on the challenging <material_0> , and competitive results on <task_12> and <material_3> .",
    "abstract_og": "deep convolutional neural networks -lrb- cnn -rrb- have shown their promise as a universal representation for recognition . however , global cnn activations lack geometric invariance , which limits their robustness for recognition and matching of highly variable scenes . to improve the invariance of cnn activations without degrading their discriminative power , this paper presents a simple but effective scheme called multi-scale orderless pooling -lrb- mop-cnn -rrb- . this scheme extracts cnn activations for local patches at multiple scale levels , performs orderless vlad pooling of these cnn activations at each level separately , and concatenates the result . the resulting mop-cnn representation can be used as a generic feature for either supervised or unsupervised recognition tasks , from image classification to instance-level retrieval ; it consistently outperforms global cnn activations without requiring any joint training of prediction layers for a particular target dataset . in absolute terms , it achieves state-of-the-art results on the challenging sun397 and mit indoor scenes classification datasets , and competitive results on ilsvrc2012/2013 classification and inria holidays retrieval datasets ."
  },
  {
    "title": "Statistical analysis of a subspace method for blind channel identification .",
    "entities": [
      "blind channel estimation of multi channel fir lters",
      "asymptotically correct weighting matrix",
      "mobile communication systems",
      "subspace estimate",
      "noise subspace",
      "asymptotic properties",
      "channel matrix",
      "digital signalling",
      "subspace method",
      "multiplicative constant",
      "orthogonality property"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "digital signalling -- USED-FOR -- mobile communication systems",
      "orthogonality property -- USED-FOR -- subspace method",
      "noise subspace -- CONJUNCTION -- channel matrix",
      "noise subspace -- FEATURE-OF -- orthogonality property"
    ],
    "abstract": "this paper considers the problem of <task_0> . this is a problem arising in e.g. <method_2> using <otherscientificterm_7> . by using the <otherscientificterm_10> between the <otherscientificterm_4> and the <otherscientificterm_6> , it has been shown in earlier work that the <otherscientificterm_6> is identiiable up to a <otherscientificterm_9> . in this article , the <otherscientificterm_5> of a <method_8> using this <otherscientificterm_10> is presented . an <otherscientificterm_1> is derived , demonstrating an attainable lower theoretical bound using the <method_3> .",
    "abstract_og": "this paper considers the problem of blind channel estimation of multi channel fir lters . this is a problem arising in e.g. mobile communication systems using digital signalling . by using the orthogonality property between the noise subspace and the channel matrix , it has been shown in earlier work that the channel matrix is identiiable up to a multiplicative constant . in this article , the asymptotic properties of a subspace method using this orthogonality property is presented . an asymptotically correct weighting matrix is derived , demonstrating an attainable lower theoretical bound using the subspace estimate ."
  },
  {
    "title": "Correspondence Expansion for Wide Baseline Stereo .",
    "entities": [
      "error tolerance constraint",
      "derived geometric structure",
      "wide baseline images",
      "point correspondences",
      "epipolar geometry",
      "image pairs",
      "motion algorithms",
      "ransac",
      "constraint"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <material> <method> <method> <otherscientificterm>",
    "relations": [
      "ransac -- USED-FOR -- epipolar geometry"
    ],
    "abstract": "we present a new method for generating large numbers of accurate <otherscientificterm_3> between two <material_2> . this is important for structure from <method_6> , which rely on many correct matches to reduce error in the <otherscientificterm_1> . given a small initial correspondence set we iteratively expand the set with nearby points exhibiting strong affine correlation , and then we constrain the set to an <method_4> using <method_7> . a key point to our algorithm is to allow a high error tolerance in the <otherscientificterm_8> , allowing the correspondence set to expand into many areas of an image before applying a lower <otherscientificterm_0> . we show that this method successfully expands a small set of initial matches , and we demonstrate it on a variety of <material_5> .",
    "abstract_og": "we present a new method for generating large numbers of accurate point correspondences between two wide baseline images . this is important for structure from motion algorithms , which rely on many correct matches to reduce error in the derived geometric structure . given a small initial correspondence set we iteratively expand the set with nearby points exhibiting strong affine correlation , and then we constrain the set to an epipolar geometry using ransac . a key point to our algorithm is to allow a high error tolerance in the constraint , allowing the correspondence set to expand into many areas of an image before applying a lower error tolerance constraint . we show that this method successfully expands a small set of initial matches , and we demonstrate it on a variety of image pairs ."
  },
  {
    "title": "Exploring the Planet of the APEs : a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing .",
    "entities": [
      "internal decoding process",
      "statistical ape methods",
      "mt output quality",
      "language pairs",
      "mt quality",
      "translation quality",
      "systematic analysis",
      "downstream processing",
      "mt system",
      "fo-cusing",
      "english"
    ],
    "types": "<method> <method> <metric> <material> <metric> <metric> <method> <method> <method> <method> <material>",
    "relations": [
      "statistical ape methods -- USED-FOR -- fo-cusing",
      "downstream processing -- USED-FOR -- translation quality"
    ],
    "abstract": "downstream processing of machine translation -lrb- mt -rrb- output promises to be a solution to improve <metric_5> , especially when the <method_8> 's <method_0> is not accessible . both rule-based and statistical automatic post-editing -lrb- ape -rrb- methods have been proposed over the years , but with contrasting results . a missing aspect in previous evaluations is the assessment of different methods : i -rrb- under comparable conditions , and ii -rrb- on different <material_3> featuring variable levels of <metric_4> . <method_9> on <method_1> -lrb- more portable across languages -rrb- , we propose the first <method_6> of two approaches . to understand their potential , we compare them in the same conditions over six <material_3> having <material_10> as source . our results evidence consistent improvements on all <material_3> , a relation between the extent of the gain and <metric_2> , slight but statistically significant performance differences between the two methods , and their possible complementarity .",
    "abstract_og": "downstream processing of machine translation -lrb- mt -rrb- output promises to be a solution to improve translation quality , especially when the mt system 's internal decoding process is not accessible . both rule-based and statistical automatic post-editing -lrb- ape -rrb- methods have been proposed over the years , but with contrasting results . a missing aspect in previous evaluations is the assessment of different methods : i -rrb- under comparable conditions , and ii -rrb- on different language pairs featuring variable levels of mt quality . fo-cusing on statistical ape methods -lrb- more portable across languages -rrb- , we propose the first systematic analysis of two approaches . to understand their potential , we compare them in the same conditions over six language pairs having english as source . our results evidence consistent improvements on all language pairs , a relation between the extent of the gain and mt output quality , slight but statistically significant performance differences between the two methods , and their possible complementarity ."
  },
  {
    "title": "A General Projection Property for Distribution Families .",
    "entities": [
      "markov decision processes",
      "natural risk criteria",
      "univariate inequalities",
      "worst-case analyses",
      "portfolio selection",
      "unimodality",
      "log-concavity",
      "optimization",
      "symmetry",
      "classification"
    ],
    "types": "<task> <task> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "unimodality -- CONJUNCTION -- log-concavity",
      "classification -- CONJUNCTION -- portfolio selection",
      "portfolio selection -- CONJUNCTION -- markov decision processes",
      "worst-case analyses -- USED-FOR -- natural risk criteria",
      "optimization -- CONJUNCTION -- portfolio selection",
      "classification -- CONJUNCTION -- optimization",
      "symmetry -- CONJUNCTION -- unimodality"
    ],
    "abstract": "surjectivity of linear projections between distribution families with fixed mean and covariance -lrb- regardless of dimension -rrb- is re-derived by a new proof . we further extend this property to distribution families that respect additional constraints , such as <otherscientificterm_8> , <otherscientificterm_5> and <otherscientificterm_6> . by combining our results with classic <otherscientificterm_2> , we provide new <method_3> for <task_1> arising in <task_9> , <task_7> , <task_4> and <task_0> .",
    "abstract_og": "surjectivity of linear projections between distribution families with fixed mean and covariance -lrb- regardless of dimension -rrb- is re-derived by a new proof . we further extend this property to distribution families that respect additional constraints , such as symmetry , unimodality and log-concavity . by combining our results with classic univariate inequalities , we provide new worst-case analyses for natural risk criteria arising in classification , optimization , portfolio selection and markov decision processes ."
  },
  {
    "title": "Structuring Techniques for Constraint Satisfaction Problems .",
    "entities": [
      "discrete constraint satisfaction problems",
      "compact representations",
      "iterative application",
      "structuring method",
      "csp",
      "interchangeabilities"
    ],
    "types": "<task> <method> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "interchangeabilities -- USED-FOR -- structuring method",
      "structuring method -- USED-FOR -- discrete constraint satisfaction problems"
    ],
    "abstract": "we present a <method_3> for <task_0> . <method_3> takes advantage of <otherscientificterm_5> to represent sets of equivalent values by meta-values and thus obtain more <method_1> . strongly related variables are clustered into meta-variables to create occurrences of inter-changeabilities . by <method_2> , a <method_4> can be transformed into an hierarchy of equivalent <task_0> , where each problem is signiicantly simpler than the original one . this structure is particularly advantageous when a large set of possible solutions must be inspected .",
    "abstract_og": "we present a structuring method for discrete constraint satisfaction problems . structuring method takes advantage of interchangeabilities to represent sets of equivalent values by meta-values and thus obtain more compact representations . strongly related variables are clustered into meta-variables to create occurrences of inter-changeabilities . by iterative application , a csp can be transformed into an hierarchy of equivalent discrete constraint satisfaction problems , where each problem is signiicantly simpler than the original one . this structure is particularly advantageous when a large set of possible solutions must be inspected ."
  },
  {
    "title": "A convex and feature-rich discriminative approach to dependency grammar induction .",
    "entities": [
      "convex and feature-rich discriminative approach",
      "dependency grammar induction\u00e9douard abstract",
      "unsupervised dependency parsing",
      "non-convex optimization problem",
      "dependency grammar induction",
      "unsupervised learning problems",
      "generative models",
      "optimization algorithm",
      "frank-wolfe algorithm",
      "features",
      "initial-ization"
    ],
    "types": "<method> <method> <task> <task> <task> <task> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "convex and feature-rich discriminative approach -- USED-FOR -- dependency grammar induction\u00e9douard abstract"
    ],
    "abstract": "a <method_0> to <method_1> in this paper , we introduce a new method for the problem of <task_2> . most current approaches are based on <method_6> . learning the parameters of such models relies on solving a <task_3> , thus making them sensitive to <otherscientificterm_10> . we propose a new convex formulation to the task of <task_4> . our approach is discriminative , allowing the use of different kinds of <otherscientificterm_9> . we describe an efficient <method_7> to learn the parameters of our model , based on the <method_8> . our method can easily be generalized to other <task_5> . we evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state-of-the-art methods .",
    "abstract_og": "a convex and feature-rich discriminative approach to dependency grammar induction\u00e9douard abstract in this paper , we introduce a new method for the problem of unsupervised dependency parsing . most current approaches are based on generative models . learning the parameters of such models relies on solving a non-convex optimization problem , thus making them sensitive to initial-ization . we propose a new convex formulation to the task of dependency grammar induction . our approach is discriminative , allowing the use of different kinds of features . we describe an efficient optimization algorithm to learn the parameters of our model , based on the frank-wolfe algorithm . our method can easily be generalized to other unsupervised learning problems . we evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state-of-the-art methods ."
  },
  {
    "title": "Joint Recovery of Dense Correspondence and Cosegmentation in Two Images .",
    "entities": [
      "hierarchical markov random field model",
      "cosegmen-tation and dense per-pixel correspondence",
      "manually obtained ground truth",
      "iterated graph cuts",
      "energy minimization framework",
      "nested image regions",
      "piecewise similarity transformations",
      "joint inference",
      "correspondence field",
      "image pairs",
      "hierarchical methods",
      "correspondence estimation",
      "structure",
      "mapping",
      "inference",
      "labeling",
      "cosegmentation",
      "segmentation"
    ],
    "types": "<method> <otherscientificterm> <material> <method> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <method> <task> <otherscientificterm> <task> <task> <otherscientificterm> <task> <method>",
    "relations": [
      "iterated graph cuts -- USED-FOR -- energy minimization framework",
      "segmentation -- FEATURE-OF -- hierarchical markov random field model",
      "nested image regions -- USED-FOR -- hierarchical markov random field model",
      "energy minimization framework -- USED-FOR -- joint inference",
      "hierarchical markov random field model -- USED-FOR -- inference",
      "piecewise similarity transformations -- USED-FOR -- correspondence field",
      "cosegmentation -- CONJUNCTION -- correspondence estimation"
    ],
    "abstract": "we propose a new technique to jointly recover <otherscientificterm_1> in two images . our method parameterizes the <otherscientificterm_8> using <otherscientificterm_6> and recovers a <task_13> between the estimated common '' foreground '' regions in the two images allowing them to be precisely aligned . our formulation is based on a <method_0> with <method_17> and transformation labels . the <method_0> uses <otherscientificterm_5> to constrain <task_14> across multiple scales . unlike prior <method_10> which assume that the <otherscientificterm_12> is given , our proposed iterative technique dynamically recovers the <otherscientificterm_12> along with the <otherscientificterm_15> . this <task_7> is performed in an <method_4> using <method_3> . we evaluate our method on a new dataset of 400 <material_9> with <material_2> , where it outperforms state-of-the-art methods designed specifically for either <task_16> or <task_11> .",
    "abstract_og": "we propose a new technique to jointly recover cosegmen-tation and dense per-pixel correspondence in two images . our method parameterizes the correspondence field using piecewise similarity transformations and recovers a mapping between the estimated common '' foreground '' regions in the two images allowing them to be precisely aligned . our formulation is based on a hierarchical markov random field model with segmentation and transformation labels . the hierarchical markov random field model uses nested image regions to constrain inference across multiple scales . unlike prior hierarchical methods which assume that the structure is given , our proposed iterative technique dynamically recovers the structure along with the labeling . this joint inference is performed in an energy minimization framework using iterated graph cuts . we evaluate our method on a new dataset of 400 image pairs with manually obtained ground truth , where it outperforms state-of-the-art methods designed specifically for either cosegmentation or correspondence estimation ."
  },
  {
    "title": "Interference-free multi-user MIMO-OFDM .",
    "entities": [
      "multiuser downlink mimo-ofdm scheme",
      "transmit power constraint",
      "multiple access technique",
      "linear processing",
      "multiuser interference",
      "sdma",
      "fdma"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "fdma -- CONJUNCTION -- sdma",
      "multiple access technique -- USED-FOR -- fdma"
    ],
    "abstract": "a <method_0> is considered . perfect channel knowledge is assumed at the base station and <method_3> is used at both the transmit and receive sides . the objective is to optimize the mean ber of the system while satisfying a <otherscientificterm_1> and fulfilling each user 's rate . <method_6> and <method_5> are investigated . we show how the <otherscientificterm_4> induced by <method_5> can be annihilated and exhibit that this <method_2> should be preferred to <method_6> .",
    "abstract_og": "a multiuser downlink mimo-ofdm scheme is considered . perfect channel knowledge is assumed at the base station and linear processing is used at both the transmit and receive sides . the objective is to optimize the mean ber of the system while satisfying a transmit power constraint and fulfilling each user 's rate . fdma and sdma are investigated . we show how the multiuser interference induced by sdma can be annihilated and exhibit that this multiple access technique should be preferred to fdma ."
  },
  {
    "title": "A noiseless code length method -LRB- NCLM -RRB- to estimate dimensionality of hyperspectral data .",
    "entities": [
      "dimension estimation step",
      "noiseless data error",
      "noisy data error",
      "hyperspectral image analysis",
      "estimation of dimensionality",
      "error comparison approach",
      "optimum subset",
      "nested subsets",
      "hyperspectral imagery",
      "hyperspec-tral data",
      "dimension estimation",
      "noiseless error",
      "hyperspectral data",
      "denoising",
      "accuracy"
    ],
    "types": "<metric> <otherscientificterm> <otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm> <material> <material> <method> <otherscientificterm> <material> <task> <metric>",
    "relations": [
      "estimation of dimensionality -- FEATURE-OF -- hyperspectral imagery",
      "accuracy -- EVALUATE-FOR -- dimension estimation step",
      "hyperspectral data -- HYPONYM-OF -- nested subsets"
    ],
    "abstract": "hyperspectral image analysis has been subjected to many improvements made in past decade . yet the accurate <task_4> is still a challenge . since <method_10> of the <material_12> is the first step in analysis of an image , the <metric_14> of analysis results highly depends on the <metric_14> of the <metric_0> . mostly , existing methods isolate the process of <method_10> and process of <task_13> which leads to an inaccurate estimation of constituent components in the signal . in this paper , the problem of estimating the dimensionality of <material_12> using the concept of '' noiseless code length '' is addressed . in our proposed method , nclm , a set of <otherscientificterm_7> including the <material_12> is generated first and then an <method_5> is utilized by estimating the <otherscientificterm_1> rather than <otherscientificterm_2> used by the existing methods to find the <otherscientificterm_6> . it has been shown that the estimated <otherscientificterm_11> has a minimum that represents the accurate estimation of the dimensionality of <material_9> . the comparison of nclm to other methods shows a substantial improvement in <task_4> in <material_8> .",
    "abstract_og": "hyperspectral image analysis has been subjected to many improvements made in past decade . yet the accurate estimation of dimensionality is still a challenge . since dimension estimation of the hyperspectral data is the first step in analysis of an image , the accuracy of analysis results highly depends on the accuracy of the dimension estimation step . mostly , existing methods isolate the process of dimension estimation and process of denoising which leads to an inaccurate estimation of constituent components in the signal . in this paper , the problem of estimating the dimensionality of hyperspectral data using the concept of '' noiseless code length '' is addressed . in our proposed method , nclm , a set of nested subsets including the hyperspectral data is generated first and then an error comparison approach is utilized by estimating the noiseless data error rather than noisy data error used by the existing methods to find the optimum subset . it has been shown that the estimated noiseless error has a minimum that represents the accurate estimation of the dimensionality of hyperspec-tral data . the comparison of nclm to other methods shows a substantial improvement in estimation of dimensionality in hyperspectral imagery ."
  },
  {
    "title": "Unsupervised modeling of user actions in a dialog corpus .",
    "entities": [
      "non-parametric bayesian hidden markov model",
      "data-driven spoken dialog system development",
      "modeling user actions",
      "automatically annotated corpus",
      "unsupervised approach",
      "dialog system",
      "semantic annotation",
      "labeling process",
      "user simulation",
      "human annotation",
      "dialog corpus"
    ],
    "types": "<method> <task> <task> <material> <method> <method> <method> <method> <method> <task> <material>",
    "relations": [
      "automatically annotated corpus -- USED-FOR -- dialog system",
      "non-parametric bayesian hidden markov model -- USED-FOR -- unsupervised approach",
      "unsupervised approach -- USED-FOR -- modeling user actions",
      "semantic annotation -- FEATURE-OF -- dialog corpus",
      "user simulation -- USED-FOR -- dialog system"
    ],
    "abstract": "in <task_1> , developers should prepare a <material_10> with <method_6> . however , the <method_7> is a laborious and time consuming task . to reduce human efforts , we propose an <method_4> based on <method_0> to the problem of <task_2> . with the <method_0> , system designers do not need to determine the number and type of user actions . in the experiments , we evaluated the clustering results by comparing them to the <task_9> . we also tested a <method_5> that used models trained from the <material_3> with a <method_8> .",
    "abstract_og": "in data-driven spoken dialog system development , developers should prepare a dialog corpus with semantic annotation . however , the labeling process is a laborious and time consuming task . to reduce human efforts , we propose an unsupervised approach based on non-parametric bayesian hidden markov model to the problem of modeling user actions . with the non-parametric bayesian hidden markov model , system designers do not need to determine the number and type of user actions . in the experiments , we evaluated the clustering results by comparing them to the human annotation . we also tested a dialog system that used models trained from the automatically annotated corpus with a user simulation ."
  },
  {
    "title": "The czech speech and prosody database both for ASR and TTS purposes .",
    "entities": [
      "automatic speech recognition",
      "raw and stylized f0 values",
      "text-to-speech synthesis",
      "word-and phoneme-level time alignment",
      "frame level energy values",
      "automatic punctuation annotation",
      "czech high-quality synthesis",
      "czech prosodic database",
      "recorded speech",
      "prosodic data",
      "prosodic module",
      "linguistical annotation",
      "tagset",
      "stylization"
    ],
    "types": "<task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <task> <material> <material> <material> <method> <task> <method> <task>",
    "relations": [
      "automatic speech recognition -- USED-FOR -- automatic speech recognition",
      "automatic speech recognition -- CONJUNCTION -- text-to-speech synthesis",
      "frame level energy values -- CONJUNCTION -- word-and phoneme-level time alignment",
      "tagset -- USED-FOR -- linguistical annotation",
      "automatic speech recognition -- USED-FOR -- prosodic module",
      "raw and stylized f0 values -- CONJUNCTION -- word-and phoneme-level time alignment",
      "recorded speech -- CONJUNCTION -- raw and stylized f0 values",
      "recorded speech -- CONJUNCTION -- frame level energy values",
      "automatic speech recognition -- USED-FOR -- automatic punctuation annotation",
      "raw and stylized f0 values -- CONJUNCTION -- frame level energy values",
      "prosodic module -- USED-FOR -- czech high-quality synthesis"
    ],
    "abstract": "this paper describes a preparation of the first large <material_7> which should be useful both in <task_0> and <task_2> . in the area of <task_0> we intend to use <task_0> for an <task_5> , in the area of <task_0> for building a <method_10> for the <task_6> . the database is based on the czech radio & tv broadcast news corpus -lrb- uwb b02 -rrb- recorded at the university of west bohemia . the configuration of the database includes <material_8> , <otherscientificterm_1> , <otherscientificterm_4> , a <otherscientificterm_3> , and a linguistically motivated description of the <material_9> . a technique of <material_9> acquisition and <task_13> is described . a new <method_12> for a <task_11> of the czech prosody is proposed and used .",
    "abstract_og": "this paper describes a preparation of the first large czech prosodic database which should be useful both in automatic speech recognition and text-to-speech synthesis . in the area of automatic speech recognition we intend to use automatic speech recognition for an automatic punctuation annotation , in the area of automatic speech recognition for building a prosodic module for the czech high-quality synthesis . the database is based on the czech radio & tv broadcast news corpus -lrb- uwb b02 -rrb- recorded at the university of west bohemia . the configuration of the database includes recorded speech , raw and stylized f0 values , frame level energy values , a word-and phoneme-level time alignment , and a linguistically motivated description of the prosodic data . a technique of prosodic data acquisition and stylization is described . a new tagset for a linguistical annotation of the czech prosody is proposed and used ."
  },
  {
    "title": "I-vector based language modeling for query representation .",
    "entities": [
      "i-vector based language modeling framework",
      "tdt-2 -lrb- topic detection and tracking -rrb- collection",
      "spoken document retrieval",
      "indexing and modeling techniques",
      "multi-levels of index features",
      "word-and subword-level units",
      "language identification",
      "i-vector framework",
      "spoken documents",
      "query reformulation",
      "multimedia data",
      "speaker recognition",
      "query formulation"
    ],
    "types": "<method> <material> <task> <method> <otherscientificterm> <otherscientificterm> <task> <method> <material> <task> <material> <task> <task>",
    "relations": [
      "i-vector framework -- USED-FOR -- speaker recognition",
      "word-and subword-level units -- HYPONYM-OF -- multi-levels of index features",
      "i-vector framework -- USED-FOR -- i-vector based language modeling framework",
      "i-vector based language modeling framework -- USED-FOR -- query formulation",
      "i-vector framework -- USED-FOR -- language identification",
      "multimedia data -- USED-FOR -- spoken document retrieval",
      "spoken documents -- USED-FOR -- multimedia data",
      "language identification -- CONJUNCTION -- speaker recognition",
      "indexing and modeling techniques -- USED-FOR -- spoken documents",
      "i-vector based language modeling framework -- USED-FOR -- language identification"
    ],
    "abstract": "since more and more <material_10> associated with <material_8> have been made available to the public , <task_2> has become an important research subject in the past two decades . following the research tendency , many efforts have been devoted towards developing <method_3> for representing <material_8> , but only few have been made on improving <task_12> for better representing users ' information needs . the <method_0> , stemming from the state-of-the-art <method_7> for <task_6> and <task_11> , has been proposed and formulated to represent documents in sdr with good promise recently . however , a major challenge of using <method_0> for <task_12> is that a query usually consists of only a few words ; thus , it is hard to learn a reliable representation accordingly . in this paper , we focus our attention on <task_9> and propose three novel methods on top of <method_0> to more accurately represent users ' information needs . in addition , we also explore the use of <otherscientificterm_4> , including <otherscientificterm_5> , to work in concert with the proposed methods . a series of empirical sdr experiments conducted on the <material_1> demonstrate the good effectiveness of our proposed methods as compared to existing state-of-the-art methods .",
    "abstract_og": "since more and more multimedia data associated with spoken documents have been made available to the public , spoken document retrieval has become an important research subject in the past two decades . following the research tendency , many efforts have been devoted towards developing indexing and modeling techniques for representing spoken documents , but only few have been made on improving query formulation for better representing users ' information needs . the i-vector based language modeling framework , stemming from the state-of-the-art i-vector framework for language identification and speaker recognition , has been proposed and formulated to represent documents in sdr with good promise recently . however , a major challenge of using i-vector based language modeling framework for query formulation is that a query usually consists of only a few words ; thus , it is hard to learn a reliable representation accordingly . in this paper , we focus our attention on query reformulation and propose three novel methods on top of i-vector based language modeling framework to more accurately represent users ' information needs . in addition , we also explore the use of multi-levels of index features , including word-and subword-level units , to work in concert with the proposed methods . a series of empirical sdr experiments conducted on the tdt-2 -lrb- topic detection and tracking -rrb- collection demonstrate the good effectiveness of our proposed methods as compared to existing state-of-the-art methods ."
  },
  {
    "title": "Evaluation of a stereo audio data hiding method using inter-channel decorrelator polarity .",
    "entities": [
      "polarity of the echoes",
      "stereo audio signals",
      "mushra standard method",
      "raw embedded data",
      "sample rate conversion",
      "spread spectrum method",
      "data hiding algorithm",
      "random bit cropping",
      "mp3 coders",
      "hiding methods",
      "high-frequency channels",
      "embedded data",
      "spread spectrum"
    ],
    "types": "<otherscientificterm> <material> <method> <material> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "mushra standard method -- COMPARE -- hiding methods",
      "data hiding algorithm -- USED-FOR -- stereo audio signals",
      "sample rate conversion -- CONJUNCTION -- random bit cropping",
      "polarity of the echoes -- USED-FOR -- data hiding algorithm"
    ],
    "abstract": "we extensively evaluated a <method_6> for <material_1> which embeds data using the <otherscientificterm_0> added to the <otherscientificterm_10> , which we have previously proposed . its performance was also compared to conventional data hiding using <otherscientificterm_12> , and those using echoes with different delays . <material_3> was detected with little or no errors for added noise at 20 db snr and above , or with <otherscientificterm_8> , although the <method_5> showed almost no errors at all . however , <otherscientificterm_4> and <otherscientificterm_7> were shown not to affect the <material_11> with the proposed <method_6> , while other methods , including <otherscientificterm_12> , showed significant amount of errors . the embedded audio quality test using the <method_2> resulted in little noticeable degradation , far better quality compared to other <method_9> .",
    "abstract_og": "we extensively evaluated a data hiding algorithm for stereo audio signals which embeds data using the polarity of the echoes added to the high-frequency channels , which we have previously proposed . its performance was also compared to conventional data hiding using spread spectrum , and those using echoes with different delays . raw embedded data was detected with little or no errors for added noise at 20 db snr and above , or with mp3 coders , although the spread spectrum method showed almost no errors at all . however , sample rate conversion and random bit cropping were shown not to affect the embedded data with the proposed data hiding algorithm , while other methods , including spread spectrum , showed significant amount of errors . the embedded audio quality test using the mushra standard method resulted in little noticeable degradation , far better quality compared to other hiding methods ."
  },
  {
    "title": "An automatic face detection and recognition system for video indexing applications .",
    "entities": [
      "principal components analysis approach",
      "face detection and recognition rate",
      "mpeg-7 video content set",
      "video indexing applications",
      "mpeg-7 evaluation group",
      "face detection stage",
      "recognition system",
      "computational cost"
    ],
    "types": "<method> <metric> <material> <task> <material> <method> <method> <metric>",
    "relations": [
      "principal components analysis approach -- USED-FOR -- video indexing applications",
      "principal components analysis approach -- USED-FOR -- face detection stage"
    ],
    "abstract": "the objective of this work is the integration and optimization of an automatic face detection and <method_6> for <task_3> . the system is composed of a <method_5> presented previously which provides good results maintaining a low <metric_7> . the <method_5> is based on the <method_0> which has been modified to cope with the <task_3> . after the integration of the two stages , several improvements are proposed which increase the <metric_1> and the overall performance of the system . good results have been obtained using the <material_2> used in the <material_4> .",
    "abstract_og": "the objective of this work is the integration and optimization of an automatic face detection and recognition system for video indexing applications . the system is composed of a face detection stage presented previously which provides good results maintaining a low computational cost . the face detection stage is based on the principal components analysis approach which has been modified to cope with the video indexing applications . after the integration of the two stages , several improvements are proposed which increase the face detection and recognition rate and the overall performance of the system . good results have been obtained using the mpeg-7 video content set used in the mpeg-7 evaluation group ."
  },
  {
    "title": "A Nonparametric Bayesian Method for Inferring Features From Similarity Judgments .",
    "entities": [
      "weighted linear function of common features",
      "fully bayesian formulation",
      "nonparametric bayesian statistics",
      "additive clustering model",
      "parameter estimation",
      "similarity judgments",
      "features",
      "similarity"
    ],
    "types": "<otherscientificterm> <method> <material> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "fully bayesian formulation -- USED-FOR -- additive clustering model",
      "fully bayesian formulation -- USED-FOR -- similarity judgments",
      "additive clustering model -- USED-FOR -- features"
    ],
    "abstract": "the <method_3> is widely used to infer the <otherscientificterm_6> of a set of stimuli from their similarities , on the assumption that <otherscientificterm_7> is a <otherscientificterm_0> . this paper develops a <method_1> of the <method_3> , using methods from <material_2> to allow the number of <otherscientificterm_6> to vary . we use this to explore several approaches to <task_4> , showing that the <method_1> provides a straightforward way to obtain estimates of both the number of <otherscientificterm_6> used in producing <otherscientificterm_5> and their importance .",
    "abstract_og": "the additive clustering model is widely used to infer the features of a set of stimuli from their similarities , on the assumption that similarity is a weighted linear function of common features . this paper develops a fully bayesian formulation of the additive clustering model , using methods from nonparametric bayesian statistics to allow the number of features to vary . we use this to explore several approaches to parameter estimation , showing that the fully bayesian formulation provides a straightforward way to obtain estimates of both the number of features used in producing similarity judgments and their importance ."
  },
  {
    "title": "Incorporating Knowledge into Structural Equation Models Using Auxiliary Variables .",
    "entities": [
      "non-zero parameter values",
      "graph-based identification methods",
      "auxiliary variables",
      "single-door criterion",
      "linear systems",
      "identification technique",
      "instrumental variables",
      "identification method",
      "bootstrapping approach",
      "model testing",
      "external knowledge",
      "half-trek criterion",
      "background knowledge",
      "d-separation",
      "over-identification",
      "z-identification",
      "identification"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "over-identification -- HYPONYM-OF -- model testing",
      "d-separation -- CONJUNCTION -- over-identification",
      "background knowledge -- USED-FOR -- graph-based identification methods",
      "instrumental variables -- HYPONYM-OF -- identification",
      "bootstrapping approach -- USED-FOR -- graph-based identification methods",
      "identification method -- USED-FOR -- linear systems",
      "d-separation -- CONJUNCTION -- d-separation",
      "auxiliary variables -- USED-FOR -- model testing",
      "half-trek criterion -- HYPONYM-OF -- identification",
      "auxiliary variables -- USED-FOR -- identification",
      "model testing -- CONJUNCTION -- z-identification",
      "instrumental variables -- CONJUNCTION -- d-separation",
      "single-door criterion -- CONJUNCTION -- instrumental variables",
      "single-door criterion -- HYPONYM-OF -- identification",
      "d-separation -- HYPONYM-OF -- model testing",
      "auxiliary variables -- USED-FOR -- graph-based identification methods",
      "instrumental variables -- CONJUNCTION -- half-trek criterion",
      "auxiliary variables -- USED-FOR -- z-identification",
      "single-door criterion -- CONJUNCTION -- half-trek criterion",
      "non-zero parameter values -- USED-FOR -- graph-based identification methods"
    ],
    "abstract": "in this paper , we extend <method_1> by allowing <otherscientificterm_12> in the form of <otherscientificterm_0> . such <otherscientificterm_12> could be obtained , for example , from a previously conducted randomized experiment , from substantive understanding of the domain , or even an <method_5> . to incorporate such <otherscientificterm_12> systematically , we propose the addition of <otherscientificterm_2> to the <method_1> , which are constructed so that certain paths will be conveniently cancelled . this cancellation allows the <otherscientificterm_2> to help conventional methods of <task_16> -lrb- e.g. , <otherscientificterm_3> , <otherscientificterm_6> , <otherscientificterm_11> -rrb- , as well as <method_9> -lrb- e.g. , <otherscientificterm_13> , <otherscientificterm_14> -rrb- . moreover , by iteratively alternating steps of <task_16> and adding <otherscientificterm_2> , we can improve the power of existing <method_1> via a <method_8> that does not require <otherscientificterm_10> . we operationalize this <method_1> for simple instrumental sets -lrb- a generalization of <otherscientificterm_6> -rrb- and show that the resulting <method_1> is able to identify at least as many models as the most general <method_7> for <method_4> known to date . we further discuss the application of <otherscientificterm_2> to the tasks of <method_9> and <task_15> .",
    "abstract_og": "in this paper , we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values . such background knowledge could be obtained , for example , from a previously conducted randomized experiment , from substantive understanding of the domain , or even an identification technique . to incorporate such background knowledge systematically , we propose the addition of auxiliary variables to the graph-based identification methods , which are constructed so that certain paths will be conveniently cancelled . this cancellation allows the auxiliary variables to help conventional methods of identification -lrb- e.g. , single-door criterion , instrumental variables , half-trek criterion -rrb- , as well as model testing -lrb- e.g. , d-separation , over-identification -rrb- . moreover , by iteratively alternating steps of identification and adding auxiliary variables , we can improve the power of existing graph-based identification methods via a bootstrapping approach that does not require external knowledge . we operationalize this graph-based identification methods for simple instrumental sets -lrb- a generalization of instrumental variables -rrb- and show that the resulting graph-based identification methods is able to identify at least as many models as the most general identification method for linear systems known to date . we further discuss the application of auxiliary variables to the tasks of model testing and z-identification ."
  },
  {
    "title": "Learning a Continuous Hidden Variable Model for Binary Data .",
    "entities": [
      "translationally invariant binary distribution",
      "directed generative model",
      "handwritten digit images",
      "principal components analysis",
      "continuous gaussian variables",
      "binary output variables",
      "hidden continuous units",
      "binary data",
      "clipping nonlinear-ity"
    ],
    "types": "<otherscientificterm> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "directed generative model -- USED-FOR -- binary data",
      "hidden continuous units -- USED-FOR -- directed generative model",
      "principal components analysis -- USED-FOR -- directed generative model",
      "continuous gaussian variables -- CONJUNCTION -- binary output variables",
      "clipping nonlinear-ity -- USED-FOR -- directed generative model",
      "translationally invariant binary distribution -- EVALUATE-FOR -- directed generative model"
    ],
    "abstract": "a <method_1> for <material_7> using a small number of <otherscientificterm_6> is investigated . a <method_8> distinguishes the <method_1> from conventional <method_3> . the relationships between the correlations of the underlying <otherscientificterm_4> and the <otherscientificterm_5> are utilized to learn the appropriate weights of the <method_1> . the advantages of this <method_1> are illustrated on a <otherscientificterm_0> and on <material_2> .",
    "abstract_og": "a directed generative model for binary data using a small number of hidden continuous units is investigated . a clipping nonlinear-ity distinguishes the directed generative model from conventional principal components analysis . the relationships between the correlations of the underlying continuous gaussian variables and the binary output variables are utilized to learn the appropriate weights of the directed generative model . the advantages of this directed generative model are illustrated on a translationally invariant binary distribution and on handwritten digit images ."
  },
  {
    "title": "The case for automatic higher-level features in forensic speaker recognition .",
    "entities": [
      "automatic higher-level systems",
      "higher-level '' features",
      "automatic speaker recognition",
      "forensic context",
      "cepstral features",
      "forensic applications",
      "automatic systems"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "cepstral features -- USED-FOR -- automatic speaker recognition"
    ],
    "abstract": "approaches from standard <task_2> , which rely on <otherscientificterm_4> , suffer the problem of lack of interpretability for <task_5> . but the growing practice of using '' <otherscientificterm_1> in <task_6> offers promise in this regard . we provide an overview of <method_0> and discuss potential advantages , as well as issues , for their use in the <otherscientificterm_3> .",
    "abstract_og": "approaches from standard automatic speaker recognition , which rely on cepstral features , suffer the problem of lack of interpretability for forensic applications . but the growing practice of using '' higher-level '' features in automatic systems offers promise in this regard . we provide an overview of automatic higher-level systems and discuss potential advantages , as well as issues , for their use in the forensic context ."
  },
  {
    "title": "Syntactic Topic Models .",
    "entities": [
      "syntactic topic model",
      "semantic insights of topic models",
      "approximate posterior inference method",
      "nonparametric bayesian model",
      "document-specific topic weights",
      "hierarchical dirichlet processes",
      "parse-tree-specific syntactic transitions",
      "parse trees",
      "synthetic data",
      "parse tree",
      "parsed documents",
      "syntactic information",
      "variational methods",
      "hand-parsed documents"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <method> <material>",
    "relations": [
      "variational methods -- USED-FOR -- hierarchical dirichlet processes",
      "document-specific topic weights -- CONJUNCTION -- parse-tree-specific syntactic transitions",
      "parse trees -- CONJUNCTION -- syntactic information",
      "variational methods -- USED-FOR -- approximate posterior inference method",
      "synthetic data -- CONJUNCTION -- hand-parsed documents",
      "approximate posterior inference method -- USED-FOR -- hierarchical dirichlet processes"
    ],
    "abstract": "we develop the <method_0> , a <method_3> of <material_10> . the <method_0> generates words that are both thematically and syntactically constrained , which combines the <method_1> with the <otherscientificterm_11> available from <otherscientificterm_7> . each word of a sentence is generated by a distribution that combines <otherscientificterm_4> and <otherscientificterm_6> . words are assumed to be generated in an order that respects the <otherscientificterm_9> . we derive an <method_2> based on <method_12> for <method_5> , and we report qualitative and quantitative results on both <material_8> and <material_13> .",
    "abstract_og": "we develop the syntactic topic model , a nonparametric bayesian model of parsed documents . the syntactic topic model generates words that are both thematically and syntactically constrained , which combines the semantic insights of topic models with the syntactic information available from parse trees . each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions . words are assumed to be generated in an order that respects the parse tree . we derive an approximate posterior inference method based on variational methods for hierarchical dirichlet processes , and we report qualitative and quantitative results on both synthetic data and hand-parsed documents ."
  },
  {
    "title": "System request detection in human conversation based on multi-resolution Gabor wavelet features .",
    "entities": [
      "voice activity detection systems",
      "power and prosody-based method",
      "detected speech section",
      "robot dialog corpus",
      "hands-free speech interface",
      "log-scale mel-frequency filter-bank",
      "prosodic articulation",
      "speech frames",
      "phoneme articulation",
      "gabor wavelet",
      "system requests",
      "system commands",
      "non-speech frames",
      "multi-resolution analysis",
      "spontaneous utterances",
      "command",
      "f-measure",
      "accuracy"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <metric> <metric>",
    "relations": [
      "f-measure -- EVALUATE-FOR -- power and prosody-based method",
      "system requests -- CONJUNCTION -- spontaneous utterances",
      "system commands -- CONJUNCTION -- spontaneous utterances",
      "log-scale mel-frequency filter-bank -- USED-FOR -- multi-resolution analysis",
      "non-speech frames -- USED-FOR -- voice activity detection systems",
      "gabor wavelet -- USED-FOR -- multi-resolution analysis"
    ],
    "abstract": "for a <otherscientificterm_4> , it is important to detect commands in <material_14> . usual <method_0> can only distinguish <otherscientificterm_7> from <otherscientificterm_12> , but they can not discriminate whether the <otherscientificterm_2> is a <otherscientificterm_15> for a system or not . in this paper , in order to analyze the difference between <otherscientificterm_10> and <material_14> , we focus on fluctuations in a long period , such as <otherscientificterm_6> , and fluctuations in a short period , such as <otherscientificterm_8> . the use of <method_13> using <otherscientificterm_9> on a <material_5> clarifies the different characteristics of <otherscientificterm_11> and <material_14> . experiments using our <material_3> show that the <metric_17> of the proposed method is 92.6 % in <metric_16> , while the conventional <method_1> is just 66.7 % .",
    "abstract_og": "for a hands-free speech interface , it is important to detect commands in spontaneous utterances . usual voice activity detection systems can only distinguish speech frames from non-speech frames , but they can not discriminate whether the detected speech section is a command for a system or not . in this paper , in order to analyze the difference between system requests and spontaneous utterances , we focus on fluctuations in a long period , such as prosodic articulation , and fluctuations in a short period , such as phoneme articulation . the use of multi-resolution analysis using gabor wavelet on a log-scale mel-frequency filter-bank clarifies the different characteristics of system commands and spontaneous utterances . experiments using our robot dialog corpus show that the accuracy of the proposed method is 92.6 % in f-measure , while the conventional power and prosody-based method is just 66.7 % ."
  },
  {
    "title": "Approximate Solutions of Interactive Dynamic Influence Diagrams Using Model Clustering .",
    "entities": [
      "interactive dynamic influence diagrams",
      "transparent and semantically clear representation",
      "sequential decision-making problem",
      "approximation technique",
      "interacting agents"
    ],
    "types": "<method> <method> <task> <method> <method>",
    "relations": [
      "interactive dynamic influence diagrams -- USED-FOR -- sequential decision-making problem",
      "transparent and semantically clear representation -- USED-FOR -- sequential decision-making problem",
      "interactive dynamic influence diagrams -- USED-FOR -- transparent and semantically clear representation"
    ],
    "abstract": "interactive dynamic influence diagrams -lrb- <method_0> -rrb- offer a <method_1> for the <task_2> over multiple time steps in the presence of other <method_4> . solving <method_0> exactly involves knowing the solutions of possible models of the other agents , which increase exponentially with the number of time steps . we present a method of solving <method_0> approximately by limiting the number of other agents ' candidate models at each time step to a constant . we do this by clustering the models and selecting a representative set from the clusters . we discuss the error bound of the <method_3> and demonstrate its empirical performance .",
    "abstract_og": "interactive dynamic influence diagrams -lrb- interactive dynamic influence diagrams -rrb- offer a transparent and semantically clear representation for the sequential decision-making problem over multiple time steps in the presence of other interacting agents . solving interactive dynamic influence diagrams exactly involves knowing the solutions of possible models of the other agents , which increase exponentially with the number of time steps . we present a method of solving interactive dynamic influence diagrams approximately by limiting the number of other agents ' candidate models at each time step to a constant . we do this by clustering the models and selecting a representative set from the clusters . we discuss the error bound of the approximation technique and demonstrate its empirical performance ."
  },
  {
    "title": "ABSORB : Atlas building by Self-Organized Registration and Bundling .",
    "entities": [
      "relative distribution of subject images",
      "synthetic and real datasets",
      "hierarchical groupwise registration framework",
      "local data distribution",
      "representative subject images",
      "groupwise registration methods",
      "registration process",
      "global structure",
      "registration accuracy",
      "self-organized registration",
      "self-organized registration",
      "image bundling",
      "image population",
      "groupwise registration",
      "registration",
      "robustness"
    ],
    "types": "<otherscientificterm> <material> <method> <task> <material> <method> <method> <otherscientificterm> <metric> <method> <method> <task> <otherscientificterm> <task> <task> <metric>",
    "relations": [
      "registration accuracy -- CONJUNCTION -- robustness",
      "self-organized registration -- HYPONYM-OF -- hierarchical groupwise registration framework",
      "synthetic and real datasets -- EVALUATE-FOR -- hierarchical groupwise registration framework",
      "hierarchical groupwise registration framework -- COMPARE -- groupwise registration methods"
    ],
    "abstract": "to achieve more accurate and consistent <task_14> in an <otherscientificterm_12> , a novel <method_2> , called atlas building by <method_9> and bundling -lrb- absorb -rrb- , is proposed in this paper . in this new <method_2> , the <otherscientificterm_7> , i.e. , the <otherscientificterm_0> is always preserved during the <method_6> by constraining each subject image to deform only locally with respect to its neighbors within the learned image manifold . to achieve this goal , two novel strategies , i.e. , the <method_10> by warping one image towards a set of its eligible neighbors and <task_11> to cluster similar images , are specially proposed . by using these two strategies , this new <method_2> can perform <task_13> in a hierarchical way . specifically , in the high level , <method_2> will perform on a much smaller dataset formed by the <material_4> of all subgroups that are generated in the previous levels of <task_14> . compared to the other <method_5> , our proposed <method_2> has several advantages : -lrb- 1 -rrb- <method_2> explores the <task_3> and uses the obtained distribution information to guide the <task_14> ; -lrb- 2 -rrb- the possible <task_14> error can be greatly reduced by requiring each individual subject to move only towards its nearby subjects with similar structures ; -lrb- 3 -rrb- <method_2> can produce a smoother <task_14> path , in general , from each subject image to the final built atlas than other <method_5> . experimental results on both <material_1> show that the proposed <method_2> can achieve substantial improvements , compared to the other two widely used <method_5> , in terms of both <metric_8> and <metric_15> .",
    "abstract_og": "to achieve more accurate and consistent registration in an image population , a novel hierarchical groupwise registration framework , called atlas building by self-organized registration and bundling -lrb- absorb -rrb- , is proposed in this paper . in this new hierarchical groupwise registration framework , the global structure , i.e. , the relative distribution of subject images is always preserved during the registration process by constraining each subject image to deform only locally with respect to its neighbors within the learned image manifold . to achieve this goal , two novel strategies , i.e. , the self-organized registration by warping one image towards a set of its eligible neighbors and image bundling to cluster similar images , are specially proposed . by using these two strategies , this new hierarchical groupwise registration framework can perform groupwise registration in a hierarchical way . specifically , in the high level , hierarchical groupwise registration framework will perform on a much smaller dataset formed by the representative subject images of all subgroups that are generated in the previous levels of registration . compared to the other groupwise registration methods , our proposed hierarchical groupwise registration framework has several advantages : -lrb- 1 -rrb- hierarchical groupwise registration framework explores the local data distribution and uses the obtained distribution information to guide the registration ; -lrb- 2 -rrb- the possible registration error can be greatly reduced by requiring each individual subject to move only towards its nearby subjects with similar structures ; -lrb- 3 -rrb- hierarchical groupwise registration framework can produce a smoother registration path , in general , from each subject image to the final built atlas than other groupwise registration methods . experimental results on both synthetic and real datasets show that the proposed hierarchical groupwise registration framework can achieve substantial improvements , compared to the other two widely used groupwise registration methods , in terms of both registration accuracy and robustness ."
  },
  {
    "title": "Effective Selectional Restrictions for Unsupervised Relation Extraction .",
    "entities": [
      "unsupervised relation extraction methods",
      "web-derived soft clustering of n-grams",
      "sparsity of the feature space",
      "selectional restrictions",
      "text corpora of unknown content",
      "fine-grained entity type system",
      "7-class named entity types",
      "discriminative power of patterns",
      "modeling sr",
      "open domain",
      "fine-granular relations",
      "selectional restrictions",
      "semantic relations",
      "ambiguities",
      "setup"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "selectional restrictions -- USED-FOR -- discriminative power of patterns",
      "fine-grained entity type system -- USED-FOR -- selectional restrictions",
      "unsupervised relation extraction methods -- USED-FOR -- ambiguities",
      "unsupervised relation extraction methods -- USED-FOR -- semantic relations"
    ],
    "abstract": "unsupervised relation extraction -lrb- <method_0> -rrb- methods automatically discover <otherscientificterm_12> in <material_4> and extract for each discovered relation a set of relation instances . due to the <otherscientificterm_2> , <method_0> is vulnerable to <otherscientificterm_13> and underspeci-fication in patterns . in this paper , we propose to increase the <otherscientificterm_7> in <method_0> using <otherscientificterm_3> . we propose a method that utilizes a <method_1> to model <otherscientificterm_11> in the <material_9> . we comparatively evaluate our method against a baseline without <otherscientificterm_3> , a <otherscientificterm_14> in which standard <otherscientificterm_6> are used as <otherscientificterm_3> and a <otherscientificterm_14> that models <otherscientificterm_3> using a <method_5> . our results indicate that <task_8> into patterns significantly improves the ability of <method_0> to discover relations and enables the discovery of more <otherscientificterm_10> .",
    "abstract_og": "unsupervised relation extraction -lrb- unsupervised relation extraction methods -rrb- methods automatically discover semantic relations in text corpora of unknown content and extract for each discovered relation a set of relation instances . due to the sparsity of the feature space , unsupervised relation extraction methods is vulnerable to ambiguities and underspeci-fication in patterns . in this paper , we propose to increase the discriminative power of patterns in unsupervised relation extraction methods using selectional restrictions . we propose a method that utilizes a web-derived soft clustering of n-grams to model selectional restrictions in the open domain . we comparatively evaluate our method against a baseline without selectional restrictions , a setup in which standard 7-class named entity types are used as selectional restrictions and a setup that models selectional restrictions using a fine-grained entity type system . our results indicate that modeling sr into patterns significantly improves the ability of unsupervised relation extraction methods to discover relations and enables the discovery of more fine-granular relations ."
  },
  {
    "title": "FAB-MAP : Appearance-Based Place Recognition and Mapping using a Learned Visual Vocabulary Model .",
    "entities": [
      "mean filter update times",
      "infrastructure-free mobile robot navigation",
      "visually repetitive environments",
      "loop closure detection",
      "robot 's camera",
      "online appearance mapping",
      "system speed",
      "probabilis-tic framework",
      "visual elements",
      "tf-idf ranking",
      "place recognition",
      "multi-hypothesis testing",
      "joint distribution",
      "bail-out strategy",
      "fab-map"
    ],
    "types": "<metric> <task> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <task> <otherscientificterm> <method> <method>",
    "relations": [
      "online appearance mapping -- CONJUNCTION -- loop closure detection",
      "online appearance mapping -- EVALUATE-FOR -- fab-map",
      "loop closure detection -- EVALUATE-FOR -- fab-map",
      "multi-hypothesis testing -- USED-FOR -- system speed",
      "fab-map -- USED-FOR -- place recognition",
      "bail-out strategy -- USED-FOR -- multi-hypothesis testing",
      "bail-out strategy -- USED-FOR -- system speed",
      "fab-map -- USED-FOR -- loop closure detection",
      "place recognition -- USED-FOR -- infrastructure-free mobile robot navigation"
    ],
    "abstract": "we present an overview of <method_14> , an algorithm for <task_10> and mapping developed for <task_1> in large environments . the <method_14> allows a robot to identify when it is revisiting a previously seen location , on the basis of imagery captured by the <otherscientificterm_4> . we outline a complete <method_7> for the task , which is applicable even in <otherscientificterm_2> where many locations may appear identical . our work introduces a number of technical innovations-notably we demonstrate that <task_10> performance can be improved by learning an approximation to the <otherscientificterm_12> over <otherscientificterm_8> . we also investigate several principled approaches to making the <method_14> robust in <otherscientificterm_2> , and define an efficient <method_13> for <task_11> to improve <otherscientificterm_6> . our model has been shown to substantially outperform standard <method_9> on our task of interest . we demonstrate the <method_14> performing reliable <task_5> and <task_3> over a 1,000 km trajectory , with <metric_0> of 14 ms.",
    "abstract_og": "we present an overview of fab-map , an algorithm for place recognition and mapping developed for infrastructure-free mobile robot navigation in large environments . the fab-map allows a robot to identify when it is revisiting a previously seen location , on the basis of imagery captured by the robot 's camera . we outline a complete probabilis-tic framework for the task , which is applicable even in visually repetitive environments where many locations may appear identical . our work introduces a number of technical innovations-notably we demonstrate that place recognition performance can be improved by learning an approximation to the joint distribution over visual elements . we also investigate several principled approaches to making the fab-map robust in visually repetitive environments , and define an efficient bail-out strategy for multi-hypothesis testing to improve system speed . our model has been shown to substantially outperform standard tf-idf ranking on our task of interest . we demonstrate the fab-map performing reliable online appearance mapping and loop closure detection over a 1,000 km trajectory , with mean filter update times of 14 ms."
  },
  {
    "title": "Statistical Analysis of Local 3D Structure in 2D Images .",
    "entities": [
      "homogeneous , edge-like , corner-like or texture-like structures",
      "edge-like or corner-like structures",
      "local image structures",
      "3d range data",
      "homogeneous image patches",
      "computer vision applications",
      "local 3d structure",
      "analysis of images",
      "intrinsic structure",
      "3d discontinuities",
      "continuous surfaces",
      "statistical analysis",
      "2d images",
      "discontinuities"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "continuous surfaces -- CONJUNCTION -- 3d discontinuities",
      "statistical analysis -- USED-FOR -- 2d images",
      "3d range data -- USED-FOR -- 3d discontinuities"
    ],
    "abstract": "for the <task_7> , a deeper understanding of their <otherscientificterm_8> is required . this has been obtained for <material_12> by means of <method_11> -lsb- 15 , 18 -rsb- . here , we analyze the relation between <otherscientificterm_2> -lrb- i.e. , <otherscientificterm_0> -rrb- and the underlying <otherscientificterm_6> , represented in terms of <otherscientificterm_10> and different kinds of <otherscientificterm_9> , using <material_3> with the true color information . we find that <otherscientificterm_4> correspond to <otherscientificterm_10> , and <otherscientificterm_13> are mainly formed by <otherscientificterm_1> . the results are discussed with regard to existing and potential <task_5> and the assumptions made by these applications .",
    "abstract_og": "for the analysis of images , a deeper understanding of their intrinsic structure is required . this has been obtained for 2d images by means of statistical analysis -lsb- 15 , 18 -rsb- . here , we analyze the relation between local image structures -lrb- i.e. , homogeneous , edge-like , corner-like or texture-like structures -rrb- and the underlying local 3d structure , represented in terms of continuous surfaces and different kinds of 3d discontinuities , using 3d range data with the true color information . we find that homogeneous image patches correspond to continuous surfaces , and discontinuities are mainly formed by edge-like or corner-like structures . the results are discussed with regard to existing and potential computer vision applications and the assumptions made by these applications ."
  },
  {
    "title": "Towards combining pitch and MFCC for speaker recognition systems .",
    "entities": [
      "voiced and unvoiced speech segments",
      "short -- term pitch information",
      "short -- term dependence",
      "speaker recognition systems",
      "voiced speech segments",
      "joint probability functions",
      "vocal tract",
      "pattern recognizers",
      "spidre corpus",
      "identification rates",
      "time duration",
      "vocal source",
      "feature vectors",
      "slp",
      "lvq",
      "gmm",
      "mfcc"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <metric> <otherscientificterm> <material> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "slp -- HYPONYM-OF -- pattern recognizers",
      "slp -- CONJUNCTION -- gmm",
      "gmm -- HYPONYM-OF -- pattern recognizers",
      "lvq -- CONJUNCTION -- gmm",
      "lvq -- HYPONYM-OF -- pattern recognizers"
    ],
    "abstract": "usually , <method_3> do not take into account the <otherscientificterm_2> between the <material_11> and the <otherscientificterm_6> . a feasibility study that retains this dependence is presented here . a model of <otherscientificterm_5> of the pitch and the <otherscientificterm_12> is proposed . three strategies are designed and compared for all female speakers taken from the <material_8> . the first operates on all <material_0> -lrb- baseline strategy -rrb- . the second strategy considers only the <otherscientificterm_4> and the last includes the <otherscientificterm_1> along with the standard <method_16> . we use two <method_7> : <method_14> -- <method_13> and <method_15> . in all cases , we observe an increase in the <metric_9> and more specifically when using a <otherscientificterm_10> of 500 ms -lrb- 6 % higher -rrb- .",
    "abstract_og": "usually , speaker recognition systems do not take into account the short -- term dependence between the vocal source and the vocal tract . a feasibility study that retains this dependence is presented here . a model of joint probability functions of the pitch and the feature vectors is proposed . three strategies are designed and compared for all female speakers taken from the spidre corpus . the first operates on all voiced and unvoiced speech segments -lrb- baseline strategy -rrb- . the second strategy considers only the voiced speech segments and the last includes the short -- term pitch information along with the standard mfcc . we use two pattern recognizers : lvq -- slp and gmm . in all cases , we observe an increase in the identification rates and more specifically when using a time duration of 500 ms -lrb- 6 % higher -rrb- ."
  },
  {
    "title": "Inducing Ontological Co-occurrence Vectors .",
    "entities": [
      "lexical co-occurrence vectors",
      "semantic information",
      "top-5 positions",
      "attachment accuracy",
      "unsupervised methodology",
      "lexical-semantic resources",
      "wordnet",
      "accuracy",
      "ontology"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <method> <material> <material> <metric> <otherscientificterm>",
    "relations": [
      "unsupervised methodology -- USED-FOR -- lexical co-occurrence vectors",
      "wordnet -- HYPONYM-OF -- ontology"
    ],
    "abstract": "in this paper , we present an <method_4> for propagating <otherscientificterm_0> into an <otherscientificterm_8> such as <material_6> . we evaluate the <method_4> on the task of automatically attaching new concepts into the <otherscientificterm_8> . experimental results show 73.9 % <metric_3> in the first position and 81.3 % <metric_7> in the <otherscientificterm_2> . this <method_4> could potentially serve as a foundation for on-tologizing <material_5> and assist the development of other large-scale and internally consistent collections of <otherscientificterm_1> .",
    "abstract_og": "in this paper , we present an unsupervised methodology for propagating lexical co-occurrence vectors into an ontology such as wordnet . we evaluate the unsupervised methodology on the task of automatically attaching new concepts into the ontology . experimental results show 73.9 % attachment accuracy in the first position and 81.3 % accuracy in the top-5 positions . this unsupervised methodology could potentially serve as a foundation for on-tologizing lexical-semantic resources and assist the development of other large-scale and internally consistent collections of semantic information ."
  },
  {
    "title": "An Agent Architecture for Prognostic Reasoning Assistance .",
    "entities": [
      "user intention recognition",
      "user 's intention",
      "software assistant agent",
      "normative reasoning",
      "assistive actions",
      "nor-mative reasoning",
      "time-constrained environment",
      "prohibitions",
      "execution",
      "replanning",
      "planning"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <task> <task> <otherscientificterm> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "user intention recognition -- CONJUNCTION -- user 's intention",
      "user intention recognition -- CONJUNCTION -- planning",
      "normative reasoning -- CONJUNCTION -- execution",
      "user 's intention -- CONJUNCTION -- execution",
      "planning -- CONJUNCTION -- normative reasoning",
      "execution -- CONJUNCTION -- replanning",
      "user intention recognition -- CONJUNCTION -- execution",
      "user 's intention -- CONJUNCTION -- normative reasoning",
      "replanning -- USED-FOR -- assistive actions",
      "planning -- CONJUNCTION -- replanning",
      "user intention recognition -- CONJUNCTION -- normative reasoning",
      "planning -- CONJUNCTION -- execution",
      "software assistant agent -- USED-FOR -- nor-mative reasoning"
    ],
    "abstract": "in this paper we describe a <method_2> that can proactively assist human users situated in a <otherscientificterm_6> to perform <task_5> -- reasoning about <otherscientificterm_7> and obligations -- so that the user can focus on her <method_10> objectives . in order to provide proactive assistance , the <method_2> must be able to 1 -rrb- recognize the user 's planned activities , 2 -rrb- reason about potential needs of assistance associated with those predicted activities , and 3 -rrb- plan to provide appropriate assistance suitable for newly identified user needs . to address these specific requirements , we develop an <method_2> that integrates <task_0> , <method_3> over a <otherscientificterm_1> , and <method_10> , <task_8> and <method_9> for <task_4> . this paper presents the <method_2> and discusses practical applications of this <method_2> .",
    "abstract_og": "in this paper we describe a software assistant agent that can proactively assist human users situated in a time-constrained environment to perform nor-mative reasoning -- reasoning about prohibitions and obligations -- so that the user can focus on her planning objectives . in order to provide proactive assistance , the software assistant agent must be able to 1 -rrb- recognize the user 's planned activities , 2 -rrb- reason about potential needs of assistance associated with those predicted activities , and 3 -rrb- plan to provide appropriate assistance suitable for newly identified user needs . to address these specific requirements , we develop an software assistant agent that integrates user intention recognition , normative reasoning over a user 's intention , and planning , execution and replanning for assistive actions . this paper presents the software assistant agent and discusses practical applications of this software assistant agent ."
  },
  {
    "title": "Affine Projection Algorithm with Selective Regressors .",
    "entities": [
      "affine projection algorithm",
      "affine projection algorithm",
      "selective regressors",
      "input vectors",
      "adaptive filter",
      "weight vector",
      "cost functions",
      "lms-type filter",
      "computational complexity",
      "minimum disturbance",
      "complexity"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <metric>",
    "relations": [
      "selective regressors -- USED-FOR -- affine projection algorithm",
      "affine projection algorithm -- USED-FOR -- weight vector",
      "minimum disturbance -- USED-FOR -- cost functions",
      "computational complexity -- EVALUATE-FOR -- affine projection algorithm"
    ],
    "abstract": "affine projection algorithm , which updates the <otherscientificterm_5> based on several previous <otherscientificterm_3> , is an useful <method_4> to improve the convergence speed of <method_7> . however , the <metric_8> of <method_0> highly depends on the number of <otherscientificterm_3> used for update . in this paper , we propose <method_1> with <method_2> whose purpose is to reduce <metric_10> by selecting a subset of input regressors at every iteration . the optimal selection of input regressors is derived by comparing the <otherscientificterm_6> based on the principle of <otherscientificterm_9> . the new <method_1> show good convergence performance as attested to by various experimental results .",
    "abstract_og": "affine projection algorithm , which updates the weight vector based on several previous input vectors , is an useful adaptive filter to improve the convergence speed of lms-type filter . however , the computational complexity of affine projection algorithm highly depends on the number of input vectors used for update . in this paper , we propose affine projection algorithm with selective regressors whose purpose is to reduce complexity by selecting a subset of input regressors at every iteration . the optimal selection of input regressors is derived by comparing the cost functions based on the principle of minimum disturbance . the new affine projection algorithm show good convergence performance as attested to by various experimental results ."
  },
  {
    "title": "Correlation Filters with Limited Boundaries .",
    "entities": [
      "inherent computational redundancies",
      "correlation filter estimation",
      "learning process",
      "fourier domain",
      "computational efficiency",
      "vision community",
      "correlation filters",
      "correlation filters",
      "frequency domain",
      "object tracking",
      "boundary effects",
      "mosse",
      "detection",
      "accuracy"
    ],
    "types": "<otherscientificterm> <task> <task> <material> <metric> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <method> <task> <metric>",
    "relations": [
      "mosse -- HYPONYM-OF -- correlation filters",
      "accuracy -- CONJUNCTION -- computational efficiency"
    ],
    "abstract": "correlation filters take advantage of specific properties in the <material_3> allowing them to be estimated efficiently : o -lrb- n d log d -rrb- in the <otherscientificterm_8> , versus o -lrb- d 3 + n d 2 -rrb- spatially where d is signal length , and n is the number of signals . recent extensions to <method_6> , such as <method_11> , have reignited interest of their use in the <method_5> due to their robustness and attractive computational properties . in this paper we demonstrate , however , that this <metric_4> comes at a cost . specifically , we demonstrate that only 1 d proportion of shifted examples are unaffected by <otherscientificterm_10> which has a dramatic effect on detection/tracking performance . in this paper , we propose a novel approach to <task_1> that : -lrb- i -rrb- takes advantage of <otherscientificterm_0> in the <otherscientificterm_8> , -lrb- ii -rrb- dramatically reduces <otherscientificterm_10> , and -lrb- iii -rrb- is able to implicitly exploit all possible patches densely extracted from training examples during <task_2> . impressive <task_9> and <task_12> results are presented in terms of both <metric_13> and <metric_4> .",
    "abstract_og": "correlation filters take advantage of specific properties in the fourier domain allowing them to be estimated efficiently : o -lrb- n d log d -rrb- in the frequency domain , versus o -lrb- d 3 + n d 2 -rrb- spatially where d is signal length , and n is the number of signals . recent extensions to correlation filters , such as mosse , have reignited interest of their use in the vision community due to their robustness and attractive computational properties . in this paper we demonstrate , however , that this computational efficiency comes at a cost . specifically , we demonstrate that only 1 d proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance . in this paper , we propose a novel approach to correlation filter estimation that : -lrb- i -rrb- takes advantage of inherent computational redundancies in the frequency domain , -lrb- ii -rrb- dramatically reduces boundary effects , and -lrb- iii -rrb- is able to implicitly exploit all possible patches densely extracted from training examples during learning process . impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency ."
  },
  {
    "title": "Nonstationary Gaussian Process Regression for Evaluating Repeated Clinical Laboratory Tests .",
    "entities": [
      "cost-driven assessments of oversampling",
      "nonstationary latent function",
      "latent physio-logic function",
      "monitoring physiologic state",
      "retrospective analysis",
      "latent func-tion",
      "latent function",
      "gaussian process",
      "adaptive strategy",
      "undersampling",
      "oversampling",
      "sampling"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <task>",
    "relations": [
      "undersampling -- COMPARE -- oversampling",
      "gaussian process -- USED-FOR -- nonstationary latent function"
    ],
    "abstract": "sampling repeated clinical laboratory tests with appropriate timing is challenging because the <otherscientificterm_2> being sampled is in general nonstation-ary . when ordering repeated tests , clinicians adopt various simple strategies that may or may not be well suited to the behavior of the function . previous research on this topic has been primarily focused on <task_0> . but for <task_3> or for <task_4> , <task_9> can be much more problematic than <method_10> . in this paper we analyze hundreds of observation sequences of four different clinical laboratory tests to provide princi-pled , data-driven estimates of <task_9> and over-sampling , and to assess whether the <task_11> adapts to changing volatility of the <otherscientificterm_6> . to do this , we developed a new method for fitting a <method_7> to samples of a <otherscientificterm_1> . our method includes an explicit estimate of the <otherscientificterm_5> 's volatility over time , which is deterministically related to its nonstationarity . we find on average that the degree of <task_9> is up to an order of magnitude greater than <method_10> , and that only a small minority are sampled with an <method_8> .",
    "abstract_og": "sampling repeated clinical laboratory tests with appropriate timing is challenging because the latent physio-logic function being sampled is in general nonstation-ary . when ordering repeated tests , clinicians adopt various simple strategies that may or may not be well suited to the behavior of the function . previous research on this topic has been primarily focused on cost-driven assessments of oversampling . but for monitoring physiologic state or for retrospective analysis , undersampling can be much more problematic than oversampling . in this paper we analyze hundreds of observation sequences of four different clinical laboratory tests to provide princi-pled , data-driven estimates of undersampling and over-sampling , and to assess whether the sampling adapts to changing volatility of the latent function . to do this , we developed a new method for fitting a gaussian process to samples of a nonstationary latent function . our method includes an explicit estimate of the latent func-tion 's volatility over time , which is deterministically related to its nonstationarity . we find on average that the degree of undersampling is up to an order of magnitude greater than oversampling , and that only a small minority are sampled with an adaptive strategy ."
  },
  {
    "title": "Covariance shrinkage for autocorrelated data .",
    "entities": [
      "real world data set",
      "high dimensional settings",
      "finite sample sizes",
      "pronounced estimation bias",
      "machine learning algorithms",
      "eeg-based brain-computer-interfacing experiment",
      "regularization strategies",
      "sancetta estimator",
      "high-dimensional limit",
      "i.i.d. data",
      "toy data",
      "analytic shrinkage",
      "sample covariance",
      "hyperparame-ter choice",
      "signal processing",
      "shrinkage framework",
      "covariance matrices"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "analytic shrinkage -- HYPONYM-OF -- regularization strategies",
      "signal processing -- CONJUNCTION -- machine learning algorithms"
    ],
    "abstract": "the accurate estimation of <otherscientificterm_16> is essential for many <task_14> and <method_4> . in <otherscientificterm_1> the <otherscientificterm_12> is known to perform poorly , hence <method_6> such as <otherscientificterm_11> of ledoit/wolf are applied . in the standard setting , <material_9> is assumed , however , in practice , time series typically exhibit strong autocorrela-tion structure , which introduces a <otherscientificterm_3> . recent work by sancetta has extended the <method_15> beyond <material_9> . we contribute in this work by showing that the <method_7> , while being consistent in the <otherscientificterm_8> , suffers from a high bias in <otherscientificterm_2> . we propose an alternative estimator , which is -lrb- 1 -rrb- unbiased , -lrb- 2 -rrb- less sensitive to <otherscientificterm_13> and -lrb- 3 -rrb- yields superior performance in simulations on <material_10> and on a <material_0> from an <otherscientificterm_5> .",
    "abstract_og": "the accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms . in high dimensional settings the sample covariance is known to perform poorly , hence regularization strategies such as analytic shrinkage of ledoit/wolf are applied . in the standard setting , i.i.d. data is assumed , however , in practice , time series typically exhibit strong autocorrela-tion structure , which introduces a pronounced estimation bias . recent work by sancetta has extended the shrinkage framework beyond i.i.d. data . we contribute in this work by showing that the sancetta estimator , while being consistent in the high-dimensional limit , suffers from a high bias in finite sample sizes . we propose an alternative estimator , which is -lrb- 1 -rrb- unbiased , -lrb- 2 -rrb- less sensitive to hyperparame-ter choice and -lrb- 3 -rrb- yields superior performance in simulations on toy data and on a real world data set from an eeg-based brain-computer-interfacing experiment ."
  },
  {
    "title": "Intelligent Agent Supporting Human-Multi-Robot Team Collaboration .",
    "entities": [
      "myopic advice optimization problem",
      "search and rescue task",
      "operator 's satisfaction",
      "field applications",
      "multi-robot systems"
    ],
    "types": "<task> <task> <metric> <task> <method>",
    "relations": [
      "multi-robot systems -- USED-FOR -- field applications"
    ],
    "abstract": "the number of <method_4> deployed in <task_3> has risen dramatically over the years . nevertheless , supervising and operating multiple robots at once is a difficult task for a single operator to execute . in this paper we propose a novel approach for utilizing advising automated agents when assisting an operator to better manage a team of multiple robots in complex environments . we introduce the <task_0> and exemplify its implementation using an agent for the <task_1> . our intelligent advising agent was evaluated through extensive field trials , with 44 non-expert human operators and 10 low-cost mobile robots , in simulation and physical deployment , and showed a significant improvement in both team performance and the <metric_2> .",
    "abstract_og": "the number of multi-robot systems deployed in field applications has risen dramatically over the years . nevertheless , supervising and operating multiple robots at once is a difficult task for a single operator to execute . in this paper we propose a novel approach for utilizing advising automated agents when assisting an operator to better manage a team of multiple robots in complex environments . we introduce the myopic advice optimization problem and exemplify its implementation using an agent for the search and rescue task . our intelligent advising agent was evaluated through extensive field trials , with 44 non-expert human operators and 10 low-cost mobile robots , in simulation and physical deployment , and showed a significant improvement in both team performance and the operator 's satisfaction ."
  },
  {
    "title": "Detection of spread-spectrum signals in a multi-user environment .",
    "entities": [
      "direct-sequence spread-spectrum multiple access",
      "linear and non-linear structures",
      "impulsive channel noise",
      "spread digital signals",
      "stochastic multi-variate signal",
      "adaptive multiuser de-modulators",
      "locally optimum detector",
      "correlator based structures",
      "adaptive multiuser demodulators",
      "asymptotic normality",
      "hybrid detector",
      "spread-spectrum signals",
      "moderate complexity",
      "performance measure",
      "distribution-free detectors",
      "complexity"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <material> <metric> <metric> <method> <metric>",
    "relations": [
      "linear and non-linear structures -- FEATURE-OF -- hybrid detector",
      "impulsive channel noise -- FEATURE-OF -- stochastic multi-variate signal",
      "adaptive multiuser de-modulators -- USED-FOR -- direct-sequence spread-spectrum multiple access",
      "locally optimum detector -- USED-FOR -- stochastic multi-variate signal"
    ],
    "abstract": "motivated by a previous study of <method_5> for <task_0> , detectors for <material_11> are investigated . due to the prohibitive <metric_15> of the <method_6> for such a <otherscientificterm_4> in <otherscientificterm_2> , <metric_12> , <method_14> are pursued . in particular , the diierential snrs -lrb- processing gain -rrb- of <otherscientificterm_7> are determined . this <metric_13> is apt given the relatively low signal strength of <material_3> . the numerical results -lrb- in the context of the prior investigation of <method_8> -rrb- impel the development of a <method_10> which is composed of <otherscientificterm_1> . the <otherscientificterm_9> of the test statistics under study is also examined .",
    "abstract_og": "motivated by a previous study of adaptive multiuser de-modulators for direct-sequence spread-spectrum multiple access , detectors for spread-spectrum signals are investigated . due to the prohibitive complexity of the locally optimum detector for such a stochastic multi-variate signal in impulsive channel noise , moderate complexity , distribution-free detectors are pursued . in particular , the diierential snrs -lrb- processing gain -rrb- of correlator based structures are determined . this performance measure is apt given the relatively low signal strength of spread digital signals . the numerical results -lrb- in the context of the prior investigation of adaptive multiuser demodulators -rrb- impel the development of a hybrid detector which is composed of linear and non-linear structures . the asymptotic normality of the test statistics under study is also examined ."
  },
  {
    "title": "Automatic Error Recovery for Pronunciation Dictionaries .",
    "entities": [
      "multilingual wiki-based open content dictionary",
      "english wik-tionary word-pronunciation pairs",
      "globalphone hausa pronunciation dictionary",
      "mandarin-english seame code-switch dictionary",
      "word error rate",
      "pronunciation modeling",
      "automatic methods",
      "pronunciation dictionaries",
      "french",
      "asr",
      "german",
      "polish",
      "czech",
      "english"
    ],
    "types": "<material> <material> <material> <material> <metric> <task> <method> <material> <material> <task> <material> <material> <material> <material>",
    "relations": [
      "french -- CONJUNCTION -- german",
      "english -- CONJUNCTION -- german",
      "german -- CONJUNCTION -- polish",
      "czech -- CONJUNCTION -- english",
      "english -- CONJUNCTION -- french",
      "czech -- CONJUNCTION -- french",
      "french -- CONJUNCTION -- polish",
      "german -- CONJUNCTION -- multilingual wiki-based open content dictionary",
      "german -- CONJUNCTION -- german",
      "polish -- CONJUNCTION -- globalphone hausa pronunciation dictionary"
    ],
    "abstract": "in this paper , we present our latest investigations on <task_5> and its impact on <task_9> . we propose completely <method_6> to detect , remove , and substitute inconsistent or flawed entries in <material_7> . the experiments were conducted on different tasks , namely -lrb- 1 -rrb- word-pronunciation pairs from the <material_12> , <material_13> , <material_8> , <material_10> , <material_11> , and spanish wiktionary -lsb- 1 -rsb- , a <material_0> , -lrb- 2 -rrb- our <material_2> -lsb- 2 -rsb- , and -lrb- 3 -rrb- pronunciations to complement our <material_3> -lsb- 3 -rsb- . in the final results , we fairly observed on average an improvement of 2.0 % relative in terms of <metric_4> and even 27.3 % for the case of <material_1> .",
    "abstract_og": "in this paper , we present our latest investigations on pronunciation modeling and its impact on asr . we propose completely automatic methods to detect , remove , and substitute inconsistent or flawed entries in pronunciation dictionaries . the experiments were conducted on different tasks , namely -lrb- 1 -rrb- word-pronunciation pairs from the czech , english , french , german , polish , and spanish wiktionary -lsb- 1 -rsb- , a multilingual wiki-based open content dictionary , -lrb- 2 -rrb- our globalphone hausa pronunciation dictionary -lsb- 2 -rsb- , and -lrb- 3 -rrb- pronunciations to complement our mandarin-english seame code-switch dictionary -lsb- 3 -rsb- . in the final results , we fairly observed on average an improvement of 2.0 % relative in terms of word error rate and even 27.3 % for the case of english wik-tionary word-pronunciation pairs ."
  },
  {
    "title": "Robust PCA neural networks for random noise reduction of the data .",
    "entities": [
      "principal component analysis approach",
      "1-dimensional and 2-dimensional data",
      "realized compression ratio",
      "reduction of noise",
      "noise strength",
      "loss tolerance",
      "lossy compression",
      "noise ltering",
      "de-compressed signal",
      "compression/decompression",
      "noise",
      "de-compression"
    ],
    "types": "<method> <material> <metric> <otherscientificterm> <metric> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "principal component analysis approach -- USED-FOR -- reduction of noise",
      "1-dimensional and 2-dimensional data -- USED-FOR -- noise ltering",
      "lossy compression -- CONJUNCTION -- de-compression",
      "noise -- CONJUNCTION -- loss tolerance"
    ],
    "abstract": "the paper presents <method_0> to the <otherscientificterm_3> contaminating the data . the <method_0> performs the role of <method_6> and <method_11> . the <method_9> provides the means of coding the data and then recovering <method_0> with some losses , dependent on the <metric_2> . in this process some part of information contained in the data is lost . when the <otherscientificterm_5> is equal to the <metric_4> , the <otherscientificterm_10> and the <otherscientificterm_5> are augmented and the <otherscientificterm_8> is deprived of <otherscientificterm_10> . this way of <task_7> has been checked on the examples of <material_1> and the results of numerical experiments have been included in the paper .",
    "abstract_og": "the paper presents principal component analysis approach to the reduction of noise contaminating the data . the principal component analysis approach performs the role of lossy compression and de-compression . the compression/decompression provides the means of coding the data and then recovering principal component analysis approach with some losses , dependent on the realized compression ratio . in this process some part of information contained in the data is lost . when the loss tolerance is equal to the noise strength , the noise and the loss tolerance are augmented and the de-compressed signal is deprived of noise . this way of noise ltering has been checked on the examples of 1-dimensional and 2-dimensional data and the results of numerical experiments have been included in the paper ."
  },
  {
    "title": "Stack-propagation : Improved Representation Learning for Syntax .",
    "entities": [
      "part-of-speech information",
      "reg-ularizer of learned representations",
      "dependency parsing and tagging",
      "predicted pos tags",
      "syntax models",
      "pos tags",
      "greedy model",
      "graph-based approach",
      "hand-tuned templates",
      "universal dependencies",
      "hidden layer",
      "tagger network",
      "features"
    ],
    "types": "<otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "graph-based approach -- COMPARE -- greedy model",
      "hand-tuned templates -- USED-FOR -- syntax models",
      "hand-tuned templates -- USED-FOR -- features",
      "part-of-speech information -- USED-FOR -- syntax models",
      "pos tags -- USED-FOR -- reg-ularizer of learned representations",
      "features -- USED-FOR -- syntax models",
      "features -- USED-FOR -- part-of-speech information"
    ],
    "abstract": "traditional <method_4> typically leverage <otherscientificterm_0> by constructing <otherscientificterm_12> from <otherscientificterm_8> . we demonstrate that a better approach is to utilize <otherscientificterm_5> as a <method_1> . we propose a simple method for learning a stacked pipeline of <method_4> which we call '' stack-propagation '' . we apply this to <task_2> , where we use the <otherscientificterm_10> of the <method_11> as a representation of the input tokens for the parser . at test time , our parser does not require <otherscientificterm_3> . on 19 languages from the <material_9> , our method is 1.3 % -lrb- absolute -rrb- more accurate than a state-of-the-art <method_7> and 2.7 % more accurate than the most comparable <method_6> .",
    "abstract_og": "traditional syntax models typically leverage part-of-speech information by constructing features from hand-tuned templates . we demonstrate that a better approach is to utilize pos tags as a reg-ularizer of learned representations . we propose a simple method for learning a stacked pipeline of syntax models which we call '' stack-propagation '' . we apply this to dependency parsing and tagging , where we use the hidden layer of the tagger network as a representation of the input tokens for the parser . at test time , our parser does not require predicted pos tags . on 19 languages from the universal dependencies , our method is 1.3 % -lrb- absolute -rrb- more accurate than a state-of-the-art graph-based approach and 2.7 % more accurate than the most comparable greedy model ."
  },
  {
    "title": "A Rote Extractor with Edit Distance-Based Generalisation and Multi-Corpora Precision Calculation .",
    "entities": [
      "pattern accuracy calculation procedure",
      "edit-distance-based pattern generalization algorithm",
      "named entity categories",
      "part-of-speech tags",
      "pattern generalization",
      "rote extrac-tor",
      "semantic relationships",
      "unrestricted text",
      "precision",
      "generalization"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <material> <metric> <task>",
    "relations": [
      "part-of-speech tags -- USED-FOR -- generalization",
      "unrestricted text -- USED-FOR -- semantic relationships",
      "precision -- EVALUATE-FOR -- rote extrac-tor",
      "edit-distance-based pattern generalization algorithm -- CONJUNCTION -- pattern accuracy calculation procedure",
      "rote extrac-tor -- USED-FOR -- semantic relationships"
    ],
    "abstract": "in this paper , we describe a <method_5> that learns patterns for finding <otherscientificterm_6> in <material_7> , with new procedures for <task_4> and scoring . these include the use of <method_3> to guide the <task_9> , <otherscientificterm_2> inside the patterns , an <method_1> , and a <method_0> based on evaluating the patterns on several test corpora . in an evaluation with 14 entities , the <method_5> attains a <metric_8> higher than 50 % for half of the relationships considered .",
    "abstract_og": "in this paper , we describe a rote extrac-tor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring . these include the use of part-of-speech tags to guide the generalization , named entity categories inside the patterns , an edit-distance-based pattern generalization algorithm , and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora . in an evaluation with 14 entities , the rote extrac-tor attains a precision higher than 50 % for half of the relationships considered ."
  },
  {
    "title": "Robust Repositioning to Counter Unpredictable Demand in Bike Sharing Systems .",
    "entities": [
      "bike sharing systems",
      "online and robust repositioning approach",
      "iterative two player game",
      "real world data",
      "scenario generation approach",
      "myopic reasoning",
      "bsss operators",
      "repositioning solution"
    ],
    "types": "<method> <method> <method> <material> <method> <method> <method> <method>",
    "relations": [
      "iterative two player game -- USED-FOR -- scenario generation approach"
    ],
    "abstract": "bike sharing systems -lrb- bsss -rrb- experience a significant loss in customer demand due to starvation -lrb- empty base stations precluding bike pickup -rrb- or congestion -lrb- full base stations precluding bike return -rrb- . therefore , <method_6> reposition bikes between stations with the help of carrier vehicles . due to unpredictable and dynamically changing nature of the demand , <method_5> typically provides a below par performance . we propose an <method_1> to min-imise the loss in customer demand while considering the possible uncertainty in future demand . specifically , we develop a <method_4> based on an <method_2> to compute a strategy of repositioning by assuming that the environment can generate a worse demand scenario -lrb- out of the feasible demand scenarios -rrb- against the current <method_7> . extensive computational results from a simulation built on <material_3> set of bike sharing company demonstrate that our <method_1> can significantly reduce the expected lost demand over the existing benchmark approaches .",
    "abstract_og": "bike sharing systems -lrb- bsss -rrb- experience a significant loss in customer demand due to starvation -lrb- empty base stations precluding bike pickup -rrb- or congestion -lrb- full base stations precluding bike return -rrb- . therefore , bsss operators reposition bikes between stations with the help of carrier vehicles . due to unpredictable and dynamically changing nature of the demand , myopic reasoning typically provides a below par performance . we propose an online and robust repositioning approach to min-imise the loss in customer demand while considering the possible uncertainty in future demand . specifically , we develop a scenario generation approach based on an iterative two player game to compute a strategy of repositioning by assuming that the environment can generate a worse demand scenario -lrb- out of the feasible demand scenarios -rrb- against the current repositioning solution . extensive computational results from a simulation built on real world data set of bike sharing company demonstrate that our online and robust repositioning approach can significantly reduce the expected lost demand over the existing benchmark approaches ."
  },
  {
    "title": "Does String-Based Neural MT Learn Source Syntax ? .",
    "entities": [
      "neural , encoder-decoder translation system",
      "local and global source syntax",
      "syntactic structure",
      "syntactic information",
      "encoder",
      "syntax"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "neural , encoder-decoder translation system -- USED-FOR -- syntactic information"
    ],
    "abstract": "we investigate whether a <method_0> learns <otherscientificterm_3> on the source side as a by-product of training . we propose two methods to detect whether the <otherscientificterm_4> has learned <otherscientificterm_1> . a fine-grained analysis of the <otherscientificterm_2> learned by the <otherscientificterm_4> reveals which kinds of <otherscientificterm_5> are learned and which are missing .",
    "abstract_og": "we investigate whether a neural , encoder-decoder translation system learns syntactic information on the source side as a by-product of training . we propose two methods to detect whether the encoder has learned local and global source syntax . a fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing ."
  },
  {
    "title": "Nuclear Norm Minimization via Active Subspace Selection .",
    "entities": [
      "alternating least squares",
      "non-smooth and smooth optimization",
      "matrix completion problems",
      "nuclear norm regulariza-tion",
      "matrix completion problem",
      "second order methods",
      "nuclear norm solvers",
      "netflix dataset",
      "active sub-space",
      "proximal gradient",
      "non-convex solvers",
      "yahoo-music dataset"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <task> <method> <method> <material> <otherscientificterm> <otherscientificterm> <method> <material>",
    "relations": [
      "alternating least squares -- HYPONYM-OF -- non-convex solvers",
      "proximal gradient -- USED-FOR -- active sub-space"
    ],
    "abstract": "we describe a novel approach to optimizing <task_2> involving <otherscientificterm_3> and apply it to the <task_4> . we combine methods from <method_1> . at each step we use the <otherscientificterm_9> to select an <otherscientificterm_8> . we then find a smooth , convex relaxation of the smaller subspace problems and solve these using <method_5> . we apply our methods to <task_2> including <material_7> , and show that they are more than 6 times faster than state-of-the-art <method_6> . also , this is the first paper to scale <method_6> to the <material_11> , and the first time in the literature that the efficiency of <method_6> can be compared and even compete with <method_10> like <method_0> .",
    "abstract_og": "we describe a novel approach to optimizing matrix completion problems involving nuclear norm regulariza-tion and apply it to the matrix completion problem . we combine methods from non-smooth and smooth optimization . at each step we use the proximal gradient to select an active sub-space . we then find a smooth , convex relaxation of the smaller subspace problems and solve these using second order methods . we apply our methods to matrix completion problems including netflix dataset , and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers . also , this is the first paper to scale nuclear norm solvers to the yahoo-music dataset , and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like alternating least squares ."
  },
  {
    "title": "A Scalable Message-Passing Algorithm for Supply Chain Formation .",
    "entities": [
      "supply chain formation",
      "max-sum loopy belief propagation",
      "decentralized scf problem",
      "binary factor graph",
      "binary variables",
      "optimization problem",
      "decentralized scf",
      "local information",
      "scalability"
    ],
    "types": "<task> <method> <task> <method> <otherscientificterm> <task> <method> <otherscientificterm> <metric>",
    "relations": [
      "optimization problem -- USED-FOR -- decentralized scf problem",
      "max-sum loopy belief propagation -- USED-FOR -- optimization problem"
    ],
    "abstract": "supply chain formation -lrb- scf -rrb- is the process of determining the participants in a supply chain , who will exchange what with whom , and the terms of the exchanges . <method_6> appears as a highly intricate task because agents only possess <otherscientificterm_7> and have limited knowledge about the capabilities of other agents . the <task_2> has been recently cast as an <task_5> that can be efficiently approximated using <method_1> . along this direction , in this paper we propose a novel encoding of the <task_2> into a <method_3> -lrb- containing only <otherscientificterm_4> -rrb- as well as an alternative algorithm . we empirically show that our approach allows to significantly increase <metric_8> , hence allowing to form supply chains in market scenarios with a large number of participants and high competition .",
    "abstract_og": "supply chain formation -lrb- scf -rrb- is the process of determining the participants in a supply chain , who will exchange what with whom , and the terms of the exchanges . decentralized scf appears as a highly intricate task because agents only possess local information and have limited knowledge about the capabilities of other agents . the decentralized scf problem has been recently cast as an optimization problem that can be efficiently approximated using max-sum loopy belief propagation . along this direction , in this paper we propose a novel encoding of the decentralized scf problem into a binary factor graph -lrb- containing only binary variables -rrb- as well as an alternative algorithm . we empirically show that our approach allows to significantly increase scalability , hence allowing to form supply chains in market scenarios with a large number of participants and high competition ."
  },
  {
    "title": "Sudden noise reduction based on GMM with noise power estimation .",
    "entities": [
      "noise detection and classification methods",
      "estimation of noise power",
      "detection and classification results",
      "sudden noise detection",
      "recognition of utterances",
      "noise power estimation",
      "noise reduction method",
      "gmm-based noise reduction",
      "sudden noise",
      "noise",
      "classification"
    ],
    "types": "<method> <method> <metric> <task> <task> <task> <method> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "estimation of noise power -- USED-FOR -- gmm-based noise reduction",
      "sudden noise detection -- CONJUNCTION -- classification",
      "estimation of noise power -- CONJUNCTION -- noise reduction method"
    ],
    "abstract": "this paper describes a method for reducing <otherscientificterm_8> using <method_0> , and <task_5> . <task_3> and <task_10> have been dealt with in our previous study . in this paper , <task_7> is performed using the <metric_2> . as a result of <task_10> , we can determine the kind of <otherscientificterm_9> we are dealing with , but the power is unknown . in this paper , this <task_7> is solved by combining an <method_1> with the <method_6> . in our experiments , the proposed method achieved good performance for <task_4> overlapped by sudden noises .",
    "abstract_og": "this paper describes a method for reducing sudden noise using noise detection and classification methods , and noise power estimation . sudden noise detection and classification have been dealt with in our previous study . in this paper , gmm-based noise reduction is performed using the detection and classification results . as a result of classification , we can determine the kind of noise we are dealing with , but the power is unknown . in this paper , this gmm-based noise reduction is solved by combining an estimation of noise power with the noise reduction method . in our experiments , the proposed method achieved good performance for recognition of utterances overlapped by sudden noises ."
  },
  {
    "title": "Gesture image sequence interpretation using the multi-PDM method and hidden Markov model .",
    "entities": [
      "hidden markov model",
      "multi-principal-distribution-model method",
      "gesture image sequence interpretation",
      "individual pdm shape model",
      "patterns of variability",
      "model transition sequence",
      "pdm shape model",
      "training hand shapes",
      "continuous gestures",
      "gesture recognition",
      "pdm model",
      "annotated images",
      "model transition",
      "hand-shape"
    ],
    "types": "<method> <method> <task> <method> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <task> <method> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "multi-principal-distribution-model method -- USED-FOR -- gesture image sequence interpretation",
      "hidden markov model -- USED-FOR -- model transition",
      "multi-principal-distribution-model method -- USED-FOR -- continuous gestures",
      "hidden markov model -- USED-FOR -- gesture image sequence interpretation",
      "multi-principal-distribution-model method -- CONJUNCTION -- hidden markov model",
      "pdm model -- USED-FOR -- multi-principal-distribution-model method",
      "hidden markov model -- USED-FOR -- pdm shape model"
    ],
    "abstract": "this paper introduces a <method_1> and <method_0> for <task_2> . to track the <otherscientificterm_13> , <method_1> uses the <method_10> which is built by learning <otherscientificterm_4> from a training set of correctly <material_11> . for <task_9> , we need to deal with a large variety of <otherscientificterm_13> . therefore , we divide all the <otherscientificterm_7> into a number of similar groups , with each group trained for an <method_3> . finally , we use the <method_0> to determine <otherscientificterm_12> among these <method_6> . from the <material_5> , <method_1> can identify the <otherscientificterm_8> denoting one-digit or two-digit numbers .",
    "abstract_og": "this paper introduces a multi-principal-distribution-model method and hidden markov model for gesture image sequence interpretation . to track the hand-shape , multi-principal-distribution-model method uses the pdm model which is built by learning patterns of variability from a training set of correctly annotated images . for gesture recognition , we need to deal with a large variety of hand-shape . therefore , we divide all the training hand shapes into a number of similar groups , with each group trained for an individual pdm shape model . finally , we use the hidden markov model to determine model transition among these pdm shape model . from the model transition sequence , multi-principal-distribution-model method can identify the continuous gestures denoting one-digit or two-digit numbers ."
  },
  {
    "title": "The ester 2 evaluation campaign for the rich transcription of French radio broadcasts .",
    "entities": [
      "automatic radio broadcasts rich transcription systems",
      "ester 2 evaluation",
      "audio event detection",
      "french language",
      "evaluation protocols",
      "information extraction",
      "speaker tracking",
      "ortho-graphic transcription",
      "tracking"
    ],
    "types": "<task> <metric> <task> <material> <metric> <task> <task> <task> <task>",
    "relations": [
      "audio event detection -- CONJUNCTION -- tracking",
      "audio event detection -- CONJUNCTION -- speaker tracking",
      "audio event detection -- CONJUNCTION -- ortho-graphic transcription",
      "speaker tracking -- CONJUNCTION -- ortho-graphic transcription",
      "ortho-graphic transcription -- CONJUNCTION -- information extraction",
      "speaker tracking -- CONJUNCTION -- tracking"
    ],
    "abstract": "this paper reports on the final results of the <metric_1> campaign held from 2007 to april 2009 . the aim of this campaign was to evaluate <task_0> for the <material_3> . the evaluation tasks were divided into three main categories : <task_2> and <task_8> -lrb- e.g. , speech vs. music , <task_6> -rrb- , <task_7> , and <task_5> . the paper describes the data provided for the campaign , the task definitions and <metric_4> as well as the results .",
    "abstract_og": "this paper reports on the final results of the ester 2 evaluation campaign held from 2007 to april 2009 . the aim of this campaign was to evaluate automatic radio broadcasts rich transcription systems for the french language . the evaluation tasks were divided into three main categories : audio event detection and tracking -lrb- e.g. , speech vs. music , speaker tracking -rrb- , ortho-graphic transcription , and information extraction . the paper describes the data provided for the campaign , the task definitions and evaluation protocols as well as the results ."
  },
  {
    "title": "Factoring Synchronous Grammars by Sorting .",
    "entities": [
      "synchronous context-free grammars",
      "machine translation applications",
      "recognizing permutations",
      "translation models",
      "computational complexity",
      "rules",
      "parsing"
    ],
    "types": "<method> <task> <task> <method> <metric> <otherscientificterm> <task>",
    "relations": [
      "synchronous context-free grammars -- USED-FOR -- translation models",
      "translation models -- USED-FOR -- machine translation applications",
      "synchronous context-free grammars -- USED-FOR -- machine translation applications",
      "synchronous context-free grammars -- USED-FOR -- parsing"
    ],
    "abstract": "synchronous context-free grammars -lrb- scfgs -rrb- have been successfully exploited as <method_3> in <task_1> . when <task_6> with an <method_0> , <metric_4> grows exponentially with the length of the <otherscientificterm_5> , in the worst case . in this paper we examine the problem of factorizing each rule of an input <method_0> to a generatively equivalent set of <otherscientificterm_5> , each having the smallest possible length . our algorithm works in time o -lrb- n log n -rrb- , for each rule of length n . this improves upon previous results and solves an open problem about <task_2> that can be factored .",
    "abstract_og": "synchronous context-free grammars -lrb- scfgs -rrb- have been successfully exploited as translation models in machine translation applications . when parsing with an synchronous context-free grammars , computational complexity grows exponentially with the length of the rules , in the worst case . in this paper we examine the problem of factorizing each rule of an input synchronous context-free grammars to a generatively equivalent set of rules , each having the smallest possible length . our algorithm works in time o -lrb- n log n -rrb- , for each rule of length n . this improves upon previous results and solves an open problem about recognizing permutations that can be factored ."
  },
  {
    "title": "Improving deep neural networks for LVCSR using dropout and shrinking structure .",
    "entities": [
      "70-hour mandarin transcription task",
      "hidden markov models",
      "309-hour switchboard task",
      "shrinking dnn structure",
      "greedy layer-wise pre-trained dnn",
      "hybrid deep neural networks",
      "relative recognition error reduction",
      "psc and swb tasks",
      "back-propagation",
      "gmm/hmms method",
      "computation time",
      "dnn prior",
      "test time",
      "swb task",
      "model size",
      "recognition accuracy",
      "hidden layers",
      "dropout",
      "dap"
    ],
    "types": "<task> <method> <material> <method> <method> <method> <metric> <task> <method> <method> <otherscientificterm> <otherscientificterm> <metric> <task> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "relative recognition error reduction -- EVALUATE-FOR -- psc and swb tasks",
      "309-hour switchboard task -- EVALUATE-FOR -- shrinking dnn structure",
      "hidden markov models -- COMPARE -- gmm/hmms method",
      "swb task -- EVALUATE-FOR -- dap",
      "hybrid deep neural networks -- CONJUNCTION -- hidden markov models",
      "shrinking dnn structure -- USED-FOR -- psc and swb tasks",
      "70-hour mandarin transcription task -- EVALUATE-FOR -- shrinking dnn structure",
      "relative recognition error reduction -- EVALUATE-FOR -- shrinking dnn structure"
    ],
    "abstract": "recently , the <method_5> and <method_1> have achieved dramatic gains over the conventional <method_9> on various large vocabulary continuous speech recognition -lrb- lvcsr -rrb- tasks . in this paper , we propose two new methods to further improve the <method_1> : i -rrb- use <otherscientificterm_17> as pre-conditioner -lrb- <method_18> -rrb- to initialize <otherscientificterm_11> to <method_8> for better <metric_15> ; ii -rrb- employ a <method_3> with <otherscientificterm_16> decreasing in size from bottom to top for the purpose of reducing <otherscientificterm_14> and expediting <otherscientificterm_10> . the proposed <method_3> is evaluated in a <task_0> and the <material_2> . compared with the traditional <method_4> , <method_3> can achieve about 10 % and 6.8 % <metric_6> for <task_7> respectively . in addition , we also evaluate <method_3> as well as its combination with <method_18> on the <task_13> . experimental results show that these methods can reduce <otherscientificterm_14> to 45 % of original size and accelerate training and <metric_12> by 55 % , without losing <metric_15> .",
    "abstract_og": "recently , the hybrid deep neural networks and hidden markov models have achieved dramatic gains over the conventional gmm/hmms method on various large vocabulary continuous speech recognition -lrb- lvcsr -rrb- tasks . in this paper , we propose two new methods to further improve the hidden markov models : i -rrb- use dropout as pre-conditioner -lrb- dap -rrb- to initialize dnn prior to back-propagation for better recognition accuracy ; ii -rrb- employ a shrinking dnn structure with hidden layers decreasing in size from bottom to top for the purpose of reducing model size and expediting computation time . the proposed shrinking dnn structure is evaluated in a 70-hour mandarin transcription task and the 309-hour switchboard task . compared with the traditional greedy layer-wise pre-trained dnn , shrinking dnn structure can achieve about 10 % and 6.8 % relative recognition error reduction for psc and swb tasks respectively . in addition , we also evaluate shrinking dnn structure as well as its combination with dap on the swb task . experimental results show that these methods can reduce model size to 45 % of original size and accelerate training and test time by 55 % , without losing recognition accuracy ."
  },
  {
    "title": "Using the GEMS System for Cancer Diagnosis and Biomarker Discovery from Microarray Gene Expression Data .",
    "entities": [
      "microarray gene expression data",
      "microarray datasets",
      "algorithmic evaluation",
      "gems system",
      "biomarker discovery",
      "human analysts"
    ],
    "types": "<material> <material> <method> <method> <task> <method>",
    "relations": [
      "microarray datasets -- USED-FOR -- algorithmic evaluation",
      "gems system -- USED-FOR -- biomarker discovery",
      "microarray datasets -- EVALUATE-FOR -- gems system",
      "algorithmic evaluation -- USED-FOR -- gems system",
      "microarray gene expression data -- USED-FOR -- gems system",
      "microarray datasets -- USED-FOR -- gems system"
    ],
    "abstract": "we will demonstrate the <method_3> for automated development and evaluation of high-quality cancer diagnostic models and <task_4> from <material_0> . the development of <method_3> was informed by the results of an extensive <method_2> using 11 <material_1> . the <method_3> was further evaluated in two cross-dataset applications and using 5 <material_1> . the performance of models produced by <method_3> is comparable or better than the results obtained by <method_5> , and these models generalize well to independent samples in cross-dataset applications . the <method_3> is freely available for download from http://www.gems-system.org for non-commercial use .",
    "abstract_og": "we will demonstrate the gems system for automated development and evaluation of high-quality cancer diagnostic models and biomarker discovery from microarray gene expression data . the development of gems system was informed by the results of an extensive algorithmic evaluation using 11 microarray datasets . the gems system was further evaluated in two cross-dataset applications and using 5 microarray datasets . the performance of models produced by gems system is comparable or better than the results obtained by human analysts , and these models generalize well to independent samples in cross-dataset applications . the gems system is freely available for download from http://www.gems-system.org for non-commercial use ."
  },
  {
    "title": "Training structural SVMs when exact inference is intractable .",
    "entities": [
      "approximate trained structural svms",
      "theoretical and empirical analysis",
      "structural svm -rrb-",
      "approximate training methods",
      "approximate training algorithms",
      "relaxed trained models",
      "relaxations -rrb- algorithms",
      "image segmentation",
      "structural svms",
      "undergenerating methods",
      "overgenerating methods",
      "machine translation",
      "relaxed predictors",
      "discriminative training",
      "approximate training",
      "non-fractional predictions",
      "clustering"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <task> <method> <method> <method> <task> <otherscientificterm> <method> <task> <otherscientificterm> <task>",
    "relations": [
      "overgenerating methods -- COMPARE -- undergenerating methods",
      "structural svm -rrb- -- USED-FOR -- machine translation",
      "machine translation -- CONJUNCTION -- image segmentation",
      "discriminative training -- USED-FOR -- machine translation",
      "image segmentation -- CONJUNCTION -- clustering",
      "relaxed predictors -- USED-FOR -- non-fractional predictions"
    ],
    "abstract": "while <method_13> -lrb- e.g. , crf , <method_2> holds much promise for <task_11> , <task_7> , and <task_16> , the complex inference these applications require make exact training intractable . this leads to a need for <method_3> . unfortunately , knowledge about how to perform efficient and effective <task_14> is limited . focusing on <method_8> , we provide and explore algorithms for two different classes of <method_4> , which we call undergenerating -lrb- e.g. , greedy -rrb- and overgenerating -lrb- e.g. , <method_6> . we provide a <method_1> of both types of <method_0> , focusing on fully connected pairwise markov random fields . we find that models trained with <method_10> have theoretic advantages over <method_9> , are empirically robust relative to their undergenerating brethren , and <method_5> favor <otherscientificterm_15> from <otherscientificterm_12> .",
    "abstract_og": "while discriminative training -lrb- e.g. , crf , structural svm -rrb- holds much promise for machine translation , image segmentation , and clustering , the complex inference these applications require make exact training intractable . this leads to a need for approximate training methods . unfortunately , knowledge about how to perform efficient and effective approximate training is limited . focusing on structural svms , we provide and explore algorithms for two different classes of approximate training algorithms , which we call undergenerating -lrb- e.g. , greedy -rrb- and overgenerating -lrb- e.g. , relaxations -rrb- algorithms . we provide a theoretical and empirical analysis of both types of approximate trained structural svms , focusing on fully connected pairwise markov random fields . we find that models trained with overgenerating methods have theoretic advantages over undergenerating methods , are empirically robust relative to their undergenerating brethren , and relaxed trained models favor non-fractional predictions from relaxed predictors ."
  },
  {
    "title": "Multi-modal sensor localization using a mobile access point .",
    "entities": [
      "mobile access point",
      "doppler and time delay",
      "randomly deployed sensor network",
      "sensor node localization",
      "radio and acoustics",
      "angle of arrival",
      "sensor local-ization algorithms",
      "pre-established sensor network",
      "radio broadcasts timing",
      "acoustic propagation effects",
      "acoustic signal parameters",
      "location information",
      "acoustic emission",
      "multi-modal approach",
      "doppler stretch",
      "time delay",
      "broadcast mode",
      "sensor node",
      "turbulent atmosphere"
    ],
    "types": "<material> <task> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "location information -- CONJUNCTION -- acoustic signal parameters",
      "acoustic signal parameters -- USED-FOR -- sensor node",
      "doppler stretch -- CONJUNCTION -- time delay",
      "turbulent atmosphere -- USED-FOR -- acoustic propagation effects",
      "radio and acoustics -- USED-FOR -- multi-modal approach",
      "mobile access point -- USED-FOR -- randomly deployed sensor network",
      "radio broadcasts timing -- CONJUNCTION -- location information",
      "time delay -- CONJUNCTION -- angle of arrival"
    ],
    "abstract": "we consider the problem of <task_3> in a <method_2> , using a <material_0> . the <material_0> can be used to localize many sensors simultaneously in a <otherscientificterm_16> , without a <method_7> . we consider a <method_13> , combining <otherscientificterm_4> . the <task_8> , <otherscientificterm_11> , and <otherscientificterm_10> . the <otherscientificterm_12> may be used at the sensor to measure <otherscientificterm_14> , <otherscientificterm_15> , and <otherscientificterm_5> . these <otherscientificterm_10> are individually sufficient to localize a <otherscientificterm_17> , or they may be advantageously combined . we focus on the cases of <task_1> . <method_6> are developed , and performance analysis includes <otherscientificterm_9> caused by the <otherscientificterm_18> .",
    "abstract_og": "we consider the problem of sensor node localization in a randomly deployed sensor network , using a mobile access point . the mobile access point can be used to localize many sensors simultaneously in a broadcast mode , without a pre-established sensor network . we consider a multi-modal approach , combining radio and acoustics . the radio broadcasts timing , location information , and acoustic signal parameters . the acoustic emission may be used at the sensor to measure doppler stretch , time delay , and angle of arrival . these acoustic signal parameters are individually sufficient to localize a sensor node , or they may be advantageously combined . we focus on the cases of doppler and time delay . sensor local-ization algorithms are developed , and performance analysis includes acoustic propagation effects caused by the turbulent atmosphere ."
  },
  {
    "title": "Tracking a hand manipulating an object .",
    "entities": [
      "pairwise markov random field",
      "anatomical hand structure",
      "individual local tracker",
      "classical anatomical constraints",
      "ground truth",
      "external forces",
      "articulated structure",
      "synthetic data",
      "partial occlusions",
      "belief propagation",
      "hand configuration",
      "soft constraints",
      "hand-tracking",
      "self-occlusions",
      "robustness"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric>",
    "relations": [
      "belief propagation -- USED-FOR -- hand configuration"
    ],
    "abstract": "we present a method for tracking a hand while it is interacting with an object . this setting is arguably the one where <method_12> has most practical relevance , but poses significant additional challenges : strong occlusions by the object as well as <otherscientificterm_13> are the norm , and <otherscientificterm_3> need to be softened due to the <otherscientificterm_5> between hand and object . to achieve <metric_14> to <otherscientificterm_8> , we use an <method_2> for each segment of the <otherscientificterm_6> . the segments are connected in a <otherscientificterm_0> , which enforces the <otherscientificterm_1> through <otherscientificterm_11> on the joints between adjacent segments . the most likely <otherscientificterm_10> is found with <method_9> . both range and color data are used as input . experiments are presented for <material_7> with <material_4> and for real data of people manipulating objects .",
    "abstract_og": "we present a method for tracking a hand while it is interacting with an object . this setting is arguably the one where hand-tracking has most practical relevance , but poses significant additional challenges : strong occlusions by the object as well as self-occlusions are the norm , and classical anatomical constraints need to be softened due to the external forces between hand and object . to achieve robustness to partial occlusions , we use an individual local tracker for each segment of the articulated structure . the segments are connected in a pairwise markov random field , which enforces the anatomical hand structure through soft constraints on the joints between adjacent segments . the most likely hand configuration is found with belief propagation . both range and color data are used as input . experiments are presented for synthetic data with ground truth and for real data of people manipulating objects ."
  },
  {
    "title": "Optimal time-of-use electricity pricing using game theory .",
    "entities": [
      "demand-side management method",
      "game theory",
      "time-of-use pricing",
      "optimal tou pricing strategy",
      "fluctuating user demands",
      "user satisfaction measurement",
      "nash equilibrium",
      "user demand",
      "utility functions",
      "iterative methods",
      "user demands",
      "backward induction"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "game theory -- USED-FOR -- optimal tou pricing strategy",
      "backward induction -- CONJUNCTION -- iterative methods",
      "iterative methods -- USED-FOR -- nash equilibrium",
      "demand-side management method -- USED-FOR -- user demands"
    ],
    "abstract": "typical <otherscientificterm_10> of electricity vary throughout the day , which increases the cost to utility companies and decreases the stability of the power system . <method_2> has been proposed as a <method_0> to influence <otherscientificterm_10> . in this paper , we describe a new approach of <method_3> based on <method_1> . we propose models for costs due to the <otherscientificterm_4> to the utility companies , as well as the <method_5> because of the difference between the demand and actual load . we design <otherscientificterm_8> for the company and the user , and obtain the <otherscientificterm_6> using <method_11> and <method_9> . numerical example shows that our method is effective in lev-eling the <metric_7> by setting optimal tou prices , in potentially increasing the profit of the utility companies and ensuring overall user benefit .",
    "abstract_og": "typical user demands of electricity vary throughout the day , which increases the cost to utility companies and decreases the stability of the power system . time-of-use pricing has been proposed as a demand-side management method to influence user demands . in this paper , we describe a new approach of optimal tou pricing strategy based on game theory . we propose models for costs due to the fluctuating user demands to the utility companies , as well as the user satisfaction measurement because of the difference between the demand and actual load . we design utility functions for the company and the user , and obtain the nash equilibrium using backward induction and iterative methods . numerical example shows that our method is effective in lev-eling the user demand by setting optimal tou prices , in potentially increasing the profit of the utility companies and ensuring overall user benefit ."
  },
  {
    "title": "Distance-Bounded Consistent Query Answering .",
    "entities": [
      "consistent query answering",
      "inconsistent data",
      "computational complexity",
      "reasoning",
      "ai"
    ],
    "types": "<task> <material> <metric> <task> <task>",
    "relations": [
      "inconsistent data -- USED-FOR -- reasoning"
    ],
    "abstract": "the ability to perform <task_3> on <material_1> is a central problem both for <task_4> and database research . one approach to deal with this situation is <task_0> , where queries are answered over all possible repairs of the database . in general , the repair may be very distant from the original database . in this work we present a new approach where this distance is bounded and analyze its <metric_2> . our results show that in many -lrb- but not all -rrb- cases the complexity drops .",
    "abstract_og": "the ability to perform reasoning on inconsistent data is a central problem both for ai and database research . one approach to deal with this situation is consistent query answering , where queries are answered over all possible repairs of the database . in general , the repair may be very distant from the original database . in this work we present a new approach where this distance is bounded and analyze its computational complexity . our results show that in many -lrb- but not all -rrb- cases the complexity drops ."
  },
  {
    "title": "Transform/subband representations for signals with arbitrarily shaped regions of support .",
    "entities": [
      "signal processing algorithms",
      "arbitrary-length 1-d signals",
      "transform/subband representations",
      "as signals",
      "as signal",
      "innite-length signals",
      "wavelet representations",
      "nite-length 1-d",
      "representations"
    ],
    "types": "<method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "wavelet representations -- USED-FOR -- arbitrary-length 1-d signals",
      "transform/subband representations -- USED-FOR -- signal processing algorithms",
      "representations -- USED-FOR -- innite-length signals"
    ],
    "abstract": "transform/subband <method_8> form a basic building block for many <method_0> and applications . most of the eort has focused on developing <method_8> for <otherscientificterm_5> , with simple extensions to <otherscientificterm_7> and rectangular support 2-d signals . however , many signals may h a v e arbitrary length or arbitrarily shaped -lrb- as -rrb- regions of support -lrb- ros -rrb- . we present a novel framework for creating critically sampled perfect reconstruction transform/subband <method_8> for <otherscientificterm_3> . our method selects an appropriate subset of vectors from an -lrb- easily obtained -rrb- basis for a larger -lrb- superset -rrb- signal space , in order to form a basis for the <otherscientificterm_4> . in particular , we have developed a number of promising <method_6> for <material_1> and as 2-d/m-d signals that provide high performance with low complexity .",
    "abstract_og": "transform/subband representations form a basic building block for many signal processing algorithms and applications . most of the eort has focused on developing representations for innite-length signals , with simple extensions to nite-length 1-d and rectangular support 2-d signals . however , many signals may h a v e arbitrary length or arbitrarily shaped -lrb- as -rrb- regions of support -lrb- ros -rrb- . we present a novel framework for creating critically sampled perfect reconstruction transform/subband representations for as signals . our method selects an appropriate subset of vectors from an -lrb- easily obtained -rrb- basis for a larger -lrb- superset -rrb- signal space , in order to form a basis for the as signal . in particular , we have developed a number of promising wavelet representations for arbitrary-length 1-d signals and as 2-d/m-d signals that provide high performance with low complexity ."
  },
  {
    "title": "Learning a Distance Metric by Empirical Loss Minimization .",
    "entities": [
      "loss function based metric learning framework",
      "smoothed hinge loss function",
      "log loss function",
      "mild conditions",
      "instance distribution"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "log loss function -- CONJUNCTION -- smoothed hinge loss function"
    ],
    "abstract": "in this paper , we study the problem of learning a metric and propose a <method_0> , in which the metric is estimated by minimizing an empirical risk over a training set . with <otherscientificterm_3> on the <otherscientificterm_4> and the used loss function , we prove that the empirical risk converges to its expected counterpart at rate of root-n . in addition , with the assumption that the best metric that minimizes the expected risk is bounded , we prove that the learned metric is consistent . two example algorithms are presented by using the proposed <method_0> , each of which uses a <otherscientificterm_2> and a <otherscientificterm_1> , respectively . experimental results suggest the effectiveness of the proposed algorithms .",
    "abstract_og": "in this paper , we study the problem of learning a metric and propose a loss function based metric learning framework , in which the metric is estimated by minimizing an empirical risk over a training set . with mild conditions on the instance distribution and the used loss function , we prove that the empirical risk converges to its expected counterpart at rate of root-n . in addition , with the assumption that the best metric that minimizes the expected risk is bounded , we prove that the learned metric is consistent . two example algorithms are presented by using the proposed loss function based metric learning framework , each of which uses a log loss function and a smoothed hinge loss function , respectively . experimental results suggest the effectiveness of the proposed algorithms ."
  },
  {
    "title": "Word Representations : A Simple and General Method for Semi-Supervised Learning .",
    "entities": [
      "supervised nlp system",
      "unsupervised word representations",
      "nlp systems",
      "word features",
      "word representations",
      "supervised baselines",
      "brown clusters",
      "ner",
      "accuracy"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <metric>",
    "relations": [
      "word features -- USED-FOR -- nlp systems",
      "unsupervised word representations -- USED-FOR -- supervised nlp system"
    ],
    "abstract": "if we take an existing <method_0> , a simple and general way to improve <metric_8> is to use <method_1> as extra <otherscientificterm_3> . we evaluate <method_6> , collobert and weston -lrb- 2008 -rrb- embeddings , and hlbl -lrb- mnih & hinton , 2009 -rrb- embeddings of words on both <otherscientificterm_7> and chunking . we use near state-of-the-art <method_5> , and find that each of the three <method_4> improves the <metric_8> of these baselines . we find further improvements by combining different <method_4> . you can download our <otherscientificterm_3> , for off-the-shelf use in existing <method_2> , as well as our <otherscientificterm_3> , here : http://metaoptimize . com/projects/wordreprs /",
    "abstract_og": "if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston -lrb- 2008 -rrb- embeddings , and hlbl -lrb- mnih & hinton , 2009 -rrb- embeddings of words on both ner and chunking . we use near state-of-the-art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off-the-shelf use in existing nlp systems , as well as our word features , here : http://metaoptimize . com/projects/wordreprs /"
  },
  {
    "title": "Using Text Reviews for Product Entity Completion .",
    "entities": [
      "real life data collection",
      "enterprise internal product descriptions",
      "external text data sources",
      "product data quality solutions",
      "domain specific rulesets",
      "composing attributes",
      "external data",
      "missing values",
      "brand name",
      "attribute-value pairs",
      "product descriptions",
      "structured information",
      "size",
      "color"
    ],
    "types": "<material> <material> <material> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "size -- CONJUNCTION -- color",
      "enterprise internal product descriptions -- CONJUNCTION -- external data"
    ],
    "abstract": "in this paper we address the problem of obtaining <otherscientificterm_11> about products in the form of <otherscientificterm_9> by leveraging a combination of <material_1> and <material_6> . <material_10> are short text <material_10> used internally within enterprises to describe a product . these <material_10> usually comprise of the <otherscientificterm_8> , name of the product , and its attributes like <otherscientificterm_12> , <otherscientificterm_13> , etc. . existing <method_3> provide us the capability to standardize and segment these descriptions into their <otherscientificterm_5> using <otherscientificterm_4> . we provide techniques that can leverage the supervision provided by these existing rulesets for extracting <otherscientificterm_7> from other <material_2> accurately . we use a large <material_0> to demonstrate the effectiveness of our approach .",
    "abstract_og": "in this paper we address the problem of obtaining structured information about products in the form of attribute-value pairs by leveraging a combination of enterprise internal product descriptions and external data . product descriptions are short text product descriptions used internally within enterprises to describe a product . these product descriptions usually comprise of the brand name , name of the product , and its attributes like size , color , etc. . existing product data quality solutions provide us the capability to standardize and segment these descriptions into their composing attributes using domain specific rulesets . we provide techniques that can leverage the supervision provided by these existing rulesets for extracting missing values from other external text data sources accurately . we use a large real life data collection to demonstrate the effectiveness of our approach ."
  },
  {
    "title": "Ask , and Shall You Receive ? Understanding Desire Fulfillment in Natural Language Text .",
    "entities": [
      "unstructured and structured models",
      "narrative and discourse structure",
      "natural language understanding",
      "fulfillment cues"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "unstructured and structured models -- USED-FOR -- fulfillment cues"
    ],
    "abstract": "the ability to comprehend wishes or desires and their fulfillment is important to <task_2> . this paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled . we propose various <method_0> that capture <otherscientificterm_3> such as the subject 's emotional state and actions . our experiments with two different datasets demonstrate the importance of understanding the <otherscientificterm_1> to address this task .",
    "abstract_og": "the ability to comprehend wishes or desires and their fulfillment is important to natural language understanding . this paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled . we propose various unstructured and structured models that capture fulfillment cues such as the subject 's emotional state and actions . our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task ."
  },
  {
    "title": "Mel , linear , and antimel frequency cepstral coefficients in broad phonetic regions for telephone speaker recognition .",
    "entities": [
      "nasal and non-nasal consonant regions",
      "speaker discriminative power of mel",
      "physiological characteristics of speakers",
      "filterbank energy f-ratio analysis",
      "speaker discriminative power",
      "nasal , vowel",
      "vowel region",
      "cepstral coefficients",
      "filterbank energies",
      "telephone bandwidth",
      "non-telephone speech",
      "telephone speech",
      "a-mfccs",
      "speech"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material> <material> <otherscientificterm> <material>",
    "relations": [
      "a-mfccs -- COMPARE -- a-mfccs",
      "telephone speech -- USED-FOR -- a-mfccs"
    ],
    "abstract": "we 've examined the <method_1> - , antimel-and linear-frequency <otherscientificterm_7> -lrb- <otherscientificterm_12> , <otherscientificterm_12> and <otherscientificterm_12> -rrb- in the <otherscientificterm_5> , and non-nasal consonant <material_13> regions . our inspiration came from the work of lu and dang in 2007 , who showed that <material_8> at some frequencies mainly outside the <otherscientificterm_9> possess more <otherscientificterm_4> due to <otherscientificterm_2> , and derived a set of <otherscientificterm_7> that outperformed <otherscientificterm_12> in <material_10> . using <material_11> , we 've discovered that <otherscientificterm_12> gave 21.5 % and 15.0 % relative eer improvements over <otherscientificterm_12> in <otherscientificterm_0> , agreeing with our <method_3> . we 've also found that using only the <otherscientificterm_6> with <otherscientificterm_12> gives a 9.1 % relative improvement over using all <material_13> . last , we 've shown that <otherscientificterm_12> are valuable in combination , contributing to a system with 17.3 % relative improvement over our baseline .",
    "abstract_og": "we 've examined the speaker discriminative power of mel - , antimel-and linear-frequency cepstral coefficients -lrb- a-mfccs , a-mfccs and a-mfccs -rrb- in the nasal , vowel , and non-nasal consonant speech regions . our inspiration came from the work of lu and dang in 2007 , who showed that filterbank energies at some frequencies mainly outside the telephone bandwidth possess more speaker discriminative power due to physiological characteristics of speakers , and derived a set of cepstral coefficients that outperformed a-mfccs in non-telephone speech . using telephone speech , we 've discovered that a-mfccs gave 21.5 % and 15.0 % relative eer improvements over a-mfccs in nasal and non-nasal consonant regions , agreeing with our filterbank energy f-ratio analysis . we 've also found that using only the vowel region with a-mfccs gives a 9.1 % relative improvement over using all speech . last , we 've shown that a-mfccs are valuable in combination , contributing to a system with 17.3 % relative improvement over our baseline ."
  },
  {
    "title": "Performance costs for theoretical minimal-length equalizers .",
    "entities": [
      "impulse response duration",
      "governing design rules",
      "equalizer implementa-tional complexity",
      "linear equalizer filter",
      "complexity reduction",
      "minimal-length equalizer",
      "linear equalizer",
      "composite distortion",
      "intersymbol interference",
      "complexity"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <metric> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "complexity -- EVALUATE-FOR -- linear equalizer filter",
      "linear equalizer -- USED-FOR -- intersymbol interference"
    ],
    "abstract": "the length and <metric_9> of a <method_3> is highly dependent on the nature of the channel effects it must mitigate . the <otherscientificterm_1> are typically stated in terms of the channel 's temporal characteristics , i.e. <otherscientificterm_0> . <metric_2> is a principal limiting factor for high bandwidth data communication applications , and consequently there is motivation for reexamining accepted design guidelines . recently , it was demonstrated in -lsb- 1 -rsb- that for relatively benign conditions on the effective channel and transmitter pulse shaping , there exists a <otherscientificterm_6> that perfectly mitigates <otherscientificterm_8> , and whose span matches that of the <otherscientificterm_7> . our paper examines the implications of the <otherscientificterm_5> in light of accepted design rules , and shows that a tangible loss in performance can be assigned to this <metric_4> . actual line-of-sight microwave radio channels are used to demonstrate the nature of the performance loss .",
    "abstract_og": "the length and complexity of a linear equalizer filter is highly dependent on the nature of the channel effects it must mitigate . the governing design rules are typically stated in terms of the channel 's temporal characteristics , i.e. impulse response duration . equalizer implementa-tional complexity is a principal limiting factor for high bandwidth data communication applications , and consequently there is motivation for reexamining accepted design guidelines . recently , it was demonstrated in -lsb- 1 -rsb- that for relatively benign conditions on the effective channel and transmitter pulse shaping , there exists a linear equalizer that perfectly mitigates intersymbol interference , and whose span matches that of the composite distortion . our paper examines the implications of the minimal-length equalizer in light of accepted design rules , and shows that a tangible loss in performance can be assigned to this complexity reduction . actual line-of-sight microwave radio channels are used to demonstrate the nature of the performance loss ."
  },
  {
    "title": "Polarization Multiplexing for Bidirectional Imaging .",
    "entities": [
      "diffuse and specular reflectance components",
      "multiple unknown light sources",
      "surface reflectance contributions",
      "light source direction",
      "overall surface reflectance",
      "polarization multiplexing",
      "appearance-based modeling",
      "intensity modulation",
      "phase his-tograms",
      "appearance modeling",
      "illumination direction",
      "bidirectional imaging",
      "intensity modulations",
      "scene properties",
      "polarization"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "intensity modulations -- USED-FOR -- phase his-tograms",
      "polarization -- USED-FOR -- diffuse and specular reflectance components",
      "light source direction -- CONJUNCTION -- intensity modulation",
      "polarization -- USED-FOR -- bidirectional imaging",
      "polarization -- USED-FOR -- surface reflectance contributions",
      "polarization -- PART-OF -- appearance-based modeling",
      "phase his-tograms -- USED-FOR -- scene properties",
      "appearance modeling -- CONJUNCTION -- bidirectional imaging"
    ],
    "abstract": "our goal is to incorporate <otherscientificterm_14> in <task_6> in an efficient and meaningful way . <otherscientificterm_14> has been used in numerous prior studies for separating <otherscientificterm_0> , but in this work we show that <otherscientificterm_14> also can be used to separate <otherscientificterm_2> from individual light sources . our approach is called <method_5> and <otherscientificterm_14> has significant impact in <task_9> and <task_11> where the image as a function of <otherscientificterm_10> is needed . <material_1> can illuminate the scene simultaneously , and the individual contributions to the <otherscientificterm_4> can be estimated . to develop the method of <method_5> , we use a relationship between <otherscientificterm_3> and <otherscientificterm_7> . inverting this transformation enables the individual intensity contributions to be estimated . in addition to <method_5> , we show that <otherscientificterm_8> from the <otherscientificterm_12> can be used to estimate <otherscientificterm_13> including the number of light sources .",
    "abstract_og": "our goal is to incorporate polarization in appearance-based modeling in an efficient and meaningful way . polarization has been used in numerous prior studies for separating diffuse and specular reflectance components , but in this work we show that polarization also can be used to separate surface reflectance contributions from individual light sources . our approach is called polarization multiplexing and polarization has significant impact in appearance modeling and bidirectional imaging where the image as a function of illumination direction is needed . multiple unknown light sources can illuminate the scene simultaneously , and the individual contributions to the overall surface reflectance can be estimated . to develop the method of polarization multiplexing , we use a relationship between light source direction and intensity modulation . inverting this transformation enables the individual intensity contributions to be estimated . in addition to polarization multiplexing , we show that phase his-tograms from the intensity modulations can be used to estimate scene properties including the number of light sources ."
  },
  {
    "title": "Regularization of Neural Networks using DropConnect .",
    "entities": [
      "regular-izing large fully-connected layers",
      "random subset of units",
      "image recognition benchmarks",
      "dropconnect-trained models",
      "neu-ral networks",
      "dropconnect",
      "dropout"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <metric> <method> <method> <method> <method>",
    "relations": [
      "dropconnect-trained models -- USED-FOR -- image recognition benchmarks",
      "dropout -- CONJUNCTION -- dropconnect"
    ],
    "abstract": "we introduce <method_5> , a generalization of <method_6> -lrb- hinton et al. , 2012 -rrb- , for <otherscientificterm_0> within <method_4> . when training with <method_6> , a randomly selected subset of activations are set to zero within each layer . <method_5> instead sets a randomly selected subset of weights within the network to zero . each unit thus receives input from a <otherscientificterm_1> in the previous layer . we derive a bound on the generalization performance of both <method_6> and <method_5> . we then evaluate <method_5> on a range of datasets , comparing to <method_6> , and show state-of-the-art results on several <metric_2> by aggregating multiple <method_3> .",
    "abstract_og": "we introduce dropconnect , a generalization of dropout -lrb- hinton et al. , 2012 -rrb- , for regular-izing large fully-connected layers within neu-ral networks . when training with dropout , a randomly selected subset of activations are set to zero within each layer . dropconnect instead sets a randomly selected subset of weights within the network to zero . each unit thus receives input from a random subset of units in the previous layer . we derive a bound on the generalization performance of both dropout and dropconnect . we then evaluate dropconnect on a range of datasets , comparing to dropout , and show state-of-the-art results on several image recognition benchmarks by aggregating multiple dropconnect-trained models ."
  },
  {
    "title": "A Universal Catalyst for First-Order Optimization .",
    "entities": [
      "accelerated prox-imal point algorithm",
      "first-order optimization methods",
      "non-strongly convex objectives",
      "well-chosen auxiliary problems",
      "block coordinate descent",
      "convex objective",
      "proximal variants",
      "gradient descent",
      "ill-conditioned problems",
      "theoretical speed-up",
      "nesterov",
      "saga",
      "sag",
      "finito/miso",
      "svrg",
      "acceleration",
      "sdca"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "sag -- CONJUNCTION -- saga",
      "block coordinate descent -- CONJUNCTION -- sag",
      "gradient descent -- CONJUNCTION -- sag",
      "block coordinate descent -- CONJUNCTION -- saga",
      "sdca -- CONJUNCTION -- finito/miso",
      "saga -- CONJUNCTION -- sdca",
      "svrg -- CONJUNCTION -- proximal variants",
      "saga -- CONJUNCTION -- svrg",
      "sdca -- CONJUNCTION -- svrg",
      "svrg -- CONJUNCTION -- finito/miso",
      "acceleration -- USED-FOR -- ill-conditioned problems",
      "gradient descent -- CONJUNCTION -- block coordinate descent",
      "sag -- CONJUNCTION -- sdca",
      "finito/miso -- CONJUNCTION -- proximal variants"
    ],
    "abstract": "we introduce a generic scheme for accelerating <method_1> in the sense of <method_10> , which builds upon a new analysis of the <method_0> . our approach consists of minimizing a <otherscientificterm_5> by approximately solving a sequence of <otherscientificterm_3> , leading to faster convergence . this strategy applies to a large class of algorithms , including <otherscientificterm_7> , <otherscientificterm_4> , <otherscientificterm_12> , <otherscientificterm_11> , <otherscientificterm_16> , <otherscientificterm_14> , <otherscientificterm_13> , and their <otherscientificterm_6> . for all of these methods , we provide <task_15> and explicit support for <otherscientificterm_2> . in addition to <otherscientificterm_9> , we also show that <task_15> is useful in practice , especially for <task_8> where we measure significant improvements .",
    "abstract_og": "we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated prox-imal point algorithm . our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence . this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants . for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives . in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements ."
  },
  {
    "title": "On simulation of first-order auto-regressive processes with near Laplace marginals .",
    "entities": [
      "stationary non-gaussian auto-regressive processes",
      "monte carlo rejection algorithms",
      "statistical signal processing",
      "time series model",
      "near-laplace marginal distributions",
      "complexity"
    ],
    "types": "<method> <method> <task> <method> <otherscientificterm> <metric>",
    "relations": [
      "stationary non-gaussian auto-regressive processes -- USED-FOR -- statistical signal processing"
    ],
    "abstract": "the focus of this paper is the modeling of a class of <method_0> that often find applications in <task_2> . we propose a general simulation procedure for constructing a <method_3> with a <otherscientificterm_4> . our approach is based on a class of <method_1> . a theoretical analysis of the average <metric_5> of the proposed algorithms for simulating the <method_3> is included .",
    "abstract_og": "the focus of this paper is the modeling of a class of stationary non-gaussian auto-regressive processes that often find applications in statistical signal processing . we propose a general simulation procedure for constructing a time series model with a near-laplace marginal distributions . our approach is based on a class of monte carlo rejection algorithms . a theoretical analysis of the average complexity of the proposed algorithms for simulating the time series model is included ."
  },
  {
    "title": "Collapsed Variational Inference for HDP .",
    "entities": [
      "collapsed variational latent dirichlet allocation",
      "dirichlet-multinomial ` topic ' models",
      "hyperparameters of dirichlet variables",
      "hierarchical dirichlet process",
      "model selection",
      "variational technique",
      "variational techniques",
      "variational algorithm",
      "marginal likelihood",
      "gibbs sampling",
      "inference",
      "hyperparameters",
      "accuracy"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <method> <method> <method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <metric>",
    "relations": [
      "variational technique -- USED-FOR -- hyperparameters",
      "collapsed variational latent dirichlet allocation -- HYPONYM-OF -- variational technique",
      "inference -- USED-FOR -- hyperparameters",
      "variational algorithm -- USED-FOR -- hierarchical dirichlet process"
    ],
    "abstract": "a wide variety of <method_1> have found interesting applications in recent years . while <method_9> remains an important method of <task_10> in such <method_1> , <method_6> have certain advantages such as easy assessment of convergence , easy optimization without the need to maintain detailed balance , a bound on the <otherscientificterm_8> , and side-stepping of issues with topic-identifiability . the most accurate <method_5> thus far , namely <otherscientificterm_0> , did not deal with <method_4> nor did <method_5> include <task_10> for <otherscientificterm_11> . we address both issues by generalizing the technique , obtaining the first <method_7> to deal with the <method_3> and to deal with <method_2> . experiments show a significant improvement in <metric_12> .",
    "abstract_og": "a wide variety of dirichlet-multinomial ` topic ' models have found interesting applications in recent years . while gibbs sampling remains an important method of inference in such dirichlet-multinomial ` topic ' models , variational techniques have certain advantages such as easy assessment of convergence , easy optimization without the need to maintain detailed balance , a bound on the marginal likelihood , and side-stepping of issues with topic-identifiability . the most accurate variational technique thus far , namely collapsed variational latent dirichlet allocation , did not deal with model selection nor did variational technique include inference for hyperparameters . we address both issues by generalizing the technique , obtaining the first variational algorithm to deal with the hierarchical dirichlet process and to deal with hyperparameters of dirichlet variables . experiments show a significant improvement in accuracy ."
  },
  {
    "title": "Organizational Issues Arising from the Integration of the Lexicon and Concept Network in a Text Understanding System .",
    "entities": [
      "lexical and encyclopaedic information",
      "knowledge bases arc",
      "text understanding system",
      "knowledge based system",
      "knowledge acquisition process",
      "text understanding",
      "conceptual information",
      "parsing process",
      "blackboard architecture",
      "target representation",
      "encyclopaedic information",
      "twig"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <task> <task> <otherscientificterm> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "lexical and encyclopaedic information -- USED-FOR -- knowledge based system",
      "knowledge based system -- USED-FOR -- text understanding",
      "conceptual information -- USED-FOR -- parsing process",
      "twig -- HYPONYM-OF -- text understanding system"
    ],
    "abstract": "a <method_3> for <task_5> will incorporate both <otherscientificterm_0> . the <otherscientificterm_0> is the basis of the <method_7> while the <otherscientificterm_10> forms the <method_9> and is used in the <task_4> . this paper describes <method_11> , a <method_2> where these two <otherscientificterm_1> integrated into one representation . there is some theoretical justification for this and <method_3> has the advantage of reducing duplication of information in the <method_3> . this integration also has the advantage of making <otherscientificterm_6> available during the <method_7> . most of all this integration of diverse information forms a natural basis for a <method_8> .",
    "abstract_og": "a knowledge based system for text understanding will incorporate both lexical and encyclopaedic information . the lexical and encyclopaedic information is the basis of the parsing process while the encyclopaedic information forms the target representation and is used in the knowledge acquisition process . this paper describes twig , a text understanding system where these two knowledge bases arc integrated into one representation . there is some theoretical justification for this and knowledge based system has the advantage of reducing duplication of information in the knowledge based system . this integration also has the advantage of making conceptual information available during the parsing process . most of all this integration of diverse information forms a natural basis for a blackboard architecture ."
  },
  {
    "title": "Global data association for multi-object tracking using network flows .",
    "entities": [
      "maximum-a-posteriori data association problem",
      "explicit occlusion model",
      "network flow based optimization method",
      "long-term inter-object occlu-sions",
      "termination of trajectories",
      "public pedestrian datasets",
      "min-cost flow algorithm",
      "multiple object tracking",
      "data association",
      "cost-flow network",
      "hypotheses pruning",
      "iterative approach",
      "non-overlap constraint"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <material> <method> <task> <task> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "iterative approach -- USED-FOR -- cost-flow network",
      "network flow based optimization method -- USED-FOR -- multiple object tracking",
      "min-cost flow algorithm -- USED-FOR -- data association",
      "min-cost flow algorithm -- COMPARE -- iterative approach",
      "network flow based optimization method -- USED-FOR -- data association",
      "data association -- USED-FOR -- multiple object tracking"
    ],
    "abstract": "we propose a <method_2> for <task_8> needed for <task_7> . the <task_0> is mapped into a <method_9> with a <otherscientificterm_12> on trajectories . the optimal <task_8> is found by a <method_6> in the <method_9> . the <method_9> is augmented to include an <method_1> to track with <otherscientificterm_3> . a solution to the <method_9> is found by an <method_11> built upon the original <method_6> . initialization and <otherscientificterm_4> and potential false observations are modeled by the formulation intrinsi-cally . the method is efficient and does not require <method_10> . performance is compared with previous results on two <material_5> to show its improvement .",
    "abstract_og": "we propose a network flow based optimization method for data association needed for multiple object tracking . the maximum-a-posteriori data association problem is mapped into a cost-flow network with a non-overlap constraint on trajectories . the optimal data association is found by a min-cost flow algorithm in the cost-flow network . the cost-flow network is augmented to include an explicit occlusion model to track with long-term inter-object occlu-sions . a solution to the cost-flow network is found by an iterative approach built upon the original min-cost flow algorithm . initialization and termination of trajectories and potential false observations are modeled by the formulation intrinsi-cally . the method is efficient and does not require hypotheses pruning . performance is compared with previous results on two public pedestrian datasets to show its improvement ."
  },
  {
    "title": "Pronunciation variation in ASR : which variation to model ? .",
    "entities": [
      "within-word and crossword pronunciation variation",
      "continuous speech recognizer",
      "modeling pronunciation variation",
      "error rates",
      "error analysis",
      "wer",
      "recognition",
      "dutch"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <metric> <method> <metric> <task> <material>",
    "relations": [
      "wer -- USED-FOR -- modeling pronunciation variation"
    ],
    "abstract": "this paper describes how the performance of a <method_1> for <material_7> has been improved by modeling <otherscientificterm_0> . a relative improvement of 8.8 % in <metric_5> was found compared to baseline system performance . however , as <metric_5> do not reveal the full effect of <otherscientificterm_2> , we performed a detailed analysis of the differences in <task_6> results that occur due to <otherscientificterm_2> and found that indeed a lot of the differences in <task_6> results are not reflected in the <metric_3> . furthermore , <method_4> revealed that testing sets of variants in isolation does not predict their behavior in combination . however , these results appeared to be corpus dependent .",
    "abstract_og": "this paper describes how the performance of a continuous speech recognizer for dutch has been improved by modeling within-word and crossword pronunciation variation . a relative improvement of 8.8 % in wer was found compared to baseline system performance . however , as wer do not reveal the full effect of modeling pronunciation variation , we performed a detailed analysis of the differences in recognition results that occur due to modeling pronunciation variation and found that indeed a lot of the differences in recognition results are not reflected in the error rates . furthermore , error analysis revealed that testing sets of variants in isolation does not predict their behavior in combination . however , these results appeared to be corpus dependent ."
  },
  {
    "title": "Hamming Distance Metric Learning .",
    "entities": [
      "loss-augmented inference algorithm",
      "sub-linear knn search",
      "triplet ranking loss",
      "latent structural svms",
      "high-dimensional data",
      "semantic similarity",
      "code length",
      "binary codes",
      "binary codes",
      "discrete mappings",
      "empirical loss",
      "large-scale applications",
      "cifar-10",
      "mnist",
      "quadratic",
      "mappings",
      "knn",
      "retrieval"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "high-dimensional data -- USED-FOR -- mappings",
      "binary codes -- USED-FOR -- sub-linear knn search",
      "binary codes -- USED-FOR -- large-scale applications",
      "cifar-10 -- CONJUNCTION -- mnist",
      "high-dimensional data -- USED-FOR -- binary codes",
      "latent structural svms -- USED-FOR -- empirical loss",
      "binary codes -- USED-FOR -- semantic similarity"
    ],
    "abstract": "motivated by large-scale multimedia applications we propose to learn <method_15> from <material_4> to <otherscientificterm_8> that preserve <otherscientificterm_5> . <otherscientificterm_7> are well suited to <task_11> as <otherscientificterm_7> are storage efficient and permit exact <task_1> . the framework is applicable to broad families of <method_15> , and uses a flexible form of <otherscientificterm_2> . we overcome discontinuous optimization of the <method_9> by minimizing a piecewise-smooth upper bound on <otherscientificterm_10> , inspired by <method_3> . we develop a new <method_0> that is <otherscientificterm_14> in the <otherscientificterm_6> . we show strong <task_17> performance on <method_12> and <method_13> , with promising classification results using no more than <otherscientificterm_16> on the <otherscientificterm_8> .",
    "abstract_og": "motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity . binary codes are well suited to large-scale applications as binary codes are storage efficient and permit exact sub-linear knn search . the framework is applicable to broad families of mappings , and uses a flexible form of triplet ranking loss . we overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss , inspired by latent structural svms . we develop a new loss-augmented inference algorithm that is quadratic in the code length . we show strong retrieval performance on cifar-10 and mnist , with promising classification results using no more than knn on the binary codes ."
  },
  {
    "title": "Speech Adaptation in Extended Ambient Intelligence Environments .",
    "entities": [
      "extended ambient intelligence",
      "mass-produced , one-size-fits-all software",
      "person 's preferences",
      "cognitive capability",
      "mobile devices",
      "personalized agents",
      "divergence detection",
      "speech patterns",
      "adaptive approach",
      "speech processing",
      "ambient intelligence",
      "conversational engagement"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <material> <method> <task> <material> <method> <method> <task> <otherscientificterm>",
    "relations": [
      "divergence detection -- USED-FOR -- speech patterns",
      "speech processing -- CONJUNCTION -- cognitive capability"
    ],
    "abstract": "this blue sky presentation focuses on a major shift toward a notion of '' <task_10> '' that transcends general applications targeted at the general population . the focus is on highly <method_5> that accommodate individual differences and changes over time . this notion of <task_0> concerns adaptation to a <otherscientificterm_2> and experiences , as well as changing capabilities , most notably in an environment where <otherscientificterm_11> is central . an important step in moving this research forward is the accommodation of different degrees of <otherscientificterm_3> -lrb- including <method_9> -rrb- that may vary over time for a given user -- whether through improvement or through deterioration . we suggest that the application of <task_6> to <material_7> may enable adaptation to a speaker 's increasing or decreasing level of speech impairment over time . taking an <method_8> toward technology development in this arena may be a first step toward empowering those with special needs so that they may live with a high quality of life . it also represents an important step toward a notion of <task_10> that is personalized beyond what can be achieved by <method_1> currently in use on <material_4> .",
    "abstract_og": "this blue sky presentation focuses on a major shift toward a notion of '' ambient intelligence '' that transcends general applications targeted at the general population . the focus is on highly personalized agents that accommodate individual differences and changes over time . this notion of extended ambient intelligence concerns adaptation to a person 's preferences and experiences , as well as changing capabilities , most notably in an environment where conversational engagement is central . an important step in moving this research forward is the accommodation of different degrees of cognitive capability -lrb- including speech processing -rrb- that may vary over time for a given user -- whether through improvement or through deterioration . we suggest that the application of divergence detection to speech patterns may enable adaptation to a speaker 's increasing or decreasing level of speech impairment over time . taking an adaptive approach toward technology development in this arena may be a first step toward empowering those with special needs so that they may live with a high quality of life . it also represents an important step toward a notion of ambient intelligence that is personalized beyond what can be achieved by mass-produced , one-size-fits-all software currently in use on mobile devices ."
  },
  {
    "title": "Bag-of-Audio-Words Approach for Multimedia Event Classification .",
    "entities": [
      "bag-of-audio words method",
      "text and image document retrieval",
      "audio document classification",
      "online video search",
      "acoustic event detection",
      "multimedia event classification",
      "online multimedia videos",
      "audio concept detectors",
      "multimedia videos",
      "unsupervised fashion",
      "audio component",
      "audio concepts",
      "supervised approaches",
      "low-level features",
      "annotated data"
    ],
    "types": "<method> <task> <task> <task> <task> <task> <material> <method> <material> <method> <method> <otherscientificterm> <method> <otherscientificterm> <material>",
    "relations": [
      "low-level features -- USED-FOR -- multimedia event classification",
      "annotated data -- USED-FOR -- supervised approaches",
      "multimedia videos -- FEATURE-OF -- audio concepts",
      "audio component -- USED-FOR -- multimedia event classification",
      "audio concept detectors -- USED-FOR -- multimedia event classification",
      "bag-of-audio words method -- USED-FOR -- audio concepts",
      "annotated data -- USED-FOR -- low-level features",
      "audio document classification -- USED-FOR -- text and image document retrieval",
      "annotated data -- USED-FOR -- audio concept detectors"
    ],
    "abstract": "with the popularity of <material_6> , there has been much interest in recent years in <task_4> and classification for the improvement of <task_3> . the <method_10> of a video has the potential to contribute significantly to <task_5> . recent research in <task_2> has drawn parallels to <task_1> by employing what is referred to as the <method_0> . compared to <method_12> where <method_7> are trained using <material_14> and extracted labels are used as <otherscientificterm_13> for <task_5> . the <method_0> extracts <otherscientificterm_11> in an <method_9> . hence this <method_0> has the advantage that <method_0> can be employed easily for a new set of <otherscientificterm_11> in <material_8> without going through a laborious annotation effort . in this paper , we explore variations of the <method_0> and present results on nist 2011 multimedia event detection -lrb- med -rrb- dataset .",
    "abstract_og": "with the popularity of online multimedia videos , there has been much interest in recent years in acoustic event detection and classification for the improvement of online video search . the audio component of a video has the potential to contribute significantly to multimedia event classification . recent research in audio document classification has drawn parallels to text and image document retrieval by employing what is referred to as the bag-of-audio words method . compared to supervised approaches where audio concept detectors are trained using annotated data and extracted labels are used as low-level features for multimedia event classification . the bag-of-audio words method extracts audio concepts in an unsupervised fashion . hence this bag-of-audio words method has the advantage that bag-of-audio words method can be employed easily for a new set of audio concepts in multimedia videos without going through a laborious annotation effort . in this paper , we explore variations of the bag-of-audio words method and present results on nist 2011 multimedia event detection -lrb- med -rrb- dataset ."
  },
  {
    "title": "Performance analysis of a recursive fractional super-exponential algorithm .",
    "entities": [
      "linear equalization of moderately distortive channels",
      "fractionally-sampled pam signals",
      "blind channel equalization",
      "speciic channel responses",
      "fast convergence rate",
      "convergence rate",
      "recursive propagation",
      "block-based technique",
      "pri-ori parameterization",
      "block length",
      "super-exponential algorithm",
      "sampling errors",
      "oversampling"
    ],
    "types": "<task> <material> <task> <otherscientificterm> <metric> <metric> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "speciic channel responses -- USED-FOR -- convergence rate",
      "super-exponential algorithm -- HYPONYM-OF -- block-based technique",
      "super-exponential algorithm -- USED-FOR -- blind channel equalization",
      "block-based technique -- USED-FOR -- blind channel equalization",
      "super-exponential algorithm -- USED-FOR -- fractionally-sampled pam signals"
    ],
    "abstract": "the <method_10> is a <method_7> for <task_2> and system identii-cation . due to its <metric_4> , and no a <method_8> other than the <otherscientificterm_9> , it is a useful tool for <task_0> . this paper presents a recursive implementation of the <method_10> for <material_1> . although the resulting <method_10> is still block-based , <method_6> of several key variables allows the <otherscientificterm_9> to be signiicantly reduced without compromising the <method_10> 's accuracy or speed , thereby enhancing its ability t o t r a c k c hannel variations . the <metric_5> is only mildly innuenced by <otherscientificterm_3> , and <method_12> provides smaller output variance and almost perfect tolerance to <otherscientificterm_11> . simulation results demonstrate the eeectiveness of the proposed <method_10> .",
    "abstract_og": "the super-exponential algorithm is a block-based technique for blind channel equalization and system identii-cation . due to its fast convergence rate , and no a pri-ori parameterization other than the block length , it is a useful tool for linear equalization of moderately distortive channels . this paper presents a recursive implementation of the super-exponential algorithm for fractionally-sampled pam signals . although the resulting super-exponential algorithm is still block-based , recursive propagation of several key variables allows the block length to be signiicantly reduced without compromising the super-exponential algorithm 's accuracy or speed , thereby enhancing its ability t o t r a c k c hannel variations . the convergence rate is only mildly innuenced by speciic channel responses , and oversampling provides smaller output variance and almost perfect tolerance to sampling errors . simulation results demonstrate the eeectiveness of the proposed super-exponential algorithm ."
  },
  {
    "title": "Improving Regressors using Boosting Techniques .",
    "entities": [
      "bagging committee machines",
      "boston housing database",
      "boosting committee machines",
      "committee of regressors",
      "non-linear functions",
      "regression trees",
      "regression context",
      "prediction error",
      "boosting",
      "bagging"
    ],
    "types": "<task> <material> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "bagging committee machines -- CONJUNCTION -- boosting committee machines",
      "boosting -- CONJUNCTION -- bagging",
      "bagging -- USED-FOR -- committee of regressors",
      "boosting -- COMPARE -- bagging"
    ],
    "abstract": "in the <otherscientificterm_6> , <method_8> and <method_9> are techniques to build a <method_3> that may be superior to a single regressor . we use <method_5> as fundamental building blocks in <task_0> and <method_2> . performance is analyzed on three <otherscientificterm_4> and the <material_1> . in all cases , <method_8> is at least equivalent , and in most cases better than <method_9> in terms of <otherscientificterm_7> .",
    "abstract_og": "in the regression context , boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor . we use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines . performance is analyzed on three non-linear functions and the boston housing database . in all cases , boosting is at least equivalent , and in most cases better than bagging in terms of prediction error ."
  },
  {
    "title": "A Global Linear Method for Camera Pose Registration .",
    "entities": [
      "triangular relationship in camera triplets",
      "pairwise translation direction constraints",
      "global camera pose registration",
      "final bundle adjustment",
      "approximate geometric error",
      "pairwise relative poses",
      "collinear motion",
      "linear method",
      "system degeneracy",
      "point triangulation",
      "linear approximation",
      "trifocal tensor",
      "linear methods",
      "algebraic error",
      "robustness",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <metric>",
    "relations": [
      "pairwise relative poses -- USED-FOR -- linear method",
      "pairwise translation direction constraints -- USED-FOR -- linear methods",
      "accuracy -- EVALUATE-FOR -- linear method",
      "robustness -- EVALUATE-FOR -- linear method",
      "collinear motion -- USED-FOR -- system degeneracy",
      "linear method -- USED-FOR -- global camera pose registration",
      "algebraic error -- HYPONYM-OF -- pairwise translation direction constraints"
    ],
    "abstract": "we present a <method_7> for <task_2> from <otherscientificterm_5> encoded in essential matrices . our <method_7> minimizes an <otherscientificterm_4> to enforce the <otherscientificterm_0> . this <method_7> does not suffer from the typical ` unbalanced scale ' problem in <method_12> relying on <otherscientificterm_1> , i.e. an <otherscientificterm_13> ; nor the <method_8> from <otherscientificterm_6> . in the case of three cameras , our <method_7> provides a good <method_10> of the <otherscientificterm_11> . <method_7> can be directly scaled up to register multiple cameras . the results obtained are accurate for <task_9> and can serve as a good initialization for <task_3> . we evaluate the <method_7> performance with different types of data and demonstrate its effectiveness . our <method_7> produces good <metric_15> , <metric_14> , and outperforms some well-known systems on efficiency .",
    "abstract_og": "we present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices . our linear method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets . this linear method does not suffer from the typical ` unbalanced scale ' problem in linear methods relying on pairwise translation direction constraints , i.e. an algebraic error ; nor the system degeneracy from collinear motion . in the case of three cameras , our linear method provides a good linear approximation of the trifocal tensor . linear method can be directly scaled up to register multiple cameras . the results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment . we evaluate the linear method performance with different types of data and demonstrate its effectiveness . our linear method produces good accuracy , robustness , and outperforms some well-known systems on efficiency ."
  },
  {
    "title": "BFGUI : An interactive tool for the synthesis and analysis of microphone array beamformers .",
    "entities": [
      "multiple analytic microphone models",
      "microphone array beamformer",
      "number and geometry",
      "distant speech capture",
      "interactive graphical tool",
      "simulating microphone arrays",
      "performance metrics",
      "direc-tivity pattern",
      "design constraints",
      "microphone arrays",
      "derived metrics",
      "microphone types",
      "real-world data",
      "front-back ratio",
      "synthesizing beamformers",
      "directivity index",
      "bfgui",
      "reverberation",
      "regulariza-tion",
      "noise",
      "beamforming"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <task> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <metric> <method> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "bfgui -- USED-FOR -- synthesizing beamformers",
      "direc-tivity pattern -- CONJUNCTION -- directivity index",
      "directivity index -- CONJUNCTION -- front-back ratio",
      "interactive graphical tool -- USED-FOR -- synthesizing beamformers",
      "noise -- CONJUNCTION -- reverberation",
      "bfgui -- HYPONYM-OF -- interactive graphical tool",
      "microphone types -- CONJUNCTION -- number and geometry",
      "regulariza-tion -- HYPONYM-OF -- design constraints",
      "microphone arrays -- USED-FOR -- distant speech capture",
      "beamforming -- USED-FOR -- noise"
    ],
    "abstract": "microphone arrays are beneficial for <otherscientificterm_3> because the signals they capture can be exploited with <method_20> to suppress <otherscientificterm_19> and <otherscientificterm_17> . the theory for the design and analysis of microphone arrays is well established , however the performance of a <method_1> is often subject to conflicting criteria that need to be assessed manually . this paper describes <method_16> , a <method_4> for <method_16> , for <task_5> and <method_14> , and whose parameters can be modified and <metric_6> monitored in real-time . primarily aimed at teaching and research , this <method_16> provides the user with an intuitive insight into the effects of <otherscientificterm_11> , <otherscientificterm_2> , and the influence of <otherscientificterm_8> such as <otherscientificterm_18> and white <otherscientificterm_19> gain on <otherscientificterm_10> . the resulting <otherscientificterm_7> , <metric_15> and <metric_13> are examples of such metrics . <method_0> are supported and external measured microphone directivity patterns can also be loaded . the <method_0> can be then exported in a variety of formats for processing of <material_12> .",
    "abstract_og": "microphone arrays are beneficial for distant speech capture because the signals they capture can be exploited with beamforming to suppress noise and reverberation . the theory for the design and analysis of microphone arrays is well established , however the performance of a microphone array beamformer is often subject to conflicting criteria that need to be assessed manually . this paper describes bfgui , a interactive graphical tool for bfgui , for simulating microphone arrays and synthesizing beamformers , and whose parameters can be modified and performance metrics monitored in real-time . primarily aimed at teaching and research , this bfgui provides the user with an intuitive insight into the effects of microphone types , number and geometry , and the influence of design constraints such as regulariza-tion and white noise gain on derived metrics . the resulting direc-tivity pattern , directivity index and front-back ratio are examples of such metrics . multiple analytic microphone models are supported and external measured microphone directivity patterns can also be loaded . the multiple analytic microphone models can be then exported in a variety of formats for processing of real-world data ."
  },
  {
    "title": "Missing feature theory applied to robust speech recognition over IP network .",
    "entities": [
      "lost regions of speech data",
      "mean burst loss length",
      "reconstruction of missing frames",
      "mobile and ip networks",
      "gilbert loss models",
      "packet loss models",
      "speech data loss",
      "packet loss environment",
      "packet loss rate",
      "word accuracy",
      "speech recognition",
      "packet loss",
      "marginal distributions",
      "missing-feature-based approaches",
      "tacking method"
    ],
    "types": "<material> <metric> <task> <task> <method> <method> <otherscientificterm> <otherscientificterm> <metric> <metric> <task> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "missing-feature-based approaches -- USED-FOR -- lost regions of speech data",
      "reconstruction of missing frames -- USED-FOR -- missing-feature-based approaches",
      "packet loss models -- USED-FOR -- missing-feature-based approaches",
      "mobile and ip networks -- USED-FOR -- speech recognition",
      "marginal distributions -- USED-FOR -- missing-feature-based approaches",
      "missing-feature-based approaches -- USED-FOR -- packet loss environment"
    ],
    "abstract": "this paper addresses the problems involved in performing <task_10> over <task_3> . the main problem is <otherscientificterm_6> caused by <otherscientificterm_11> in the network . we present two <method_13> that recover <material_0> . these <method_13> are based on <task_2> or on <otherscientificterm_12> . for comparison , we also use a <method_14> , which recognizes only received data . we evaluate these <method_13> with <method_5> , i.e. , random loss and <method_4> . the results show that the <method_13> is most effective for a <otherscientificterm_7> ; the degradation of <metric_9> is only 5 % when the <metric_8> is 30 % and only 3 % when <metric_1> is 24 frames .",
    "abstract_og": "this paper addresses the problems involved in performing speech recognition over mobile and ip networks . the main problem is speech data loss caused by packet loss in the network . we present two missing-feature-based approaches that recover lost regions of speech data . these missing-feature-based approaches are based on reconstruction of missing frames or on marginal distributions . for comparison , we also use a tacking method , which recognizes only received data . we evaluate these missing-feature-based approaches with packet loss models , i.e. , random loss and gilbert loss models . the results show that the missing-feature-based approaches is most effective for a packet loss environment ; the degradation of word accuracy is only 5 % when the packet loss rate is 30 % and only 3 % when mean burst loss length is 24 frames ."
  },
  {
    "title": "Low-Complexity Fuzzy Video Rate Controller for Streaming .",
    "entities": [
      "video rate control algorithm",
      "low-complexity fuzzy video rate control algorithm",
      "quantization scale",
      "variable bitrate video",
      "variable bitrate benefits",
      "real-time streaming applications",
      "fuzzy controller",
      "buffer constraint",
      "constant bitrate",
      "encoded video",
      "streaming application",
      "average quality",
      "visual quality",
      "streaming constraints"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <material> <task> <metric> <metric> <otherscientificterm>",
    "relations": [
      "buffer constraint -- USED-FOR -- low-complexity fuzzy video rate control algorithm",
      "low-complexity fuzzy video rate control algorithm -- USED-FOR -- real-time streaming applications",
      "quantization scale -- USED-FOR -- encoded video",
      "fuzzy controller -- USED-FOR -- quantization scale",
      "average quality -- EVALUATE-FOR -- encoded video",
      "video rate control algorithm -- USED-FOR -- variable bitrate video"
    ],
    "abstract": "in this paper we propose a <method_1> with <otherscientificterm_7> designed for <task_5> . while in low delay video communications bit streams with <otherscientificterm_8> are required , in <task_10> more delay and variation in bitrate is acceptable . the described <method_0> provides a <otherscientificterm_3> by control of the <method_2> on picture basis . the <method_2> is mainly controlled by a <method_6> such that it minimizes the variation of <method_2> to provide <material_9> with high <metric_12> so as to utilize the <otherscientificterm_4> as much as possible . the proposed rate control algorithm -lrb- rca -rrb- has been implemented in the mpeg-4 , h. 263 and h. 264/avc standard video codecs and the experimental results show that it provides high level <metric_11> for <material_9> while it strictly obeys <otherscientificterm_13> .",
    "abstract_og": "in this paper we propose a low-complexity fuzzy video rate control algorithm with buffer constraint designed for real-time streaming applications . while in low delay video communications bit streams with constant bitrate are required , in streaming application more delay and variation in bitrate is acceptable . the described video rate control algorithm provides a variable bitrate video by control of the quantization scale on picture basis . the quantization scale is mainly controlled by a fuzzy controller such that it minimizes the variation of quantization scale to provide encoded video with high visual quality so as to utilize the variable bitrate benefits as much as possible . the proposed rate control algorithm -lrb- rca -rrb- has been implemented in the mpeg-4 , h. 263 and h. 264/avc standard video codecs and the experimental results show that it provides high level average quality for encoded video while it strictly obeys streaming constraints ."
  },
  {
    "title": "A posterior approach for microphone array based speech recognition .",
    "entities": [
      "automatic speech recognition",
      "time-domain signal processing theory",
      "microphone array speech recognition",
      "adverse acoustic conditions",
      "tandem ann-hmm system",
      "posterior phone probabilities",
      "heterogeneous channels",
      "geometric analysis",
      "multiparty meetings",
      "background noise",
      "posterior-based approach",
      "microphone arrays",
      "beamforming techniques",
      "geometric properties",
      "speech signals",
      "asr accuracy",
      "beamforming",
      "reverberation",
      "cross-talk"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <material> <metric> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "background noise -- CONJUNCTION -- reverberation",
      "time-domain signal processing theory -- USED-FOR -- beamforming techniques",
      "reverberation -- HYPONYM-OF -- adverse acoustic conditions",
      "background noise -- HYPONYM-OF -- adverse acoustic conditions",
      "posterior phone probabilities -- USED-FOR -- tandem ann-hmm system",
      "posterior-based approach -- USED-FOR -- microphone array speech recognition",
      "reverberation -- CONJUNCTION -- cross-talk",
      "beamforming -- COMPARE -- beamforming",
      "cross-talk -- HYPONYM-OF -- adverse acoustic conditions",
      "beamforming -- CONJUNCTION -- posterior-based approach",
      "posterior-based approach -- COMPARE -- beamforming",
      "speech signals -- USED-FOR -- posterior phone probabilities",
      "microphone arrays -- USED-FOR -- asr accuracy"
    ],
    "abstract": "automatic speech recognition -lrb- asr -rrb- is difficult in environments such as <otherscientificterm_8> because of <otherscientificterm_3> : <otherscientificterm_9> , <otherscientificterm_17> and <otherscientificterm_18> . <otherscientificterm_11> can increase <metric_15> dramatically in such situations . however , most existing <method_12> use <method_1> and are based on a <method_7> of the relationship between sources and microphones . this limits their application , and leads to performance degradation when the <otherscientificterm_13> are unavailable , or <otherscientificterm_6> are used . we present a new <method_10> for <task_2> . instead of enhancing <material_14> , we enhance <otherscientificterm_5> which are used in a <method_4> . significant improvements were achieved over a single channel baseline . combining <method_16> and our <method_10> is significantly better than <method_16> alone , especially in a moving speakers scenario .",
    "abstract_og": "automatic speech recognition -lrb- asr -rrb- is difficult in environments such as multiparty meetings because of adverse acoustic conditions : background noise , reverberation and cross-talk . microphone arrays can increase asr accuracy dramatically in such situations . however , most existing beamforming techniques use time-domain signal processing theory and are based on a geometric analysis of the relationship between sources and microphones . this limits their application , and leads to performance degradation when the geometric properties are unavailable , or heterogeneous channels are used . we present a new posterior-based approach for microphone array speech recognition . instead of enhancing speech signals , we enhance posterior phone probabilities which are used in a tandem ann-hmm system . significant improvements were achieved over a single channel baseline . combining beamforming and our posterior-based approach is significantly better than beamforming alone , especially in a moving speakers scenario ."
  },
  {
    "title": "Automated Dysarthria Severity Classification for Improved Objective Intelligibility Assessment of Spastic Dysarthric Speech .",
    "entities": [
      "ma-halanobis distance-based discriminant analysis classifier",
      "9-dimensional intelligibility prediction mapping",
      "automatic dysarthria severity classification",
      "voice pathology assessment",
      "intelligibility prediction tasks",
      "spastic dysarthric speech",
      "subjective intelligibility ratings",
      "two-level severity classifier",
      "disorder severity classification",
      "salient features",
      "acoustic features",
      "root-mean-square error",
      "intelligibility accuracy",
      "intelligibility prediction",
      "classification errors",
      "feature selection"
    ],
    "types": "<method> <method> <task> <task> <task> <material> <metric> <method> <task> <otherscientificterm> <otherscientificterm> <metric> <metric> <task> <otherscientificterm> <method>",
    "relations": [
      "feature selection -- USED-FOR -- salient features",
      "acoustic features -- USED-FOR -- ma-halanobis distance-based discriminant analysis classifier",
      "salient features -- USED-FOR -- disorder severity classification",
      "feature selection -- USED-FOR -- disorder severity classification",
      "disorder severity classification -- CONJUNCTION -- intelligibility prediction tasks",
      "feature selection -- USED-FOR -- intelligibility prediction tasks",
      "intelligibility prediction -- CONJUNCTION -- voice pathology assessment"
    ],
    "abstract": "in this paper , <task_2> is explored as a tool to advance objective intelli-gibility prediction of <material_5> . a <method_0> is developed based on a set of <otherscientificterm_10> formerly proposed for <task_13> and <task_3> . <method_15> is used to sift <otherscientificterm_9> for both the <task_8> and <task_4> . experimental results show that a <method_7> combined with a <method_1> can achieve 0.92 correlation and 12.52 <metric_11> with <metric_6> . the effects of <otherscientificterm_14> on <metric_12> are also explored and shown to be insignificant .",
    "abstract_og": "in this paper , automatic dysarthria severity classification is explored as a tool to advance objective intelli-gibility prediction of spastic dysarthric speech . a ma-halanobis distance-based discriminant analysis classifier is developed based on a set of acoustic features formerly proposed for intelligibility prediction and voice pathology assessment . feature selection is used to sift salient features for both the disorder severity classification and intelligibility prediction tasks . experimental results show that a two-level severity classifier combined with a 9-dimensional intelligibility prediction mapping can achieve 0.92 correlation and 12.52 root-mean-square error with subjective intelligibility ratings . the effects of classification errors on intelligibility accuracy are also explored and shown to be insignificant ."
  },
  {
    "title": "An improved parallel architecture for MPEG-4 motion estimation in 3G mobile applications .",
    "entities": [
      "full-search block matching algorithm",
      "preload and alignment cycles",
      "low clock rate requirements",
      "high-parallel vlsi core architecture",
      "3g mobile applications",
      "mpeg-4 motion estimation",
      "one-dimensional tree architecture",
      "low memory bandwidth",
      "16-pe array",
      "dual-register/buffer technique",
      "motion vectors"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <task> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "dual-register/buffer technique -- USED-FOR -- preload and alignment cycles",
      "high-parallel vlsi core architecture -- USED-FOR -- preload and alignment cycles",
      "low memory bandwidth -- CONJUNCTION -- low clock rate requirements",
      "16-pe array -- USED-FOR -- full-search block matching algorithm",
      "dual-register/buffer technique -- USED-FOR -- high-parallel vlsi core architecture"
    ],
    "abstract": "a <method_3> for <task_5> is proposed in this paper . <method_3> possesses the characteristics of <otherscientificterm_7> and <otherscientificterm_2> , thus primarily aiming at <task_4> . based on a <method_6> , the <method_3> employs the <method_9> to reduce the <otherscientificterm_1> . as an example , <method_0> has been mapped onto this <method_3> using a <method_8> that has the ability to calculate the <otherscientificterm_10> of qcif video sequences in real time at 1 mhz clock rate and using 15.5 mbytes/s memory bandwidth .",
    "abstract_og": "a high-parallel vlsi core architecture for mpeg-4 motion estimation is proposed in this paper . high-parallel vlsi core architecture possesses the characteristics of low memory bandwidth and low clock rate requirements , thus primarily aiming at 3g mobile applications . based on a one-dimensional tree architecture , the high-parallel vlsi core architecture employs the dual-register/buffer technique to reduce the preload and alignment cycles . as an example , full-search block matching algorithm has been mapped onto this high-parallel vlsi core architecture using a 16-pe array that has the ability to calculate the motion vectors of qcif video sequences in real time at 1 mhz clock rate and using 15.5 mbytes/s memory bandwidth ."
  },
  {
    "title": "Phase-only information loss .",
    "entities": [
      "complex sinusoid in white noise",
      "von mises distributional assumptions",
      "asymptotic distributional properties",
      "complex-valued random variables",
      "asymptotic variance",
      "statistical information",
      "system parameters",
      "complex distribution",
      "signal processing"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "statistical information -- CONJUNCTION -- asymptotic variance",
      "complex-valued random variables -- USED-FOR -- system parameters"
    ],
    "abstract": "in many areas of <task_8> , the phases of <otherscientificterm_3> are used to estimate <otherscientificterm_6> , the magnitudes being discarded . in this paper , we consider the implications of doing this : the loss of <otherscientificterm_5> and subsequent increase in <otherscientificterm_4> . two particular cases , those of estimating the phase of the mean of a <task_7> , and estimating the frequency of a <otherscientificterm_0> , are considered . the estimators are motivated by estimation under <otherscientificterm_1> . the <otherscientificterm_2> are obtained under general assumptions , and are tested using a small number of simulations .",
    "abstract_og": "in many areas of signal processing , the phases of complex-valued random variables are used to estimate system parameters , the magnitudes being discarded . in this paper , we consider the implications of doing this : the loss of statistical information and subsequent increase in asymptotic variance . two particular cases , those of estimating the phase of the mean of a complex distribution , and estimating the frequency of a complex sinusoid in white noise , are considered . the estimators are motivated by estimation under von mises distributional assumptions . the asymptotic distributional properties are obtained under general assumptions , and are tested using a small number of simulations ."
  },
  {
    "title": "Task Space Behavior Learning for Humanoid Robots using Gaussian Mixture Models .",
    "entities": [
      "robot behavior acquisition",
      "constrained reaching gestures",
      "gaussian mixture models",
      "kinesthetic demonstrations",
      "humanoid robot",
      "learning algorithm",
      "imitation trajectory"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "humanoid robot -- USED-FOR -- constrained reaching gestures",
      "gaussian mixture models -- USED-FOR -- learning algorithm"
    ],
    "abstract": "in this paper a system was developed for <task_0> using <method_3> . it enables a <method_4> to imitate <otherscientificterm_1> directed towards a target using a <method_5> based on <method_2> . the <otherscientificterm_6> can be reshaped in order to satisfy the constraints of the task and <otherscientificterm_6> can adapt to changes in the initial conditions and to target displacements occurring during movement execution . the potential of this method was evaluated using experiments with the nao , aldebaran 's <method_4> .",
    "abstract_og": "in this paper a system was developed for robot behavior acquisition using kinesthetic demonstrations . it enables a humanoid robot to imitate constrained reaching gestures directed towards a target using a learning algorithm based on gaussian mixture models . the imitation trajectory can be reshaped in order to satisfy the constraints of the task and imitation trajectory can adapt to changes in the initial conditions and to target displacements occurring during movement execution . the potential of this method was evaluated using experiments with the nao , aldebaran 's humanoid robot ."
  },
  {
    "title": "Empirical Study of Utilizing Morph-Syntactic Information in SMT .",
    "entities": [
      "morphological and relative positional information",
      "statistical machine translation framework",
      "class-based n-gram language model",
      "morphologically rich language pairs",
      "part-of-speech and base form",
      "local word orders",
      "relative positional features",
      "word-based language model",
      "word-based/class-based language models",
      "relative positional information",
      "data sparseness problem",
      "morph-syntactical information",
      "base form",
      "word group",
      "morpho-syntactical information",
      "morph-syntactical similarity",
      "multilingual translations",
      "log-linear model",
      "translation quality",
      "translation models",
      "part-of-speech"
    ],
    "types": "<otherscientificterm> <method> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <metric> <method> <otherscientificterm>",
    "relations": [
      "morphological and relative positional information -- USED-FOR -- multilingual translations",
      "base form -- CONJUNCTION -- part-of-speech",
      "relative positional information -- PART-OF -- statistical machine translation framework",
      "part-of-speech -- CONJUNCTION -- relative positional information",
      "data sparseness problem -- USED-FOR -- class-based n-gram language model",
      "relative positional information -- HYPONYM-OF -- morpho-syntactical information",
      "relative positional features -- USED-FOR -- local word orders",
      "morphological and relative positional information -- USED-FOR -- morphologically rich language pairs",
      "part-of-speech -- HYPONYM-OF -- morpho-syntactical information",
      "base form -- HYPONYM-OF -- morpho-syntactical information"
    ],
    "abstract": "in this paper , we present an empirical study that utilizes <otherscientificterm_11> to improve <metric_18> . with three kinds of language pairs matched according to <otherscientificterm_15> or difference , we investigate the effects of various <otherscientificterm_14> , such as <otherscientificterm_12> , <otherscientificterm_20> , and the <otherscientificterm_9> of a word in a <method_1> . we learn not only <method_19> but also <method_8> by manipulating <otherscientificterm_0> . and we integrate the models into a <method_17> . experiments on <material_16> showed that such <otherscientificterm_0> as <otherscientificterm_4> are effective for improving performance in <material_3> and that the <otherscientificterm_6> in a <otherscientificterm_13> are useful for reordering the <otherscientificterm_5> . moreover , the use of a <method_2> improves performance by alleviating the <task_10> in a <method_7> .",
    "abstract_og": "in this paper , we present an empirical study that utilizes morph-syntactical information to improve translation quality . with three kinds of language pairs matched according to morph-syntactical similarity or difference , we investigate the effects of various morpho-syntactical information , such as base form , part-of-speech , and the relative positional information of a word in a statistical machine translation framework . we learn not only translation models but also word-based/class-based language models by manipulating morphological and relative positional information . and we integrate the models into a log-linear model . experiments on multilingual translations showed that such morphological and relative positional information as part-of-speech and base form are effective for improving performance in morphologically rich language pairs and that the relative positional features in a word group are useful for reordering the local word orders . moreover , the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model ."
  },
  {
    "title": "Lossless Compression of 4D Medical Images using H. 264/AVC .",
    "entities": [
      "dimensional medical data",
      "spatial and temporal redundancies",
      "lossless compression technique",
      "4d compression methods",
      "2d image slices",
      "4d medical images",
      "3d images",
      "diagnostic purposes",
      "image quality",
      "volumetric images",
      "compression"
    ],
    "types": "<material> <otherscientificterm> <method> <method> <material> <material> <material> <task> <metric> <material> <otherscientificterm>",
    "relations": [
      "2d image slices -- CONJUNCTION -- spatial and temporal redundancies",
      "image quality -- EVALUATE-FOR -- compression",
      "2d image slices -- CONJUNCTION -- 3d images",
      "image quality -- EVALUATE-FOR -- diagnostic purposes",
      "lossless compression technique -- COMPARE -- 4d compression methods",
      "lossless compression technique -- USED-FOR -- 4d medical images"
    ],
    "abstract": "four <material_0> are sequences of <material_9> captured in time . these data sets are typically very large in size and demand a great amount of resources for storage and transmission . in this paper , we present a <method_2> for <material_5> which is based on the h. 264/avc video coding standard . our <method_2> efficiently exploits <otherscientificterm_1> between <material_4> and <material_6> in <material_5> and eliminates any concerns regarding the effects of <otherscientificterm_10> on <metric_8> for <task_7> . performance evaluations have shown that the proposed <method_2> outperforms current <method_3> by 70 % .",
    "abstract_og": "four dimensional medical data are sequences of volumetric images captured in time . these data sets are typically very large in size and demand a great amount of resources for storage and transmission . in this paper , we present a lossless compression technique for 4d medical images which is based on the h. 264/avc video coding standard . our lossless compression technique efficiently exploits spatial and temporal redundancies between 2d image slices and 3d images in 4d medical images and eliminates any concerns regarding the effects of compression on image quality for diagnostic purposes . performance evaluations have shown that the proposed lossless compression technique outperforms current 4d compression methods by 70 % ."
  },
  {
    "title": "Variational Mixture of Gaussian Process Experts .",
    "entities": [
      "mixture of gaussian processes models",
      "reduction of training complexity",
      "gaussian mixture model",
      "variational bayesian algorithm",
      "large-scale data sets",
      "gaussian components",
      "generative approaches",
      "experts model",
      "linear model",
      "inference algorithms",
      "gaussian process",
      "gibbs sampling"
    ],
    "types": "<method> <metric> <method> <method> <material> <method> <method> <method> <method> <method> <method> <method>",
    "relations": [
      "gibbs sampling -- USED-FOR -- inference algorithms",
      "gaussian process -- USED-FOR -- mixture of gaussian processes models",
      "linear model -- USED-FOR -- gaussian process",
      "inference algorithms -- USED-FOR -- mixture of gaussian processes models",
      "variational bayesian algorithm -- COMPARE -- generative approaches"
    ],
    "abstract": "mixture of gaussian processes <method_0> extended a single <method_10> with ability of modeling multi-modal data and <metric_1> . previous <method_9> for these <method_0> are mostly based on <method_11> , which can be very slow , particularly for <material_4> . we present a new generative mixture of <method_7> . each expert is still a <method_10> but is reformulated by a <method_8> . this breaks the dependency among training outputs and enables us to use a much faster <method_3> for training . our <method_3> is more flexible than previous <method_6> as inputs for each expert are modeled by a <method_2> . the number of experts and number of <method_5> for an expert are inferred automatically . a variety of tests show the advantages of our method .",
    "abstract_og": "mixture of gaussian processes mixture of gaussian processes models extended a single gaussian process with ability of modeling multi-modal data and reduction of training complexity . previous inference algorithms for these mixture of gaussian processes models are mostly based on gibbs sampling , which can be very slow , particularly for large-scale data sets . we present a new generative mixture of experts model . each expert is still a gaussian process but is reformulated by a linear model . this breaks the dependency among training outputs and enables us to use a much faster variational bayesian algorithm for training . our variational bayesian algorithm is more flexible than previous generative approaches as inputs for each expert are modeled by a gaussian mixture model . the number of experts and number of gaussian components for an expert are inferred automatically . a variety of tests show the advantages of our method ."
  },
  {
    "title": "VMF-SNE : Embedding for spherical data .",
    "entities": [
      "von mises-fisher distribution",
      "simulation data set",
      "vmf-sne embedding algorithm",
      "data visualization",
      "local proximity",
      "spherical data",
      "iterative process",
      "euclidean space",
      "high-dimensional data",
      "gaussian distributions",
      "t-sne",
      "embeddings",
      "embedding"
    ],
    "types": "<otherscientificterm> <material> <method> <task> <otherscientificterm> <material> <method> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "t-sne -- COMPARE -- t-sne",
      "von mises-fisher distribution -- USED-FOR -- local proximity",
      "simulation data set -- EVALUATE-FOR -- t-sne",
      "t-sne -- USED-FOR -- spherical data",
      "gaussian distributions -- USED-FOR -- local proximity",
      "high-dimensional data -- USED-FOR -- data visualization",
      "t-sne -- USED-FOR -- high-dimensional data",
      "iterative process -- USED-FOR -- embedding",
      "vmf-sne embedding algorithm -- USED-FOR -- spherical data"
    ],
    "abstract": "-- <method_10> is a well-known approach to embedding <material_8> and has been widely used in <task_3> . the basic assumption of <method_10> is that the data are non-constrained in the <otherscientificterm_7> and the <otherscientificterm_4> can be modelled by <otherscientificterm_9> . this assumption does not hold for a wide range of data types in practical applications , for instance <material_5> for which the <otherscientificterm_4> is better modelled by the <otherscientificterm_0> instead of the <otherscientificterm_9> . this paper presents a <method_2> to embed <material_5> . an <method_6> is derived to produce an efficient <method_12> . the results on a <material_1> demonstrated that <method_10> produces better <otherscientificterm_11> than <method_10> for <material_5> .",
    "abstract_og": "-- t-sne is a well-known approach to embedding high-dimensional data and has been widely used in data visualization . the basic assumption of t-sne is that the data are non-constrained in the euclidean space and the local proximity can be modelled by gaussian distributions . this assumption does not hold for a wide range of data types in practical applications , for instance spherical data for which the local proximity is better modelled by the von mises-fisher distribution instead of the gaussian distributions . this paper presents a vmf-sne embedding algorithm to embed spherical data . an iterative process is derived to produce an efficient embedding . the results on a simulation data set demonstrated that t-sne produces better embeddings than t-sne for spherical data ."
  },
  {
    "title": "Bookmark Hierarchies and Collaborative Recommendation .",
    "entities": [
      "recommendation and search engine",
      "intelligent information retrieval techniques",
      "semantic similarity measure",
      "collaborative filtering",
      "bookmark files",
      "ranking algorithms",
      "novelty measures",
      "web pages",
      "hierarchical structure",
      "similarity-induced network",
      "bookmarks",
      "givealink.org",
      "urls",
      "tags"
    ],
    "types": "<method> <method> <metric> <method> <material> <method> <metric> <material> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "similarity-induced network -- USED-FOR -- novelty measures",
      "ranking algorithms -- USED-FOR -- recommendation and search engine",
      "semantic similarity measure -- USED-FOR -- urls",
      "novelty measures -- USED-FOR -- ranking algorithms",
      "similarity-induced network -- USED-FOR -- ranking algorithms",
      "novelty measures -- USED-FOR -- recommendation and search engine"
    ],
    "abstract": "givealink.org is a social bookmarking site where users may donate and view their personal <material_4> online securely . the <otherscientificterm_10> are analyzed to build a new generation of <method_1> to recommend , search , and personalize the web . <method_11> does not use <otherscientificterm_13> , content , or links in the submitted <material_7> . instead we present a <metric_2> for <otherscientificterm_12> that takes advantage both of the <otherscientificterm_8> in the <material_4> of individual users , and of <method_3> across users . in addition , we build a <method_0> from <method_5> based on popularity and <metric_6> extracted from the <method_9> . search results can be personalized using the <otherscientificterm_10> submitted by a user . we evaluate a subset of the proposed <metric_2> by conducting a study with human subjects .",
    "abstract_og": "givealink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely . the bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend , search , and personalize the web . givealink.org does not use tags , content , or links in the submitted web pages . instead we present a semantic similarity measure for urls that takes advantage both of the hierarchical structure in the bookmark files of individual users , and of collaborative filtering across users . in addition , we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network . search results can be personalized using the bookmarks submitted by a user . we evaluate a subset of the proposed semantic similarity measure by conducting a study with human subjects ."
  },
  {
    "title": "Automatic Generation of High-Level State Features for Generalized Planning .",
    "entities": [
      "diverse generalized planning problems",
      "computation of generalized plans",
      "high-level state features",
      "classical planning problems",
      "generalized planning problem",
      "computation of features",
      "generalized plans",
      "generalized planning",
      "classification tasks",
      "classical planning",
      "conjunctive queries",
      "features"
    ],
    "types": "<task> <task> <otherscientificterm> <task> <task> <otherscientificterm> <method> <task> <task> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "features -- USED-FOR -- generalized plans",
      "features -- USED-FOR -- diverse generalized planning problems",
      "high-level state features -- USED-FOR -- generalized planning problem"
    ],
    "abstract": "in many domains <method_6> can only be computed if certain <otherscientificterm_2> , i.e. <otherscientificterm_11> that capture key concepts to accurately distinguish between states and make good decisions , are available . in most applications of <task_7> such <otherscientificterm_11> are hand-coded by an expert . this paper presents a novel method to automatically generate <otherscientificterm_2> for solving a <task_4> . our method extends a compilation of <task_7> into <task_9> and integrates the <task_1> with the <otherscientificterm_5> , in the form of <otherscientificterm_10> . experiments show that we generate <otherscientificterm_11> for <task_0> and hence , compute <method_6> without providing a prior high-level representation of the states . we also bring a new landscape of challenging benchmarks to <task_9> since our compilation naturally models <task_8> as <task_3> .",
    "abstract_og": "in many domains generalized plans can only be computed if certain high-level state features , i.e. features that capture key concepts to accurately distinguish between states and make good decisions , are available . in most applications of generalized planning such features are hand-coded by an expert . this paper presents a novel method to automatically generate high-level state features for solving a generalized planning problem . our method extends a compilation of generalized planning into classical planning and integrates the computation of generalized plans with the computation of features , in the form of conjunctive queries . experiments show that we generate features for diverse generalized planning problems and hence , compute generalized plans without providing a prior high-level representation of the states . we also bring a new landscape of challenging benchmarks to classical planning since our compilation naturally models classification tasks as classical planning problems ."
  },
  {
    "title": "Learning Partially Observable Deterministic Action Models .",
    "entities": [
      "partially observable strips domains",
      "reinforcement learning",
      "autonomous agent",
      "logical filtering",
      "conditional effects",
      "strips actions",
      "theoretical guarantees",
      "version spaces",
      "decision making",
      "deterministic domains",
      "hmms",
      "diagnosis"
    ],
    "types": "<material> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <method> <task>",
    "relations": [
      "hmms -- CONJUNCTION -- reinforcement learning",
      "version spaces -- CONJUNCTION -- logical filtering",
      "decision making -- CONJUNCTION -- diagnosis",
      "conditional effects -- HYPONYM-OF -- deterministic domains"
    ],
    "abstract": "we present the first tractable , exact solution for the problem of identifying actions ' effects in <material_0> . our algorithms resemble <method_7> and <method_3> , and they identify all the models that are consistent with observations . they apply in other <otherscientificterm_9> -lrb- e.g. , with <otherscientificterm_4> -rrb- , but are inexact -lrb- may return false positives -rrb- or inefficient -lrb- we could not bound the representation size -rrb- . our experiments verify the <otherscientificterm_6> , and show that we learn <otherscientificterm_5> efficiently , with time that is significantly better than approaches for <method_10> and <method_1> -lrb- which are inexact -rrb- . our results are especially surprising because of the inherent intractability of the general deterministic case . these results have been applied to an <method_2> in a virtual world , facilitating <task_8> , <task_11> , and exploration .",
    "abstract_og": "we present the first tractable , exact solution for the problem of identifying actions ' effects in partially observable strips domains . our algorithms resemble version spaces and logical filtering , and they identify all the models that are consistent with observations . they apply in other deterministic domains -lrb- e.g. , with conditional effects -rrb- , but are inexact -lrb- may return false positives -rrb- or inefficient -lrb- we could not bound the representation size -rrb- . our experiments verify the theoretical guarantees , and show that we learn strips actions efficiently , with time that is significantly better than approaches for hmms and reinforcement learning -lrb- which are inexact -rrb- . our results are especially surprising because of the inherent intractability of the general deterministic case . these results have been applied to an autonomous agent in a virtual world , facilitating decision making , diagnosis , and exploration ."
  },
  {
    "title": "Optimal design of spectrum constrained signal sets with correlation analysis .",
    "entities": [
      "quadratic phase structure",
      "prescribed magnitude spectrum",
      "lowest maximum cross-correlation",
      "mathematical analysis",
      "analog signals",
      "analytic expression",
      "maximum cross-correlation"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "analytic expression -- USED-FOR -- maximum cross-correlation",
      "prescribed magnitude spectrum -- CONJUNCTION -- quadratic phase structure",
      "prescribed magnitude spectrum -- FEATURE-OF -- analog signals"
    ],
    "abstract": "this paper is concerned with the design of an optimal set of <material_4> with <otherscientificterm_1> and <otherscientificterm_0> such that the <otherscientificterm_6> is minimized . an <otherscientificterm_5> for the <otherscientificterm_6> between two signals is derived through <method_3> . the optimal set of signals with the <otherscientificterm_2> is explicitly characterized under certain conditions .",
    "abstract_og": "this paper is concerned with the design of an optimal set of analog signals with prescribed magnitude spectrum and quadratic phase structure such that the maximum cross-correlation is minimized . an analytic expression for the maximum cross-correlation between two signals is derived through mathematical analysis . the optimal set of signals with the lowest maximum cross-correlation is explicitly characterized under certain conditions ."
  },
  {
    "title": "Learning to Solve QBF .",
    "entities": [
      "quantified boolean formulas",
      "dynamic , online approach",
      "search-based qbf solver",
      "machine learning techniques",
      "dynamic method variables",
      "unsolved problem instances",
      "variable assignment",
      "optimal heuristics",
      "heuristics",
      "classifier",
      "heuristic"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "search-based qbf solver -- CONJUNCTION -- machine learning techniques"
    ],
    "abstract": "we present a novel approach to solving <method_0> that combines a <method_2> with <method_3> . we show how classification methods can be used to predict run-times and to choose <method_7> both within a portfolio-based , and within a <method_1> . in the <otherscientificterm_4> are set to a truth value according to a scheme that tries to maximize the probability of successfully solving the remaining sub-problem efficiently . since each <otherscientificterm_6> can drastically change the problem-structure , new <method_8> are chosen dynamically , and a <method_9> is used online to predict the usefulness of each <method_10> . experimental results on a large corpus of example problems show the usefulness of our approach in terms of run-time as well as the ability to solve previously <otherscientificterm_5> .",
    "abstract_og": "we present a novel approach to solving quantified boolean formulas that combines a search-based qbf solver with machine learning techniques . we show how classification methods can be used to predict run-times and to choose optimal heuristics both within a portfolio-based , and within a dynamic , online approach . in the dynamic method variables are set to a truth value according to a scheme that tries to maximize the probability of successfully solving the remaining sub-problem efficiently . since each variable assignment can drastically change the problem-structure , new heuristics are chosen dynamically , and a classifier is used online to predict the usefulness of each heuristic . experimental results on a large corpus of example problems show the usefulness of our approach in terms of run-time as well as the ability to solve previously unsolved problem instances ."
  },
  {
    "title": "Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras ? .",
    "entities": [
      "quasi-pinhole central cameras",
      "central camera settings",
      "ad hoc models",
      "image formation model",
      "parametric approaches",
      "unconstrained model",
      "imaging devices",
      "camera models",
      "free parameters",
      "precision calibration",
      "pinhole cameras",
      "calibration approach",
      "radial distortion",
      "estimation process",
      "non-linearities",
      "correction"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <method> <method> <otherscientificterm> <metric> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "correction -- FEATURE-OF -- ad hoc models",
      "unconstrained model -- COMPARE -- ad hoc models",
      "ad hoc models -- USED-FOR -- imaging devices",
      "precision calibration -- EVALUATE-FOR -- calibration approach",
      "unconstrained model -- USED-FOR -- quasi-pinhole central cameras",
      "ad hoc models -- USED-FOR -- radial distortion",
      "unconstrained model -- USED-FOR -- central camera settings",
      "unconstrained model -- USED-FOR -- free parameters",
      "correction -- USED-FOR -- radial distortion"
    ],
    "abstract": "traditional <method_7> are often the result of a compromise between the ability to account for <otherscientificterm_14> in the <method_3> and the need for a feasible number of degrees of freedom in the <task_13> . these considerations led to the definition of several <method_2> that best adapt to different <method_6> , ranging from <method_10> with no <otherscientificterm_12> to the more complex catadioptric or polydioptric optics . in this paper we propose the use of an <method_5> even in standard <otherscientificterm_1> dominated by the <method_2> , and introduce a novel <method_11> that can deal effectively with the huge number of <otherscientificterm_8> associated with <method_5> , resulting in a higher <metric_9> than what is possible with the standard <method_2> with <otherscientificterm_15> for <otherscientificterm_12> . this effectively extends the use of general models to settings that traditionally have been ruled by <method_4> out of practical considerations . the benefit of such an <method_5> to <otherscientificterm_0> is supported by an extensive experimental validation .",
    "abstract_og": "traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process . these considerations led to the definition of several ad hoc models that best adapt to different imaging devices , ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics . in this paper we propose the use of an unconstrained model even in standard central camera settings dominated by the ad hoc models , and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with unconstrained model , resulting in a higher precision calibration than what is possible with the standard ad hoc models with correction for radial distortion . this effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations . the benefit of such an unconstrained model to quasi-pinhole central cameras is supported by an extensive experimental validation ."
  },
  {
    "title": "A Transcription Task for Crowdsourcing with Automatic Quality Control .",
    "entities": [
      "support vector machine classifier",
      "amazon mechanical turk",
      "two-stage transcription task design",
      "word level confidence scores",
      "automatic quality control mechanism",
      "academic lecture speech",
      "rover-based method",
      "baseline transcripts",
      "acoustic cues",
      "instantaneous feedback",
      "technical material",
      "transcription quality",
      "transcriber effort",
      "language patterns",
      "crowdsourcing",
      "mturk"
    ],
    "types": "<method> <method> <method> <metric> <method> <material> <method> <material> <otherscientificterm> <otherscientificterm> <material> <metric> <otherscientificterm> <otherscientificterm> <task> <material>",
    "relations": [
      "academic lecture speech -- EVALUATE-FOR -- two-stage transcription task design",
      "amazon mechanical turk -- USED-FOR -- two-stage transcription task design",
      "rover-based method -- USED-FOR -- baseline transcripts",
      "word level confidence scores -- USED-FOR -- transcription quality",
      "two-stage transcription task design -- USED-FOR -- crowdsourcing",
      "acoustic cues -- CONJUNCTION -- language patterns",
      "transcriber effort -- EVALUATE-FOR -- two-stage transcription task design"
    ],
    "abstract": "in this paper , we propose a <method_2> for <task_14> with an <method_4> embedded in each stage . for the first stage , a <method_0> is utilized to quickly filter poor quality transcripts based on <otherscientificterm_8> and <otherscientificterm_13> in the transcript . in the second stage , <metric_3> are used to estimate a <metric_11> and provide <otherscientificterm_9> to the transcriber . the proposed <method_2> was evaluated using <method_1> and tested on seven hours of <material_5> , which is typically conversational in nature and contains <material_10> . compared to <material_7> which were also collected from <material_15> using a <method_6> , we observed that the new <method_2> resulted in higher quality transcripts while requiring less <otherscientificterm_12> .",
    "abstract_og": "in this paper , we propose a two-stage transcription task design for crowdsourcing with an automatic quality control mechanism embedded in each stage . for the first stage , a support vector machine classifier is utilized to quickly filter poor quality transcripts based on acoustic cues and language patterns in the transcript . in the second stage , word level confidence scores are used to estimate a transcription quality and provide instantaneous feedback to the transcriber . the proposed two-stage transcription task design was evaluated using amazon mechanical turk and tested on seven hours of academic lecture speech , which is typically conversational in nature and contains technical material . compared to baseline transcripts which were also collected from mturk using a rover-based method , we observed that the new two-stage transcription task design resulted in higher quality transcripts while requiring less transcriber effort ."
  },
  {
    "title": "Modeling tones in hakka on the basis of the command-response model .",
    "entities": [
      "subjective and relative nature",
      "f 0 contour generation",
      "5-level notation of tones",
      "continuous f 0 contours",
      "quantitative approximations",
      "lexical tones",
      "command-response model",
      "tone type",
      "tone system",
      "phonological descriptions",
      "chinese dialects",
      "hakka"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <material> <material>",
    "relations": [
      "phonological descriptions -- USED-FOR -- continuous f 0 contours",
      "lexical tones -- FEATURE-OF -- tone system",
      "command-response model -- USED-FOR -- f 0 contour generation"
    ],
    "abstract": "as one of the major <material_10> , <material_11> typically has a <method_8> with six <otherscientificterm_5> . the traditional <otherscientificterm_2> in <material_11> varies in previous references due to its <otherscientificterm_0> . in order to overcome the limitations of the traditional approach , the <method_6> for the process of <task_1> is employed to analyze quantitatively the tones in continuous speech of two varieties of <material_11> , spoken in meixian and in shataukok , respectively . by providing both <otherscientificterm_9> to each <otherscientificterm_7> and <otherscientificterm_4> to <otherscientificterm_3> , the model-based approach provides an efficient connection between phonetics and phonology of <material_11> tones .",
    "abstract_og": "as one of the major chinese dialects , hakka typically has a tone system with six lexical tones . the traditional 5-level notation of tones in hakka varies in previous references due to its subjective and relative nature . in order to overcome the limitations of the traditional approach , the command-response model for the process of f 0 contour generation is employed to analyze quantitatively the tones in continuous speech of two varieties of hakka , spoken in meixian and in shataukok , respectively . by providing both phonological descriptions to each tone type and quantitative approximations to continuous f 0 contours , the model-based approach provides an efficient connection between phonetics and phonology of hakka tones ."
  },
  {
    "title": "Matrix parametrization of compactly supported orthonormal wavelets .",
    "entities": [
      "orthogonal wavelet of compact support",
      "regular objective function",
      "nonlinear optimization problem",
      "arbitrary decision vector",
      "two-scale difference equation",
      "classical orthogonal wavelets",
      "customized orthonormal wavelets",
      "filter coefficients",
      "filter size",
      "decision vector",
      "filter design"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "filter size -- FEATURE-OF -- arbitrary decision vector",
      "filter coefficients -- USED-FOR -- two-scale difference equation",
      "regular objective function -- FEATURE-OF -- filter design"
    ],
    "abstract": "we derive a new set of necessary and sufficient conditions for the <otherscientificterm_7> of the <otherscientificterm_4> to yield an <otherscientificterm_0> . the conditions constitute a linear set of equations of an <otherscientificterm_3> of half the <otherscientificterm_8> . the vector of the <otherscientificterm_7> is a differentiable function of the <otherscientificterm_9> . the formulation enables the optimization of the <method_10> under any <otherscientificterm_1> . the proposed parametrization is used to design <otherscientificterm_6> and to reproduce the <otherscientificterm_5> as a solution of a <task_2> .",
    "abstract_og": "we derive a new set of necessary and sufficient conditions for the filter coefficients of the two-scale difference equation to yield an orthogonal wavelet of compact support . the conditions constitute a linear set of equations of an arbitrary decision vector of half the filter size . the vector of the filter coefficients is a differentiable function of the decision vector . the formulation enables the optimization of the filter design under any regular objective function . the proposed parametrization is used to design customized orthonormal wavelets and to reproduce the classical orthogonal wavelets as a solution of a nonlinear optimization problem ."
  },
  {
    "title": "A theory of plenoptic multiplexing .",
    "entities": [
      "multiplex-ing the dimensions",
      "generic reconstruction algorithm",
      "plenoptic multiplexing schemes",
      "multiplexed imaging applications",
      "high-dimensional image data",
      "optical heterodyn-ing",
      "noise analysis",
      "plenoptic function",
      "fourier domain",
      "light fields",
      "image sensor",
      "spatial multiplexing",
      "color channels",
      "bayer patterns",
      "multiplexing"
    ],
    "types": "<otherscientificterm> <method> <method> <task> <material> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm> <task>",
    "relations": [
      "multiplexing -- USED-FOR -- high-dimensional image data",
      "bayer patterns -- PART-OF -- spatial multiplexing",
      "noise analysis -- HYPONYM-OF -- plenoptic multiplexing schemes",
      "optical heterodyn-ing -- USED-FOR -- light fields"
    ],
    "abstract": "multiplexing is a common technique for encoding <material_4> into a single , two-dimensional image . examples of <task_11> include <otherscientificterm_13> to capture <material_12> , and integral images to encode <otherscientificterm_9> . in the <material_8> , <otherscientificterm_5> has been used to acquire <otherscientificterm_9> . in this paper , we develop a general theory of <otherscientificterm_0> of the <otherscientificterm_7> onto an <otherscientificterm_10> . our theory enables a principled comparison of <method_2> , including <method_6> , as well as the development of a <method_1> . the framework also aides in the identification and optimization of novel <task_3> .",
    "abstract_og": "multiplexing is a common technique for encoding high-dimensional image data into a single , two-dimensional image . examples of spatial multiplexing include bayer patterns to capture color channels , and integral images to encode light fields . in the fourier domain , optical heterodyn-ing has been used to acquire light fields . in this paper , we develop a general theory of multiplex-ing the dimensions of the plenoptic function onto an image sensor . our theory enables a principled comparison of plenoptic multiplexing schemes , including noise analysis , as well as the development of a generic reconstruction algorithm . the framework also aides in the identification and optimization of novel multiplexed imaging applications ."
  },
  {
    "title": "Data spectroscopy : learning mixture models using eigenspaces of convolution operators .",
    "entities": [
      "kernel principal components analysis",
      "parametric distribution <i> p </i>",
      "gaussian mixture models",
      "identification of substances",
      "estimating mixture distributions",
      "gaussian mixture",
      "spectral techniques",
      "spectral clustering",
      "theoretical framework",
      "sampled data",
      "spectral framework",
      "mixture components",
      "spectroscopy"
    ],
    "types": "<method> <method> <method> <task> <task> <method> <method> <method> <method> <material> <method> <method> <method>",
    "relations": [
      "spectral framework -- USED-FOR -- estimating mixture distributions",
      "spectral clustering -- HYPONYM-OF -- spectral techniques",
      "spectroscopy -- USED-FOR -- identification of substances",
      "kernel principal components analysis -- HYPONYM-OF -- spectral techniques",
      "spectral clustering -- CONJUNCTION -- kernel principal components analysis"
    ],
    "abstract": "in this paper we develop a <method_10> for <task_4> , specifically <method_2> . in physics , <method_12> is often used for the <task_3> through their spectrum . treating a kernel function <i> k -lrb- x , y -rrb- </i> as `` light '' and the <material_9> as `` substance '' , the spectrum of their interaction -lrb- eigenvalues and eigenvectors of the kernel matrix <i> k </i> -rrb- unveils certain aspects of the underlying <method_1> , such as the parameters of a <method_5> . our <method_10> extends the intuitions and analyses underlying the existing <method_6> , such as <method_7> and <method_0> . we construct <method_10> to estimate parameters of <method_2> , including the number of <method_11> , their means and covariance matrices , which are important in many practical applications . we provide a <method_8> and show encouraging experimental results .",
    "abstract_og": "in this paper we develop a spectral framework for estimating mixture distributions , specifically gaussian mixture models . in physics , spectroscopy is often used for the identification of substances through their spectrum . treating a kernel function <i> k -lrb- x , y -rrb- </i> as `` light '' and the sampled data as `` substance '' , the spectrum of their interaction -lrb- eigenvalues and eigenvectors of the kernel matrix <i> k </i> -rrb- unveils certain aspects of the underlying parametric distribution <i> p </i> , such as the parameters of a gaussian mixture . our spectral framework extends the intuitions and analyses underlying the existing spectral techniques , such as spectral clustering and kernel principal components analysis . we construct spectral framework to estimate parameters of gaussian mixture models , including the number of mixture components , their means and covariance matrices , which are important in many practical applications . we provide a theoretical framework and show encouraging experimental results ."
  },
  {
    "title": "Gauss-Seidel based non-negative matrix factorization for gene expression clustering .",
    "entities": [
      "gene expression data",
      "cancer expression datasets",
      "representative nmf methods",
      "matrix inverse operators",
      "gene expression clustering",
      "genome-wide expression data",
      "projected data",
      "cluster centroids",
      "factor matrix",
      "raw data",
      "large-scale data",
      "linear system",
      "gauss-seidel method",
      "imbalance deficiency",
      "probed genes",
      "clustering methods",
      "features",
      "gsnmf",
      "clustering"
    ],
    "types": "<material> <material> <method> <method> <task> <material> <material> <method> <method> <material> <material> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "gsnmf -- USED-FOR -- gene expression data",
      "clustering -- USED-FOR -- gene expression data",
      "linear system -- USED-FOR -- factor matrix",
      "cancer expression datasets -- EVALUATE-FOR -- gsnmf",
      "projected data -- USED-FOR -- cluster centroids",
      "gauss-seidel method -- USED-FOR -- linear system",
      "gsnmf -- USED-FOR -- gene expression clustering",
      "gsnmf -- COMPARE -- clustering methods",
      "representative nmf methods -- CONJUNCTION -- clustering methods",
      "linear system -- USED-FOR -- gsnmf",
      "clustering methods -- USED-FOR -- gene expression clustering",
      "gsnmf -- COMPARE -- representative nmf methods"
    ],
    "abstract": "genome-wide expression data consists of millions of measurements towards large number of genes , and thus <material_0> is challenging for human beings to directly analyze such <material_10> . <method_18> provides a more convenient way to analyze <material_0> because <material_0> can subdivide <material_9> into comprehensive classes . however , the number of <otherscientificterm_14> is rather greater than the number of samples , and this makes conventional <method_15> perform unsatisfactorily . in this paper , we propose a gauss-seidel based non-negative matrix factorization -lrb- <method_17> -rrb- method to overcome such <otherscientificterm_13> between <otherscientificterm_16> and samples . in particular , <method_17> iteratively projects <material_0> onto the learned subspace followed by adaptively updating the <method_7> based on the <material_6> . since this <material_0> significantly reduces the influence of imbalance between the number of samples and the number of genes , <method_17> performs better than traditional <method_15> in <task_4> . since <method_17> updates each <method_8> by solution of a <method_11> obtained by the <method_12> , <material_0> converges rapidly without neither complex line search nor <method_3> . experimental results on several <material_1> confirm both efficiency and effectiveness of <method_17> comparing with the <method_2> and conventional <method_15> .",
    "abstract_og": "genome-wide expression data consists of millions of measurements towards large number of genes , and thus gene expression data is challenging for human beings to directly analyze such large-scale data . clustering provides a more convenient way to analyze gene expression data because gene expression data can subdivide raw data into comprehensive classes . however , the number of probed genes is rather greater than the number of samples , and this makes conventional clustering methods perform unsatisfactorily . in this paper , we propose a gauss-seidel based non-negative matrix factorization -lrb- gsnmf -rrb- method to overcome such imbalance deficiency between features and samples . in particular , gsnmf iteratively projects gene expression data onto the learned subspace followed by adaptively updating the cluster centroids based on the projected data . since this gene expression data significantly reduces the influence of imbalance between the number of samples and the number of genes , gsnmf performs better than traditional clustering methods in gene expression clustering . since gsnmf updates each factor matrix by solution of a linear system obtained by the gauss-seidel method , gene expression data converges rapidly without neither complex line search nor matrix inverse operators . experimental results on several cancer expression datasets confirm both efficiency and effectiveness of gsnmf comparing with the representative nmf methods and conventional clustering methods ."
  },
  {
    "title": "Automatic Term Ambiguity Detection .",
    "entities": [
      "information extraction systems",
      "resolution of term ambiguity",
      "term ambiguity detection problem",
      "ambiguous and unambigu-ous cases",
      "language models",
      "topic mod-eling",
      "baseline f-measure",
      "ambiguity detection",
      "entity",
      "ontologies"
    ],
    "types": "<task> <task> <task> <otherscientificterm> <method> <otherscientificterm> <metric> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "ontologies -- CONJUNCTION -- topic mod-eling",
      "language models -- CONJUNCTION -- ontologies"
    ],
    "abstract": "while the <task_1> is important for <task_0> , the cost of resolving each instance of an <otherscientificterm_8> can be prohibitively expensive on large datasets . to combat this , this work looks at <task_7> at the term , rather than the instance , level . by making a judgment about the general ambiguity of a term , a system is able to handle <otherscientificterm_3> differently , improving through-put and quality . to address the <task_2> , we employ a model that combines data from <method_4> , <otherscientificterm_9> , and <otherscientificterm_5> . results over a dataset of entities from four product domains show that the proposed approach achieves significantly above <metric_6> of 0.96 .",
    "abstract_og": "while the resolution of term ambiguity is important for information extraction systems , the cost of resolving each instance of an entity can be prohibitively expensive on large datasets . to combat this , this work looks at ambiguity detection at the term , rather than the instance , level . by making a judgment about the general ambiguity of a term , a system is able to handle ambiguous and unambigu-ous cases differently , improving through-put and quality . to address the term ambiguity detection problem , we employ a model that combines data from language models , ontologies , and topic mod-eling . results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline f-measure of 0.96 ."
  },
  {
    "title": "Minimal local reconstruction error measure based discriminant feature extraction and classification .",
    "entities": [
      "geometric meaning of the minimal local reconstruction error",
      "mlre measure based discriminant feature extraction method",
      "minimal local reconstruction error",
      "mlre-based feature extraction and classification method",
      "within-class and between-class local scatters",
      "cenparmi handwritten numeral database",
      "feret face image database",
      "mlre-based classification method",
      "nearest neighbor classifier",
      "nearest neighbor line",
      "plane classifiers",
      "mlre-based classifier",
      "similarity measure"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <material> <material> <method> <method> <method> <method> <method> <metric>",
    "relations": [
      "cenparmi handwritten numeral database -- CONJUNCTION -- feret face image database",
      "nearest neighbor line -- CONJUNCTION -- plane classifiers",
      "minimal local reconstruction error -- USED-FOR -- within-class and between-class local scatters",
      "cenparmi handwritten numeral database -- EVALUATE-FOR -- mlre-based feature extraction and classification method",
      "feret face image database -- EVALUATE-FOR -- mlre-based feature extraction and classification method"
    ],
    "abstract": "this paper introduces the <otherscientificterm_2> as a <metric_12> and presents a <method_11> . from the <otherscientificterm_0> , we derive that the <method_11> is a generalization of the conventional <method_8> and the <method_9> and <method_10> . we further apply the <otherscientificterm_2> to characterize the <otherscientificterm_4> and then develop a <method_1> . the proposed <method_1> is in line with the <method_7> in spirit , thus the two methods can be seamlessly combined in applications . the experimental results on the <material_5> and the <material_6> show effectiveness of the proposed <method_3> .",
    "abstract_og": "this paper introduces the minimal local reconstruction error as a similarity measure and presents a mlre-based classifier . from the geometric meaning of the minimal local reconstruction error , we derive that the mlre-based classifier is a generalization of the conventional nearest neighbor classifier and the nearest neighbor line and plane classifiers . we further apply the minimal local reconstruction error to characterize the within-class and between-class local scatters and then develop a mlre measure based discriminant feature extraction method . the proposed mlre measure based discriminant feature extraction method is in line with the mlre-based classification method in spirit , thus the two methods can be seamlessly combined in applications . the experimental results on the cenparmi handwritten numeral database and the feret face image database show effectiveness of the proposed mlre-based feature extraction and classification method ."
  },
  {
    "title": "Spatial audio activity detection for hearing aids .",
    "entities": [
      "multi-microphone signal activity detection scheme",
      "hearing aid domain",
      "practical reverberant conditions",
      "hearing aids",
      "directional processing",
      "detection"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "multi-microphone signal activity detection scheme -- USED-FOR -- hearing aids",
      "multi-microphone signal activity detection scheme -- USED-FOR -- detection"
    ],
    "abstract": "we present a <method_0> for <otherscientificterm_3> to differentiate between the periods of activity of desired and interfering sources . the <method_0> is designed to provide robust performance in the presence of simultaneously active desired and interfering sources . we exploit knowledge from the <material_1> , and the <method_4> present in modern <otherscientificterm_3> , to present a <method_0> to design appropriate thresholds for the <task_5> . experiments confirm robust performance under <otherscientificterm_2> .",
    "abstract_og": "we present a multi-microphone signal activity detection scheme for hearing aids to differentiate between the periods of activity of desired and interfering sources . the multi-microphone signal activity detection scheme is designed to provide robust performance in the presence of simultaneously active desired and interfering sources . we exploit knowledge from the hearing aid domain , and the directional processing present in modern hearing aids , to present a multi-microphone signal activity detection scheme to design appropriate thresholds for the detection . experiments confirm robust performance under practical reverberant conditions ."
  },
  {
    "title": "Hierarchical Part-Template Matching for Human Detection and Segmentation .",
    "entities": [
      "local part-based and global template-based schemes",
      "global shape template-based human detectors",
      "detecting and segmenting human shapes",
      "local part-based human detectors",
      "fine occlusion analysis",
      "bayesian map framework",
      "global likelihood re-evaluation",
      "background subtraction",
      "human shapes",
      "severe occlusion",
      "human detection",
      "detection hypotheses",
      "partial occlusions",
      "bayesian approach",
      "part-template tree",
      "images",
      "detection"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <task>",
    "relations": [
      "local part-based human detectors -- USED-FOR -- partial occlusions",
      "bayesian approach -- USED-FOR -- human shapes",
      "fine occlusion analysis -- USED-FOR -- bayesian map framework",
      "global likelihood re-evaluation -- CONJUNCTION -- fine occlusion analysis",
      "part-template tree -- USED-FOR -- detection hypotheses",
      "bayesian approach -- USED-FOR -- human detection",
      "bayesian map framework -- USED-FOR -- bayesian approach",
      "global likelihood re-evaluation -- USED-FOR -- bayesian map framework",
      "images -- EVALUATE-FOR -- bayesian approach"
    ],
    "abstract": "local part-based human detectors are capable of handling <otherscientificterm_12> efficiently and modeling shape ar-ticulations flexibly , while <otherscientificterm_1> are capable of <task_2> simultaneously . we describe a <method_13> to <task_10> and segmentation combining <method_0> . the <method_13> relies on the key ideas of matching a <otherscientificterm_14> to <material_15> hierarchically to generate a reliable set of <otherscientificterm_11> and optimizing <method_13> under a <method_5> through <task_6> and <method_4> . in addition to <task_16> , our <method_13> is able to obtain <otherscientificterm_8> and poses simultaneously . we applied the <method_13> to <task_10> and segmentation in crowded scenes with and without <otherscientificterm_7> . experimental results show that our <method_13> achieves good performance on <material_15> and video sequences with <otherscientificterm_9> .",
    "abstract_og": "local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape ar-ticulations flexibly , while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously . we describe a bayesian approach to human detection and segmentation combining local part-based and global template-based schemes . the bayesian approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing bayesian approach under a bayesian map framework through global likelihood re-evaluation and fine occlusion analysis . in addition to detection , our bayesian approach is able to obtain human shapes and poses simultaneously . we applied the bayesian approach to human detection and segmentation in crowded scenes with and without background subtraction . experimental results show that our bayesian approach achieves good performance on images and video sequences with severe occlusion ."
  },
  {
    "title": "Modality and Component Aware Feature Fusion for RGB-D Scene Classification .",
    "entities": [
      "convolutional neural networks",
      "fv gmm components",
      "proposal-based fv features",
      "augmented pixel-wise representation",
      "exclusive group lasso",
      "global cn-n features",
      "full-image cnn features",
      "scene images",
      "sunrgbd dataset",
      "modal non-sparsity",
      "cnn features",
      "rgb-d data",
      "discriminative components",
      "gmm components",
      "group lasso",
      "scene discriminability",
      "scene classification",
      "regularization terms",
      "region proposals",
      "object recognition",
      "regres-sors",
      "modalities",
      "rgb",
      "hha"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <material> <method> <method> <method> <otherscientificterm> <task> <method> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "regres-sors -- USED-FOR -- proposal-based fv features",
      "full-image cnn features -- USED-FOR -- scene classification",
      "rgb -- FEATURE-OF -- augmented pixel-wise representation",
      "augmented pixel-wise representation -- USED-FOR -- cnn features",
      "fv gmm components -- USED-FOR -- scene discriminability",
      "gmm components -- CONJUNCTION -- exclusive group lasso",
      "proposal-based fv features -- CONJUNCTION -- global cn-n features",
      "modalities -- FEATURE-OF -- augmented pixel-wise representation",
      "regres-sors -- USED-FOR -- scene classification",
      "rgb -- CONJUNCTION -- hha",
      "convolutional neural networks -- USED-FOR -- object recognition",
      "group lasso -- USED-FOR -- gmm components"
    ],
    "abstract": "while <method_0> have been excellent for <task_19> , the greater spatial variability in <material_7> typically meant that the standard <otherscientificterm_6> are suboptimal for <task_16> . in this paper , we investigate a framework allowing greater spatial flexibility , in which the fisher vector -lrb- fv -rrb- encoded distribution of local <otherscientificterm_10> , obtained from a multitude of <method_18> per image , is considered instead . the <otherscientificterm_10> are computed from an <method_3> comprising multiple <otherscientificterm_21> of <otherscientificterm_22> , <otherscientificterm_23> and surface normals , as extracted from <material_11> . more significantly , we make two postulates : -lrb- 1 -rrb- component sparsity -- that only a small variety of <method_18> and their corresponding <method_1> contribute to <otherscientificterm_15> , and -lrb- 2 -rrb- <otherscientificterm_9> -- within these <method_12> , all <otherscientificterm_21> have important contribution . in our framework , these are implemented through <method_17> applying <method_14> to <method_13> and <otherscientificterm_4> across <otherscientificterm_21> . by learning and combining <method_20> for both <otherscientificterm_2> and <otherscientificterm_5> , we were able to achieve state-of-the-art <task_16> performance on the <material_8> and nyu depth dataset v2 .",
    "abstract_og": "while convolutional neural networks have been excellent for object recognition , the greater spatial variability in scene images typically meant that the standard full-image cnn features are suboptimal for scene classification . in this paper , we investigate a framework allowing greater spatial flexibility , in which the fisher vector -lrb- fv -rrb- encoded distribution of local cnn features , obtained from a multitude of region proposals per image , is considered instead . the cnn features are computed from an augmented pixel-wise representation comprising multiple modalities of rgb , hha and surface normals , as extracted from rgb-d data . more significantly , we make two postulates : -lrb- 1 -rrb- component sparsity -- that only a small variety of region proposals and their corresponding fv gmm components contribute to scene discriminability , and -lrb- 2 -rrb- modal non-sparsity -- within these discriminative components , all modalities have important contribution . in our framework , these are implemented through regularization terms applying group lasso to gmm components and exclusive group lasso across modalities . by learning and combining regres-sors for both proposal-based fv features and global cn-n features , we were able to achieve state-of-the-art scene classification performance on the sunrgbd dataset and nyu depth dataset v2 ."
  },
  {
    "title": "Efficient Kernel Machines Using the Improved Fast Gauss Transform .",
    "entities": [
      "moderate size problems",
      "approximation technique",
      "kernel machines",
      "uci datasets",
      "large datasets",
      "complexity"
    ],
    "types": "<otherscientificterm> <method> <method> <material> <material> <metric>",
    "relations": [
      "complexity -- USED-FOR -- moderate size problems"
    ],
    "abstract": "the computation and memory required for <method_2> with n training samples is at least o -lrb- n 2 -rrb- . such a <metric_5> is significant even for <otherscientificterm_0> and is prohibitive for <material_4> . we present an <method_1> based on the improved fast gauss transform to reduce the computation to o -lrb- n -rrb- . we also give an error bound for the approximation , and provide experimental results on the <material_3> .",
    "abstract_og": "the computation and memory required for kernel machines with n training samples is at least o -lrb- n 2 -rrb- . such a complexity is significant even for moderate size problems and is prohibitive for large datasets . we present an approximation technique based on the improved fast gauss transform to reduce the computation to o -lrb- n -rrb- . we also give an error bound for the approximation , and provide experimental results on the uci datasets ."
  },
  {
    "title": "Variance reduction by using separate genuine - impostor statistics in multimodal biometrics .",
    "entities": [
      "matching score level fusion methods",
      "biometric matching score level fusion",
      "genuine and impostor variances",
      "normalization and fusion techniques",
      "polycost and xm2vts databases",
      "genuine and impostor statistics",
      "global score statistics",
      "multimodal biometric",
      "monomodal scores",
      "person verification",
      "multimodal scores",
      "multimodal statistics",
      "weighting method"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <method>",
    "relations": [
      "normalization and fusion techniques -- USED-FOR -- biometric matching score level fusion",
      "global score statistics -- USED-FOR -- matching score level fusion methods"
    ],
    "abstract": "in this paper , we propose some novel <method_3> for <task_1> in <task_9> . while conventional <method_0> use <otherscientificterm_6> , we consider in this work both <material_5> separately . performing a joint mean normalization of the separate <otherscientificterm_8> , <otherscientificterm_10> with less separate variance than the monomodal ones are obtained . furthermore , a <method_12> has been designed in order to minimize the variance sum of the separate <material_11> . this <method_12> obtains a minor sum of <otherscientificterm_2> for the <otherscientificterm_7> than that of the monomodal ones . the results obtained in speech and face scores fusion upon <material_4> show that the proposed <method_3> provide better results than the conventional methods .",
    "abstract_og": "in this paper , we propose some novel normalization and fusion techniques for biometric matching score level fusion in person verification . while conventional matching score level fusion methods use global score statistics , we consider in this work both genuine and impostor statistics separately . performing a joint mean normalization of the separate monomodal scores , multimodal scores with less separate variance than the monomodal ones are obtained . furthermore , a weighting method has been designed in order to minimize the variance sum of the separate multimodal statistics . this weighting method obtains a minor sum of genuine and impostor variances for the multimodal biometric than that of the monomodal ones . the results obtained in speech and face scores fusion upon polycost and xm2vts databases show that the proposed normalization and fusion techniques provide better results than the conventional methods ."
  },
  {
    "title": "Modeling Interaction via the Principle of Maximum Causal Entropy .",
    "entities": [
      "sequentially revealed side information",
      "maximum causal entropy",
      "causally conditioned probabilities",
      "statistical models",
      "sequential data",
      "maximum entropy",
      "marginal distributions",
      "interaction",
      "feedback"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "causally conditioned probabilities -- USED-FOR -- maximum causal entropy",
      "interaction -- CONJUNCTION -- feedback"
    ],
    "abstract": "the principle of <method_5> provides a powerful framework for <method_3> of joint , conditional , and <otherscientificterm_6> . however , there are many important distributions with elements of <otherscientificterm_7> and <otherscientificterm_8> where its applicability has not been established . this work presents the principle of <otherscientificterm_1> -- an approach based on <otherscientificterm_2> that can appropriately model the availability and influence of <otherscientificterm_0> . using this principle , we derive models for <material_4> with revealed information , <otherscientificterm_7> , and <otherscientificterm_8> , and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks .",
    "abstract_og": "the principle of maximum entropy provides a powerful framework for statistical models of joint , conditional , and marginal distributions . however , there are many important distributions with elements of interaction and feedback where its applicability has not been established . this work presents the principle of maximum causal entropy -- an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information . using this principle , we derive models for sequential data with revealed information , interaction , and feedback , and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks ."
  },
  {
    "title": "A Novel Representation for Riemannian Analysis of Elastic Curves in Rn .",
    "entities": [
      "continuous , closed curves",
      "elastic shape metric",
      "shape analysis",
      "3-d examples",
      "path-straightening methods",
      "computing geodesics",
      "fast algorithm",
      "step-by-step algorithms",
      "features",
      "geodesics",
      "path-straightening"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <material> <method> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "elastic shape metric -- CONJUNCTION -- path-straightening methods",
      "step-by-step algorithms -- USED-FOR -- computing geodesics"
    ],
    "abstract": "we propose a novel representation of <otherscientificterm_0> in \u211d -lrb- n -rrb- that is quite efficient for analyzing their shapes . we combine the strengths of two important ideas - <otherscientificterm_1> and <method_4> - in <task_2> and present a <method_6> for finding <otherscientificterm_9> in shape spaces . the <method_6> allows for optimal matching of <otherscientificterm_8> while <otherscientificterm_10> provides <otherscientificterm_9> between curves . efficiency results from the fact that the <method_6> becomes the simple -lrb- 2 -rrb- metric in the proposed representation . we present <method_7> for <task_5> in this <method_6> , and demonstrate <method_7> with 2-d as well as <material_3> .",
    "abstract_og": "we propose a novel representation of continuous , closed curves in \u211d -lrb- n -rrb- that is quite efficient for analyzing their shapes . we combine the strengths of two important ideas - elastic shape metric and path-straightening methods - in shape analysis and present a fast algorithm for finding geodesics in shape spaces . the fast algorithm allows for optimal matching of features while path-straightening provides geodesics between curves . efficiency results from the fact that the fast algorithm becomes the simple -lrb- 2 -rrb- metric in the proposed representation . we present step-by-step algorithms for computing geodesics in this fast algorithm , and demonstrate step-by-step algorithms with 2-d as well as 3-d examples ."
  },
  {
    "title": "Modelling activity global temporal dependencies using Time Delayed Probabilistic Graphical Model .",
    "entities": [
      "time delayed probabilistic graphical model",
      "globally optimised time-delayed dependencies",
      "global behaviour anomalies",
      "real-time anomaly detection",
      "semantically decomposed regions",
      "cumulative abnormality score",
      "multi-camera activities",
      "camera network",
      "disjoint cameras",
      "log-likelihood score"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <metric> <task> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "cumulative abnormality score -- USED-FOR -- real-time anomaly detection",
      "cumulative abnormality score -- COMPARE -- log-likelihood score",
      "log-likelihood score -- USED-FOR -- real-time anomaly detection",
      "time delayed probabilistic graphical model -- USED-FOR -- multi-camera activities"
    ],
    "abstract": "we present a novel approach for detecting <otherscientificterm_2> in multiple <otherscientificterm_8> by learning time delayed dependencies between activities cross camera views . specifically , we propose to model <task_6> using a <method_0> with different nodes representing activities in different <otherscientificterm_4> from different camera views , and the directed links between nodes encoding causal relationships between the activities . a novel two-stage structure learning algorithm is formulated to learn <otherscientificterm_1> . a new <metric_5> is also introduced to replace the conventional <otherscientificterm_9> for gaining significantly more robust and reliable <task_3> . the effectiveness of the proposed approach is validated using a <method_7> installed at a busy underground station .",
    "abstract_og": "we present a novel approach for detecting global behaviour anomalies in multiple disjoint cameras by learning time delayed dependencies between activities cross camera views . specifically , we propose to model multi-camera activities using a time delayed probabilistic graphical model with different nodes representing activities in different semantically decomposed regions from different camera views , and the directed links between nodes encoding causal relationships between the activities . a novel two-stage structure learning algorithm is formulated to learn globally optimised time-delayed dependencies . a new cumulative abnormality score is also introduced to replace the conventional log-likelihood score for gaining significantly more robust and reliable real-time anomaly detection . the effectiveness of the proposed approach is validated using a camera network installed at a busy underground station ."
  },
  {
    "title": "A Bayesian framework for the multifractal analysis of images using data augmentation and a whittle approximation .",
    "entities": [
      "-lrb- linear regression based -rrb- estimation",
      "bayesian estimation of multifractal parameters",
      "regularity fluctuations of image intensity",
      "-lrb- wavelet -rrb- leaders",
      "fourier coefficients of log-leaders",
      "conditional posterior distributions",
      "nonstandard posterior distributions",
      "synthetic multifractal images",
      "image processing task",
      "data-augmented bayesian model",
      "texture analysis",
      "estimation quality",
      "inherent constraints",
      "bayesian model",
      "generative model",
      "mathematical framework",
      "computational cost",
      "numerical simulations",
      "statistical model",
      "reparametrization"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <task> <method> <task> <metric> <otherscientificterm> <method> <method> <method> <metric> <method> <method> <method>",
    "relations": [
      "generative model -- USED-FOR -- fourier coefficients of log-leaders",
      "synthetic multifractal images -- USED-FOR -- numerical simulations",
      "generative model -- USED-FOR -- inherent constraints",
      "estimation quality -- CONJUNCTION -- computational cost",
      "conditional posterior distributions -- USED-FOR -- data-augmented bayesian model",
      "bayesian model -- USED-FOR -- texture analysis",
      "data-augmented bayesian model -- USED-FOR -- conditional posterior distributions",
      "reparametrization -- USED-FOR -- inherent constraints",
      "texture analysis -- HYPONYM-OF -- image processing task",
      "estimation quality -- EVALUATE-FOR -- statistical model",
      "computational cost -- EVALUATE-FOR -- statistical model",
      "texture analysis -- USED-FOR -- image processing task",
      "synthetic multifractal images -- USED-FOR -- statistical model",
      "statistical model -- USED-FOR -- bayesian estimation of multifractal parameters",
      "statistical model -- COMPARE -- -lrb- linear regression based -rrb- estimation"
    ],
    "abstract": "texture analysis is an <task_8> that can be conducted using the <method_15> of <task_10> to study the <otherscientificterm_2> and the practical tools for their assessment , such as <otherscientificterm_3> . a recently introduced <method_18> for leaders enables the <task_1> . <method_18> significantly improves performance over standard <method_0> . however , the <metric_16> induced by the associated <otherscientificterm_6> limits its application . the present work proposes an alternative <method_13> for <task_10> that leads to more efficient algorithms . <method_18> relies on three original contributions : a novel <method_14> for the <method_4> ; an appropriate <method_19> for handling its <otherscientificterm_12> ; a <method_9> yielding standard <otherscientificterm_5> that can be sampled exactly . <method_17> using <material_7> demonstrate the excellent performance of the proposed <method_18> , both in terms of <metric_11> and <metric_16> .",
    "abstract_og": "texture analysis is an image processing task that can be conducted using the mathematical framework of texture analysis to study the regularity fluctuations of image intensity and the practical tools for their assessment , such as -lrb- wavelet -rrb- leaders . a recently introduced statistical model for leaders enables the bayesian estimation of multifractal parameters . statistical model significantly improves performance over standard -lrb- linear regression based -rrb- estimation . however , the computational cost induced by the associated nonstandard posterior distributions limits its application . the present work proposes an alternative bayesian model for texture analysis that leads to more efficient algorithms . statistical model relies on three original contributions : a novel generative model for the fourier coefficients of log-leaders ; an appropriate reparametrization for handling its inherent constraints ; a data-augmented bayesian model yielding standard conditional posterior distributions that can be sampled exactly . numerical simulations using synthetic multifractal images demonstrate the excellent performance of the proposed statistical model , both in terms of estimation quality and computational cost ."
  },
  {
    "title": "The Method of Quantum Clustering .",
    "entities": [
      "known data sets",
      "scale-space probability function",
      "support-vector clustering",
      "variable parameter",
      "probability function",
      "scale-space clustering",
      "schr\u00f6dinger equation",
      "cluster centers",
      "clustering method",
      "hilbert space",
      "gaussian kernel",
      "ideas"
    ],
    "types": "<material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "scale-space clustering -- CONJUNCTION -- support-vector clustering"
    ],
    "abstract": "we propose a novel <method_8> that is an extension of <otherscientificterm_11> inherent to <task_5> and <task_2> . like the latter , <method_8> associates every data point with a vector in <otherscientificterm_9> , and like the former <method_8> puts emphasis on their total sum , that is equal to the <otherscientificterm_1> . the novelty of our <method_8> is the study of an operator in <otherscientificterm_9> , represented by the <otherscientificterm_6> of which the <otherscientificterm_4> is a solution . this <otherscientificterm_6> contains a potential function that can be derived analytically from the <otherscientificterm_4> . we associate minima of the potential with <method_7> . the <method_8> has one <otherscientificterm_3> , the scale of its <method_10> . we demonstrate its applicability on <material_0> . by limiting the evaluation of the schr\u00f6dinger potential to the locations of data points , we can apply this <method_8> to problems in high dimensions .",
    "abstract_og": "we propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering . like the latter , clustering method associates every data point with a vector in hilbert space , and like the former clustering method puts emphasis on their total sum , that is equal to the scale-space probability function . the novelty of our clustering method is the study of an operator in hilbert space , represented by the schr\u00f6dinger equation of which the probability function is a solution . this schr\u00f6dinger equation contains a potential function that can be derived analytically from the probability function . we associate minima of the potential with cluster centers . the clustering method has one variable parameter , the scale of its gaussian kernel . we demonstrate its applicability on known data sets . by limiting the evaluation of the schr\u00f6dinger potential to the locations of data points , we can apply this clustering method to problems in high dimensions ."
  },
  {
    "title": "Linear interpolation of cepstral variance for noisy speech recognition .",
    "entities": [
      "pattern classification rate",
      "speech model combination",
      "cepstral variance",
      "recognition rate",
      "spectral statistics",
      "mapping process",
      "pattern classification",
      "noise variances",
      "background noise",
      "cepstral domain",
      "linear interpolation",
      "spectral domain",
      "noisy speech",
      "environmental adaptation",
      "speech feature",
      "speech recognition",
      "computation",
      "mapping"
    ],
    "types": "<metric> <task> <otherscientificterm> <metric> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <material> <method> <material> <material> <task> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "speech feature -- USED-FOR -- pattern classification",
      "spectral statistics -- USED-FOR -- speech recognition",
      "spectral statistics -- USED-FOR -- speech model combination",
      "cepstral domain -- USED-FOR -- speech recognition",
      "background noise -- USED-FOR -- speech model combination",
      "spectral statistics -- USED-FOR -- cepstral domain"
    ],
    "abstract": "speech <task_1> with the <otherscientificterm_8> has been shown effective to improve the <metric_0> of <material_12> . the <task_1> can be performed by the addition of the <otherscientificterm_4> such as the means and the variances . since the <otherscientificterm_14> for <task_6> has to be expressed in the <material_9> , the combined <otherscientificterm_4> have to be transferred into the <material_9> for <task_15> . in our previous study , we have proposed a direct adaptation scheme of the <otherscientificterm_2> that is without the <task_17> from the <material_11> to the <material_9> . in this paper , an improved version to perform the adaptation is proposed . from the study , it is observed that the adapted variance can be expressed as a <method_10> of the speech and the <otherscientificterm_7> to obtain a comparable <metric_3> that is obtained with the <method_5> . due to the direct adaptation of the variances , a lot of <otherscientificterm_16> can be reduced to perform the <task_13> .",
    "abstract_og": "speech speech model combination with the background noise has been shown effective to improve the pattern classification rate of noisy speech . the speech model combination can be performed by the addition of the spectral statistics such as the means and the variances . since the speech feature for pattern classification has to be expressed in the cepstral domain , the combined spectral statistics have to be transferred into the cepstral domain for speech recognition . in our previous study , we have proposed a direct adaptation scheme of the cepstral variance that is without the mapping from the spectral domain to the cepstral domain . in this paper , an improved version to perform the adaptation is proposed . from the study , it is observed that the adapted variance can be expressed as a linear interpolation of the speech and the noise variances to obtain a comparable recognition rate that is obtained with the mapping process . due to the direct adaptation of the variances , a lot of computation can be reduced to perform the environmental adaptation ."
  },
  {
    "title": "An efficient fractional sample delayer for digital beam steering .",
    "entities": [
      "fractional sample delayer",
      "35-tap finite impulse response filter",
      "digital fractional sample delayers",
      "flat magnitude response",
      "baseband sampling frequency",
      "baseband sampling time",
      "baseband width",
      "signal delay",
      "baseband rate",
      "20-bit resolution",
      "tracking"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "20-bit resolution -- USED-FOR -- 35-tap finite impulse response filter"
    ],
    "abstract": "in this paper we propose to use <method_2> to perform high precision beam steering at the <otherscientificterm_4> . the major advantages of the proposed technique are that the <method_0> used has a very <otherscientificterm_3> within the <otherscientificterm_6> allowing greater than <otherscientificterm_9> for a <method_1> . it also has a delay which is continuously variable providing resolutions greater than 220,000 ths of the <otherscientificterm_5> . owing to the <otherscientificterm_7> being performed at the <otherscientificterm_8> , elements with different delays may be placed in parallel , allowing for the formation of multiple beams -lrb- e.g. <task_10> and surveillance capability simultaneously -rrb- .",
    "abstract_og": "in this paper we propose to use digital fractional sample delayers to perform high precision beam steering at the baseband sampling frequency . the major advantages of the proposed technique are that the fractional sample delayer used has a very flat magnitude response within the baseband width allowing greater than 20-bit resolution for a 35-tap finite impulse response filter . it also has a delay which is continuously variable providing resolutions greater than 220,000 ths of the baseband sampling time . owing to the signal delay being performed at the baseband rate , elements with different delays may be placed in parallel , allowing for the formation of multiple beams -lrb- e.g. tracking and surveillance capability simultaneously -rrb- ."
  },
  {
    "title": "DFT domain subspace based noise tracking for speech enhancement .",
    "entities": [
      "noise power spectral density",
      "dft domain based speech enhancement methods",
      "eigenvalue decompositions of correlation matrices",
      "non-stationary noise sources",
      "noise tracking algorithms",
      "noisy dft coefficients",
      "estimation error",
      "enhancement system",
      "noise tracking",
      "spectral regions",
      "segmental snr",
      "noise psd"
    ],
    "types": "<metric> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "non-stationary noise sources -- USED-FOR -- noise psd"
    ],
    "abstract": "most <method_1> are dependent on an estimate of the <metric_0> . for <otherscientificterm_3> it is desirable to estimate the <otherscientificterm_11> also in <otherscientificterm_9> where speech is present . in this paper a new method for <task_8> is presented , based on <method_2> that are constructed from time series of <otherscientificterm_5> . the presented method can estimate the <otherscientificterm_11> at time-frequency points where both speech and noise are present . in comparison to state-of-the-art <method_4> the proposed algorithm reduces the <otherscientificterm_6> between the estimated and the true <otherscientificterm_11> and improves <method_10> when combined with an <method_7> with several db .",
    "abstract_og": "most dft domain based speech enhancement methods are dependent on an estimate of the noise power spectral density . for non-stationary noise sources it is desirable to estimate the noise psd also in spectral regions where speech is present . in this paper a new method for noise tracking is presented , based on eigenvalue decompositions of correlation matrices that are constructed from time series of noisy dft coefficients . the presented method can estimate the noise psd at time-frequency points where both speech and noise are present . in comparison to state-of-the-art noise tracking algorithms the proposed algorithm reduces the estimation error between the estimated and the true noise psd and improves segmental snr when combined with an enhancement system with several db ."
  },
  {
    "title": "Active Frame , Location , and Detector Selection for Automated and Manual Video Annotation .",
    "entities": [
      "semantic class label uncertainty",
      "information-driven active selection approach",
      "paragon '' algorithm",
      "uncertainty bound",
      "labeling mechanism",
      "noisy detectors",
      "computational cost",
      "pixel",
      "detectors"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <method> <method> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "information-driven active selection approach -- USED-FOR -- labeling mechanism",
      "information-driven active selection approach -- USED-FOR -- detectors"
    ],
    "abstract": "we describe an <method_1> to determine which <otherscientificterm_8> to deploy at which location in which frame of a video to minimize <otherscientificterm_0> at every <otherscientificterm_7> , with the smallest <metric_6> that ensures a given <otherscientificterm_3> . we show minimal performance reduction compared to a '' <method_2> running all <otherscientificterm_8> at all locations in all frames , at a small fraction of the <metric_6> . our <method_1> can handle uncertainty in the <method_4> , so <method_1> can handle both '' oracles '' -lrb- manual annotation -rrb- or <method_5> -lrb- automated annotation -rrb- .",
    "abstract_og": "we describe an information-driven active selection approach to determine which detectors to deploy at which location in which frame of a video to minimize semantic class label uncertainty at every pixel , with the smallest computational cost that ensures a given uncertainty bound . we show minimal performance reduction compared to a '' paragon '' algorithm running all detectors at all locations in all frames , at a small fraction of the computational cost . our information-driven active selection approach can handle uncertainty in the labeling mechanism , so information-driven active selection approach can handle both '' oracles '' -lrb- manual annotation -rrb- or noisy detectors -lrb- automated annotation -rrb- ."
  },
  {
    "title": "A Complete Epistemic Planner without the Epistemic Closed World Assumption .",
    "entities": [
      "epistemic closed-world assumption",
      "weak minimal epistemic cnf",
      "weak minimal epistemic dnf",
      "progression and entailment algorithms",
      "epistemic planning problems",
      "main planning algorithm",
      "single-agent epis-temic planner",
      "epistemic planning scenarios",
      "planning communities",
      "epistemic planner",
      "epistemic formulas",
      "epistemic goals",
      "contingent planning",
      "generic plan",
      "epistemic planning",
      "single-agent case"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <method> <task> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <method> <material>",
    "relations": [
      "single-agent epis-temic planner -- USED-FOR -- epistemic planning problems",
      "weak minimal epistemic cnf -- HYPONYM-OF -- epistemic formulas",
      "single-agent epis-temic planner -- HYPONYM-OF -- epistemic planner",
      "weak minimal epistemic dnf -- CONJUNCTION -- weak minimal epistemic cnf",
      "weak minimal epistemic dnf -- HYPONYM-OF -- epistemic formulas",
      "single-agent epis-temic planner -- USED-FOR -- contingent planning",
      "epistemic closed-world assumption -- USED-FOR -- single-agent epis-temic planner"
    ],
    "abstract": "planning with <otherscientificterm_11> has received attention from both the dynamic logic and <method_8> . in the <material_15> , under the <method_0> , <method_14> can be reduced to <task_12> . however , it is inappropriate to make the <method_0> in some <task_7> , for example , when the agent is not fully introspective , or when the agent wants to devise a <otherscientificterm_13> that applies to a wide range of situations . in this paper , we propose a complete <method_6> without the <method_0> . we identify two normal forms of <method_10> : <otherscientificterm_2> and <otherscientificterm_1> , and present the <method_3> based on these normal forms . we adapt the <method_6> for <task_12> from the literature as the <method_5> and develop a complete <method_9> called <method_6> . our experimental results show that <method_6> can generate solutions effectively for most of the <task_4> we have considered including those without the <method_0> .",
    "abstract_og": "planning with epistemic goals has received attention from both the dynamic logic and planning communities . in the single-agent case , under the epistemic closed-world assumption , epistemic planning can be reduced to contingent planning . however , it is inappropriate to make the epistemic closed-world assumption in some epistemic planning scenarios , for example , when the agent is not fully introspective , or when the agent wants to devise a generic plan that applies to a wide range of situations . in this paper , we propose a complete single-agent epis-temic planner without the epistemic closed-world assumption . we identify two normal forms of epistemic formulas : weak minimal epistemic dnf and weak minimal epistemic cnf , and present the progression and entailment algorithms based on these normal forms . we adapt the single-agent epis-temic planner for contingent planning from the literature as the main planning algorithm and develop a complete epistemic planner called single-agent epis-temic planner . our experimental results show that single-agent epis-temic planner can generate solutions effectively for most of the epistemic planning problems we have considered including those without the epistemic closed-world assumption ."
  },
  {
    "title": "Rapid design of discrete orthonormal wavelet transforms using silicon IP components .",
    "entities": [
      "single and multi-stage wavelet analysis",
      "fpga and pld implementations",
      "orthonormal wavelet transform cores",
      "cascaded silicon cores",
      "wavelet based system",
      "data word length",
      "coefficient word length",
      "interface glue logic",
      "wavelet transform filters",
      "rapid design methodology",
      "wavelet family",
      "design time",
      "silicon layout",
      "hand-crafted designs",
      "time-interleaved coefficients",
      "control circuit",
      "wavelet type",
      "vhdl",
      "foundries"
    ],
    "types": "<task> <task> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "data word length -- CONJUNCTION -- coefficient word length",
      "time-interleaved coefficients -- USED-FOR -- rapid design methodology",
      "rapid design methodology -- USED-FOR -- fpga and pld implementations",
      "rapid design methodology -- USED-FOR -- orthonormal wavelet transform cores",
      "wavelet family -- CONJUNCTION -- wavelet type",
      "wavelet type -- CONJUNCTION -- data word length",
      "time-interleaved coefficients -- USED-FOR -- wavelet transform filters",
      "cascaded silicon cores -- USED-FOR -- single and multi-stage wavelet analysis",
      "design time -- USED-FOR -- silicon layout"
    ],
    "abstract": "a <method_9> for <otherscientificterm_2> has been developed . this <method_9> is based on a generic , scaleable <method_9> utilising <otherscientificterm_14> for the <method_8> . the <method_9> has been captured in <method_17> and parameterised in terms of <otherscientificterm_10> , <otherscientificterm_16> , <otherscientificterm_5> and <otherscientificterm_6> . the <otherscientificterm_15> is embedded within the cores and allows <otherscientificterm_15> to be cascaded without any <otherscientificterm_7> for any desired level of decomposition . case studies for stand alone and <material_3> for <task_0> respectively are reported . the <metric_11> to produce <otherscientificterm_12> of a <method_4> has been reduced to typically less than a day . the cores are comparable in area and performance to <otherscientificterm_13> . the <method_9> are portable across a range of <otherscientificterm_18> and are also applicable to <task_1> .",
    "abstract_og": "a rapid design methodology for orthonormal wavelet transform cores has been developed . this rapid design methodology is based on a generic , scaleable rapid design methodology utilising time-interleaved coefficients for the wavelet transform filters . the rapid design methodology has been captured in vhdl and parameterised in terms of wavelet family , wavelet type , data word length and coefficient word length . the control circuit is embedded within the cores and allows control circuit to be cascaded without any interface glue logic for any desired level of decomposition . case studies for stand alone and cascaded silicon cores for single and multi-stage wavelet analysis respectively are reported . the design time to produce silicon layout of a wavelet based system has been reduced to typically less than a day . the cores are comparable in area and performance to hand-crafted designs . the rapid design methodology are portable across a range of foundries and are also applicable to fpga and pld implementations ."
  },
  {
    "title": "Detection of multipath random signals by multiresolution subspace design .",
    "entities": [
      "modied de blockinection sense",
      "multipath signal subspace s",
      "multipath signal set s",
      "multipath constrained environments",
      "random signal case",
      "representation subspace g",
      "gap metric sense",
      "modi-ed de blockinection",
      "linear subspace",
      "random process",
      "transmitted signal",
      "orthogonal projection",
      "subspaces"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "modi-ed de blockinection -- USED-FOR -- random signal case",
      "orthogonal projection -- USED-FOR -- multipath signal subspace s",
      "orthogonal projection -- USED-FOR -- orthogonal projection",
      "representation subspace g -- USED-FOR -- multipath signal set s",
      "modied de blockinection sense -- FEATURE-OF -- multipath signal set s"
    ],
    "abstract": "in our earlier work -lsb- 1 , 2 -rsb- , we developed a robust detector for <otherscientificterm_3> when the <material_10> is known . in this paper , we extend these results to the case where the <material_10> is a <otherscientificterm_9> . the approach i n -lsb- 1 , 2 -rsb- is to replace the <otherscientificterm_11> on the <otherscientificterm_1> by the <otherscientificterm_11> on a <otherscientificterm_5> , such that g and s are close in the <otherscientificterm_6> . when the signal is random , s is no longer a <otherscientificterm_8> but a set with a given structure . the gap metric applies only when s and g are <otherscientificterm_12> . in this paper , we introduce the <method_7> as the appropriate measure to be used in the <task_4> . we design the <otherscientificterm_5> to match the <otherscientificterm_2> in the <otherscientificterm_0> . wavelet multiresolution tools are used to facilitate the design .",
    "abstract_og": "in our earlier work -lsb- 1 , 2 -rsb- , we developed a robust detector for multipath constrained environments when the transmitted signal is known . in this paper , we extend these results to the case where the transmitted signal is a random process . the approach i n -lsb- 1 , 2 -rsb- is to replace the orthogonal projection on the multipath signal subspace s by the orthogonal projection on a representation subspace g , such that g and s are close in the gap metric sense . when the signal is random , s is no longer a linear subspace but a set with a given structure . the gap metric applies only when s and g are subspaces . in this paper , we introduce the modi-ed de blockinection as the appropriate measure to be used in the random signal case . we design the representation subspace g to match the multipath signal set s in the modied de blockinection sense . wavelet multiresolution tools are used to facilitate the design ."
  },
  {
    "title": "Cyclic Causal Models with Discrete Variables : Markov Chain Equilibrium Semantics and Sample Ordering .",
    "entities": [
      "structural equation models",
      "equilibrium distribution",
      "markov chain equilibrium semantics",
      "cyclic structural equation models",
      "cyclic causal models",
      "discrete cyclic sems",
      "discrete variables",
      "causal graph",
      "markov chain",
      "sample order",
      "sample ordering",
      "independent noise",
      "noise",
      "semantics"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "cyclic causal models -- USED-FOR -- discrete variables"
    ],
    "abstract": "we analyze the foundations of <method_4> for <otherscientificterm_6> , and compare <method_0> to an alternative <otherscientificterm_13> as the <otherscientificterm_1> of a <otherscientificterm_8> . we show under general conditions , <method_5> can not have <otherscientificterm_11> ; even in the simplest case , <method_3> imply constraints on the <otherscientificterm_12> . we give a formalization of an alternative <otherscientificterm_2> which requires not only the <otherscientificterm_7> , but also a <otherscientificterm_9> . we show how the resulting equilibrium is a function of the <otherscientificterm_10> , both theoretically and empirically .",
    "abstract_og": "we analyze the foundations of cyclic causal models for discrete variables , and compare structural equation models to an alternative semantics as the equilibrium distribution of a markov chain . we show under general conditions , discrete cyclic sems can not have independent noise ; even in the simplest case , cyclic structural equation models imply constraints on the noise . we give a formalization of an alternative markov chain equilibrium semantics which requires not only the causal graph , but also a sample order . we show how the resulting equilibrium is a function of the sample ordering , both theoretically and empirically ."
  },
  {
    "title": "The Shapley Value as a Function of the Quota in Weighted Voting Games .",
    "entities": [
      "player 's power",
      "weighted voting games",
      "shapley value-maximizing quota",
      "voter 's rank",
      "overall weight distribution",
      "quota manipulation",
      "algorithmic issues",
      "shapley value",
      "quota",
      "heuristic"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "algorithmic issues -- USED-FOR -- quota manipulation",
      "voter 's rank -- CONJUNCTION -- overall weight distribution"
    ],
    "abstract": "in <method_1> , each agent has a weight , and a coalition of players is deemed to be winning if its weight meets or exceeds the given <otherscientificterm_8> . an agent 's power in such games is usually measured by her <otherscientificterm_7> , which depends both on the agent 's weight and the <otherscientificterm_8> . -lsb- zuckerman et al. , 2008 -rsb- show that one can alter a <otherscientificterm_0> significantly by modifying the <otherscientificterm_8> , and investigate some of the related <otherscientificterm_6> . in this paper , we answer a number of questions that were left open by -lsb- zuckerman et al. , 2008 -rsb- : we show that , even though deciding whether a <otherscientificterm_8> maximizes or minimizes an agent 's <otherscientificterm_7> is conp-hard , finding a <otherscientificterm_2> is easy . minimizing a <otherscientificterm_0> appears to be more difficult . however , we propose and evaluate a <method_9> for this problem , which takes into account the <otherscientificterm_3> and the <metric_4> . we also explore a number of other <otherscientificterm_6> related to <task_5> .",
    "abstract_og": "in weighted voting games , each agent has a weight , and a coalition of players is deemed to be winning if its weight meets or exceeds the given quota . an agent 's power in such games is usually measured by her shapley value , which depends both on the agent 's weight and the quota . -lsb- zuckerman et al. , 2008 -rsb- show that one can alter a player 's power significantly by modifying the quota , and investigate some of the related algorithmic issues . in this paper , we answer a number of questions that were left open by -lsb- zuckerman et al. , 2008 -rsb- : we show that , even though deciding whether a quota maximizes or minimizes an agent 's shapley value is conp-hard , finding a shapley value-maximizing quota is easy . minimizing a player 's power appears to be more difficult . however , we propose and evaluate a heuristic for this problem , which takes into account the voter 's rank and the overall weight distribution . we also explore a number of other algorithmic issues related to quota manipulation ."
  },
  {
    "title": "Speech analysis by rule extraction from trained artificial neural networks .",
    "entities": [
      "timit and ogi numbers95 speech corpora",
      "neural network feature extractors",
      "english fricative classes",
      "rule extraction technique",
      "transformed feature representation",
      "feature extraction",
      "input features",
      "pa-rameterization method",
      "if-then rules",
      "neural network",
      "class discrim-inability",
      "parameterized signal"
    ],
    "types": "<material> <method> <otherscientificterm> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "neural network feature extractors -- USED-FOR -- feature extraction"
    ],
    "abstract": "a recent development in <task_5> is the use of <method_1> , where the <otherscientificterm_11> is passed through a <method_9> trained to discriminate between targets representing e.g. different phone classes or speakers . while the <method_4> often enhances <otherscientificterm_10> and thereby overall performance , the transformation performed by the <method_9> can not directly be interpreted by human experts . however , explicit knowledge about this transformation could lead to the definition of a simpler function on the <otherscientificterm_6> which might eventually be incorporated into the basic <method_7> . in this paper we investigate a <method_3> for transforming the trained <method_9> into a set of <otherscientificterm_8> capable of representing the transformation in a more transparent way , and apply it to the problem of distinguishing between the <otherscientificterm_2> / f , v / and / s , z / from the <material_0> .",
    "abstract_og": "a recent development in feature extraction is the use of neural network feature extractors , where the parameterized signal is passed through a neural network trained to discriminate between targets representing e.g. different phone classes or speakers . while the transformed feature representation often enhances class discrim-inability and thereby overall performance , the transformation performed by the neural network can not directly be interpreted by human experts . however , explicit knowledge about this transformation could lead to the definition of a simpler function on the input features which might eventually be incorporated into the basic pa-rameterization method . in this paper we investigate a rule extraction technique for transforming the trained neural network into a set of if-then rules capable of representing the transformation in a more transparent way , and apply it to the problem of distinguishing between the english fricative classes / f , v / and / s , z / from the timit and ogi numbers95 speech corpora ."
  },
  {
    "title": "ARCH and GARCH parameter estimation in presence of additive noise using particle methods .",
    "entities": [
      "generalized autoregressive conditional heteroscedasticity models",
      "maximum likelihood estimation",
      "autoregressive conditional heteroscedasticity",
      "gradient based optimization algorithm",
      "gradient descend method",
      "active set method",
      "particle methods",
      "particle filters",
      "stationarity constraints",
      "gradient"
    ],
    "types": "<method> <task> <method> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "maximum likelihood estimation -- USED-FOR -- generalized autoregressive conditional heteroscedasticity models",
      "gradient descend method -- CONJUNCTION -- active set method",
      "particle methods -- USED-FOR -- gradient based optimization algorithm",
      "particle filters -- USED-FOR -- maximum likelihood estimation"
    ],
    "abstract": "in this paper , we propose a new method based on <method_7> for <task_1> of the parameters of <method_2> and <method_0> . our method is based on <method_4> and <method_5> for maximizing the likelihood function over parameters under <otherscientificterm_8> . the <otherscientificterm_9> of the likelihood function of observation given the parameters of the model , which is needed for <method_3> , is estimated using <method_6> . simulation results show the advantage of the proposed method over competing techniques .",
    "abstract_og": "in this paper , we propose a new method based on particle filters for maximum likelihood estimation of the parameters of autoregressive conditional heteroscedasticity and generalized autoregressive conditional heteroscedasticity models . our method is based on gradient descend method and active set method for maximizing the likelihood function over parameters under stationarity constraints . the gradient of the likelihood function of observation given the parameters of the model , which is needed for gradient based optimization algorithm , is estimated using particle methods . simulation results show the advantage of the proposed method over competing techniques ."
  },
  {
    "title": "Optimal dimensionality of metric space for classification .",
    "entities": [
      "discriminant adjacent matrix",
      "curse of dimensionality",
      "finite training samples",
      "high-dimensional patterns",
      "classification task",
      "metric space",
      "euclidean distance",
      "spectral analysis",
      "classification"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "spectral analysis -- USED-FOR -- metric space",
      "metric space -- USED-FOR -- classification"
    ],
    "abstract": "in many real-world applications , <otherscientificterm_6> in the original space is not good due to the <otherscientificterm_1> . in this paper , we propose a new method , called discriminant neighborhood embedding -lrb- dne -rrb- , to learn an appropriate <otherscientificterm_5> for <task_8> given <material_2> . we define a <otherscientificterm_0> in favor of <task_4> , i.e. , neighboring samples in the same class are squeezed but those in different classes are separated as far as possible . the optimal dimensionality of the <otherscientificterm_5> can be estimated by <method_7> in the proposed method , which is of great significance for <otherscientificterm_3> . experiments with various datasets demonstrate the effectiveness of our method .",
    "abstract_og": "in many real-world applications , euclidean distance in the original space is not good due to the curse of dimensionality . in this paper , we propose a new method , called discriminant neighborhood embedding -lrb- dne -rrb- , to learn an appropriate metric space for classification given finite training samples . we define a discriminant adjacent matrix in favor of classification task , i.e. , neighboring samples in the same class are squeezed but those in different classes are separated as far as possible . the optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method , which is of great significance for high-dimensional patterns . experiments with various datasets demonstrate the effectiveness of our method ."
  },
  {
    "title": "Online Learning : Stochastic , Constrained , and Smoothed Adversaries .",
    "entities": [
      "stochastic and non-stochastic assumptions",
      "smoothed online learning scenario",
      "distribution-dependent rademacher complexity",
      "sequential symmetrization approach",
      "classical statistical setting",
      "infinite littlestone dimension",
      "fixed distribution",
      "adversarial scenario",
      "learning theory",
      "minimax value",
      "variation-type bounds",
      "function classes",
      "bounds"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <metric> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "infinite littlestone dimension -- FEATURE-OF -- function classes"
    ],
    "abstract": "learning theory has largely focused on two main learning scenarios : the <method_4> where instances are drawn i.i.d. from a <otherscientificterm_6> , and the <otherscientificterm_7> wherein , at every time step , an adversarially chosen instance is revealed to the player . it can be argued that in the real world neither of these assumptions is reasonable . we define the <otherscientificterm_9> of a game where the adversary is restricted in his moves , capturing <otherscientificterm_0> on data . building on the <method_3> , we define a notion of <metric_2> for the spectrum of problems ranging from i.i.d. to worst-case . the <otherscientificterm_12> let us immediately deduce <otherscientificterm_10> . we study a <otherscientificterm_1> and show that exponentially small amount of noise can make <otherscientificterm_11> with <otherscientificterm_5> learnable .",
    "abstract_og": "learning theory has largely focused on two main learning scenarios : the classical statistical setting where instances are drawn i.i.d. from a fixed distribution , and the adversarial scenario wherein , at every time step , an adversarially chosen instance is revealed to the player . it can be argued that in the real world neither of these assumptions is reasonable . we define the minimax value of a game where the adversary is restricted in his moves , capturing stochastic and non-stochastic assumptions on data . building on the sequential symmetrization approach , we define a notion of distribution-dependent rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case . the bounds let us immediately deduce variation-type bounds . we study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite littlestone dimension learnable ."
  },
  {
    "title": "One Permutation Hashing .",
    "entities": [
      "-lrb- k-permutation -rrb- minwise hashing",
      "sublinear time near-neighbor search",
      "binary data matrix",
      "b-bit minwise hashing",
      "massive binary data",
      "minwise hashing",
      "k-permutation scheme",
      "permutation hashing",
      "permutation scheme",
      "data vector",
      "permuted columns",
      "probability analysis",
      "large-scale learning",
      "pre-processing",
      "search"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <material> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <task>",
    "relations": [
      "b-bit minwise hashing -- USED-FOR -- large-scale learning",
      "permutation scheme -- COMPARE -- -lrb- k-permutation -rrb- minwise hashing"
    ],
    "abstract": "minwise hashing is a standard procedure in the context of <task_14> , for efficiently estimating set similarities in <material_4> such as text . recently , <method_3> has been applied to <task_12> and <task_1> . the major drawback of <method_5> is the expensive <method_13> , as the method requires applying -lrb- e.g. , -rrb- k = 200 to 500 permutations on the data . this paper presents a simple solution called one <method_7> . conceptually , given a <otherscientificterm_2> , we permute the columns once and divide the <otherscientificterm_10> evenly into k bins ; and we store , for each <otherscientificterm_9> , the smallest nonzero location in each bin . the <method_11> illustrates that this one <method_8> should perform similarly to the original <method_0> . our experiments with training svm and logistic regression confirm that one <method_7> can achieve similar -lrb- or even better -rrb- accuracies compared to the <method_6> . see more details in arxiv :1208.1259 .",
    "abstract_og": "minwise hashing is a standard procedure in the context of search , for efficiently estimating set similarities in massive binary data such as text . recently , b-bit minwise hashing has been applied to large-scale learning and sublinear time near-neighbor search . the major drawback of minwise hashing is the expensive pre-processing , as the method requires applying -lrb- e.g. , -rrb- k = 200 to 500 permutations on the data . this paper presents a simple solution called one permutation hashing . conceptually , given a binary data matrix , we permute the columns once and divide the permuted columns evenly into k bins ; and we store , for each data vector , the smallest nonzero location in each bin . the probability analysis illustrates that this one permutation scheme should perform similarly to the original -lrb- k-permutation -rrb- minwise hashing . our experiments with training svm and logistic regression confirm that one permutation hashing can achieve similar -lrb- or even better -rrb- accuracies compared to the k-permutation scheme . see more details in arxiv :1208.1259 ."
  },
  {
    "title": "Automatic Depiction of Spatial Descriptions .",
    "entities": [
      "wip -lrb- words into pictures -rrb- system",
      "natural language spatial predications",
      "conceptual representation of objects",
      "deictic/intrinsic reference frame ambiguity",
      "spatial occupancy models",
      "qualitative layer",
      "field model",
      "quantitative layer",
      "projective prepositions"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "qualitative layer -- USED-FOR -- conceptual representation of objects"
    ],
    "abstract": "a novel combination of ideas from cognitive linguistics and <method_4> in robotics has led to the <method_0> . <method_0> automatically generates depictions of natural language descriptions of indoor scenes . a <otherscientificterm_5> in the <otherscientificterm_2> underlies a mechanism by which alternative depictions arise for qualitatively distinct interpretations , as often occurs as a result of <otherscientificterm_3> . at the same time , a <otherscientificterm_7> , in conjunction with a potential <method_6> of the semantics of <otherscientificterm_8> , is used in the process of capturing the inherently fuzzy character of the meaning of <task_1> .",
    "abstract_og": "a novel combination of ideas from cognitive linguistics and spatial occupancy models in robotics has led to the wip -lrb- words into pictures -rrb- system . wip -lrb- words into pictures -rrb- system automatically generates depictions of natural language descriptions of indoor scenes . a qualitative layer in the conceptual representation of objects underlies a mechanism by which alternative depictions arise for qualitatively distinct interpretations , as often occurs as a result of deictic/intrinsic reference frame ambiguity . at the same time , a quantitative layer , in conjunction with a potential field model of the semantics of projective prepositions , is used in the process of capturing the inherently fuzzy character of the meaning of natural language spatial predications ."
  },
  {
    "title": "AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems .",
    "entities": [
      "event-based hardware vision system",
      "data-driven adaptive real time vision systems",
      "event-based hardware vision system",
      "spatio-temporal trajectories",
      "rotating disk",
      "aer system",
      "object chip"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "event-based hardware vision system -- USED-FOR -- spatio-temporal trajectories",
      "event-based hardware vision system -- HYPONYM-OF -- aer system"
    ],
    "abstract": "we describe the construction and characterization of an <method_0> that learns to classify <otherscientificterm_3> . our characterization so far showed that stimuli of two different shapes on a <otherscientificterm_4> could simultaneously be discriminated and their position extracted at level of the <otherscientificterm_6> . <method_0> is the largest <method_5> yet assembled . <method_0> is a step towards efficient architectures for <task_1> .",
    "abstract_og": "we describe the construction and characterization of an event-based hardware vision system that learns to classify spatio-temporal trajectories . our characterization so far showed that stimuli of two different shapes on a rotating disk could simultaneously be discriminated and their position extracted at level of the object chip . event-based hardware vision system is the largest aer system yet assembled . event-based hardware vision system is a step towards efficient architectures for data-driven adaptive real time vision systems ."
  },
  {
    "title": "Combining multi-party speech and text exchanges over the internet .",
    "entities": [
      "fully situated human-human spoken conversation",
      "spoken and text chat records",
      "spoken information representation",
      "meeting history tools",
      "on-line multi-speaker conversation",
      "video conferencing",
      "magic lounge",
      "spoken conversation",
      "on-line communication",
      "text chat",
      "internet",
      "speech"
    ],
    "types": "<material> <material> <method> <method> <material> <material> <method> <material> <task> <material> <material> <material>",
    "relations": [
      "fully situated human-human spoken conversation -- HYPONYM-OF -- on-line communication",
      "text chat -- CONJUNCTION -- on-line communication",
      "fully situated human-human spoken conversation -- CONJUNCTION -- video conferencing",
      "text chat -- CONJUNCTION -- speech",
      "text chat -- CONJUNCTION -- spoken conversation",
      "on-line multi-speaker conversation -- USED-FOR -- text chat"
    ],
    "abstract": "bilateral or group text chatting over the <material_10> has become a favoured pastime for many people across the world . yet it would seem that , in general , <material_9> is a severely impoverished mode of <task_8> compared to , e.g. , <material_0> , <material_5> , or even speaking over the telephone . this paper explores what happens when <material_4> over the <material_10> is added to <material_9> , creating what may become a widespread mode of communication in the near future . the system used is called the <method_6> . <method_6> offers a multimodal combination of <material_9> and <material_7> for meetings and other encounters among ubiquitous users who may join the communication from workstations , pdas and wap phones . in addition , the system has a series of <method_3> which provide various forms of structure to the <material_1> of the meeting as it unfolds and after the meeting . the paper presents rather clear-cut results on the respective communicative roles of <material_11> and <material_9> from a series of user tests with the system in which different groups of users performed scenarios designed to explore the combined use of <material_9> and <material_11> . the results reported may generalise to a wide range of applications which combine text and <method_2> .",
    "abstract_og": "bilateral or group text chatting over the internet has become a favoured pastime for many people across the world . yet it would seem that , in general , text chat is a severely impoverished mode of on-line communication compared to , e.g. , fully situated human-human spoken conversation , video conferencing , or even speaking over the telephone . this paper explores what happens when on-line multi-speaker conversation over the internet is added to text chat , creating what may become a widespread mode of communication in the near future . the system used is called the magic lounge . magic lounge offers a multimodal combination of text chat and spoken conversation for meetings and other encounters among ubiquitous users who may join the communication from workstations , pdas and wap phones . in addition , the system has a series of meeting history tools which provide various forms of structure to the spoken and text chat records of the meeting as it unfolds and after the meeting . the paper presents rather clear-cut results on the respective communicative roles of speech and text chat from a series of user tests with the system in which different groups of users performed scenarios designed to explore the combined use of text chat and speech . the results reported may generalise to a wide range of applications which combine text and spoken information representation ."
  },
  {
    "title": "Visual tracking via geometric particle filtering on the affine group with optimal importance functions .",
    "entities": [
      "geometrically defined optimal importance function",
      "2-d affine motion",
      "principal component analysis",
      "coordinate-invariant particle filtering",
      "object template",
      "geometric method",
      "visual tracking",
      "video sequence"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <task> <material>",
    "relations": [
      "geometric method -- USED-FOR -- visual tracking",
      "coordinate-invariant particle filtering -- USED-FOR -- 2-d affine motion",
      "geometrically defined optimal importance function -- USED-FOR -- visual tracking"
    ],
    "abstract": "we propose a <method_5> for <task_6> , in which the <otherscientificterm_1> of a given <otherscientificterm_4> is estimated in a <material_7> by means of <method_3> on the 2-d affine group aff -lrb- 2 -rrb- . <task_6> performance is further enhanced through a <otherscientificterm_0> , obtained explicitly via taylor expansion of a <method_2> based measurement function on aff -lrb- 2 -rrb- . the efficiency of our <method_5> to <task_6> is demonstrated via comparative experiments .",
    "abstract_og": "we propose a geometric method for visual tracking , in which the 2-d affine motion of a given object template is estimated in a video sequence by means of coordinate-invariant particle filtering on the 2-d affine group aff -lrb- 2 -rrb- . visual tracking performance is further enhanced through a geometrically defined optimal importance function , obtained explicitly via taylor expansion of a principal component analysis based measurement function on aff -lrb- 2 -rrb- . the efficiency of our geometric method to visual tracking is demonstrated via comparative experiments ."
  },
  {
    "title": "Language identification on code-switching utterances using multiple cues .",
    "entities": [
      "acoustic , prosodic and phonetic features",
      "asr -rrb- error rate reduction",
      "switching linguistic unit",
      "posteriori decision rule",
      "mandarin-taiwanese code-switching utterance",
      "stage lvcsr-based system",
      "language identifier",
      "language model",
      "acoustic model",
      "code-switching speech",
      "duration model",
      "speech recognizer",
      "two-stage framework"
    ],
    "types": "<otherscientificterm> <metric> <method> <otherscientificterm> <material> <method> <method> <method> <method> <material> <method> <method> <method>",
    "relations": [
      "duration model -- CONJUNCTION -- language model",
      "language identifier -- CONJUNCTION -- speech recognizer",
      "asr -rrb- error rate reduction -- EVALUATE-FOR -- stage lvcsr-based system",
      "language identifier -- PART-OF -- two-stage framework"
    ],
    "abstract": "code-switching speech is an utterance containing two or more languages . usually , the <method_2> is in clause or word levels . in this paper , a <method_12> is proposed , containing a <method_6> and then a <method_11> , to evaluate on a <material_4> . in the <method_6> , we use multiple cues including <otherscientificterm_0> . in order to integrate the cues to distinguish one language from another , we used a maximum a <otherscientificterm_3> to connect an <method_8> , a <method_10> and a <method_7> . in the experiments , we have achieved 34.5 % -lrb- lid -rrb- and 17.7 % -lrb- <metric_1> comparing with one <method_5> .",
    "abstract_og": "code-switching speech is an utterance containing two or more languages . usually , the switching linguistic unit is in clause or word levels . in this paper , a two-stage framework is proposed , containing a language identifier and then a speech recognizer , to evaluate on a mandarin-taiwanese code-switching utterance . in the language identifier , we use multiple cues including acoustic , prosodic and phonetic features . in order to integrate the cues to distinguish one language from another , we used a maximum a posteriori decision rule to connect an acoustic model , a duration model and a language model . in the experiments , we have achieved 34.5 % -lrb- lid -rrb- and 17.7 % -lrb- asr -rrb- error rate reduction comparing with one stage lvcsr-based system ."
  },
  {
    "title": "Physical models of the human vocal tract with gel-type material .",
    "entities": [
      "default low tongue height",
      "articulatory training",
      "ultra-malleable model",
      "gel-type tongue",
      "speech science",
      "speech pathology",
      "language learning",
      "talking heads"
    ],
    "types": "<otherscientificterm> <task> <method> <otherscientificterm> <task> <task> <task> <otherscientificterm>",
    "relations": [
      "speech pathology -- CONJUNCTION -- language learning",
      "articulatory training -- USED-FOR -- language learning",
      "speech pathology -- CONJUNCTION -- articulatory training"
    ],
    "abstract": "beginning in 2001 , we have been developing models of the vocal tract to promote a more intuitive understanding of the theories for <task_4> for technical and non-technical students . in this paper , we compared and contrasted four newer models of the <otherscientificterm_7> : our original model with a <otherscientificterm_3> , a similar model including teeth and palate , a third model with teeth and palate having a <otherscientificterm_0> , and a fourth <method_2> made completely of gel material . results are discussed regarding their strengths and weaknesses for educational purposes and <task_1> in <task_5> and <task_6> .",
    "abstract_og": "beginning in 2001 , we have been developing models of the vocal tract to promote a more intuitive understanding of the theories for speech science for technical and non-technical students . in this paper , we compared and contrasted four newer models of the talking heads : our original model with a gel-type tongue , a similar model including teeth and palate , a third model with teeth and palate having a default low tongue height , and a fourth ultra-malleable model made completely of gel material . results are discussed regarding their strengths and weaknesses for educational purposes and articulatory training in speech pathology and language learning ."
  },
  {
    "title": "Theory and design of a class of cosine-modulated non-uniform filter banks .",
    "entities": [
      "pr cosine-modulated nonuniform filter bank",
      "pr nonuniform filter banks",
      "high stopband attenuation",
      "uniform filter bank",
      "filter representations",
      "merging techniques",
      "synthesis section",
      "design procedure",
      "filter quality",
      "spectrum inversion",
      "design examples",
      "arithmetic complexity",
      "protrusion cancellation",
      "filter bank",
      "implementation complexities",
      "decimation",
      "cox",
      "cmfb",
      "simplifications"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <metric> <method> <material> <metric> <otherscientificterm> <method> <metric> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "spectrum inversion -- CONJUNCTION -- filter representations",
      "protrusion cancellation -- HYPONYM-OF -- merging techniques",
      "filter quality -- CONJUNCTION -- implementation complexities",
      "filter representations -- CONJUNCTION -- protrusion cancellation",
      "spectrum inversion -- HYPONYM-OF -- merging techniques"
    ],
    "abstract": "in this paper , the theory and design of a class of <task_0> is proposed . <task_0> is based on a structure previously proposed by <method_16> , where the outputs of a <otherscientificterm_3> is combined or merged by means of the <method_6> of another <method_13> with smaller channel number . <otherscientificterm_18> are imposed on this structure so that the <method_7> can be considerably simplified . due to the use of <method_17> as the original and recombination filter banks , excellent <metric_8> and low design and <metric_14> can be achieved . problems with these <method_5> such as <method_9> , equivalent <method_4> and <otherscientificterm_12> are also addressed . as the merging is performed after the <otherscientificterm_15> , the <metric_11> is lower than other conventional approaches . <material_10> show that <method_1> with <otherscientificterm_2> and low design and <metric_14> can be obtained by the proposed <task_0> .",
    "abstract_og": "in this paper , the theory and design of a class of pr cosine-modulated nonuniform filter bank is proposed . pr cosine-modulated nonuniform filter bank is based on a structure previously proposed by cox , where the outputs of a uniform filter bank is combined or merged by means of the synthesis section of another filter bank with smaller channel number . simplifications are imposed on this structure so that the design procedure can be considerably simplified . due to the use of cmfb as the original and recombination filter banks , excellent filter quality and low design and implementation complexities can be achieved . problems with these merging techniques such as spectrum inversion , equivalent filter representations and protrusion cancellation are also addressed . as the merging is performed after the decimation , the arithmetic complexity is lower than other conventional approaches . design examples show that pr nonuniform filter banks with high stopband attenuation and low design and implementation complexities can be obtained by the proposed pr cosine-modulated nonuniform filter bank ."
  },
  {
    "title": "An Analysis of Inference with the Universum .",
    "entities": [
      "positive and negative data",
      "pattern classification algorithm",
      "data-dependent reduced kernel",
      "fisher discriminant analysis",
      "inductive principle",
      "projected subspace",
      "universum",
      "pca"
    ],
    "types": "<material> <method> <otherscientificterm> <method> <method> <otherscientificterm> <material> <method>",
    "relations": [
      "inductive principle -- USED-FOR -- pattern classification algorithm",
      "fisher discriminant analysis -- CONJUNCTION -- pca"
    ],
    "abstract": "we study a <method_1> which has recently been proposed by vapnik and coworkers . <method_1> builds on a new <method_4> which assumes that in addition to <material_0> , a third class of data is available , termed the <material_6> . we assay the behavior of the <method_1> by establishing links with <method_3> and oriented <method_7> , as well as with an <method_7> in a <otherscientificterm_5> -lrb- or , equivalently , with a <otherscientificterm_2> -rrb- . we also provide experimental results .",
    "abstract_og": "we study a pattern classification algorithm which has recently been proposed by vapnik and coworkers . pattern classification algorithm builds on a new inductive principle which assumes that in addition to positive and negative data , a third class of data is available , termed the universum . we assay the behavior of the pattern classification algorithm by establishing links with fisher discriminant analysis and oriented pca , as well as with an pca in a projected subspace -lrb- or , equivalently , with a data-dependent reduced kernel -rrb- . we also provide experimental results ."
  },
  {
    "title": "Approximate eigenvalue decomposition of para-Hermitian systems through successive FIR paraunitary transformations .",
    "entities": [
      "zeroth order diagonal energy",
      "signal-adapted pu filter bank",
      "para-hermitian system",
      "approximate evd",
      "eigenvalue decomposition",
      "unitary matrices",
      "approximate diag-onalization",
      "hermitian matrix"
    ],
    "types": "<method> <method> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "approximate evd -- USED-FOR -- para-hermitian system",
      "eigenvalue decomposition -- PART-OF -- hermitian matrix",
      "unitary matrices -- FEATURE-OF -- hermitian matrix"
    ],
    "abstract": "the <task_4> of a <otherscientificterm_7> in terms of <otherscientificterm_5> is well known . in this paper , we present an algorithm for the <method_3> of a <method_2> . here , the <otherscientificterm_6> is carried out successively by applying degree-1 finite impulse response -lrb- fir -rrb- paraunitary -lrb- pu -rrb- transformations . the system parameters are chosen to make the <method_0> nondecreasing at each stage . simulation results presented for the design of a <method_1> show close agreement with the behavior of the infinite order principal component fb -lrb- pcfb -rrb- .",
    "abstract_og": "the eigenvalue decomposition of a hermitian matrix in terms of unitary matrices is well known . in this paper , we present an algorithm for the approximate evd of a para-hermitian system . here , the approximate diag-onalization is carried out successively by applying degree-1 finite impulse response -lrb- fir -rrb- paraunitary -lrb- pu -rrb- transformations . the system parameters are chosen to make the zeroth order diagonal energy nondecreasing at each stage . simulation results presented for the design of a signal-adapted pu filter bank show close agreement with the behavior of the infinite order principal component fb -lrb- pcfb -rrb- ."
  },
  {
    "title": "Segmentation and relevance measure for speaker verification .",
    "entities": [
      "nist 2003 speaker evaluation database",
      "speaker recognition systems",
      "non speech frames",
      "decision score",
      "likelihood ratio",
      "segment position",
      "automatic segmentation"
    ],
    "types": "<material> <method> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "decision score -- EVALUATE-FOR -- speaker recognition systems"
    ],
    "abstract": "in all the efficient <method_1> , the <metric_3> is based on the average of the <otherscientificterm_4> computed on each frame of the sentence . except for the <otherscientificterm_2> which are rejected , each one has the same weight in this summation . this paper deals with the study of the speaker relevance of each frame . an <method_6> provides quasi stationary segments of variable length ; a weight is allocated to each frame in function of its <otherscientificterm_5> and a weighted mean of the <otherscientificterm_4> is then computed . experiments are performed with <material_0> . they show that the frames near segment frontiers , that is to say the transient ones , are more speaker relevant than the middle frames of long segments which correspond to the steady parts of the phones .",
    "abstract_og": "in all the efficient speaker recognition systems , the decision score is based on the average of the likelihood ratio computed on each frame of the sentence . except for the non speech frames which are rejected , each one has the same weight in this summation . this paper deals with the study of the speaker relevance of each frame . an automatic segmentation provides quasi stationary segments of variable length ; a weight is allocated to each frame in function of its segment position and a weighted mean of the likelihood ratio is then computed . experiments are performed with nist 2003 speaker evaluation database . they show that the frames near segment frontiers , that is to say the transient ones , are more speaker relevant than the middle frames of long segments which correspond to the steady parts of the phones ."
  },
  {
    "title": "Detection of Concentric Circles for Camera Calibration .",
    "entities": [
      "patterns of pairs of concentric circles",
      "image of the circle center",
      "fully automatic calibration system",
      "plane-based calibration methods",
      "feature detection",
      "pattern features",
      "user interaction",
      "geometric method",
      "construction method",
      "homolog-ical constraints",
      "features",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "user interaction -- USED-FOR -- feature detection",
      "homolog-ical constraints -- USED-FOR -- fully automatic calibration system",
      "accuracy -- EVALUATE-FOR -- fully automatic calibration system",
      "patterns of pairs of concentric circles -- USED-FOR -- fully automatic calibration system",
      "construction method -- USED-FOR -- pattern features"
    ],
    "abstract": "the geometry of <method_3> is well understood , but some <otherscientificterm_6> is often needed in practice for <task_4> . this paper presents a <method_2> that uses <otherscientificterm_0> . the key observation is to introduce a <method_7> that constructs a sequence of points strictly convergent to the <otherscientificterm_1> from an arbitrary point . the <method_2> automatically detects the points of the <otherscientificterm_5> by the <method_8> , and identify them by invariants . <method_2> then takes advantage of <otherscientificterm_9> to consistently and optimally estimate the <otherscientificterm_10> in the image . the experiments demonstrate the ro-bustness and the <metric_11> of the new <method_2> .",
    "abstract_og": "the geometry of plane-based calibration methods is well understood , but some user interaction is often needed in practice for feature detection . this paper presents a fully automatic calibration system that uses patterns of pairs of concentric circles . the key observation is to introduce a geometric method that constructs a sequence of points strictly convergent to the image of the circle center from an arbitrary point . the fully automatic calibration system automatically detects the points of the pattern features by the construction method , and identify them by invariants . fully automatic calibration system then takes advantage of homolog-ical constraints to consistently and optimally estimate the features in the image . the experiments demonstrate the ro-bustness and the accuracy of the new fully automatic calibration system ."
  },
  {
    "title": "Optimal VLC sequence decoding exploiting additional video stream properties .",
    "entities": [
      "maximum likelihood decoder",
      "image and video streaming transmission",
      "vlc sequence unit",
      "prefix-based decoder",
      "awgn channels",
      "vlc codeword",
      "vlc codewords",
      "video decoding",
      "unreliable links",
      "vlc decoders",
      "wireless networks"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <method>",
    "relations": [
      "wireless networks -- HYPONYM-OF -- unreliable links"
    ],
    "abstract": "codes -lrb- vlc -rrb- for <task_1> over <otherscientificterm_8> , such as <method_10> , is a subject of increasing interest . this paper proposes an optimum <method_0> of vlc sequences which exploits additional inherent redundancy in the source information , namely -lrb- i -rrb- the correlation between bits inside a <otherscientificterm_5> as well as -lrb- ii -rrb- the correlation between <otherscientificterm_6> of a <otherscientificterm_2> -lrb- e.g. corresponding to one image block -rrb- . performance results for improving <task_7> over <material_4> are then presented and compared to the <method_3> as well as the recently proposed <method_9> .",
    "abstract_og": "codes -lrb- vlc -rrb- for image and video streaming transmission over unreliable links , such as wireless networks , is a subject of increasing interest . this paper proposes an optimum maximum likelihood decoder of vlc sequences which exploits additional inherent redundancy in the source information , namely -lrb- i -rrb- the correlation between bits inside a vlc codeword as well as -lrb- ii -rrb- the correlation between vlc codewords of a vlc sequence unit -lrb- e.g. corresponding to one image block -rrb- . performance results for improving video decoding over awgn channels are then presented and compared to the prefix-based decoder as well as the recently proposed vlc decoders ."
  },
  {
    "title": "A Joint Model for Discovery of Aspects in Utterances .",
    "entities": [
      "natural language utterance understanding model",
      "unstructured web search query logs",
      "supervised joint learning model",
      "labeled and unlabeled utterances",
      "multi-layer generative approach",
      "natural language utterances",
      "generative process",
      "semantic component",
      "semantic units",
      "cascaded approach"
    ],
    "types": "<method> <material> <method> <material> <method> <material> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "labeled and unlabeled utterances -- USED-FOR -- multi-layer generative approach",
      "unstructured web search query logs -- USED-FOR -- natural language utterance understanding model",
      "multi-layer generative approach -- COMPARE -- cascaded approach"
    ],
    "abstract": "we describe a <method_4> for understanding user actions in <material_5> . our <method_4> uses both <material_3> to jointly learn aspects regarding utterance 's target domain -lrb- e.g. movies -rrb- , intention -lrb- e.g. , finding a movie -rrb- along with other <otherscientificterm_8> -lrb- e.g. , movie name -rrb- . we inject information extracted from <material_1> as prior information to enhance the <method_6> of the <method_0> . using utterances from five domains , our <method_4> shows up to 4.5 % improvement on domain and dialog act performance over <method_9> in which each <method_7> is learned sequentially and a <method_2> -lrb- which requires fully labeled data -rrb- .",
    "abstract_og": "we describe a multi-layer generative approach for understanding user actions in natural language utterances . our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance 's target domain -lrb- e.g. movies -rrb- , intention -lrb- e.g. , finding a movie -rrb- along with other semantic units -lrb- e.g. , movie name -rrb- . we inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model . using utterances from five domains , our multi-layer generative approach shows up to 4.5 % improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model -lrb- which requires fully labeled data -rrb- ."
  },
  {
    "title": "Haar filter banks for I-D space signals .",
    "entities": [
      "notions of signal and filter spaces",
      "1-d space signal processing",
      "symmetric space shift operation",
      "directed time shift operation",
      "1-d space signals",
      "haar filter bank",
      "space signal processing",
      "filters",
      "subspaces",
      "fourier"
    ],
    "types": "<otherscientificterm> <task> <method> <method> <material> <method> <task> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "haar filter bank -- USED-FOR -- 1-d space signal processing",
      "haar filter bank -- USED-FOR -- 1-d space signals",
      "haar filter bank -- USED-FOR -- space signal processing",
      "symmetric space shift operation -- USED-FOR -- haar filter bank"
    ],
    "abstract": "we derive the <method_5> for <material_4> , based on our recently introduced framework for <task_1> , termed this way since <method_5> is built on a <method_2> in contrast to the <method_3> . the framework includes the proper <otherscientificterm_0> , '' z-transform , '' convo-lution , and <method_9> transform , each of which is different from their time equivalents . in this paper , we extend this framework by deriving the proper notions of a <method_5> for <task_6> , and show that <method_5> has a similar yet different form compared to the time case . our derivation also sheds light on the nature of filter banks and makes a case for viewing them as projections on <otherscientificterm_8> rather than as based on <otherscientificterm_7> .",
    "abstract_og": "we derive the haar filter bank for 1-d space signals , based on our recently introduced framework for 1-d space signal processing , termed this way since haar filter bank is built on a symmetric space shift operation in contrast to the directed time shift operation . the framework includes the proper notions of signal and filter spaces , '' z-transform , '' convo-lution , and fourier transform , each of which is different from their time equivalents . in this paper , we extend this framework by deriving the proper notions of a haar filter bank for space signal processing , and show that haar filter bank has a similar yet different form compared to the time case . our derivation also sheds light on the nature of filter banks and makes a case for viewing them as projections on subspaces rather than as based on filters ."
  },
  {
    "title": "Analogy-preserving Semantic Embedding for Visual Object Categorization .",
    "entities": [
      "analogy-preserving semantic embedding",
      "discriminatively learned label embedding",
      "higher-order geometric constraints",
      "attribute-based class descriptions",
      "multi-class categorization tasks",
      "visual recognition datasets",
      "semantic analogies",
      "analogy completion",
      "inter-class confusion",
      "visual recognition",
      "semantic distances",
      "object taxonomy",
      "semantic relationships",
      "convex regularizer",
      "recognition accuracy",
      "analogical parallelograms",
      "pairwise structures"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "object taxonomy -- USED-FOR -- visual recognition",
      "convex regularizer -- USED-FOR -- discriminatively learned label embedding",
      "semantic analogies -- PART-OF -- higher-order geometric constraints",
      "semantic analogies -- USED-FOR -- discriminatively learned label embedding",
      "analogical parallelograms -- HYPONYM-OF -- higher-order geometric constraints",
      "semantic analogies -- USED-FOR -- convex regularizer",
      "visual recognition datasets -- EVALUATE-FOR -- analogy-preserving semantic embedding",
      "recognition accuracy -- CONJUNCTION -- analogy completion",
      "analogical parallelograms -- USED-FOR -- semantic analogies"
    ],
    "abstract": "in <task_4> , knowledge about the classes ' <otherscientificterm_12> can provide valuable information beyond the class labels themselves . however , existing techniques focus on preserving the <otherscientificterm_10> between classes -lrb- e.g. , according to a given <method_11> for <task_9> -rrb- , limiting the influence to <otherscientificterm_16> . we propose to model analogies that reflect the relationships between multiple pairs of classes simultaneously , in the form '' p is to q , as r is to s '' . we translate <otherscientificterm_6> into <otherscientificterm_2> called <otherscientificterm_15> , and use <otherscientificterm_6> in a novel <method_13> for a <otherscientificterm_1> . furthermore , we show how to discover analogies from <otherscientificterm_3> , and how to prioritize those likely to reduce <otherscientificterm_8> . evaluating our <method_0> on two <material_5> , we demonstrate clear improvements over existing approaches , both in terms of <metric_14> and <task_7> .",
    "abstract_og": "in multi-class categorization tasks , knowledge about the classes ' semantic relationships can provide valuable information beyond the class labels themselves . however , existing techniques focus on preserving the semantic distances between classes -lrb- e.g. , according to a given object taxonomy for visual recognition -rrb- , limiting the influence to pairwise structures . we propose to model analogies that reflect the relationships between multiple pairs of classes simultaneously , in the form '' p is to q , as r is to s '' . we translate semantic analogies into higher-order geometric constraints called analogical parallelograms , and use semantic analogies in a novel convex regularizer for a discriminatively learned label embedding . furthermore , we show how to discover analogies from attribute-based class descriptions , and how to prioritize those likely to reduce inter-class confusion . evaluating our analogy-preserving semantic embedding on two visual recognition datasets , we demonstrate clear improvements over existing approaches , both in terms of recognition accuracy and analogy completion ."
  },
  {
    "title": "A General Framework for Generating Multivariate Explanations in Bayesian Networks .",
    "entities": [
      "maximum a posteriori assignment",
      "most relevant explanation",
      "most probable explanation",
      "reversible jump mcmc",
      "generalized bayes factor",
      "relevance measure",
      "generating explanations",
      "approximate algorithm",
      "explanation methods",
      "simulated annealing",
      "bayesian networks",
      "pri-ori"
    ],
    "types": "<method> <method> <method> <method> <method> <metric> <task> <method> <method> <task> <method> <method>",
    "relations": [
      "maximum a posteriori assignment -- HYPONYM-OF -- bayesian networks",
      "explanation methods -- PART-OF -- bayesian networks",
      "approximate algorithm -- USED-FOR -- most relevant explanation",
      "simulated annealing -- USED-FOR -- approximate algorithm",
      "reversible jump mcmc -- CONJUNCTION -- simulated annealing",
      "maximum a posteriori assignment -- HYPONYM-OF -- explanation methods",
      "maximum a posteriori assignment -- CONJUNCTION -- most probable explanation",
      "reversible jump mcmc -- USED-FOR -- approximate algorithm",
      "most probable explanation -- HYPONYM-OF -- bayesian networks"
    ],
    "abstract": "many existing <method_8> in <method_10> , such as <method_0> and <method_2> , generate complete assignments for target variables . a <method_11> , the set of target variables is often large , but only a few of them may be most relevant in explaining given evidence . <task_6> with all the target variables is hence not always desirable . this paper addresses the problem by proposing a new framework called <method_1> , which aims to automatically identify the most relevant target variables . we will also discuss in detail a specific instance of the framework that uses <method_4> as the <metric_5> . finally we will propose an <method_7> based on <method_3> and <task_9> to solve <method_1> . empirical results show that the new <method_7> typically finds much more concise explanations than existing methods .",
    "abstract_og": "many existing explanation methods in bayesian networks , such as maximum a posteriori assignment and most probable explanation , generate complete assignments for target variables . a pri-ori , the set of target variables is often large , but only a few of them may be most relevant in explaining given evidence . generating explanations with all the target variables is hence not always desirable . this paper addresses the problem by proposing a new framework called most relevant explanation , which aims to automatically identify the most relevant target variables . we will also discuss in detail a specific instance of the framework that uses generalized bayes factor as the relevance measure . finally we will propose an approximate algorithm based on reversible jump mcmc and simulated annealing to solve most relevant explanation . empirical results show that the new approximate algorithm typically finds much more concise explanations than existing methods ."
  },
  {
    "title": "The robustness of an almost-parsing language model given errorful training data .",
    "entities": [
      "word error rate",
      "inconsistent and flawed training data",
      "almost-parsing 1 language model",
      "uniform linguistic structure",
      "parser-based language models",
      "syntactic constraints",
      "lvcsr tasks",
      "lexical features"
    ],
    "types": "<metric> <material> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "lexical features -- PART-OF -- uniform linguistic structure",
      "lexical features -- CONJUNCTION -- syntactic constraints",
      "syntactic constraints -- PART-OF -- uniform linguistic structure"
    ],
    "abstract": "an <method_2> has been developed -lsb- 1 -rsb- that provides a framework for tightly integrating multiple knowledge sources . <otherscientificterm_7> and <otherscientificterm_5> are integrated into a <otherscientificterm_3> -lrb- called a superarv -rrb- that is associated with words in the lexicon . the <method_2> has been found able to reduce perplexity and <metric_0> compared to trigram , part-of-speech-based , and <method_4> on the darpa wall street journal -lrb- wsj -rrb- csr task . in this paper we further investigate the robustness of the <method_2> to possibly <material_1> , as well as its ability to scale up to sophisticated <task_6> by comparing performance on the darpa wsj and hub4 -lrb- broadcast news -rrb- csr tasks .",
    "abstract_og": "an almost-parsing 1 language model has been developed -lsb- 1 -rsb- that provides a framework for tightly integrating multiple knowledge sources . lexical features and syntactic constraints are integrated into a uniform linguistic structure -lrb- called a superarv -rrb- that is associated with words in the lexicon . the almost-parsing 1 language model has been found able to reduce perplexity and word error rate compared to trigram , part-of-speech-based , and parser-based language models on the darpa wall street journal -lrb- wsj -rrb- csr task . in this paper we further investigate the robustness of the almost-parsing 1 language model to possibly inconsistent and flawed training data , as well as its ability to scale up to sophisticated lvcsr tasks by comparing performance on the darpa wsj and hub4 -lrb- broadcast news -rrb- csr tasks ."
  },
  {
    "title": "Finite-State Controllers Based on Mealy Machines for Centralized and Decentralized POMDPs .",
    "entities": [
      "centralized and decentralized pomdps",
      "existing controller-based approaches",
      "solution methods",
      "mealy machine",
      "controller-based algorithms",
      "controller-based approaches",
      "mealy machines",
      "moore machines",
      "automata"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <method> <method>",
    "relations": [
      "mealy machines -- USED-FOR -- controller-based algorithms",
      "controller-based algorithms -- COMPARE -- existing controller-based approaches",
      "existing controller-based approaches -- USED-FOR -- centralized and decentralized pomdps",
      "mealy machine -- HYPONYM-OF -- automata",
      "moore machines -- HYPONYM-OF -- automata",
      "mealy machines -- COMPARE -- moore machines",
      "automata -- USED-FOR -- existing controller-based approaches",
      "existing controller-based approaches -- COMPARE -- existing controller-based approaches"
    ],
    "abstract": "existing <method_5> for <method_0> are based on <method_8> with output known as <method_7> . in this paper , we show that several advantages can be gained by utilizing another type of <method_8> , the <method_3> . <method_6> are more powerful than <method_7> , provide a richer structure that can be exploited by <method_2> , and can be easily incorporated into current <method_5> . to demonstrate this , we adapted some existing <method_4> to use <method_6> and obtained results on a set of benchmark domains . the <method_1> always outperformed the <method_1> and often out-performed the state-of-the-art <method_4> for both <method_0> . these findings provide fresh and general insights for the improvement of existing <method_4> and the development of new <method_1> .",
    "abstract_og": "existing controller-based approaches for centralized and decentralized pomdps are based on automata with output known as moore machines . in this paper , we show that several advantages can be gained by utilizing another type of automata , the mealy machine . mealy machines are more powerful than moore machines , provide a richer structure that can be exploited by solution methods , and can be easily incorporated into current controller-based approaches . to demonstrate this , we adapted some existing controller-based algorithms to use mealy machines and obtained results on a set of benchmark domains . the existing controller-based approaches always outperformed the existing controller-based approaches and often out-performed the state-of-the-art controller-based algorithms for both centralized and decentralized pomdps . these findings provide fresh and general insights for the improvement of existing controller-based algorithms and the development of new existing controller-based approaches ."
  },
  {
    "title": "The Parameterized Complexity of Global Constraints .",
    "entities": [
      "constraint programming",
      "value symmetry",
      "symmetry breaking",
      "constraint propagation",
      "dynamic program",
      "natural parameters",
      "cycle cutset",
      "global constraints",
      "parameterized complexity",
      "decomposition",
      "backdoor",
      "symmetries"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "parameterized complexity -- USED-FOR -- constraint programming",
      "symmetry breaking -- HYPONYM-OF -- constraint programming",
      "parameterized complexity -- USED-FOR -- global constraints"
    ],
    "abstract": "we argue that <otherscientificterm_8> is a useful tool with which to study <otherscientificterm_7> . in particular , we show that many <otherscientificterm_7> which are intractable to propagate completely have <otherscientificterm_5> which make them fixed-parameter tractable and which are easy to compute . this tractability tends either to be the result of a simple <method_4> or of a <method_9> which has a strong <otherscientificterm_10> of bounded size . this strong <otherscientificterm_10> is often a <otherscientificterm_6> . we also show that <otherscientificterm_8> can be used to study other aspects of <method_0> like <task_2> . for instance , we prove that <otherscientificterm_1> is fixed-parameter tractable to break in the number of <otherscientificterm_11> . finally , we argue that <otherscientificterm_8> can be used to derive results about the approximability of <method_3> .",
    "abstract_og": "we argue that parameterized complexity is a useful tool with which to study global constraints . in particular , we show that many global constraints which are intractable to propagate completely have natural parameters which make them fixed-parameter tractable and which are easy to compute . this tractability tends either to be the result of a simple dynamic program or of a decomposition which has a strong backdoor of bounded size . this strong backdoor is often a cycle cutset . we also show that parameterized complexity can be used to study other aspects of constraint programming like symmetry breaking . for instance , we prove that value symmetry is fixed-parameter tractable to break in the number of symmetries . finally , we argue that parameterized complexity can be used to derive results about the approximability of constraint propagation ."
  },
  {
    "title": "Memory efficient JPEG2000 architecture with stripe pipeline scheme .",
    "entities": [
      "level switch discrete wavelet transform",
      "code-block switch embedded block coding",
      "jpeg 2000 architectures",
      "stripe pipeline scheme",
      "memory issue",
      "inter-leaved scheme",
      "code-blocks",
      "ls-dwt"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <otherscientificterm> <method>",
    "relations": [
      "level switch discrete wavelet transform -- CONJUNCTION -- code-block switch embedded block coding",
      "level switch discrete wavelet transform -- PART-OF -- stripe pipeline scheme",
      "code-block switch embedded block coding -- USED-FOR -- code-blocks",
      "memory issue -- PART-OF -- jpeg 2000 architectures"
    ],
    "abstract": "memory issue is the most critical problem for a high performance jpeg 2000 <method_3> . the <task_4> occupies more than 50 % of area in conventional <method_2> . to solve this problem , we propose a <method_3> . for this <method_3> , a <method_0> and a <method_1> are proposed . with small additional memory , the <method_7> and the <method_1> can process multiple levels and <otherscientificterm_6> in parallel by an <method_5> . as a result of above techniques , the overall memory requirements of the proposed <method_3> can be reduced to only 8.5 % comparing with conventional architectures .",
    "abstract_og": "memory issue is the most critical problem for a high performance jpeg 2000 stripe pipeline scheme . the memory issue occupies more than 50 % of area in conventional jpeg 2000 architectures . to solve this problem , we propose a stripe pipeline scheme . for this stripe pipeline scheme , a level switch discrete wavelet transform and a code-block switch embedded block coding are proposed . with small additional memory , the ls-dwt and the code-block switch embedded block coding can process multiple levels and code-blocks in parallel by an inter-leaved scheme . as a result of above techniques , the overall memory requirements of the proposed stripe pipeline scheme can be reduced to only 8.5 % comparing with conventional architectures ."
  },
  {
    "title": "On the consistency of l1-norm based ar parameters estimation in a sparse multipath environment .",
    "entities": [
      "symmetric finite impulse response filter",
      "autoregressive process",
      "sparse multipath environment",
      "ar process",
      "ar parameters",
      "0-norm minimization",
      "1-norm minimization",
      "reflection gains",
      "multipath reflections",
      "sparsity",
      "poles",
      "consistency",
      "zeros"
    ],
    "types": "<method> <method> <otherscientificterm> <task> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "sparse multipath environment -- USED-FOR -- autoregressive process",
      "ar parameters -- CONJUNCTION -- reflection gains"
    ],
    "abstract": "when an <method_1> is observed through a <otherscientificterm_2> , its <otherscientificterm_4> may be estimated by searching for a <method_0> , which , when convolved with the observed signal 's autocorrelation sequence , yields the sparsest output . the <otherscientificterm_12> of that filter would then correspond to the <otherscientificterm_10> of the <task_3> . when the 0-norm of the output is used as a measure of its <metric_9> , <metric_11> of the resulting estimate -lrb- under some simple conditions -rrb- is readily obtained . however , due to problematic aspects of <task_5> , it is often more convenient to resort to <method_6> . a question of major interest in this context is whether -lrb- and if so , under what conditions -rrb- <metric_11> of the resulting estimate is maintained . by analyzing the perturbations of the 1-norm about the desired solution , we derive -lrb- and illustrate -rrb- specific conditions for <metric_11> . we show that when the <otherscientificterm_8> are sufficiently sparse , <metric_11> is guaranteed for a very wide range of <otherscientificterm_4> and <otherscientificterm_7> .",
    "abstract_og": "when an autoregressive process is observed through a sparse multipath environment , its ar parameters may be estimated by searching for a symmetric finite impulse response filter , which , when convolved with the observed signal 's autocorrelation sequence , yields the sparsest output . the zeros of that filter would then correspond to the poles of the ar process . when the 0-norm of the output is used as a measure of its sparsity , consistency of the resulting estimate -lrb- under some simple conditions -rrb- is readily obtained . however , due to problematic aspects of 0-norm minimization , it is often more convenient to resort to 1-norm minimization . a question of major interest in this context is whether -lrb- and if so , under what conditions -rrb- consistency of the resulting estimate is maintained . by analyzing the perturbations of the 1-norm about the desired solution , we derive -lrb- and illustrate -rrb- specific conditions for consistency . we show that when the multipath reflections are sufficiently sparse , consistency is guaranteed for a very wide range of ar parameters and reflection gains ."
  },
  {
    "title": "Keeping Flexible Active Contours on Track using Metropolis Updates .",
    "entities": [
      "computationally burdensome number of particles",
      "active contour '' framework",
      "likelihood-weighted particle filtering",
      "flexible contours",
      "video sequences",
      "metropolis algorithm",
      "condensation algorithm",
      "contour distribution",
      "video sequence",
      "condensation",
      "contours",
      "shape-subspace",
      "condensation"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <material> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "computationally burdensome number of particles -- USED-FOR -- contour distribution",
      "metropolis algorithm -- COMPARE -- condensation algorithm",
      "video sequence -- USED-FOR -- condensation",
      "metropolis algorithm -- USED-FOR -- condensation",
      "condensation -- HYPONYM-OF -- likelihood-weighted particle filtering",
      "video sequence -- USED-FOR -- metropolis algorithm"
    ],
    "abstract": "condensation , a form of <method_2> , has been successfully used to infer the shapes of highly constrained '' active '' <otherscientificterm_10> in <material_4> . however , when the <otherscientificterm_10> are highly flexible -lrb- e.g. for tracking fingers of a hand -rrb- , a <otherscientificterm_0> is needed to successfully approximate the <otherscientificterm_7> . we show how the <method_5> can be used to update a particle set representing a distribution over <otherscientificterm_10> at each frame in a <material_8> . we compare this <method_5> to <otherscientificterm_12> using a <material_8> that requires highly <otherscientificterm_3> , and show that the new <method_5> performs dramatically better that the <method_6> . we discuss the incorporation of this <method_5> into the '' <method_1> where a <otherscientificterm_11> is used constrain shape variation .",
    "abstract_og": "condensation , a form of likelihood-weighted particle filtering , has been successfully used to infer the shapes of highly constrained '' active '' contours in video sequences . however , when the contours are highly flexible -lrb- e.g. for tracking fingers of a hand -rrb- , a computationally burdensome number of particles is needed to successfully approximate the contour distribution . we show how the metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence . we compare this metropolis algorithm to condensation using a video sequence that requires highly flexible contours , and show that the new metropolis algorithm performs dramatically better that the condensation algorithm . we discuss the incorporation of this metropolis algorithm into the '' active contour '' framework where a shape-subspace is used constrain shape variation ."
  },
  {
    "title": "Pliable Rejection Sampling .",
    "entities": [
      "pliable rejection sampling",
      "adaptive rejection sampling methods",
      "kernel estimator",
      "rejection rate",
      "sampling proposal",
      "rejection sampling",
      "sampling"
    ],
    "types": "<method> <method> <method> <metric> <method> <method> <task>",
    "relations": [
      "kernel estimator -- USED-FOR -- sampling proposal"
    ],
    "abstract": "rejection <task_6> is a technique for <task_6> from difficult distributions . however , its use is limited due to a high <metric_3> . common <method_1> either work only for very specific distributions or without performance guarantees . in this paper , we present <method_0> , a new approach to <method_5> , where we learn the <method_4> using a <method_2> . since our method builds on <method_5> , the samples obtained are with high probability i.i.d. and distributed according to f. moreover , <method_0> comes with a guarantee on the number of accepted samples .",
    "abstract_og": "rejection sampling is a technique for sampling from difficult distributions . however , its use is limited due to a high rejection rate . common adaptive rejection sampling methods either work only for very specific distributions or without performance guarantees . in this paper , we present pliable rejection sampling , a new approach to rejection sampling , where we learn the sampling proposal using a kernel estimator . since our method builds on rejection sampling , the samples obtained are with high probability i.i.d. and distributed according to f. moreover , pliable rejection sampling comes with a guarantee on the number of accepted samples ."
  },
  {
    "title": "Product Sparse Coding .",
    "entities": [
      "codebook size k",
      "sparse coding problem",
      "image retrieval",
      "computer vision",
      "sparse coding",
      "time complexity",
      "computational cost",
      "codebook size",
      "image classification",
      "sparse coding"
    ],
    "types": "<otherscientificterm> <task> <task> <task> <method> <metric> <metric> <otherscientificterm> <task> <task>",
    "relations": [
      "sparse coding -- PART-OF -- computer vision",
      "image classification -- CONJUNCTION -- image retrieval"
    ],
    "abstract": "sparse coding is a widely involved technique in <task_3> . however , the expensive <metric_6> can hamper its applications , typically when the <otherscientificterm_7> must be limited due to concerns on running time . in this paper , we study a special case of <task_9> in which the codebook is a cartesian product of two subcodebooks . we present algorithms to decompose this <task_1> into smaller subproblems , which can be separately solved . our solution , named as product sparse coding -lrb- psc -rrb- , reduces the <metric_5> from o -lrb- k -rrb- to o -lrb- \u221a k -rrb- in the <otherscientificterm_0> . in practice , this can be 20-100 \u00d7 faster than standard <task_9> . in experiments we demonstrate the efficiency and quality of this method on the applications of <task_8> and <task_2> .",
    "abstract_og": "sparse coding is a widely involved technique in computer vision . however , the expensive computational cost can hamper its applications , typically when the codebook size must be limited due to concerns on running time . in this paper , we study a special case of sparse coding in which the codebook is a cartesian product of two subcodebooks . we present algorithms to decompose this sparse coding problem into smaller subproblems , which can be separately solved . our solution , named as product sparse coding -lrb- psc -rrb- , reduces the time complexity from o -lrb- k -rrb- to o -lrb- \u221a k -rrb- in the codebook size k . in practice , this can be 20-100 \u00d7 faster than standard sparse coding . in experiments we demonstrate the efficiency and quality of this method on the applications of image classification and image retrieval ."
  },
  {
    "title": "Automatic Language Identification in music videos with low level audio and visual features .",
    "entities": [
      "automatic language identification",
      "corpus of 25000 music videos",
      "linear svm classifiers",
      "bag-of-words '' approach",
      "automatic lid",
      "music videos",
      "accuracy"
    ],
    "types": "<task> <material> <method> <method> <task> <material> <metric>",
    "relations": [
      "linear svm classifiers -- USED-FOR -- automatic lid",
      "corpus of 25000 music videos -- EVALUATE-FOR -- bag-of-words '' approach",
      "accuracy -- EVALUATE-FOR -- bag-of-words '' approach"
    ],
    "abstract": "automatic language identification -lrb- lid -rrb- in music has received significantly less attention than lid in speech . here , we study the problem of lid in <material_5> uploaded on youtube . we use a '' <method_3> based on state-of-the-art content based audiovisual features and <method_2> for <task_4> . our <method_3> obtains 48 % <metric_6> for a <material_1> and 25 different languages .",
    "abstract_og": "automatic language identification -lrb- lid -rrb- in music has received significantly less attention than lid in speech . here , we study the problem of lid in music videos uploaded on youtube . we use a '' bag-of-words '' approach based on state-of-the-art content based audiovisual features and linear svm classifiers for automatic lid . our bag-of-words '' approach obtains 48 % accuracy for a corpus of 25000 music videos and 25 different languages ."
  },
  {
    "title": "A hybrid method for deconvolution of Bernoulli-Gaussian processes .",
    "entities": [
      "stochastic inference methods",
      "parameter estimation methods",
      "bernoulli-gaussian prior model",
      "state inference",
      "deterministic inference",
      "student-t model",
      "hybrid method",
      "bernoulli-gaussian process",
      "simulation studies",
      "sem",
      "signal",
      "filter"
    ],
    "types": "<method> <method> <method> <task> <method> <method> <method> <material> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "signal -- CONJUNCTION -- filter",
      "student-t model -- USED-FOR -- bernoulli-gaussian prior model",
      "sem -- HYPONYM-OF -- stochastic inference methods",
      "deterministic inference -- USED-FOR -- bernoulli-gaussian prior model"
    ],
    "abstract": "we investigate a <method_6> which improves the quality of <task_3> and parameter estimation in blind decon-volution of a sparse source modeled by a <material_7> . in this problem , when both the <otherscientificterm_10> and the <otherscientificterm_11> are jointly estimated , the true posterior is typically highly multimodal . therefore , when not properly initialized , standard <method_0> , -lrb- mcem , <method_9> or saem -rrb- , tend to get stuck and suffer from poor convergence . in our <method_6> , we first relax the <method_2> by a <method_5> . our simulations suggest that <method_4> in the <method_2> is not only efficient , but also provides a very good initialization for the <method_2> . we provide <method_8> that compare the results obtained with and without our <method_2> for several combinations of <task_3> and <method_1> used for the <method_2> .",
    "abstract_og": "we investigate a hybrid method which improves the quality of state inference and parameter estimation in blind decon-volution of a sparse source modeled by a bernoulli-gaussian process . in this problem , when both the signal and the filter are jointly estimated , the true posterior is typically highly multimodal . therefore , when not properly initialized , standard stochastic inference methods , -lrb- mcem , sem or saem -rrb- , tend to get stuck and suffer from poor convergence . in our hybrid method , we first relax the bernoulli-gaussian prior model by a student-t model . our simulations suggest that deterministic inference in the bernoulli-gaussian prior model is not only efficient , but also provides a very good initialization for the bernoulli-gaussian prior model . we provide simulation studies that compare the results obtained with and without our bernoulli-gaussian prior model for several combinations of state inference and parameter estimation methods used for the bernoulli-gaussian prior model ."
  },
  {
    "title": "An HMM/n-gram-based linguistic processing approach for Mandarin spoken document retrieval .",
    "entities": [
      "minimum classification error training algorithms",
      "vector space model approach",
      "mandarin spoken document retrieval",
      "hmm/n-gram-based linguistic processing approach",
      "spoken document retrieval",
      "retrieval capabilities",
      "word-and syllable-levels",
      "discrimination capabilities",
      "hmms"
    ],
    "types": "<method> <method> <task> <method> <task> <metric> <otherscientificterm> <metric> <method>",
    "relations": [
      "hmm/n-gram-based linguistic processing approach -- USED-FOR -- mandarin spoken document retrieval"
    ],
    "abstract": "in this paper an <method_3> for <task_2> is presented . the underlying characteristics and different structures of this <method_3> were extensively investigated . the <metric_5> were verified by tests with indexing features of word-and syllable -lrb- subword -rrb- - levels and comparison with the conventional <method_1> . to further improve the <metric_7> of the <method_8> , both the expectation-maximization -lrb- em -rrb- and <method_0> were introduced in training . the information fusion of indexing features of <otherscientificterm_6> was also investigated . the <task_4> experiments were performed on the topic detection and tracking corpora -lrb- tdt-2 and tdt-3 -rrb- . very encouraging retrieval performance was obtained .",
    "abstract_og": "in this paper an hmm/n-gram-based linguistic processing approach for mandarin spoken document retrieval is presented . the underlying characteristics and different structures of this hmm/n-gram-based linguistic processing approach were extensively investigated . the retrieval capabilities were verified by tests with indexing features of word-and syllable -lrb- subword -rrb- - levels and comparison with the conventional vector space model approach . to further improve the discrimination capabilities of the hmms , both the expectation-maximization -lrb- em -rrb- and minimum classification error training algorithms were introduced in training . the information fusion of indexing features of word-and syllable-levels was also investigated . the spoken document retrieval experiments were performed on the topic detection and tracking corpora -lrb- tdt-2 and tdt-3 -rrb- . very encouraging retrieval performance was obtained ."
  },
  {
    "title": "Adaptive step-size parameter control for real-world blind source separation .",
    "entities": [
      "blind source separation",
      "complex gradient theory",
      "newton 's method",
      "adaptive step-size control",
      "robot audition systems",
      "fixed step-size parameter",
      "simultaneous speeches",
      "separation matrix",
      "honda asimo",
      "step-size parameter",
      "environmental changes",
      "real-world applications"
    ],
    "types": "<task> <method> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "blind source separation -- USED-FOR -- real-world applications",
      "complex gradient theory -- USED-FOR -- newton 's method",
      "robot audition systems -- HYPONYM-OF -- real-world applications",
      "step-size parameter -- USED-FOR -- separation matrix"
    ],
    "abstract": "this paper describes a method to adaptively control a <otherscientificterm_9> which is used for updating a <otherscientificterm_7> to extract a target sound source accurately in <task_0> . the design of the <otherscientificterm_9> is essential when we apply <task_0> to <task_11> such as <method_4> , because the surrounding environment dynamically changes in the real world . it is common to use a <otherscientificterm_5> that is obtained empirically . however , due to <otherscientificterm_10> and noises , the performance of <task_0> with the <otherscientificterm_5> deteriorates and the <otherscientificterm_7> sometimes diverges . we propose a general method that allows <method_3> . the proposed method is an extension of <method_2> utilizing a <method_1> and is applicable to any <task_0> . actually , we applied it to six types of <task_0> for an 8 ch microphone array embedded in <material_8> . experimental results show that the proposed method improves the performance of these six <task_0> through experiments of separation and recognition for two <material_6> .",
    "abstract_og": "this paper describes a method to adaptively control a step-size parameter which is used for updating a separation matrix to extract a target sound source accurately in blind source separation . the design of the step-size parameter is essential when we apply blind source separation to real-world applications such as robot audition systems , because the surrounding environment dynamically changes in the real world . it is common to use a fixed step-size parameter that is obtained empirically . however , due to environmental changes and noises , the performance of blind source separation with the fixed step-size parameter deteriorates and the separation matrix sometimes diverges . we propose a general method that allows adaptive step-size control . the proposed method is an extension of newton 's method utilizing a complex gradient theory and is applicable to any blind source separation . actually , we applied it to six types of blind source separation for an 8 ch microphone array embedded in honda asimo . experimental results show that the proposed method improves the performance of these six blind source separation through experiments of separation and recognition for two simultaneous speeches ."
  },
  {
    "title": "GPU-accelerated scene categorization under multiscale category-specific visual word strategy .",
    "entities": [
      "bag of word models",
      "calculation of euclidean distance",
      "multimedia information retrieval",
      "visual word quantization",
      "scene categorization",
      "computer vision",
      "multimedia analysis",
      "feature clustering",
      "gpu implementations"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <task> <task> <task> <method> <method>",
    "relations": [
      "feature clustering -- CONJUNCTION -- visual word quantization",
      "gpu implementations -- USED-FOR -- multimedia analysis",
      "multimedia analysis -- EVALUATE-FOR -- gpu implementations",
      "gpu implementations -- USED-FOR -- multimedia information retrieval",
      "computer vision -- CONJUNCTION -- multimedia information retrieval",
      "gpu implementations -- USED-FOR -- computer vision"
    ],
    "abstract": "we utilize <method_8> to accelerate an essential component for <task_5> and <task_2> , i.e. <task_4> . to construct <method_0> , we modify <otherscientificterm_1> so that <method_7> and <otherscientificterm_3> can be processed in a parallel manner . we provide details of <method_8> and conduct comprehensive experiments to verify the efficiency of <method_8> on <task_6> .",
    "abstract_og": "we utilize gpu implementations to accelerate an essential component for computer vision and multimedia information retrieval , i.e. scene categorization . to construct bag of word models , we modify calculation of euclidean distance so that feature clustering and visual word quantization can be processed in a parallel manner . we provide details of gpu implementations and conduct comprehensive experiments to verify the efficiency of gpu implementations on multimedia analysis ."
  },
  {
    "title": "Learning semantic hierarchy with distributed representations for unsupervised spoken language understanding .",
    "entities": [
      "frame-semantic based unsupervised slot induction approach",
      "unsupervised ontology learning",
      "hierarchical semantic structure",
      "high-level semantic estimation",
      "spoken dialogue systems",
      "hierarchical agglomerative clustering",
      "coherent semantic hierarchy",
      "high-level semantic information",
      "slot induction",
      "topically-related slots",
      "slot importance",
      "location-related information",
      "semantic decoder",
      "semantic understanding",
      "cross-slot relations",
      "hand-crafted grammars",
      "unlabelled conversations",
      "f-measure"
    ],
    "types": "<method> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <metric>",
    "relations": [
      "frame-semantic based unsupervised slot induction approach -- USED-FOR -- topically-related slots",
      "unsupervised ontology learning -- USED-FOR -- semantic understanding",
      "hierarchical agglomerative clustering -- USED-FOR -- frame-semantic based unsupervised slot induction approach",
      "spoken dialogue systems -- USED-FOR -- semantic understanding",
      "unsupervised ontology learning -- USED-FOR -- spoken dialogue systems",
      "f-measure -- EVALUATE-FOR -- semantic decoder",
      "hand-crafted grammars -- USED-FOR -- semantic decoder"
    ],
    "abstract": "we study the problem of <method_1> for <task_13> in <method_4> , in particular , learning the <otherscientificterm_2> from the data . given <otherscientificterm_16> , we augment a <method_0> with <method_5> to merge <otherscientificterm_9> -lrb- e.g. , both slots '' direction '' and '' locale '' convey <otherscientificterm_11> -rrb- for building a <otherscientificterm_6> , and then estimate the <otherscientificterm_10> at different levels . the <task_3> involves not only within-slot but also <otherscientificterm_14> . the experiments show that <otherscientificterm_7> can accurately estimate the prominence of slots , significantly improving the <task_8> performance ; furthermore , a <method_12> trained on the data with automatically extracted slots achieves about 68 % <metric_17> , which is close to the one from <method_15> .",
    "abstract_og": "we study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems , in particular , learning the hierarchical semantic structure from the data . given unlabelled conversations , we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots -lrb- e.g. , both slots '' direction '' and '' locale '' convey location-related information -rrb- for building a coherent semantic hierarchy , and then estimate the slot importance at different levels . the high-level semantic estimation involves not only within-slot but also cross-slot relations . the experiments show that high-level semantic information can accurately estimate the prominence of slots , significantly improving the slot induction performance ; furthermore , a semantic decoder trained on the data with automatically extracted slots achieves about 68 % f-measure , which is close to the one from hand-crafted grammars ."
  },
  {
    "title": "Quantization Noise Shaping for Information Maximizing ADCs .",
    "entities": [
      "common time and frequency interleaved multi channel structures",
      "wireline and wireless style channels",
      "analog and digital worlds",
      "non constant information",
      "fixed power budget",
      "quantization noise shaping",
      "loop filter design",
      "con-figurable adc",
      "digital domain",
      "quantization noise",
      "adcs"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <method> <material> <otherscientificterm> <material>",
    "relations": [
      "loop filter design -- USED-FOR -- quantization noise shaping",
      "non constant information -- FEATURE-OF -- con-figurable adc"
    ],
    "abstract": "adcs sit at the interface of the <otherscientificterm_2> and fundamentally determine what information is available in the <material_8> for processing . this paper shows that a <method_7> can be designed for signals with <otherscientificterm_3> as a function of frequency such that within a <otherscientificterm_4> the <material_10> maximizes the information in the converted signal by frequency shaping the <otherscientificterm_9> . <task_5> can be realized via <method_6> for a single channel delta sigma <material_10> and extended to <otherscientificterm_0> . results are presented for example <material_1> .",
    "abstract_og": "adcs sit at the interface of the analog and digital worlds and fundamentally determine what information is available in the digital domain for processing . this paper shows that a con-figurable adc can be designed for signals with non constant information as a function of frequency such that within a fixed power budget the adcs maximizes the information in the converted signal by frequency shaping the quantization noise . quantization noise shaping can be realized via loop filter design for a single channel delta sigma adcs and extended to common time and frequency interleaved multi channel structures . results are presented for example wireline and wireless style channels ."
  },
  {
    "title": "The relevance of feature type for the automatic classification of emotional user states : low level descriptors and functionals .",
    "entities": [
      "low level descriptor types",
      "support vector machines",
      "information gain ratio",
      "german database",
      "random forests",
      "features",
      "classification"
    ],
    "types": "<otherscientificterm> <method> <metric> <material> <material> <otherscientificterm> <task>",
    "relations": [
      "support vector machines -- CONJUNCTION -- random forests"
    ],
    "abstract": "in this paper , we report on <task_6> results for emotional user states -lrb- 4 classes , <material_3> of children interacting with a pet robot -rrb- . six sites computed acoustic and linguistic <otherscientificterm_5> independently from each other , following in part different strategies . a total of 4244 <otherscientificterm_5> were pooled together and grouped into 12 <otherscientificterm_0> and 6 functional types . for each of these groups , <task_6> results using <method_1> and <material_4> are reported for the full set of <otherscientificterm_5> , and for 150 <otherscientificterm_5> each with the highest individual <metric_2> . the performance for the different groups varies mostly between \u2248 50 % and \u2248 60 % .",
    "abstract_og": "in this paper , we report on classification results for emotional user states -lrb- 4 classes , german database of children interacting with a pet robot -rrb- . six sites computed acoustic and linguistic features independently from each other , following in part different strategies . a total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types . for each of these groups , classification results using support vector machines and random forests are reported for the full set of features , and for 150 features each with the highest individual information gain ratio . the performance for the different groups varies mostly between \u2248 50 % and \u2248 60 % ."
  },
  {
    "title": "Multi-Relational Latent Semantic Analysis .",
    "entities": [
      "multi-relational latent semantic analysis",
      "latent semantic analysis",
      "homogeneous and heterogeneous information sources",
      "latent semantic space",
      "latent square matrix",
      "linear algebraic operations",
      "3-way tensor",
      "low-rank approximation",
      "tensor decomposition",
      "antonymy"
    ],
    "types": "<method> <method> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "multi-relational latent semantic analysis -- USED-FOR -- latent semantic analysis"
    ],
    "abstract": "we present <method_0> which generalizes <method_1> . <method_0> provides an elegant approach to combining multiple relations between words by constructing a <otherscientificterm_6> . similar to <method_0> , a <method_7> of the tensor is derived using a <method_8> . each word in the vocabulary is thus represented by a vector in the <otherscientificterm_3> and each relation is captured by a <otherscientificterm_4> . the degree of two words having a specific relation can then be measured through simple <method_5> . we demonstrate that by integrating multiple relations from both <material_2> , <method_0> achieves state-of-the-art performance on existing benchmark datasets for two relations , <otherscientificterm_9> and is-a .",
    "abstract_og": "we present multi-relational latent semantic analysis which generalizes latent semantic analysis . multi-relational latent semantic analysis provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor . similar to multi-relational latent semantic analysis , a low-rank approximation of the tensor is derived using a tensor decomposition . each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix . the degree of two words having a specific relation can then be measured through simple linear algebraic operations . we demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources , multi-relational latent semantic analysis achieves state-of-the-art performance on existing benchmark datasets for two relations , antonymy and is-a ."
  },
  {
    "title": "Automatic Construction of Polarity-Tagged Corpus from HTML Documents .",
    "entities": [
      "html documents",
      "linguistic pattern",
      "layout structures",
      "polarity-tagged corpus"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "layout structures -- CONJUNCTION -- linguistic pattern"
    ],
    "abstract": "this paper proposes a novel method of building <material_3> from <material_0> . the characteristics of this method is that it is fully automatic and can be applied to arbitrary <material_0> . the idea behind our method is to utilize certain <otherscientificterm_2> and <otherscientificterm_1> . by using them , we can automatically extract such sentences that express opinion . in our experiment , the method could construct a corpus consisting of 126,610 sentences .",
    "abstract_og": "this paper proposes a novel method of building polarity-tagged corpus from html documents . the characteristics of this method is that it is fully automatic and can be applied to arbitrary html documents . the idea behind our method is to utilize certain layout structures and linguistic pattern . by using them , we can automatically extract such sentences that express opinion . in our experiment , the method could construct a corpus consisting of 126,610 sentences ."
  },
  {
    "title": "A non-Gaussian LCMV beamformer for MEG source reconstruction .",
    "entities": [
      "linearly constraint minimum variance beamformer",
      "source probability density function",
      "non-gaussian source estimation of stationary signals",
      "magnetoencephalo-gram data",
      "real meg measurements",
      "localising brain activity",
      "lcmv beamformer",
      "bayesian formulation",
      "non-gaussian distribution",
      "source localisation",
      "gaussian behaviour",
      "non-gaussian signal",
      "meg data",
      "spatial estimates",
      "gaussian"
    ],
    "types": "<method> <method> <task> <material> <otherscientificterm> <task> <method> <method> <otherscientificterm> <task> <otherscientificterm> <material> <material> <otherscientificterm> <method>",
    "relations": [
      "non-gaussian distribution -- FEATURE-OF -- magnetoencephalo-gram data",
      "non-gaussian signal -- CONJUNCTION -- real meg measurements",
      "bayesian formulation -- USED-FOR -- linearly constraint minimum variance beamformer",
      "source probability density function -- COMPARE -- lcmv beamformer",
      "spatial estimates -- EVALUATE-FOR -- source probability density function",
      "gaussian behaviour -- USED-FOR -- source localisation",
      "spatial estimates -- EVALUATE-FOR -- lcmv beamformer",
      "non-gaussian source estimation of stationary signals -- USED-FOR -- localising brain activity"
    ],
    "abstract": "-- evidence suggests that <material_3> have characteristics with <otherscientificterm_8> , however , standard methods for <task_9> assume <otherscientificterm_10> . we present a new general method for <task_2> for <task_5> in the <material_12> . by providing a <method_7> for <method_0> , we extend this <method_7> and show that how the <method_1> , which is not necessarily <method_14> , can be estimated . the proposed <method_1> is shown to give better <otherscientificterm_13> than the <method_6> , in both simulations incorporating <material_11> and in <otherscientificterm_4> .",
    "abstract_og": "-- evidence suggests that magnetoencephalo-gram data have characteristics with non-gaussian distribution , however , standard methods for source localisation assume gaussian behaviour . we present a new general method for non-gaussian source estimation of stationary signals for localising brain activity in the meg data . by providing a bayesian formulation for linearly constraint minimum variance beamformer , we extend this bayesian formulation and show that how the source probability density function , which is not necessarily gaussian , can be estimated . the proposed source probability density function is shown to give better spatial estimates than the lcmv beamformer , in both simulations incorporating non-gaussian signal and in real meg measurements ."
  },
  {
    "title": "Automatic prominence identification and prosodic typology .",
    "entities": [
      "fundamental frequency movements",
      "automatic detection of prosodic prominence",
      "typological point of view",
      "overall syllable energy",
      "syllable nuclei duration",
      "acoustic parameters",
      "continuous speech",
      "pitch accent",
      "prosodic prominence",
      "prosodic parameters",
      "prominence phenomenon",
      "prosodic features",
      "mid-to-high-frequency emphasis",
      "stress-accented languages",
      "automatic system",
      "inter-human agreement",
      "stress"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <metric> <otherscientificterm>",
    "relations": [
      "mid-to-high-frequency emphasis -- HYPONYM-OF -- acoustic parameters",
      "prosodic features -- PART-OF -- prosodic prominence",
      "syllable nuclei duration -- CONJUNCTION -- mid-to-high-frequency emphasis",
      "overall syllable energy -- CONJUNCTION -- mid-to-high-frequency emphasis",
      "overall syllable energy -- HYPONYM-OF -- acoustic parameters",
      "overall syllable energy -- CONJUNCTION -- stress",
      "pitch accent -- CONJUNCTION -- overall syllable energy",
      "pitch accent -- USED-FOR -- prosodic prominence",
      "pitch accent -- CONJUNCTION -- stress",
      "pitch accent -- CONJUNCTION -- syllable nuclei duration",
      "syllable nuclei duration -- HYPONYM-OF -- acoustic parameters",
      "acoustic parameters -- CONJUNCTION -- prominence phenomenon",
      "overall syllable energy -- CONJUNCTION -- syllable nuclei duration"
    ],
    "abstract": "this paper presents a follow up of a study on the <task_1> in <material_6> . <otherscientificterm_8> involves two different <otherscientificterm_11> , <otherscientificterm_7> and <otherscientificterm_16> , that are typically based on four <otherscientificterm_5> : <otherscientificterm_0> , <otherscientificterm_3> , <otherscientificterm_4> and <otherscientificterm_12> . a careful measurement of these <otherscientificterm_5> , as well as the identification of their connection to <otherscientificterm_9> , makes it possible to build an <method_14> capable of identifying prominent syllables in utterances with performance comparable with the <metric_15> reported in the literature . this <method_14> has been used to cast light on the actual correlation among the <otherscientificterm_5> and the <otherscientificterm_10> from an <otherscientificterm_2> , by examining data derived from some <material_13> .",
    "abstract_og": "this paper presents a follow up of a study on the automatic detection of prosodic prominence in continuous speech . prosodic prominence involves two different prosodic features , pitch accent and stress , that are typically based on four acoustic parameters : fundamental frequency movements , overall syllable energy , syllable nuclei duration and mid-to-high-frequency emphasis . a careful measurement of these acoustic parameters , as well as the identification of their connection to prosodic parameters , makes it possible to build an automatic system capable of identifying prominent syllables in utterances with performance comparable with the inter-human agreement reported in the literature . this automatic system has been used to cast light on the actual correlation among the acoustic parameters and the prominence phenomenon from an typological point of view , by examining data derived from some stress-accented languages ."
  },
  {
    "title": "Modulation spectrogram features for improved speaker diarization .",
    "entities": [
      "nist rich transcription 2007 task",
      "icsi speaker diarization system",
      "mfcc only system",
      "modulation spectrogram features",
      "relative der",
      "speaker diarization",
      "mfccs"
    ],
    "types": "<material> <method> <method> <method> <metric> <task> <method>",
    "relations": [
      "relative der -- EVALUATE-FOR -- icsi speaker diarization system",
      "modulation spectrogram features -- COMPARE -- mfccs",
      "nist rich transcription 2007 task -- EVALUATE-FOR -- icsi speaker diarization system",
      "modulation spectrogram features -- USED-FOR -- speaker diarization"
    ],
    "abstract": "we propose the use of <method_3> in <task_5> . these <method_3> carry longer term characteristics of the acoustic signals than the widely used <method_6> , thus providing potential improvement by using both <method_3> in combination . using the state-of-the-art <method_1> , an improvement of 20.77 % <metric_4> is obtained on the <material_0> with respect to the <method_2> .",
    "abstract_og": "we propose the use of modulation spectrogram features in speaker diarization . these modulation spectrogram features carry longer term characteristics of the acoustic signals than the widely used mfccs , thus providing potential improvement by using both modulation spectrogram features in combination . using the state-of-the-art icsi speaker diarization system , an improvement of 20.77 % relative der is obtained on the nist rich transcription 2007 task with respect to the mfcc only system ."
  },
  {
    "title": "Image interpolation using across-scale pixel correlation .",
    "entities": [
      "available image data",
      "unknown pixel values",
      "priori similarity",
      "high-resolution image",
      "pixel correlation",
      "local regions",
      "image interpolation",
      "evaluation"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "available image data -- USED-FOR -- priori similarity",
      "unknown pixel values -- PART-OF -- high-resolution image"
    ],
    "abstract": "in this paper , a novel method is proposed for <task_6> . it is assumed that the <otherscientificterm_4> between <otherscientificterm_5> across scales would remain similar . in addition , this a <otherscientificterm_2> could be extracted from a set of <material_0> that have the same content but different resolutions . a simple architecture is devised to estimate the correlation efficiently , which is then used to predict the <otherscientificterm_1> in a <material_3> . <task_7> shows a promising performance of the proposed algorithm .",
    "abstract_og": "in this paper , a novel method is proposed for image interpolation . it is assumed that the pixel correlation between local regions across scales would remain similar . in addition , this a priori similarity could be extracted from a set of available image data that have the same content but different resolutions . a simple architecture is devised to estimate the correlation efficiently , which is then used to predict the unknown pixel values in a high-resolution image . evaluation shows a promising performance of the proposed algorithm ."
  },
  {
    "title": "Learning to Freestyle : Hip Hop Challenge-Response Induction via Transduction Rule Segmentation .",
    "entities": [
      "token based and rule segmentation induction method",
      "under-explored natural language genre of music lyrics",
      "strictly unsupervised transduction grammar induction approach",
      "out-of-the-box phrase based smt system",
      "bottom-up token based rule induction",
      "maghrebi french hip hop lyrics",
      "pri-ori linguistic or phonetic information",
      "dedicated rhyme scheme detection module",
      "top-down rule segmentation strategies",
      "rhyming and fluent responses",
      "stochas-tic transduction grammar",
      "hip hop lyrics",
      "rhyming criteria",
      "unsupervised learning",
      "human evaluators"
    ],
    "types": "<method> <material> <method> <method> <method> <material> <otherscientificterm> <method> <method> <otherscientificterm> <method> <material> <metric> <method> <otherscientificterm>",
    "relations": [
      "bottom-up token based rule induction -- CONJUNCTION -- top-down rule segmentation strategies",
      "unsupervised learning -- USED-FOR -- strictly unsupervised transduction grammar induction approach",
      "top-down rule segmentation strategies -- USED-FOR -- stochas-tic transduction grammar",
      "dedicated rhyme scheme detection module -- USED-FOR -- strictly unsupervised transduction grammar induction approach",
      "unsupervised learning -- USED-FOR -- dedicated rhyme scheme detection module"
    ],
    "abstract": "we present a novel model , freestyle , that learns to improvise <otherscientificterm_9> upon being challenged with a line of <material_11> , by combining both <method_4> and <method_8> to learn a <method_10> that simultaneously learns both phrasing and rhyming associations . in this attack on the woefully <material_1> , we exploit a <method_2> . our <method_2> is particularly ambitious in that no use of any a <otherscientificterm_6> is allowed , even though the domain of <material_11> is particularly noisy and unstructured . we evaluate the performance of the learned model against a model learned only using the more conventional <method_4> , and demonstrate the superiority of our combined <method_0> toward generating higher quality improvised responses , measured on fluency and <metric_12> as judged by <otherscientificterm_14> . to highlight some of the inherent challenges in adapting other algorithms to this novel <method_2> , we also compare the quality of the responses generated by our model to those generated by an <method_3> . we tackle the challenge of selecting appropriate training data for our <method_2> via a <method_7> , which is also acquired via <method_13> and report improved quality of the generated responses . finally , we report results with <material_5> indicating that our model performs surprisingly well with no special adaptation to other languages .",
    "abstract_og": "we present a novel model , freestyle , that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics , by combining both bottom-up token based rule induction and top-down rule segmentation strategies to learn a stochas-tic transduction grammar that simultaneously learns both phrasing and rhyming associations . in this attack on the woefully under-explored natural language genre of music lyrics , we exploit a strictly unsupervised transduction grammar induction approach . our strictly unsupervised transduction grammar induction approach is particularly ambitious in that no use of any a pri-ori linguistic or phonetic information is allowed , even though the domain of hip hop lyrics is particularly noisy and unstructured . we evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction , and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses , measured on fluency and rhyming criteria as judged by human evaluators . to highlight some of the inherent challenges in adapting other algorithms to this novel strictly unsupervised transduction grammar induction approach , we also compare the quality of the responses generated by our model to those generated by an out-of-the-box phrase based smt system . we tackle the challenge of selecting appropriate training data for our strictly unsupervised transduction grammar induction approach via a dedicated rhyme scheme detection module , which is also acquired via unsupervised learning and report improved quality of the generated responses . finally , we report results with maghrebi french hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages ."
  },
  {
    "title": "A Risk Minimization Principle for a Class of Parzen Estimators .",
    "entities": [
      "maximal average margin optimality principle",
      "o algorithm",
      "classical parzen window classifier",
      "risk minimization principle",
      "ordinal regression tasks",
      "learning algorithms",
      "margin transformation",
      "facilitating analysis",
      "learning machines",
      "rademacher complexities"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "maximal average margin optimality principle -- USED-FOR -- learning algorithms",
      "maximal average margin optimality principle -- USED-FOR -- ordinal regression tasks"
    ],
    "abstract": "this paper 1 explores the use of a <method_0> for the design of <method_5> . it is shown that the application of this <method_3> results in a class of -lrb- computationally -rrb- simple <method_8> similar to the <method_2> . a direct relation with the <otherscientificterm_9> is established , as such <method_7> and providing a notion of certainty of prediction . this analysis is related to support vector machines by means of a <method_6> . the power of the <method_0> is illustrated further by application to <task_4> , resulting in an <method_1> able to process large datasets in reasonable time .",
    "abstract_og": "this paper 1 explores the use of a maximal average margin optimality principle for the design of learning algorithms . it is shown that the application of this risk minimization principle results in a class of -lrb- computationally -rrb- simple learning machines similar to the classical parzen window classifier . a direct relation with the rademacher complexities is established , as such facilitating analysis and providing a notion of certainty of prediction . this analysis is related to support vector machines by means of a margin transformation . the power of the maximal average margin optimality principle is illustrated further by application to ordinal regression tasks , resulting in an o algorithm able to process large datasets in reasonable time ."
  },
  {
    "title": "Phoneme recognition based on fisher weight map to higher-order local auto-correlation .",
    "entities": [
      "fisher weight map",
      "higher-order local auto-correlation",
      "feature extraction method",
      "local auto-correlation features",
      "two-dimensional local regions",
      "total phoneme recognition",
      "fisher weight map",
      "discriminative areas",
      "global regions",
      "vowel recognition",
      "score map",
      "temporal dynamics",
      "features"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "higher-order local auto-correlation -- USED-FOR -- feature extraction method",
      "temporal dynamics -- FEATURE-OF -- higher-order local auto-correlation",
      "vowel recognition -- CONJUNCTION -- fisher weight map",
      "vowel recognition -- CONJUNCTION -- total phoneme recognition",
      "fisher weight map -- USED-FOR -- feature extraction method",
      "discriminative areas -- USED-FOR -- local auto-correlation features",
      "higher-order local auto-correlation -- CONJUNCTION -- fisher weight map",
      "fisher weight map -- CONJUNCTION -- fisher weight map",
      "two-dimensional local regions -- FEATURE-OF -- local auto-correlation features"
    ],
    "abstract": "in this paper , we propose a new <method_2> based on <method_1> and <method_0> . widely used <method_1> lack <otherscientificterm_11> . to solve this problem , 35 types of <otherscientificterm_3> are computed within <otherscientificterm_4> . these <otherscientificterm_3> are accumulated over more <otherscientificterm_8> by weight-ing high scores on the <otherscientificterm_7> where the typical <otherscientificterm_12> among all phonemes are well expressed . this <otherscientificterm_10> is called <method_6> . we verified the effectiveness of the <method_0> and <method_0> through <task_9> and <task_5> .",
    "abstract_og": "in this paper , we propose a new feature extraction method based on higher-order local auto-correlation and fisher weight map . widely used higher-order local auto-correlation lack temporal dynamics . to solve this problem , 35 types of local auto-correlation features are computed within two-dimensional local regions . these local auto-correlation features are accumulated over more global regions by weight-ing high scores on the discriminative areas where the typical features among all phonemes are well expressed . this score map is called fisher weight map . we verified the effectiveness of the fisher weight map and fisher weight map through vowel recognition and total phoneme recognition ."
  },
  {
    "title": "Estimating the spectral envelope of voiced speech using multi-frame analysis .",
    "entities": [
      "multi-frame analysis",
      "spectral envelope of voiced speech",
      "vocal-tract transfer function",
      "transfer characteristics",
      "harmonic structure",
      "spectral envelope",
      "vocal-tract shape",
      "voiced speech",
      "speech"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material>",
    "relations": [
      "multi-frame analysis -- USED-FOR -- spectral envelope",
      "harmonic structure -- FEATURE-OF -- spectral envelope of voiced speech"
    ],
    "abstract": "this paper proposes a novel approach for estimating the <otherscientificterm_1> independently of its <otherscientificterm_4> . because of the quasi-periodicity of <material_7> , its spectrum indicates <otherscientificterm_4> and only has energy at frequencies corresponding to integral multiples of 1/4 . it is hence impossible to identify <otherscientificterm_3> between the adjacent harmonics . in order to resolve this problem , <task_0> is introduced . the mfa estimates a <otherscientificterm_5> using many portions of <material_8> which are vo-calised using the same <otherscientificterm_6> . since each of the portions usually has a different 1/4 and ensuing different <otherscientificterm_4> , a number of harmonics can be obtained at various frequencies to form a <otherscientificterm_5> . the method thereby gives a closer approximation to the <otherscientificterm_2> .",
    "abstract_og": "this paper proposes a novel approach for estimating the spectral envelope of voiced speech independently of its harmonic structure . because of the quasi-periodicity of voiced speech , its spectrum indicates harmonic structure and only has energy at frequencies corresponding to integral multiples of 1/4 . it is hence impossible to identify transfer characteristics between the adjacent harmonics . in order to resolve this problem , multi-frame analysis is introduced . the mfa estimates a spectral envelope using many portions of speech which are vo-calised using the same vocal-tract shape . since each of the portions usually has a different 1/4 and ensuing different harmonic structure , a number of harmonics can be obtained at various frequencies to form a spectral envelope . the method thereby gives a closer approximation to the vocal-tract transfer function ."
  },
  {
    "title": "Stereo reconstruction with mixed pixels using adaptive over-segmentation .",
    "entities": [
      "over-segmentation based , dense stereo algorithm",
      "middlebury stereo evaluation",
      "fronto-parallel planar segments",
      "segmentation based methods",
      "depth estimates",
      "shape constraints",
      "depth estimation",
      "generative model",
      "map estimation",
      "segment boundaries",
      "image formation",
      "belief propagation",
      "reference view",
      "mixed pixels",
      "segmentation",
      "color",
      "depth",
      "opacity"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <metric>",
    "relations": [
      "middlebury stereo evaluation -- EVALUATE-FOR -- over-segmentation based , dense stereo algorithm",
      "generative model -- USED-FOR -- image formation",
      "shape constraints -- USED-FOR -- over-segmentation based , dense stereo algorithm",
      "generative model -- USED-FOR -- mixed pixels",
      "belief propagation -- USED-FOR -- depth estimates",
      "over-segmentation based , dense stereo algorithm -- USED-FOR -- segmentation",
      "over-segmentation based , dense stereo algorithm -- COMPARE -- segmentation based methods",
      "generative model -- USED-FOR -- fronto-parallel planar segments",
      "over-segmentation based , dense stereo algorithm -- USED-FOR -- mixed pixels",
      "map estimation -- USED-FOR -- over-segmentation based , dense stereo algorithm"
    ],
    "abstract": "we present an <method_0> that jointly estimates <otherscientificterm_14> and <metric_16> . for <otherscientificterm_13> on <otherscientificterm_9> , the <method_0> computes foreground <metric_17> -lrb- alpha -rrb- , as well as <otherscientificterm_15> and <metric_16> for the foreground and background . we model the scene as a collection of <otherscientificterm_2> in a <otherscientificterm_12> , and use a <method_7> for <task_10> that handles <otherscientificterm_13> at <otherscientificterm_9> . our <method_0> iteratively updates the <otherscientificterm_14> based on <otherscientificterm_15> , <metric_16> and <otherscientificterm_5> using <method_8> . given a <otherscientificterm_14> , the <method_4> are updated using <method_11> . we show that our <method_0> is competitive with the state-of-the-art based on the new <method_1> , and that <method_0> overcomes limitations of traditional <method_3> while properly handling <otherscientificterm_13> . z-keying results show the advantages of combining <metric_17> and <method_6> .",
    "abstract_og": "we present an over-segmentation based , dense stereo algorithm that jointly estimates segmentation and depth . for mixed pixels on segment boundaries , the over-segmentation based , dense stereo algorithm computes foreground opacity -lrb- alpha -rrb- , as well as color and depth for the foreground and background . we model the scene as a collection of fronto-parallel planar segments in a reference view , and use a generative model for image formation that handles mixed pixels at segment boundaries . our over-segmentation based , dense stereo algorithm iteratively updates the segmentation based on color , depth and shape constraints using map estimation . given a segmentation , the depth estimates are updated using belief propagation . we show that our over-segmentation based , dense stereo algorithm is competitive with the state-of-the-art based on the new middlebury stereo evaluation , and that over-segmentation based , dense stereo algorithm overcomes limitations of traditional segmentation based methods while properly handling mixed pixels . z-keying results show the advantages of combining opacity and depth estimation ."
  },
  {
    "title": "Color Constancy , Intrinsic Images , and Shape Estimation .",
    "entities": [
      "mit intrinsic images dataset",
      "modified problem formulation",
      "intrinsic image decomposition",
      "sirfs -lrb- shape",
      "chromatic illumination",
      "unified model",
      "optimization scheme",
      "inference problem",
      "priors",
      "illumination",
      "reflectance"
    ],
    "types": "<material> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "chromatic illumination -- CONJUNCTION -- reflectance",
      "illumination -- CONJUNCTION -- reflectance",
      "sirfs -lrb- shape -- CONJUNCTION -- illumination",
      "unified model -- USED-FOR -- chromatic illumination",
      "optimization scheme -- USED-FOR -- inference problem",
      "reflectance -- CONJUNCTION -- illumination",
      "reflectance -- CONJUNCTION -- priors"
    ],
    "abstract": "we present <otherscientificterm_3> , <otherscientificterm_9> , and <otherscientificterm_10> from shading -rrb- , the first <method_5> for recovering shape , <otherscientificterm_4> , and <otherscientificterm_10> from a single image . our model is an extension of our previous work -lsb- 1 -rsb- , which addressed the achromatic version of this problem . dealing with color requires a <method_1> , novel <otherscientificterm_8> on <otherscientificterm_10> and <otherscientificterm_9> , and a new <method_6> for dealing with the resulting <task_7> . our approach outperforms all previously published algorithms for <task_2> and shape-from-shading on the <material_0> -lsb- 1 , 2 -rsb- and on our own '' naturally '' illuminated version of that dataset .",
    "abstract_og": "we present sirfs -lrb- shape , illumination , and reflectance from shading -rrb- , the first unified model for recovering shape , chromatic illumination , and reflectance from a single image . our model is an extension of our previous work -lsb- 1 -rsb- , which addressed the achromatic version of this problem . dealing with color requires a modified problem formulation , novel priors on reflectance and illumination , and a new optimization scheme for dealing with the resulting inference problem . our approach outperforms all previously published algorithms for intrinsic image decomposition and shape-from-shading on the mit intrinsic images dataset -lsb- 1 , 2 -rsb- and on our own '' naturally '' illuminated version of that dataset ."
  },
  {
    "title": "The Infinite Hierarchical Factor Regression Model .",
    "entities": [
      "nonparametric bayesian factor regression model",
      "gene-expression data analysis",
      "indian buffet process",
      "kingman 's coalescent",
      "hierarchical model",
      "factor regression",
      "factor analysis"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "kingman 's coalescent -- USED-FOR -- hierarchical model",
      "factor regression -- FEATURE-OF -- gene-expression data analysis"
    ],
    "abstract": "we propose a <method_0> that accounts for uncertainty in the number of factors , and the relationship between factors . to accomplish <method_0> , we propose a sparse variant of the <method_2> and couple <method_0> with a <method_4> over factors , based on <otherscientificterm_3> . we apply <method_0> model to two problems -lrb- <method_6> and <method_5> -rrb- in <task_1> .",
    "abstract_og": "we propose a nonparametric bayesian factor regression model that accounts for uncertainty in the number of factors , and the relationship between factors . to accomplish nonparametric bayesian factor regression model , we propose a sparse variant of the indian buffet process and couple nonparametric bayesian factor regression model with a hierarchical model over factors , based on kingman 's coalescent . we apply nonparametric bayesian factor regression model model to two problems -lrb- factor analysis and factor regression -rrb- in gene-expression data analysis ."
  },
  {
    "title": "A Confidence Measure for Boundary Detection and Object Selection .",
    "entities": [
      "detection of object boundaries",
      "user-guided image segmentation environment",
      "object boundary definition",
      "object houndury",
      "graph arc",
      "confidence measure",
      "conjidence measure",
      "fixed-length paths",
      "edge"
    ],
    "types": "<task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <metric> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "confidence measure -- USED-FOR -- detection of object boundaries"
    ],
    "abstract": "we introduce a <method_6> that estimates the assurance that a <otherscientificterm_4> -lrb- or <otherscientificterm_8> -rrb- corresponds to an <otherscientificterm_3> in an image . a weighted , planar graph is imposed onto the watershed lines of a gradient magnitude image and the <method_6> is afunction of the cost of <otherscientificterm_7> emanating from and extending to each end of a <otherscientificterm_4> . the <metric_5> is applied to automate the <task_0> and thereby reduces -lrb- often greatly -rrb- the time and effort required for <task_2> within a <otherscientificterm_1> .",
    "abstract_og": "we introduce a conjidence measure that estimates the assurance that a graph arc -lrb- or edge -rrb- corresponds to an object houndury in an image . a weighted , planar graph is imposed onto the watershed lines of a gradient magnitude image and the conjidence measure is afunction of the cost of fixed-length paths emanating from and extending to each end of a graph arc . the confidence measure is applied to automate the detection of object boundaries and thereby reduces -lrb- often greatly -rrb- the time and effort required for object boundary definition within a user-guided image segmentation environment ."
  },
  {
    "title": "Ritel : an open-domain , human-computer dialog system .",
    "entities": [
      "open-domain question answering system",
      "spoken language dialog system",
      "human-computer dialog corpus",
      "ritel platform",
      "qa system",
      "collected corpus"
    ],
    "types": "<method> <method> <material> <method> <method> <material>",
    "relations": [
      "ritel platform -- USED-FOR -- human-computer dialog corpus",
      "spoken language dialog system -- CONJUNCTION -- open-domain question answering system",
      "ritel platform -- USED-FOR -- spoken language dialog system",
      "spoken language dialog system -- PART-OF -- qa system"
    ],
    "abstract": "the project <method_3> aims at integrating a <method_1> and an <method_0> to allow a human to ask general questions -lrb- '' who is currently presiding the senate ? '' -rrb- and refine the search interactively . as this point in time the <method_3> is being used to collect a <material_2> . the user can receive factual answers to some questions -lrb- q : who is the president of france , r : jacques chirac is the president for france since may 1995 -rrb- . this paper briefly presents the current system , the <material_5> , the problems encountered by such a system and our first answers to these problems . when the system is more advance , it will allow measuring the net worth of integrating a <method_1> into a <method_4> . does allowing such a <method_1> really enables to reach faster and more precisely the '' right '' answer to a question ?",
    "abstract_og": "the project ritel platform aims at integrating a spoken language dialog system and an open-domain question answering system to allow a human to ask general questions -lrb- '' who is currently presiding the senate ? '' -rrb- and refine the search interactively . as this point in time the ritel platform is being used to collect a human-computer dialog corpus . the user can receive factual answers to some questions -lrb- q : who is the president of france , r : jacques chirac is the president for france since may 1995 -rrb- . this paper briefly presents the current system , the collected corpus , the problems encountered by such a system and our first answers to these problems . when the system is more advance , it will allow measuring the net worth of integrating a spoken language dialog system into a qa system . does allowing such a spoken language dialog system really enables to reach faster and more precisely the '' right '' answer to a question ?"
  },
  {
    "title": "SMT-Based Validation of Timed Failure Propagation Graphs .",
    "entities": [
      "satisfiability modulo theories engines",
      "timed failure propagation graphs",
      "dynamic partially observable system",
      "model-based diagnosis approach",
      "model-checking techniques",
      "model-based diagnosis",
      "failure propagation"
    ],
    "types": "<method> <method> <method> <method> <method> <task> <otherscientificterm>",
    "relations": [
      "model-checking techniques -- USED-FOR -- timed failure propagation graphs",
      "failure propagation -- PART-OF -- dynamic partially observable system",
      "satisfiability modulo theories engines -- USED-FOR -- timed failure propagation graphs",
      "timed failure propagation graphs -- USED-FOR -- model-based diagnosis"
    ],
    "abstract": "timed failure propagation graphs -lrb- <method_1> -rrb- are a formalism used in industry to describe <otherscientificterm_6> in a <method_2> . <method_1> are commonly used to perform <task_5> . as in any <method_3> , however , the quality of the diagnosis strongly depends on the quality of the <method_1> . approaches to certify the quality of the <method_1> are limited and mainly rely on testing . in this work we address this problem by leverag-ing efficient <method_0> to perform exhaustive reasoning on <method_1> . we apply <method_4> to certify that a given <method_1> satisfies -lrb- or not -rrb- a property of interest . moreover , we discuss the problem of refinement and diagnosability testing and empirically show that our <method_4> can be used to efficiently solve them .",
    "abstract_og": "timed failure propagation graphs -lrb- timed failure propagation graphs -rrb- are a formalism used in industry to describe failure propagation in a dynamic partially observable system . timed failure propagation graphs are commonly used to perform model-based diagnosis . as in any model-based diagnosis approach , however , the quality of the diagnosis strongly depends on the quality of the timed failure propagation graphs . approaches to certify the quality of the timed failure propagation graphs are limited and mainly rely on testing . in this work we address this problem by leverag-ing efficient satisfiability modulo theories engines to perform exhaustive reasoning on timed failure propagation graphs . we apply model-checking techniques to certify that a given timed failure propagation graphs satisfies -lrb- or not -rrb- a property of interest . moreover , we discuss the problem of refinement and diagnosability testing and empirically show that our model-checking techniques can be used to efficiently solve them ."
  },
  {
    "title": "Apply n-best list re-ranking to acoustic model combinations of boosting training .",
    "entities": [
      "utterance level error rate",
      "frame level decoding",
      "word error rate",
      "improvement of system",
      "n-best list re-ranking",
      "boosting training method",
      "word level",
      "object function",
      "re-ranked hypotheses",
      "performance metric",
      "speech recognition",
      "top-1 hypotheses",
      "confidence feature",
      "acoustic modeling",
      "rover"
    ],
    "types": "<metric> <task> <metric> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <task> <otherscientificterm> <method> <task> <method>",
    "relations": [
      "n-best list re-ranking -- CONJUNCTION -- rover",
      "acoustic modeling -- USED-FOR -- re-ranked hypotheses",
      "confidence feature -- USED-FOR -- frame level decoding",
      "utterance level error rate -- EVALUATE-FOR -- acoustic modeling",
      "object function -- USED-FOR -- boosting training method",
      "re-ranked hypotheses -- COMPARE -- top-1 hypotheses",
      "utterance level error rate -- EVALUATE-FOR -- boosting training method",
      "acoustic modeling -- COMPARE -- top-1 hypotheses"
    ],
    "abstract": "the <otherscientificterm_7> for <method_5> in <task_13> aims to reduce <metric_0> . this is different from the most commonly used <metric_9> in <task_10> , <metric_2> . this paper proposes that the combination of <method_4> and <method_14> can partly address this problem . in particular , <task_13> is applied to <otherscientificterm_8> rather than to the original <otherscientificterm_11> and carried on <otherscientificterm_6> . <method_3> performance is observed in our experiments . in addition , we describe and evaluate a new <method_12> that measures the correctness of <task_1> result .",
    "abstract_og": "the object function for boosting training method in acoustic modeling aims to reduce utterance level error rate . this is different from the most commonly used performance metric in speech recognition , word error rate . this paper proposes that the combination of n-best list re-ranking and rover can partly address this problem . in particular , acoustic modeling is applied to re-ranked hypotheses rather than to the original top-1 hypotheses and carried on word level . improvement of system performance is observed in our experiments . in addition , we describe and evaluate a new confidence feature that measures the correctness of frame level decoding result ."
  },
  {
    "title": "Efficient Processing of Underspecified Discourse Representations .",
    "entities": [
      "processing partially disambiguated discourse structure",
      "underspecified discourse description",
      "rst discourse treebank",
      "weighted tree grammars",
      "dominance graphs",
      "discourse representation",
      "underspecification-based algorithms"
    ],
    "types": "<task> <task> <material> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "dominance graphs -- CONJUNCTION -- weighted tree grammars",
      "underspecification-based algorithms -- USED-FOR -- processing partially disambiguated discourse structure"
    ],
    "abstract": "underspecification-based algorithms for <task_0> must cope with extremely high numbers of readings . based on previous work on <otherscientificterm_4> and <method_3> , we provide the first possibility for computing an <task_1> and a best <method_5> efficiently enough to process even the longest discourses in the <material_2> .",
    "abstract_og": "underspecification-based algorithms for processing partially disambiguated discourse structure must cope with extremely high numbers of readings . based on previous work on dominance graphs and weighted tree grammars , we provide the first possibility for computing an underspecified discourse description and a best discourse representation efficiently enough to process even the longest discourses in the rst discourse treebank ."
  },
  {
    "title": "Facial Contour Labeling via Congealing .",
    "entities": [
      "histogram of oriented gradient",
      "automatic or semi-supervised fashion",
      "manual landmark labels",
      "dual-curve congealing manner",
      "parametric curve representation",
      "non-rigid shape deformation",
      "semi-supervised approach",
      "inter-person database",
      "labeling accuracy",
      "object class",
      "congealing framework",
      "closed contour",
      "appearance information",
      "300-image ensemble",
      "image ensemble",
      "vision problem",
      "congealing-like process",
      "features",
      "curve",
      "congealing"
    ],
    "types": "<method> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <method> <material> <metric> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <task> <task> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "histogram of oriented gradient -- HYPONYM-OF -- features",
      "features -- PART-OF -- congealing framework",
      "non-rigid shape deformation -- USED-FOR -- image ensemble",
      "congealing-like process -- USED-FOR -- semi-supervised approach",
      "semi-supervised approach -- USED-FOR -- manual landmark labels",
      "histogram of oriented gradient -- USED-FOR -- congealing framework",
      "vision problem -- USED-FOR -- non-rigid shape deformation",
      "congealing-like process -- USED-FOR -- manual landmark labels"
    ],
    "abstract": "it is a challenging <task_15> to discover <otherscientificterm_5> for an <task_14> belonging to a single <otherscientificterm_9> , in an <otherscientificterm_1> . the conventional <method_6> -lsb- 1 -rsb- uses a <method_16> to propagate <material_2> from a few images to a large ensemble . although effective on an <material_7> with a large population , there is potential for increased <metric_8> . with the goal of providing highly accurate labels , in this paper we present a <method_4> for each of the seven major facial contours . the <otherscientificterm_12> along the <otherscientificterm_18> , named <otherscientificterm_18> descriptor , is extracted and used for <task_19> . furthermore , we demonstrate that advanced <otherscientificterm_17> such as <method_0> can be utilized in the proposed <method_10> , which operates in a <otherscientificterm_3> for the case of a <otherscientificterm_11> . with extensive experiments on a <material_13> that exhibits moderate variation in facial pose and shape , we show that substantial progress has been achieved in the <metric_8> compared to the previous state-of-the-art approach .",
    "abstract_og": "it is a challenging vision problem to discover non-rigid shape deformation for an image ensemble belonging to a single object class , in an automatic or semi-supervised fashion . the conventional semi-supervised approach -lsb- 1 -rsb- uses a congealing-like process to propagate manual landmark labels from a few images to a large ensemble . although effective on an inter-person database with a large population , there is potential for increased labeling accuracy . with the goal of providing highly accurate labels , in this paper we present a parametric curve representation for each of the seven major facial contours . the appearance information along the curve , named curve descriptor , is extracted and used for congealing . furthermore , we demonstrate that advanced features such as histogram of oriented gradient can be utilized in the proposed congealing framework , which operates in a dual-curve congealing manner for the case of a closed contour . with extensive experiments on a 300-image ensemble that exhibits moderate variation in facial pose and shape , we show that substantial progress has been achieved in the labeling accuracy compared to the previous state-of-the-art approach ."
  },
  {
    "title": "Fast polygonal integration and its application in extending haar-like features to improve object detection .",
    "entities": [
      "viola and jones ' object detection framework",
      "mars ' surface terrain assessment",
      "fixed-pose hand detection",
      "polyg-onal haar-like features",
      "classical haar-like features",
      "frontal face detection",
      "image resolution",
      "object detection",
      "rectangular region",
      "rock detection",
      "integration time",
      "rectilinear",
      "polygon",
      "function"
    ],
    "types": "<method> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <task> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "rock detection -- USED-FOR -- mars ' surface terrain assessment",
      "frontal face detection -- CONJUNCTION -- fixed-pose hand detection",
      "fixed-pose hand detection -- CONJUNCTION -- rock detection"
    ],
    "abstract": "the integral image is typically used for fast integrating a <otherscientificterm_13> over a <otherscientificterm_8> in an image . we propose a method that extends the integral image to do fast integration over the interior of any <otherscientificterm_12> that is not necessarily <otherscientificterm_11> . the <metric_10> of the method is fast , independent of the <otherscientificterm_6> , and only linear to the <otherscientificterm_12> 's number of vertices . we apply the method to <method_0> , in which we propose to improve <otherscientificterm_4> with <otherscientificterm_3> . we show that the extended feature set improves <task_7> 's performance . the experiments are conducted in three domains : <task_5> , <task_2> , and <task_9> for <task_1> .",
    "abstract_og": "the integral image is typically used for fast integrating a function over a rectangular region in an image . we propose a method that extends the integral image to do fast integration over the interior of any polygon that is not necessarily rectilinear . the integration time of the method is fast , independent of the image resolution , and only linear to the polygon 's number of vertices . we apply the method to viola and jones ' object detection framework , in which we propose to improve classical haar-like features with polyg-onal haar-like features . we show that the extended feature set improves object detection 's performance . the experiments are conducted in three domains : frontal face detection , fixed-pose hand detection , and rock detection for mars ' surface terrain assessment ."
  },
  {
    "title": "Computation as estimation : Estimation-theoretic IC design improves robustness and reduces power consumption .",
    "entities": [
      "modern integrated circuits",
      "massively parallel systems",
      "silicon feature sizes",
      "design optimization formalization",
      "moore 's law",
      "viewing hardware errors",
      "estimation-theoretic framework",
      "estimation theory",
      "power savings",
      "particle hits",
      "power/reliability trade-off",
      "system reliability",
      "measurement"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <method> <task> <method> <method> <metric> <otherscientificterm> <otherscientificterm> <metric> <task>",
    "relations": [
      "estimation-theoretic framework -- USED-FOR -- power/reliability trade-off",
      "power savings -- CONJUNCTION -- system reliability",
      "estimation-theoretic framework -- USED-FOR -- design optimization formalization"
    ],
    "abstract": "modern integrated circuits -lrb- ics -rrb- are designed as <method_1> as a consequence of diminishing <otherscientificterm_2> . this has adversely impacted reliability because of increased errors due to process and environmental variations , and <otherscientificterm_9> . <task_5> as analogous to <task_12> or system noise allows us to borrow results from <method_7> and extend <method_4> . the <method_6> provides a <method_3> that enables <otherscientificterm_10> in broad classes of applications . two applications described here show that specific instantiations of the <method_6> yield significant <metric_8> and <metric_11> .",
    "abstract_og": "modern integrated circuits -lrb- ics -rrb- are designed as massively parallel systems as a consequence of diminishing silicon feature sizes . this has adversely impacted reliability because of increased errors due to process and environmental variations , and particle hits . viewing hardware errors as analogous to measurement or system noise allows us to borrow results from estimation theory and extend moore 's law . the estimation-theoretic framework provides a design optimization formalization that enables power/reliability trade-off in broad classes of applications . two applications described here show that specific instantiations of the estimation-theoretic framework yield significant power savings and system reliability ."
  },
  {
    "title": "Snapshot spectral imaging via compressive random convolution .",
    "entities": [
      "random convolution snapshot spectral imaging",
      "coded aperture spectral snapshot imaging",
      "focal plane array measurement",
      "psnr spectral image cube reconstructions",
      "compressive sensing reconstruction algorithm",
      "3d spectral cube",
      "spatial light modulator",
      "wide-area airborne surveillance",
      "shot 2d measurement",
      "remote sensing",
      "cassi systems",
      "random convolutions",
      "fpa measurements",
      "tissue spectroscopy",
      "compres-sive sensing",
      "spectral imaging"
    ],
    "types": "<method> <method> <method> <task> <method> <otherscientificterm> <method> <task> <method> <task> <method> <otherscientificterm> <method> <task> <task> <method>",
    "relations": [
      "wide-area airborne surveillance -- CONJUNCTION -- remote sensing",
      "coded aperture spectral snapshot imaging -- USED-FOR -- 3d spectral cube",
      "psnr spectral image cube reconstructions -- EVALUATE-FOR -- cassi systems",
      "compressive sensing reconstruction algorithm -- USED-FOR -- 3d spectral cube",
      "focal plane array measurement -- USED-FOR -- coded aperture spectral snapshot imaging",
      "remote sensing -- CONJUNCTION -- tissue spectroscopy"
    ],
    "abstract": "spectral imaging is of interest in many applications , including <task_7> , <task_9> , and <task_13> . <method_1> provides an efficient mechanism to capture a <otherscientificterm_5> with a single <method_8> . <method_1> uses a <method_2> of a spectrally dispersed , aperture coded , source . the <otherscientificterm_5> is then attained using a <method_4> . in this paper , we explore a new approach referred to as <method_0> . it is based on <method_12> of spectrally dispersed coherent sources that have been randomly convoluted by a <method_6> . the new method , based on the theory of <task_14> via <otherscientificterm_11> , is shown to outperform traditional <method_10> in terms of <task_3> .",
    "abstract_og": "spectral imaging is of interest in many applications , including wide-area airborne surveillance , remote sensing , and tissue spectroscopy . coded aperture spectral snapshot imaging provides an efficient mechanism to capture a 3d spectral cube with a single shot 2d measurement . coded aperture spectral snapshot imaging uses a focal plane array measurement of a spectrally dispersed , aperture coded , source . the 3d spectral cube is then attained using a compressive sensing reconstruction algorithm . in this paper , we explore a new approach referred to as random convolution snapshot spectral imaging . it is based on fpa measurements of spectrally dispersed coherent sources that have been randomly convoluted by a spatial light modulator . the new method , based on the theory of compres-sive sensing via random convolutions , is shown to outperform traditional cassi systems in terms of psnr spectral image cube reconstructions ."
  },
  {
    "title": "Neural CRF Parsing .",
    "entities": [
      "rich nonlinear fea-turization of neural net approaches",
      "baseline crf model",
      "anchored rule productions",
      "linear potential functions",
      "feedforward neu-ral network",
      "dense features",
      "crf parsing",
      "penn tree-bank",
      "sparse features",
      "computing gradients",
      "error signal",
      "anchored rules",
      "nonlinear potentials",
      "parsing model",
      "parser"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "parsing model -- COMPARE -- baseline crf model",
      "penn tree-bank -- EVALUATE-FOR -- parsing model",
      "sparse features -- USED-FOR -- linear potential functions",
      "nonlinear potentials -- USED-FOR -- linear potential functions",
      "feedforward neu-ral network -- USED-FOR -- nonlinear potentials"
    ],
    "abstract": "this paper describes a <method_13> that combines the exact dynamic programming of <method_6> with the <method_0> . our <method_13> is structurally a <method_6> that factors over <otherscientificterm_2> , but instead of <otherscientificterm_3> based on <otherscientificterm_8> , we use <otherscientificterm_12> computed via a <method_4> . because potentials are still local to <otherscientificterm_11> , structured inference -lrb- cky -rrb- is unchanged from the sparse case . <otherscientificterm_9> during learning involves backpropagating an <otherscientificterm_10> formed from standard <method_6> sufficient statistics -lrb- expected rule counts -rrb- . using only <otherscientificterm_5> , our <method_13> already exceeds a strong <method_1> -lrb- hall et al. , 2014 -rrb- . in combination with <otherscientificterm_8> , our <method_13> 1 achieves 91.1 f 1 on section 23 of the <material_7> , and more generally outperforms the best prior single <method_14> results on a range of languages .",
    "abstract_og": "this paper describes a parsing model that combines the exact dynamic programming of crf parsing with the rich nonlinear fea-turization of neural net approaches . our parsing model is structurally a crf parsing that factors over anchored rule productions , but instead of linear potential functions based on sparse features , we use nonlinear potentials computed via a feedforward neu-ral network . because potentials are still local to anchored rules , structured inference -lrb- cky -rrb- is unchanged from the sparse case . computing gradients during learning involves backpropagating an error signal formed from standard crf parsing sufficient statistics -lrb- expected rule counts -rrb- . using only dense features , our parsing model already exceeds a strong baseline crf model -lrb- hall et al. , 2014 -rrb- . in combination with sparse features , our parsing model 1 achieves 91.1 f 1 on section 23 of the penn tree-bank , and more generally outperforms the best prior single parser results on a range of languages ."
  },
  {
    "title": "Packetized video transmission for OFDM wireless systems with dynamic ordered subcarrier selection algorithm .",
    "entities": [
      "dynamic ordered subcarrier selection algorithm",
      "bit error rate",
      "unequal error protection",
      "ofdm based video transmission system",
      "highest channel gain",
      "ossa"
    ],
    "types": "<method> <metric> <method> <task> <otherscientificterm> <method>",
    "relations": [
      "dynamic ordered subcarrier selection algorithm -- USED-FOR -- ofdm based video transmission system",
      "dynamic ordered subcarrier selection algorithm -- COMPARE -- ossa",
      "bit error rate -- EVALUATE-FOR -- dynamic ordered subcarrier selection algorithm",
      "ossa -- USED-FOR -- unequal error protection"
    ],
    "abstract": "in this paper , we proposed a <method_0> for <task_3> . the proposed <method_0> is shown to achieve lower <metric_1> than the previously proposed <method_5> by \u017frst selecting a fraction of the subcarriers with <otherscientificterm_4> . the content information is then exploited in order to extend the <method_5> to achieve <method_2> for packets of different importance . simulation results show that <method_0> that utilizes the proposed <method_0> can achieve higher <metric_1> , especially at low snr , compared to those that use the equal error protection -lrb- eep -rrb- <method_5> .",
    "abstract_og": "in this paper , we proposed a dynamic ordered subcarrier selection algorithm for ofdm based video transmission system . the proposed dynamic ordered subcarrier selection algorithm is shown to achieve lower bit error rate than the previously proposed ossa by \u017frst selecting a fraction of the subcarriers with highest channel gain . the content information is then exploited in order to extend the ossa to achieve unequal error protection for packets of different importance . simulation results show that dynamic ordered subcarrier selection algorithm that utilizes the proposed dynamic ordered subcarrier selection algorithm can achieve higher bit error rate , especially at low snr , compared to those that use the equal error protection -lrb- eep -rrb- ossa ."
  },
  {
    "title": "Novel approach to AM-FM decomposition with applications to speech and music analysis .",
    "entities": [
      "k-nearest neighbour -lrb- ` k-nn ' -rrb- framework",
      "freqeuency modulation components",
      "discrete-energy separation algorithm",
      "amplitude modulation",
      "envelope and frequency estimates",
      "zero-crossing instant information",
      "bandpass filtered speech",
      "time-varying lowpass filter",
      "teager energy operator",
      "zero-crossing based algorithm",
      "fine-structured modulations",
      "analysis frame",
      "bandpass signal",
      "micro-time scale",
      "fm component",
      "instantaneous frequency",
      "coherent demodulation"
    ],
    "types": "<method> <method> <method> <method> <metric> <otherscientificterm> <material> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "coherent demodulation -- USED-FOR -- fm component",
      "envelope and frequency estimates -- EVALUATE-FOR -- discrete-energy separation algorithm",
      "time-varying lowpass filter -- USED-FOR -- coherent demodulation",
      "zero-crossing instant information -- USED-FOR -- fm component",
      "zero-crossing based algorithm -- COMPARE -- discrete-energy separation algorithm",
      "zero-crossing instant information -- USED-FOR -- zero-crossing based algorithm",
      "zero-crossing based algorithm -- USED-FOR -- bandpass signal",
      "teager energy operator -- USED-FOR -- discrete-energy separation algorithm",
      "amplitude modulation -- CONJUNCTION -- freqeuency modulation components",
      "zero-crossing based algorithm -- USED-FOR -- fine-structured modulations",
      "time-varying lowpass filter -- USED-FOR -- fm component"
    ],
    "abstract": "we present a new <method_9> for decomposing a <otherscientificterm_12> into the <method_3> and <method_1> . in this <method_9> , the <method_14> is first estimated using <otherscientificterm_5> in a <method_0> . the <method_14> is estimated by <method_16> using a <method_7> that uses the estimated <otherscientificterm_15> . simulation results show that the proposed <method_9> gives more accurate <metric_4> compared to the <method_2> which uses the <method_8> . using the proposed <method_9> on <material_6> and music we can extract the <otherscientificterm_10> that occur on a <otherscientificterm_13> , within an <otherscientificterm_11> .",
    "abstract_og": "we present a new zero-crossing based algorithm for decomposing a bandpass signal into the amplitude modulation and freqeuency modulation components . in this zero-crossing based algorithm , the fm component is first estimated using zero-crossing instant information in a k-nearest neighbour -lrb- ` k-nn ' -rrb- framework . the fm component is estimated by coherent demodulation using a time-varying lowpass filter that uses the estimated instantaneous frequency . simulation results show that the proposed zero-crossing based algorithm gives more accurate envelope and frequency estimates compared to the discrete-energy separation algorithm which uses the teager energy operator . using the proposed zero-crossing based algorithm on bandpass filtered speech and music we can extract the fine-structured modulations that occur on a micro-time scale , within an analysis frame ."
  },
  {
    "title": "Robust Matched Filters for Target Detection in Hyperspectral Imaging Data .",
    "entities": [
      "uncertainty and/or variability of target signatures",
      "robust matched \u00bf lter",
      "hyperspectral imaging data",
      "hyperspectral imaging applications",
      "spectral space",
      "spectral variability",
      "target mismatch",
      "hydice sensor",
      "spectral signature",
      "target signature",
      "detection algorithms",
      "ellipsoid",
      "spectra"
    ],
    "types": "<otherscientificterm> <method> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "robust matched \u00bf lter -- USED-FOR -- spectral variability",
      "detection algorithms -- USED-FOR -- hyperspectral imaging applications",
      "robust matched \u00bf lter -- USED-FOR -- target mismatch",
      "target mismatch -- CONJUNCTION -- spectral variability",
      "spectral space -- FEATURE-OF -- ellipsoid"
    ],
    "abstract": "most <method_10> for <task_3> assume a target with a perfectly known <otherscientificterm_8> . in practice , the <otherscientificterm_9> is either imperfectly measured -lrb- <otherscientificterm_6> -rrb- and/or it exhibits <otherscientificterm_5> . the objective of this paper is to introduce a <method_1> that takes the <otherscientificterm_0> into account . it is shown that , if we describe this uncertainty with an <otherscientificterm_11> in the <otherscientificterm_4> , we can design a matched \u00bf lter that provides a response of the same magnitude for all <otherscientificterm_12> within this <otherscientificterm_11> . thus , by changing the size of this <otherscientificterm_11> , we can control the '' spectral selectivity '' of the matched \u00bf lter . the ability of the <method_1> to deal effectively with <otherscientificterm_6> and <otherscientificterm_5> is demonstrated with <material_2> from the <method_7> .",
    "abstract_og": "most detection algorithms for hyperspectral imaging applications assume a target with a perfectly known spectral signature . in practice , the target signature is either imperfectly measured -lrb- target mismatch -rrb- and/or it exhibits spectral variability . the objective of this paper is to introduce a robust matched \u00bf lter that takes the uncertainty and/or variability of target signatures into account . it is shown that , if we describe this uncertainty with an ellipsoid in the spectral space , we can design a matched \u00bf lter that provides a response of the same magnitude for all spectra within this ellipsoid . thus , by changing the size of this ellipsoid , we can control the '' spectral selectivity '' of the matched \u00bf lter . the ability of the robust matched \u00bf lter to deal effectively with target mismatch and spectral variability is demonstrated with hyperspectral imaging data from the hydice sensor ."
  },
  {
    "title": "The Parameterized Complexity of Abduction .",
    "entities": [
      "parameterized complexity analysis of abduction",
      "fundamental reasoning methods",
      "propositional logic",
      "logic-based abduction",
      "reverse inference",
      "proposi-tional formulas",
      "computational complexity",
      "abduction"
    ],
    "types": "<task> <method> <otherscientificterm> <task> <task> <otherscientificterm> <metric> <task>",
    "relations": [
      "computational complexity -- EVALUATE-FOR -- logic-based abduction"
    ],
    "abstract": "abduction belongs to the most <method_1> . it is a method for <task_4> , this means one is interested in explaining observed behavior by finding appropriate causes . we study <task_3> , where knowledge is represented by <otherscientificterm_5> . the <metric_6> of this <task_3> is highly intractable in many interesting settings . in this work we therefore present an extensive <task_0> within various fragments of <otherscientificterm_2> together with -lrb- combinations of -rrb- natural parameters .",
    "abstract_og": "abduction belongs to the most fundamental reasoning methods . it is a method for reverse inference , this means one is interested in explaining observed behavior by finding appropriate causes . we study logic-based abduction , where knowledge is represented by proposi-tional formulas . the computational complexity of this logic-based abduction is highly intractable in many interesting settings . in this work we therefore present an extensive parameterized complexity analysis of abduction within various fragments of propositional logic together with -lrb- combinations of -rrb- natural parameters ."
  },
  {
    "title": "Interpolatory Mercer kernel construction for kernel direct LDA on face recognition .",
    "entities": [
      "self-constructed interpolatory mercer kernel",
      "rbf kernel based kdda method",
      "im kernel based kdda approach",
      "feret and cmu pie databases",
      "lagrange interpolatory basis functions",
      "mercer kernel function",
      "cmu pie database",
      "mercer kernel construction",
      "nonlinear mapping \u03c6",
      "gram matrix",
      "interpolatory strategy",
      "face recognition",
      "kernel function",
      "face databases",
      "cholesky decomposition"
    ],
    "types": "<method> <method> <method> <material> <otherscientificterm> <method> <material> <task> <method> <otherscientificterm> <method> <task> <otherscientificterm> <material> <method>",
    "relations": [
      "self-constructed interpolatory mercer kernel -- USED-FOR -- gram matrix",
      "mercer kernel function -- USED-FOR -- kernel function",
      "feret and cmu pie databases -- EVALUATE-FOR -- im kernel based kdda approach",
      "cmu pie database -- EVALUATE-FOR -- im kernel based kdda approach",
      "face databases -- EVALUATE-FOR -- rbf kernel based kdda method",
      "feret and cmu pie databases -- HYPONYM-OF -- face databases",
      "interpolatory strategy -- USED-FOR -- mercer kernel construction",
      "lagrange interpolatory basis functions -- USED-FOR -- nonlinear mapping \u03c6"
    ],
    "abstract": "this paper proposes a novel methodology on <task_7> using <method_10> . based on a given symmetric and positive semi-definite matrix -lrb- <otherscientificterm_9> -rrb- and <method_14> , it first constructs a <method_8> , which is well-defined on the training data . this <method_8> is then extended to the whole input feature space by utilizing <otherscientificterm_4> . the <otherscientificterm_12> constructed by inner product is proven to be a <method_5> . the <method_0> keeps the <otherscientificterm_9> unchanged on the training samples . to evaluate the performance of the proposed <method_0> , a popular kernel direct linear discriminant analysis -lrb- kdda -rrb- method for <task_11> is selected . comparing with <method_1> on two <material_13> , namely <material_3> , the <method_2> could increase the performance by around 20 % on <material_6> .",
    "abstract_og": "this paper proposes a novel methodology on mercer kernel construction using interpolatory strategy . based on a given symmetric and positive semi-definite matrix -lrb- gram matrix -rrb- and cholesky decomposition , it first constructs a nonlinear mapping \u03c6 , which is well-defined on the training data . this nonlinear mapping \u03c6 is then extended to the whole input feature space by utilizing lagrange interpolatory basis functions . the kernel function constructed by inner product is proven to be a mercer kernel function . the self-constructed interpolatory mercer kernel keeps the gram matrix unchanged on the training samples . to evaluate the performance of the proposed self-constructed interpolatory mercer kernel , a popular kernel direct linear discriminant analysis -lrb- kdda -rrb- method for face recognition is selected . comparing with rbf kernel based kdda method on two face databases , namely feret and cmu pie databases , the im kernel based kdda approach could increase the performance by around 20 % on cmu pie database ."
  },
  {
    "title": "The Use of DBN-HMMs for Mispronunciation Detection and Diagnosis in L2 English to Support Computer-Aided Pronunciation Training .",
    "entities": [
      "word pronunciation error rate",
      "unannotated l2 data",
      "hybrid dbn-hmm framework",
      "supervised manner",
      "asr engine",
      "unsupervised manner",
      "unsupervised pre-training",
      "annotated data",
      "acoustic modeling",
      "mispronunciation detection",
      "features",
      "gmm-hmm",
      "dbn-hmm",
      "ml"
    ],
    "types": "<metric> <material> <method> <method> <task> <method> <method> <material> <task> <task> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "annotated data -- USED-FOR -- asr engine",
      "mispronunciation detection -- USED-FOR -- acoustic modeling",
      "ml -- CONJUNCTION -- gmm-hmm",
      "unannotated l2 data -- USED-FOR -- unsupervised manner",
      "unsupervised manner -- USED-FOR -- asr engine",
      "supervised manner -- USED-FOR -- asr engine",
      "hybrid dbn-hmm framework -- USED-FOR -- mispronunciation detection",
      "unannotated l2 data -- USED-FOR -- asr engine",
      "unsupervised pre-training -- USED-FOR -- dbn-hmm",
      "hybrid dbn-hmm framework -- USED-FOR -- acoustic modeling"
    ],
    "abstract": "this paper investigates <task_8> using the <method_2> in <task_9> and diagnosis of l2 english . this is one of the first efforts that compare the performance of <method_12> with that of the best-tuned <method_11> trained in <method_13> and mwe on the same set of <otherscientificterm_10> . previous work in <task_4> has also shown the necessity of <method_6> for <method_12> to work well . we explore further the effect of training our <task_4> in an <method_5> with additional <material_1> from the test speakers . this is compared with the original <task_4> that has been trained with <material_7> in a <method_3> . experiments show that <method_12> can give significant improvement -lrb- between 13-18 % relative in <metric_0> -rrb- but is computationally more expensive .",
    "abstract_og": "this paper investigates acoustic modeling using the hybrid dbn-hmm framework in mispronunciation detection and diagnosis of l2 english . this is one of the first efforts that compare the performance of dbn-hmm with that of the best-tuned gmm-hmm trained in ml and mwe on the same set of features . previous work in asr engine has also shown the necessity of unsupervised pre-training for dbn-hmm to work well . we explore further the effect of training our asr engine in an unsupervised manner with additional unannotated l2 data from the test speakers . this is compared with the original asr engine that has been trained with annotated data in a supervised manner . experiments show that dbn-hmm can give significant improvement -lrb- between 13-18 % relative in word pronunciation error rate -rrb- but is computationally more expensive ."
  },
  {
    "title": "Generalization in Decision Trees and DNF : Does Size Matter ? .",
    "entities": [
      "class of node decision functions",
      "decision tree of depth",
      "two-layer neural networks",
      "thresh-olded real-valued functions",
      "support vector machines",
      "misclassiication probability",
      "boolean functions",
      "vc theory",
      "pattern classiication",
      "dnf formulae",
      "sig-moid networks",
      "distribution"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm>",
    "relations": [
      "support vector machines -- HYPONYM-OF -- thresh-olded real-valued functions",
      "sig-moid networks -- HYPONYM-OF -- thresh-olded real-valued functions",
      "thresh-olded real-valued functions -- FEATURE-OF -- pattern classiication",
      "support vector machines -- CONJUNCTION -- sig-moid networks"
    ],
    "abstract": "recent theoretical results for <task_8> with <otherscientificterm_3> -lrb- such as <method_4> , <method_10> , and boosting -rrb- give bounds on <otherscientificterm_5> that do not depend on the size of the classiier , and hence can be considerably smaller than the bounds that follow from the <method_7> . in this paper , we show that these techniques can be more widely applied , by representing other <otherscientificterm_6> as <method_2> -lrb- thresholded convex combinations of <otherscientificterm_6> -rrb- . for example , we show that with high probability any <otherscientificterm_1> no more than d that is consistent with m training examples has <otherscientificterm_5> no more than o ? 1 m ? n ee vcdim -lrb- u -rrb- log 2 m log d 1 = 2 , where u is the <otherscientificterm_0> , and n ee n can be thought of as the eeective number of leaves -lrb- it becomes small as the <otherscientificterm_11> on the leaves induced by the training data gets far from uniform -rrb- . this bound is qualitatively diierent from the vc bound and can be considerably smaller . we use the same technique to give similar results for <method_9> .",
    "abstract_og": "recent theoretical results for pattern classiication with thresh-olded real-valued functions -lrb- such as support vector machines , sig-moid networks , and boosting -rrb- give bounds on misclassiication probability that do not depend on the size of the classiier , and hence can be considerably smaller than the bounds that follow from the vc theory . in this paper , we show that these techniques can be more widely applied , by representing other boolean functions as two-layer neural networks -lrb- thresholded convex combinations of boolean functions -rrb- . for example , we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassiication probability no more than o ? 1 m ? n ee vcdim -lrb- u -rrb- log 2 m log d 1 = 2 , where u is the class of node decision functions , and n ee n can be thought of as the eeective number of leaves -lrb- it becomes small as the distribution on the leaves induced by the training data gets far from uniform -rrb- . this bound is qualitatively diierent from the vc bound and can be considerably smaller . we use the same technique to give similar results for dnf formulae ."
  },
  {
    "title": "Salient Color Names for Person Re-identification .",
    "entities": [
      "semantic analysis of images",
      "user 's feedback optimization",
      "computer vision applications",
      "metric learning method",
      "salient color names",
      "color name",
      "color distributions",
      "background information",
      "person re-identification",
      "color naming",
      "color names",
      "color spaces",
      "feature representation",
      "scncd"
    ],
    "types": "<task> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "semantic analysis of images -- USED-FOR -- computer vision applications",
      "color distributions -- USED-FOR -- feature representation",
      "background information -- USED-FOR -- person re-identification",
      "salient color names -- USED-FOR -- scncd"
    ],
    "abstract": "color naming , which relates colors with <otherscientificterm_10> , can help people with a <task_0> in many <task_2> . in this paper , we propose a novel <otherscientificterm_4> based color descriptor -lrb- <method_13> -rrb- to describe colors . <method_13> utilizes <otherscientificterm_4> to guarantee that a higher probability will be assigned to the <otherscientificterm_5> which is nearer to the color . based on <method_13> , <otherscientificterm_6> over <otherscientificterm_10> in different <otherscientificterm_11> are then obtained and fused to generate a <method_12> . moreover , the effect of <otherscientificterm_7> is employed and analyzed for <task_8> . with a simple <method_3> , the proposed <method_13> outperforms the state-of-the-art performance -lrb- without <method_1> -rrb- on two challenging datasets -lrb- viper and prid 450s -rrb- . more importantly , the proposed <method_3> can be obtained very fast if we compute <method_13> of each color in advance .",
    "abstract_og": "color naming , which relates colors with color names , can help people with a semantic analysis of images in many computer vision applications . in this paper , we propose a novel salient color names based color descriptor -lrb- scncd -rrb- to describe colors . scncd utilizes salient color names to guarantee that a higher probability will be assigned to the color name which is nearer to the color . based on scncd , color distributions over color names in different color spaces are then obtained and fused to generate a feature representation . moreover , the effect of background information is employed and analyzed for person re-identification . with a simple metric learning method , the proposed scncd outperforms the state-of-the-art performance -lrb- without user 's feedback optimization -rrb- on two challenging datasets -lrb- viper and prid 450s -rrb- . more importantly , the proposed metric learning method can be obtained very fast if we compute scncd of each color in advance ."
  },
  {
    "title": "Implementation of parallel cosine and sine modulated filter banks for equalized transmultiplexer systems .",
    "entities": [
      "reconstruction cosine modulated transmultiplexer systems",
      "filter bank based transmultiplexer systems",
      "parallel filter bank system",
      "sine modulated filter banks",
      "frequency selective channels",
      "dft-based multicarrier systems",
      "channel equalization idea",
      "realization structures",
      "receiver end",
      "parallel cosine",
      "data transmission",
      "equalizer"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "filter bank based transmultiplexer systems -- COMPARE -- dft-based multicarrier systems",
      "parallel cosine -- USED-FOR -- equalizer",
      "filter bank based transmultiplexer systems -- USED-FOR -- data transmission",
      "parallel cosine -- CONJUNCTION -- sine modulated filter banks",
      "sine modulated filter banks -- USED-FOR -- equalizer",
      "realization structures -- USED-FOR -- parallel filter bank system",
      "frequency selective channels -- USED-FOR -- data transmission"
    ],
    "abstract": "filter bank based transmultiplexer systems have certain advantages compared with existing <method_5> and <method_1> are promising candidates for <task_10> in <material_4> . we have recently proposed a novel and efficient <method_6> to be used with critically decimated perfect <method_0> . the <otherscientificterm_11> utilizes <otherscientificterm_9> and <otherscientificterm_3> in the <otherscientificterm_8> . this paper explores efficient <otherscientificterm_7> for the needed <method_2> , which finds applications also in other areas .",
    "abstract_og": "filter bank based transmultiplexer systems have certain advantages compared with existing dft-based multicarrier systems and filter bank based transmultiplexer systems are promising candidates for data transmission in frequency selective channels . we have recently proposed a novel and efficient channel equalization idea to be used with critically decimated perfect reconstruction cosine modulated transmultiplexer systems . the equalizer utilizes parallel cosine and sine modulated filter banks in the receiver end . this paper explores efficient realization structures for the needed parallel filter bank system , which finds applications also in other areas ."
  },
  {
    "title": "Zero resource spoken audio corpus analysis .",
    "entities": [
      "segmental dynamic time warping algorithm",
      "expectation-maximization algorithm",
      "zero-resource speech processing",
      "acoustic pattern discovery",
      "audio document collection",
      "latent probability distributions",
      "acoustic summaries",
      "zero-resource system",
      "probabilistic model",
      "automatic analysis",
      "topical themes",
      "audio corpus",
      "hidden variables",
      "speech data",
      "pseudo-words"
    ],
    "types": "<method> <method> <task> <task> <task> <otherscientificterm> <material> <method> <method> <task> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "segmental dynamic time warping algorithm -- USED-FOR -- zero-resource system",
      "zero-resource system -- USED-FOR -- topical themes",
      "zero-resource system -- USED-FOR -- latent probability distributions",
      "zero-resource system -- USED-FOR -- acoustic pattern discovery",
      "segmental dynamic time warping algorithm -- USED-FOR -- acoustic pattern discovery",
      "topical themes -- PART-OF -- audio corpus"
    ],
    "abstract": "zero-resource speech processing involves the <task_9> of a collection of <material_13> in a completely unsupervised fashion without the benefit of any transcriptions or annotations of the data . in this paper , our <method_7> seeks to automatically discover important words , phrases and <otherscientificterm_10> present in an <material_11> . this <method_7> employs a <method_0> for <task_3> in conjunction with a <method_8> which treats the topic and pseudo-word identity of each discovered pattern as <otherscientificterm_12> . by applying an <method_1> , our <method_7> estimates the <otherscientificterm_5> over the <otherscientificterm_14> and topics associated with the discovered patterns . using this information , we produce <material_6> of the dominant <otherscientificterm_10> of the <task_4> .",
    "abstract_og": "zero-resource speech processing involves the automatic analysis of a collection of speech data in a completely unsupervised fashion without the benefit of any transcriptions or annotations of the data . in this paper , our zero-resource system seeks to automatically discover important words , phrases and topical themes present in an audio corpus . this zero-resource system employs a segmental dynamic time warping algorithm for acoustic pattern discovery in conjunction with a probabilistic model which treats the topic and pseudo-word identity of each discovered pattern as hidden variables . by applying an expectation-maximization algorithm , our zero-resource system estimates the latent probability distributions over the pseudo-words and topics associated with the discovered patterns . using this information , we produce acoustic summaries of the dominant topical themes of the audio document collection ."
  },
  {
    "title": "Manifold alignment using Procrustes analysis .",
    "entities": [
      "`` semi-supervised alignment ''",
      "dimensionality reduction method",
      "cross-lingual information retrieval",
      "markov decision processes",
      "manifold alignment",
      "procrustes analysis",
      "transfer learning",
      "mapping"
    ],
    "types": "<method> <method> <task> <method> <task> <method> <task> <method>",
    "relations": [
      "cross-lingual information retrieval -- CONJUNCTION -- transfer learning",
      "transfer learning -- PART-OF -- markov decision processes"
    ],
    "abstract": "in this paper we introduce a novel approach to <task_4> , based on <method_5> . our approach differs from <method_0> in that it results in a <method_7> that is defined everywhere - when used with a suitable <method_1> - rather than just on the training data points . we describe and evaluate our approach both theoretically and experimentally , providing results showing useful knowledge transfer from one domain to another . novel applications of our method including <task_2> and <task_6> in <method_3> are presented .",
    "abstract_og": "in this paper we introduce a novel approach to manifold alignment , based on procrustes analysis . our approach differs from `` semi-supervised alignment '' in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points . we describe and evaluate our approach both theoretically and experimentally , providing results showing useful knowledge transfer from one domain to another . novel applications of our method including cross-lingual information retrieval and transfer learning in markov decision processes are presented ."
  },
  {
    "title": "An EM Algorithm for Video SummarizationGenerative Model Approach .",
    "entities": [
      "visual video summarization problem",
      "linear dynamical system theory",
      "evolution of appearance",
      "temporal transformation symmetries",
      "image-like data structures",
      "time correlated frames",
      "time evolution models",
      "probabilistic approach",
      "bayesian framework",
      "generative model",
      "temporal information",
      "perceptual similarity",
      "video sequence",
      "sequence evolution",
      "invariance"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "bayesian framework -- USED-FOR -- visual video summarization problem",
      "linear dynamical system theory -- USED-FOR -- visual video summarization problem",
      "probabilistic approach -- USED-FOR -- visual video summarization problem"
    ],
    "abstract": "in this paper , we address the <task_0> in a <method_8> in order to detect and describe the underlying <otherscientificterm_3> in a <material_12> . given a set of <otherscientificterm_5> , we attempt to extract a reduced number of <otherscientificterm_4> which are semantically meaningful and that have the ability of representing the <otherscientificterm_13> . to this end , we present a <method_9> which involves jointly the representation and the <otherscientificterm_2> . applying <method_1> to this <task_0> , we discuss how the <otherscientificterm_10> is encoded yielding a manner of grouping the iconic representations of the <material_12> in terms of <otherscientificterm_14> . the formulation of this <task_0> is driven in terms of a <method_7> , which affords a measure of <otherscientificterm_11> taking both learned appearance and <method_6> into account .",
    "abstract_og": "in this paper , we address the visual video summarization problem in a bayesian framework in order to detect and describe the underlying temporal transformation symmetries in a video sequence . given a set of time correlated frames , we attempt to extract a reduced number of image-like data structures which are semantically meaningful and that have the ability of representing the sequence evolution . to this end , we present a generative model which involves jointly the representation and the evolution of appearance . applying linear dynamical system theory to this visual video summarization problem , we discuss how the temporal information is encoded yielding a manner of grouping the iconic representations of the video sequence in terms of invariance . the formulation of this visual video summarization problem is driven in terms of a probabilistic approach , which affords a measure of perceptual similarity taking both learned appearance and time evolution models into account ."
  },
  {
    "title": "Hybrid weak-perspective and full-perspective matching .",
    "entities": [
      "t h e weak-perspective algorithm",
      "3d model-based robot navigation",
      "probabilistic combinatorial optimization algorithm",
      "w eak-perspective algorithm",
      "h ybrid algorithm",
      "2d images",
      "full-perspective algorithm",
      "2d projection",
      "3d landmark",
      "weak-perspective mappings",
      "full-perspective mappings",
      "landmark",
      "rotation",
      "scaling",
      "translation",
      "robot"
    ],
    "types": "<method> <task> <method> <method> <method> <material> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm>",
    "relations": [
      "translation -- CONJUNCTION -- scaling",
      "rotation -- CONJUNCTION -- translation"
    ],
    "abstract": "full-perspective mappings between 3d objects and <material_5> are more complicated than <method_9> , which consider only <otherscientificterm_12> , <task_14> and <task_13> . therefore , in <task_1> , it is important to understand how and when full-perspective m ust be taken into account . in this paper we use a <method_2> to search for an optimal match between <otherscientificterm_8> and 2d image features . three variations are considered : a <method_3> rotates , translates and scales an initial <otherscientificterm_7> of the <otherscientificterm_8> . a <method_6> always recomputes the <otherscientificterm_15> 's pose and repro-jects the <otherscientificterm_11> when testing alternative matches . finally , a <method_4> uses weak-perspective t o select a most promising alternative , but then updates the pose and reprojects the <otherscientificterm_11> . the <method_6> appears to combine the best attributes of the other two . like the <method_6> , it reliably recovers the true pose of the <otherscientificterm_15> , and like <method_0> , it runs 5 to 10 faster than the <method_6> .",
    "abstract_og": "full-perspective mappings between 3d objects and 2d images are more complicated than weak-perspective mappings , which consider only rotation , translation and scaling . therefore , in 3d model-based robot navigation , it is important to understand how and when full-perspective m ust be taken into account . in this paper we use a probabilistic combinatorial optimization algorithm to search for an optimal match between 3d landmark and 2d image features . three variations are considered : a w eak-perspective algorithm rotates , translates and scales an initial 2d projection of the 3d landmark . a full-perspective algorithm always recomputes the robot 's pose and repro-jects the landmark when testing alternative matches . finally , a h ybrid algorithm uses weak-perspective t o select a most promising alternative , but then updates the pose and reprojects the landmark . the full-perspective algorithm appears to combine the best attributes of the other two . like the full-perspective algorithm , it reliably recovers the true pose of the robot , and like t h e weak-perspective algorithm , it runs 5 to 10 faster than the full-perspective algorithm ."
  },
  {
    "title": "Fast Newton-type Methods for Total Variation Regularization .",
    "entities": [
      "anisotropic -lrb- \u2113 1-based -rrb- tv",
      "total variation penalties",
      "newton-type methods",
      "machine learning",
      "1d-tv algorithms",
      "optimization problems",
      "image de-noising",
      "tv solvers",
      "signal processing",
      "fused-lasso"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <method> <task> <task> <method> <task> <otherscientificterm>",
    "relations": [
      "signal processing -- CONJUNCTION -- machine learning",
      "total variation penalties -- USED-FOR -- machine learning"
    ],
    "abstract": "numerous applications in statistics , <task_8> , and <task_3> regularize using <otherscientificterm_1> . we study <method_0> and also a related \u2113 2-norm variant . we consider for both variants associated -lrb- 1d -rrb- proximity operators , which lead to challenging <task_5> . we solve these problems by developing <method_2> that outperform the state-of-the-art algorithms . more importantly , our <method_4> serve as building blocks for solving the harder task of computing 2 - -lrb- and higher -rrb- - dimensional tv proximity . we illustrate the computational benefits of our <method_4> by applying <method_4> to several applications : -lrb- i -rrb- <task_6> ; -lrb- ii -rrb- image deconvolution -lrb- by plugging in our <method_7> into publicly available software -rrb- ; and -lrb- iii -rrb- four variants of <otherscientificterm_9> . the results show large speedups -- and to support our claims , we provide software accompanying this paper .",
    "abstract_og": "numerous applications in statistics , signal processing , and machine learning regularize using total variation penalties . we study anisotropic -lrb- \u2113 1-based -rrb- tv and also a related \u2113 2-norm variant . we consider for both variants associated -lrb- 1d -rrb- proximity operators , which lead to challenging optimization problems . we solve these problems by developing newton-type methods that outperform the state-of-the-art algorithms . more importantly , our 1d-tv algorithms serve as building blocks for solving the harder task of computing 2 - -lrb- and higher -rrb- - dimensional tv proximity . we illustrate the computational benefits of our 1d-tv algorithms by applying 1d-tv algorithms to several applications : -lrb- i -rrb- image de-noising ; -lrb- ii -rrb- image deconvolution -lrb- by plugging in our tv solvers into publicly available software -rrb- ; and -lrb- iii -rrb- four variants of fused-lasso . the results show large speedups -- and to support our claims , we provide software accompanying this paper ."
  },
  {
    "title": "Resampling auxiliary data for language model adaptation in machine translation for speech .",
    "entities": [
      "speech to speech translation system",
      "n-gram language models",
      "sparse in-domain resources",
      "in-domain conversational data",
      "language modeling community",
      "domain textual data",
      "resampled language models",
      "auxiliary textual material",
      "language models",
      "n-gram ratios",
      "newswire text",
      "s2s system",
      "cen-troid similarity",
      "bleu score"
    ],
    "types": "<task> <method> <material> <material> <method> <material> <method> <material> <method> <otherscientificterm> <material> <task> <otherscientificterm> <metric>",
    "relations": [
      "auxiliary textual material -- USED-FOR -- language modeling community",
      "n-gram ratios -- CONJUNCTION -- resampled language models",
      "domain textual data -- USED-FOR -- s2s system",
      "newswire text -- HYPONYM-OF -- domain textual data",
      "auxiliary textual material -- USED-FOR -- sparse in-domain resources",
      "cen-troid similarity -- CONJUNCTION -- n-gram ratios",
      "language models -- USED-FOR -- speech to speech translation system"
    ],
    "abstract": "performance of <method_1> depends to a large extent on the amount of training text material available for building the <method_1> and the degree to which this text matches the domain of interest . the <method_4> is showing a growing interest in using large collections of <material_7> to supplement <material_2> . one of the problems in using such <method_1> is that <method_1> may differ significantly from the specific nature of the domain of interest . in this paper , we propose three different methods for adapting <method_8> for a <task_0> when <method_1> are of different genre and domain . the proposed methods are based on <otherscientificterm_12> , <otherscientificterm_9> and <method_6> . we show how these methods can be used to select out of <material_5> such as <material_10> to improve a <task_11> . we were able to achieve an overall relative improvement of 3.8 % in <metric_13> over a baseline system that uses only <material_3> .",
    "abstract_og": "performance of n-gram language models depends to a large extent on the amount of training text material available for building the n-gram language models and the degree to which this text matches the domain of interest . the language modeling community is showing a growing interest in using large collections of auxiliary textual material to supplement sparse in-domain resources . one of the problems in using such n-gram language models is that n-gram language models may differ significantly from the specific nature of the domain of interest . in this paper , we propose three different methods for adapting language models for a speech to speech translation system when n-gram language models are of different genre and domain . the proposed methods are based on cen-troid similarity , n-gram ratios and resampled language models . we show how these methods can be used to select out of domain textual data such as newswire text to improve a s2s system . we were able to achieve an overall relative improvement of 3.8 % in bleu score over a baseline system that uses only in-domain conversational data ."
  },
  {
    "title": "Learning Taxonomies by Dependence Maximization .",
    "entities": [
      "image and text data",
      "numerical taxonomy clustering",
      "dependence maximization approach",
      "spectral clustering",
      "unsupervised algorithms",
      "taxonomy",
      "clustering"
    ],
    "types": "<material> <method> <method> <method> <method> <method> <method>",
    "relations": [
      "spectral clustering -- CONJUNCTION -- dependence maximization approach"
    ],
    "abstract": "we introduce a family of <method_4> , <method_1> , to simultaneously cluster data , and to learn a <method_5> that encodes the relationship between the clusters . the <method_4> work by maximizing the dependence between the <method_5> and the original data . the resulting <method_5> is a more informative visualization of complex data than simple <method_6> ; in addition , taking into account the relations between different clusters is shown to substantially improve the quality of the <method_6> , when compared with state-of-the-art <method_4> in the literature -lrb- both <method_3> and a previous <method_2> -rrb- . we demonstrate our algorithm on <material_0> .",
    "abstract_og": "we introduce a family of unsupervised algorithms , numerical taxonomy clustering , to simultaneously cluster data , and to learn a taxonomy that encodes the relationship between the clusters . the unsupervised algorithms work by maximizing the dependence between the taxonomy and the original data . the resulting taxonomy is a more informative visualization of complex data than simple clustering ; in addition , taking into account the relations between different clusters is shown to substantially improve the quality of the clustering , when compared with state-of-the-art unsupervised algorithms in the literature -lrb- both spectral clustering and a previous dependence maximization approach -rrb- . we demonstrate our algorithm on image and text data ."
  },
  {
    "title": "On Kernel Methods for Relational Learning .",
    "entities": [
      "high dimensional implicit space",
      "structured and relational data",
      "machine learning community",
      "restricted feature space",
      "natural language processing",
      "high-dimensional feature spaces",
      "description language",
      "limited expressivity",
      "relational domains",
      "kernel operations",
      "time complexity",
      "relational kernels",
      "relational learning",
      "kernel methods",
      "kernel learning",
      "kernel functions",
      "feature space",
      "generalization ability",
      "kernels"
    ],
    "types": "<otherscientificterm> <material> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "structured and relational data -- USED-FOR -- kernel operations",
      "kernel methods -- PART-OF -- machine learning community",
      "relational domains -- FEATURE-OF -- kernel learning"
    ],
    "abstract": "kernel methods have gained a great deal of popularity in the <task_2> as a method to learn indirectly in <otherscientificterm_5> . those interested in <method_12> have recently begun to cast learning from <material_1> in terms of <task_9> . we describe a general family of <otherscientificterm_15> built up from a <otherscientificterm_6> of <otherscientificterm_7> and use it to study the benefits and drawbacks of <task_14> in <material_8> . learning with <otherscientificterm_18> in this family directly models learning over an expanded <otherscientificterm_16> constructed using the same <otherscientificterm_6> . this allows us to examine issues of <otherscientificterm_10> in terms of learning with these and other <otherscientificterm_11> , and how these relate to <otherscientificterm_17> . the tradeoffs between using <otherscientificterm_18> in a very <otherscientificterm_0> versus a <otherscientificterm_3> , is highlighted through two experiments , in bioinformatics and in <task_4> .",
    "abstract_og": "kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in high-dimensional feature spaces . those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations . we describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains . learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language . this allows us to examine issues of time complexity in terms of learning with these and other relational kernels , and how these relate to generalization ability . the tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space , is highlighted through two experiments , in bioinformatics and in natural language processing ."
  },
  {
    "title": "Convolution Kernels on Constituent , Dependency and Sequential Structures for Relation Extraction .",
    "entities": [
      "constituent and dependency parse trees",
      "target relation extraction task",
      "syntactic and semantic structures",
      "automated relation extraction",
      "support vector machines",
      "word sequence kernels",
      "ace 2004 corpus",
      "lexical sequences",
      "syntactic tree",
      "partial tree",
      "entity types",
      "semantics concerns",
      "syntax"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <task> <method> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "partial tree -- CONJUNCTION -- word sequence kernels",
      "syntactic tree -- CONJUNCTION -- partial tree",
      "constituent and dependency parse trees -- USED-FOR -- syntax"
    ],
    "abstract": "this paper explores the use of innovative kernels based on <otherscientificterm_2> for a <task_1> . <material_12> is derived from <otherscientificterm_0> whereas <otherscientificterm_11> to <otherscientificterm_10> and <material_7> . we investigate the effectiveness of such representations in the <task_3> from text . we process the above data by means of <method_4> along with the <otherscientificterm_8> , the <otherscientificterm_9> and the <otherscientificterm_5> . our study on the <material_6> illustrates that the combination of the above kernels achieves high effectiveness and significantly improves the current state-of-the-art .",
    "abstract_og": "this paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task . syntax is derived from constituent and dependency parse trees whereas semantics concerns to entity types and lexical sequences . we investigate the effectiveness of such representations in the automated relation extraction from text . we process the above data by means of support vector machines along with the syntactic tree , the partial tree and the word sequence kernels . our study on the ace 2004 corpus illustrates that the combination of the above kernels achieves high effectiveness and significantly improves the current state-of-the-art ."
  },
  {
    "title": "Situation Testing-Based Discrimination Discovery : A Causal Inference Approach .",
    "entities": [
      "legally grounded situation testing methodology",
      "causal bayesian networks",
      "discrimination discovery",
      "real dataset",
      "protected-by-law group",
      "causal inference",
      "historical dataset",
      "causal structure",
      "similarity measurement",
      "discrimination",
      "eciency",
      "accuracy"
    ],
    "types": "<method> <method> <task> <material> <otherscientificterm> <method> <material> <otherscientificterm> <method> <otherscientificterm> <metric> <metric>",
    "relations": [
      "accuracy -- CONJUNCTION -- eciency"
    ],
    "abstract": "discrimination discovery is to unveil <otherscientificterm_9> against a specific individual by analyzing the <material_6> . in this paper , we develop a general technique to capture <otherscientificterm_9> based on the <method_0> . for any individual , we find pairs of tuples from the dataset with similar characteristics apart from belonging or not to the <otherscientificterm_4> and assign them in two groups . the individual is considered as discriminated if significant di \u21b5 erence is observed between the decisions from the two groups . to find similar tuples , we make use of the <method_1> and the associated <method_5> as a guideline . the <otherscientificterm_7> of the dataset and the causal e \u21b5 ect of each attribute on the decision are used to facilitate the <method_8> . through empirical assessments on a <material_3> , our approach shows good ecacy both in <metric_11> and <metric_10> .",
    "abstract_og": "discrimination discovery is to unveil discrimination against a specific individual by analyzing the historical dataset . in this paper , we develop a general technique to capture discrimination based on the legally grounded situation testing methodology . for any individual , we find pairs of tuples from the dataset with similar characteristics apart from belonging or not to the protected-by-law group and assign them in two groups . the individual is considered as discriminated if significant di \u21b5 erence is observed between the decisions from the two groups . to find similar tuples , we make use of the causal bayesian networks and the associated causal inference as a guideline . the causal structure of the dataset and the causal e \u21b5 ect of each attribute on the decision are used to facilitate the similarity measurement . through empirical assessments on a real dataset , our approach shows good ecacy both in accuracy and eciency ."
  },
  {
    "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate .",
    "entities": [
      "variance-reduced stochastic gradient technique",
      "principal component analysis",
      "singular value decomposition",
      "slow convergence",
      "runtime scales",
      "non-convex problem",
      "stochastic iterations",
      "data size"
    ],
    "types": "<method> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "principal component analysis -- CONJUNCTION -- singular value decomposition"
    ],
    "abstract": "we describe and analyze a simple algorithm for <task_1> and <task_2> , vr-pca , which uses computa-tionally cheap <otherscientificterm_6> , yet converges exponentially fast to the optimal solution . in contrast , existing algorithms suffer either from <otherscientificterm_3> , or computationally intensive iterations whose <otherscientificterm_4> with the <otherscientificterm_7> . the algorithm builds on a recent <method_0> , which was previously analyzed for strongly convex optimization , whereas here we apply it to an inherently <task_5> , using a very different analysis .",
    "abstract_og": "we describe and analyze a simple algorithm for principal component analysis and singular value decomposition , vr-pca , which uses computa-tionally cheap stochastic iterations , yet converges exponentially fast to the optimal solution . in contrast , existing algorithms suffer either from slow convergence , or computationally intensive iterations whose runtime scales with the data size . the algorithm builds on a recent variance-reduced stochastic gradient technique , which was previously analyzed for strongly convex optimization , whereas here we apply it to an inherently non-convex problem , using a very different analysis ."
  },
  {
    "title": "Information-bearing components of speech intelligibility under babble-noise and bandlimiting distortions .",
    "entities": [
      "spectro-temporal modulation index",
      "spectral and temporal modulations",
      "additive babble noise",
      "intelligibility of speech",
      "representation of sound",
      "biological model",
      "adverse conditions",
      "information-bearing features",
      "psychoacoustic tests",
      "low-pass filtering",
      "speech intelligibility",
      "noisy speech",
      "continuous speech",
      "features",
      "noise",
      "babble"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <method> <metric> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "spectral and temporal modulations -- USED-FOR -- intelligibility of speech",
      "spectral and temporal modulations -- FEATURE-OF -- intelligibility of speech",
      "low-pass filtering -- CONJUNCTION -- additive babble noise",
      "adverse conditions -- FEATURE-OF -- intelligibility of speech"
    ],
    "abstract": "performance of speech technologies can benefit greatly from a deeper appreciation of the nature of the <otherscientificterm_7> in <material_12> . to explore these <otherscientificterm_13> , we focus here on the role of the <otherscientificterm_1> in maintaining the <otherscientificterm_3> as it becomes severely degraded by <method_9> and <otherscientificterm_2> . these <otherscientificterm_1> are estimated using a <method_5> of auditory processing which approximates the <otherscientificterm_4> in the cortex . intelligibility of the <material_11> is computed directly from this model via the <method_0> -lsb- 1 -rsb- , and the validity of this metric is confirmed by a detailed comparison with results of <material_8> . our analysis reveals quantitatively why certain types of <otherscientificterm_14> are more disruptive to <metric_10> than others -lrb- e.g. , <otherscientificterm_15> vs. white <otherscientificterm_14> -rrb- . it also highlights the important contribution of both <otherscientificterm_1> in accurately predicting the <otherscientificterm_3> under <otherscientificterm_6> .",
    "abstract_og": "performance of speech technologies can benefit greatly from a deeper appreciation of the nature of the information-bearing features in continuous speech . to explore these features , we focus here on the role of the spectral and temporal modulations in maintaining the intelligibility of speech as it becomes severely degraded by low-pass filtering and additive babble noise . these spectral and temporal modulations are estimated using a biological model of auditory processing which approximates the representation of sound in the cortex . intelligibility of the noisy speech is computed directly from this model via the spectro-temporal modulation index -lsb- 1 -rsb- , and the validity of this metric is confirmed by a detailed comparison with results of psychoacoustic tests . our analysis reveals quantitatively why certain types of noise are more disruptive to speech intelligibility than others -lrb- e.g. , babble vs. white noise -rrb- . it also highlights the important contribution of both spectral and temporal modulations in accurately predicting the intelligibility of speech under adverse conditions ."
  },
  {
    "title": "Temporal Planning with Semantic Attachment of Non-Linear Monotonic Continuous Behaviours .",
    "entities": [
      "existent temporal planning techniques",
      "non-linear continuous monotonic functions",
      "non-linear continuous change",
      "semantic attachment mechanism",
      "external libraries",
      "physical systems",
      "planning problems",
      "native pddl",
      "planning system",
      "linear programming",
      "real-world problems",
      "planning process"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <material> <method> <method> <task> <task>",
    "relations": [
      "planning system -- USED-FOR -- planning problems"
    ],
    "abstract": "non-linear continuous change is common in <task_10> , especially those that model <method_5> . we present an algorithm which builds upon <method_0> based on <method_9> to approximate <otherscientificterm_1> . these are integrated through a <method_3> , allowing <otherscientificterm_4> or functions that are difficult to model in <material_7> to be evaluated during the <task_11> . a new <method_8> implementing this algorithm was developed and evaluated . results show that the addition of this algorithm to the <task_11> can enable <method_8> to solve a broader set of <task_6> .",
    "abstract_og": "non-linear continuous change is common in real-world problems , especially those that model physical systems . we present an algorithm which builds upon existent temporal planning techniques based on linear programming to approximate non-linear continuous monotonic functions . these are integrated through a semantic attachment mechanism , allowing external libraries or functions that are difficult to model in native pddl to be evaluated during the planning process . a new planning system implementing this algorithm was developed and evaluated . results show that the addition of this algorithm to the planning process can enable planning system to solve a broader set of planning problems ."
  },
  {
    "title": "Comparing Agents ' Success against People in Security Domains .",
    "entities": [
      "peer designed agents",
      "computer agents",
      "autonomous agents",
      "human subjects",
      "security domains"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <material>",
    "relations": [
      "security domains -- FEATURE-OF -- autonomous agents"
    ],
    "abstract": "the interaction of people with <method_2> has become increasingly prevalent . some of these settings include <material_4> , where people can be characterized as uncooperative , hostile , manipulative , and tending to take advantage of the situation for their own needs . this makes it challenging to design proficient agents to interact with people in such environments . evaluating the success of the agents automatically before evaluating them with people or deploying them could alleviate this challenge and result in better designed agents . in this paper we show how <method_0> -- <method_1> developed by <otherscientificterm_3> -- can be used as a method for evaluating <method_2> in <material_4> . such evaluation can reduce the effort and costs involved in evaluating <method_2> interacting with people to validate their efficacy . our experiments included more than 70 <otherscientificterm_3> and 40 <method_0> developed by students . the study provides empirical support that <method_0> can be used to compare the proficiency of <method_2> when matched with people in <material_4> .",
    "abstract_og": "the interaction of people with autonomous agents has become increasingly prevalent . some of these settings include security domains , where people can be characterized as uncooperative , hostile , manipulative , and tending to take advantage of the situation for their own needs . this makes it challenging to design proficient agents to interact with people in such environments . evaluating the success of the agents automatically before evaluating them with people or deploying them could alleviate this challenge and result in better designed agents . in this paper we show how peer designed agents -- computer agents developed by human subjects -- can be used as a method for evaluating autonomous agents in security domains . such evaluation can reduce the effort and costs involved in evaluating autonomous agents interacting with people to validate their efficacy . our experiments included more than 70 human subjects and 40 peer designed agents developed by students . the study provides empirical support that peer designed agents can be used to compare the proficiency of autonomous agents when matched with people in security domains ."
  },
  {
    "title": "Symmetry Breaking via LexLeader Feasibility Checkers .",
    "entities": [
      "lexleader feasibility checkers",
      "partial assignments",
      "partial assignment",
      "value symmetries",
      "variable orderings",
      "matrix models",
      "canonical solution",
      "snakelex",
      "doublelex"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "partial assignments -- CONJUNCTION -- variable orderings",
      "variable orderings -- CONJUNCTION -- value symmetries",
      "doublelex -- CONJUNCTION -- snakelex"
    ],
    "abstract": "this paper considers <method_5> , a class of <method_5> which generally exhibit significant symmetries . it proposed the idea of <method_0> that verify , during search , whether the current <otherscientificterm_2> can be extended into a <method_6> . the <method_0> are based on a novel result by -lsb- katsirelos et al. , 2010 -rsb- on how to check efficiently whether a solution is canonical . the paper generalizes this result to <otherscientificterm_1> , various <otherscientificterm_4> , and <otherscientificterm_3> . empirical results on 5 standard benchmarks shows that <method_0> may bring significant performance gains , when jointly used with <method_8> or <method_7> .",
    "abstract_og": "this paper considers matrix models , a class of matrix models which generally exhibit significant symmetries . it proposed the idea of lexleader feasibility checkers that verify , during search , whether the current partial assignment can be extended into a canonical solution . the lexleader feasibility checkers are based on a novel result by -lsb- katsirelos et al. , 2010 -rsb- on how to check efficiently whether a solution is canonical . the paper generalizes this result to partial assignments , various variable orderings , and value symmetries . empirical results on 5 standard benchmarks shows that lexleader feasibility checkers may bring significant performance gains , when jointly used with doublelex or snakelex ."
  },
  {
    "title": "Complex Newton algorithm for blind signal extraction of speech in diffuse noise .",
    "entities": [
      "frequency domain blind signal extraction method",
      "frequency domain blind signal separation",
      "blind signal separation approach",
      "diffuse background noise",
      "nonlin-ear post filter",
      "cost function",
      "speech enhancement",
      "diffuse noise",
      "gradient descent",
      "newton algorithm",
      "newton method",
      "minimization"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "frequency domain blind signal extraction method -- USED-FOR -- diffuse noise",
      "minimization -- USED-FOR -- frequency domain blind signal extraction method",
      "diffuse noise -- CONJUNCTION -- nonlin-ear post filter",
      "gradient descent -- CONJUNCTION -- blind signal separation approach",
      "diffuse background noise -- USED-FOR -- speech enhancement"
    ],
    "abstract": "several recent methods for <task_6> in presence of <otherscientificterm_3> use <task_1> to estimate the <otherscientificterm_7> and a <method_4> to suppress this estimated noise . this paper presents a <method_0> for estimating the <otherscientificterm_7> in place of the <task_1> . the <method_0> is based on the <method_11> by means of a complex <method_9> of a <otherscientificterm_5> depending of the modulus of the extracted component . the proposed complex <method_10> is compared to the <otherscientificterm_8> on the same <otherscientificterm_5> and to the <method_2> .",
    "abstract_og": "several recent methods for speech enhancement in presence of diffuse background noise use frequency domain blind signal separation to estimate the diffuse noise and a nonlin-ear post filter to suppress this estimated noise . this paper presents a frequency domain blind signal extraction method for estimating the diffuse noise in place of the frequency domain blind signal separation . the frequency domain blind signal extraction method is based on the minimization by means of a complex newton algorithm of a cost function depending of the modulus of the extracted component . the proposed complex newton method is compared to the gradient descent on the same cost function and to the blind signal separation approach ."
  },
  {
    "title": "Stereophonic acoustic echo canceller without pre-processing .",
    "entities": [
      "stereophonic acoustic echo canceller",
      "filter coefficients",
      "echo-path identification",
      "convergence analysis",
      "stable adaptation",
      "adaptive step-size",
      "filter coefficient",
      "convergence detection",
      "pre-processing",
      "echo-paths",
      "step-size"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "convergence detection -- CONJUNCTION -- adaptive step-size"
    ],
    "abstract": "this paper proposes a <method_0> without <method_8> which can identify the correct <otherscientificterm_9> . by dividing the <otherscientificterm_1> into two portions and update one part at a time , the <otherscientificterm_6> have an unique solution . <method_3> clarifies the condition for correct <task_2> . for fast convergence and <task_4> , a <method_7> and an <method_5> are also introduced . the modification amount of the <otherscientificterm_1> detects the convergence and also determines the <otherscientificterm_10> . computer simulations show 10db smaller coefficient error than those of the conventional algorithms .",
    "abstract_og": "this paper proposes a stereophonic acoustic echo canceller without pre-processing which can identify the correct echo-paths . by dividing the filter coefficients into two portions and update one part at a time , the filter coefficient have an unique solution . convergence analysis clarifies the condition for correct echo-path identification . for fast convergence and stable adaptation , a convergence detection and an adaptive step-size are also introduced . the modification amount of the filter coefficients detects the convergence and also determines the step-size . computer simulations show 10db smaller coefficient error than those of the conventional algorithms ."
  },
  {
    "title": "Convex Subspace Representation Learning from Multi-View Data .",
    "entities": [
      "convex subspace representation learning method",
      "convex min-max dual formulation",
      "common subspace representation matrix",
      "group sparsity inducing norm",
      "sparsity inducing trace norm",
      "proximal bundle optimization algorithm",
      "joint optimization problem",
      "multi-view clustering methods",
      "unsuper-vised multi-view clustering",
      "sub-space learning",
      "multi-view clustering",
      "dual norms",
      "multi-view data"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <task> <method> <task> <method> <method> <otherscientificterm> <material>",
    "relations": [
      "convex subspace representation learning method -- COMPARE -- multi-view clustering methods",
      "convex subspace representation learning method -- USED-FOR -- multi-view clustering",
      "joint optimization problem -- USED-FOR -- sub-space learning",
      "convex subspace representation learning method -- USED-FOR -- unsuper-vised multi-view clustering",
      "proximal bundle optimization algorithm -- USED-FOR -- joint optimization problem",
      "sparsity inducing trace norm -- FEATURE-OF -- convex min-max dual formulation"
    ],
    "abstract": "learning from <material_12> is important in many applications . in this paper , we propose a novel <method_0> for <task_8> . we first formulate the <method_9> with multiple views as a <task_6> with a <method_2> and a <method_3> . by exploiting the properties of <otherscientificterm_11> , we then show a <method_1> with a <otherscientificterm_4> can be obtained . we develop a <method_5> to globally solve the <task_6> . our empirical study shows the proposed <method_0> can effectively facilitate <method_10> and induce superior clustering results than alternative <method_7> .",
    "abstract_og": "learning from multi-view data is important in many applications . in this paper , we propose a novel convex subspace representation learning method for unsuper-vised multi-view clustering . we first formulate the sub-space learning with multiple views as a joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm . by exploiting the properties of dual norms , we then show a convex min-max dual formulation with a sparsity inducing trace norm can be obtained . we develop a proximal bundle optimization algorithm to globally solve the joint optimization problem . our empirical study shows the proposed convex subspace representation learning method can effectively facilitate multi-view clustering and induce superior clustering results than alternative multi-view clustering methods ."
  },
  {
    "title": "Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation .",
    "entities": [
      "hidden layer size",
      "unsupervised feature learning",
      "mnist dataset",
      "classification problem",
      "ddfa features",
      "unsupervised approaches",
      "features",
      "autoencoders"
    ],
    "types": "<otherscientificterm> <method> <material> <task> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "ddfa features -- USED-FOR -- features",
      "autoencoders -- HYPONYM-OF -- unsupervised approaches"
    ],
    "abstract": "unlike <method_5> such as <method_7> that learn to reconstruct their inputs , this paper introduces an alternative approach to <method_1> called divergent discriminative feature accumulation -lrb- <otherscientificterm_4> -rrb- that instead continually accumulates <otherscientificterm_6> that make novel discriminations among the training set . thus <otherscientificterm_4> are inherently discriminative from the start even though <otherscientificterm_4> are trained without knowledge of the ultimate <task_3> . interestingly , <otherscientificterm_4> also continues to add new <otherscientificterm_6> indefinitely -lrb- so <otherscientificterm_6> does not depend on a <otherscientificterm_0> -rrb- , is not based on minimizing error , and is inherently divergent instead of convergent , thereby providing a unique direction of research for <method_1> . in this paper the quality of its learned <otherscientificterm_6> is demonstrated on the <material_2> , where its performance confirms that indeed <otherscientificterm_4> is a viable technique for learning useful <otherscientificterm_6> .",
    "abstract_og": "unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs , this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation -lrb- ddfa features -rrb- that instead continually accumulates features that make novel discriminations among the training set . thus ddfa features are inherently discriminative from the start even though ddfa features are trained without knowledge of the ultimate classification problem . interestingly , ddfa features also continues to add new features indefinitely -lrb- so features does not depend on a hidden layer size -rrb- , is not based on minimizing error , and is inherently divergent instead of convergent , thereby providing a unique direction of research for unsupervised feature learning . in this paper the quality of its learned features is demonstrated on the mnist dataset , where its performance confirms that indeed ddfa features is a viable technique for learning useful features ."
  },
  {
    "title": "Decision tree based frame mode selection for AMR-WB + .",
    "entities": [
      "decision tree based coding mode selection method",
      "open loop mode selection module",
      "closed loop mode selection",
      "speech and music materials",
      "amr-wb + audio coder",
      "mode selection accuracy",
      "decision tree classifier",
      "amr-wb +",
      "pruning"
    ],
    "types": "<method> <method> <method> <material> <method> <metric> <method> <method> <otherscientificterm>",
    "relations": [
      "open loop mode selection module -- USED-FOR -- amr-wb +",
      "closed loop mode selection -- USED-FOR -- decision tree classifier",
      "decision tree based coding mode selection method -- USED-FOR -- amr-wb + audio coder",
      "mode selection accuracy -- EVALUATE-FOR -- decision tree based coding mode selection method",
      "mode selection accuracy -- EVALUATE-FOR -- open loop mode selection module"
    ],
    "abstract": "in this paper , we propose a <method_0> for the <method_4> . in order to obtain improved performance with reduced computation , <method_6> is adopted with the <method_2> results as the target classification labels . to secure the practical feasibility of this <method_0> , the size of the <method_6> is controlled by <otherscientificterm_8> . through an evaluation on a database covering both <material_3> , the proposed <method_0> is found to increase the <metric_5> compared with the <method_1> in the <method_7> .",
    "abstract_og": "in this paper , we propose a decision tree based coding mode selection method for the amr-wb + audio coder . in order to obtain improved performance with reduced computation , decision tree classifier is adopted with the closed loop mode selection results as the target classification labels . to secure the practical feasibility of this decision tree based coding mode selection method , the size of the decision tree classifier is controlled by pruning . through an evaluation on a database covering both speech and music materials , the proposed decision tree based coding mode selection method is found to increase the mode selection accuracy compared with the open loop mode selection module in the amr-wb + ."
  },
  {
    "title": "Sparse Finite Elements for Geodesic Contours with Level-Sets .",
    "entities": [
      "automatic detection of nested contours",
      "signed distance constraint",
      "geodesic contour problem",
      "finite difference representations",
      "numerical scheme",
      "level-set function",
      "flexible topology",
      "manual initialisation",
      "banded computation",
      "closed curves",
      "computational complexity",
      "banded algorithms",
      "level-set methods",
      "re-initialisation procedures",
      "maximum sparsity",
      "finite elements",
      "optimisation problems",
      "images",
      "re-initialisation"
    ],
    "types": "<task> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <method> <method> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm>",
    "relations": [
      "level-set methods -- USED-FOR -- optimisation problems",
      "finite elements -- USED-FOR -- banded computation",
      "banded algorithms -- USED-FOR -- computational complexity",
      "signed distance constraint -- PART-OF -- numerical scheme"
    ],
    "abstract": "level-set methods have been shown to be an effective way to solve <task_16> that involve <otherscientificterm_9> . they are well known for their capacity to deal with <otherscientificterm_6> and do not require <otherscientificterm_7> . <metric_10> has previously been addressed by using <method_11> which restrict computation to the vicinity of the zero set of the <otherscientificterm_5> . so far , such schemes have used <method_3> which suffer from limited accuracy and require <method_13> to stabilise the evolution . this paper shows how <method_8> can be achieved using <otherscientificterm_15> . we give details of the novel representation and show how to build the <otherscientificterm_1> into the presented <method_4> . we apply the algorithm to the <task_2> -lrb- including the <task_0> -rrb- and demonstrate its performance on a variety of <material_17> . the resulting algorithm has several advantages which are demonstrated in the paper : it is inherently stable and avoids <otherscientificterm_18> ; it is convergent and more accurate because of the capabilities of <otherscientificterm_15> ; it achieves <otherscientificterm_14> because with <otherscientificterm_15> the band can be effectively of width 1 .",
    "abstract_og": "level-set methods have been shown to be an effective way to solve optimisation problems that involve closed curves . they are well known for their capacity to deal with flexible topology and do not require manual initialisation . computational complexity has previously been addressed by using banded algorithms which restrict computation to the vicinity of the zero set of the level-set function . so far , such schemes have used finite difference representations which suffer from limited accuracy and require re-initialisation procedures to stabilise the evolution . this paper shows how banded computation can be achieved using finite elements . we give details of the novel representation and show how to build the signed distance constraint into the presented numerical scheme . we apply the algorithm to the geodesic contour problem -lrb- including the automatic detection of nested contours -rrb- and demonstrate its performance on a variety of images . the resulting algorithm has several advantages which are demonstrated in the paper : it is inherently stable and avoids re-initialisation ; it is convergent and more accurate because of the capabilities of finite elements ; it achieves maximum sparsity because with finite elements the band can be effectively of width 1 ."
  },
  {
    "title": "The Complexity of Subsumption in Fuzzy EL .",
    "entities": [
      "fuzzy description logics",
      "finitely valued fuzzy dls",
      "vague and imprecise knowledge",
      "\u0142ukasiewicz extensions of el",
      "lightweight dl el",
      "truth value",
      "classical dl",
      "application domains",
      "\u0142ukasiewicz semantics",
      "biomedical ontologies",
      "reasoning",
      "exptime"
    ],
    "types": "<method> <material> <otherscientificterm> <method> <method> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <task> <method>",
    "relations": [
      "fuzzy description logics -- COMPARE -- exptime"
    ],
    "abstract": "fuzzy description logics -lrb- dls -rrb- are used to represent and reason about <otherscientificterm_2> that is inherent to many <material_7> . it was recently shown that the complexity of <task_10> in <material_1> is often not higher than that of the underlying <material_6> . we show that this does not hold for fuzzy extensions of the <method_4> , which is used in many <otherscientificterm_9> , under the <otherscientificterm_8> . the complexity of <task_10> increases from <method_0> to <method_11> , even if only one additional <otherscientificterm_5> is introduced . the same lower bound holds also for infinitely valued <method_3> .",
    "abstract_og": "fuzzy description logics -lrb- dls -rrb- are used to represent and reason about vague and imprecise knowledge that is inherent to many application domains . it was recently shown that the complexity of reasoning in finitely valued fuzzy dls is often not higher than that of the underlying classical dl . we show that this does not hold for fuzzy extensions of the lightweight dl el , which is used in many biomedical ontologies , under the \u0142ukasiewicz semantics . the complexity of reasoning increases from fuzzy description logics to exptime , even if only one additional truth value is introduced . the same lower bound holds also for infinitely valued \u0142ukasiewicz extensions of el ."
  },
  {
    "title": "Analysis of HMM temporal evolution for automatic speech recognition and utterance verification .",
    "entities": [
      "temporal evolution of hmm 's state scores",
      "viterbi grammar-free decoding step",
      "hmm-based acoustic modeling",
      "utterance verification system",
      "baseline verification algorithm",
      "acoustic parameters",
      "state scores",
      "utterance verification",
      "speech recognition",
      "acoustic models",
      "decoding",
      "hmms"
    ],
    "types": "<metric> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <task> <task> <method> <method> <method>",
    "relations": [
      "acoustic models -- USED-FOR -- decoding",
      "hmms -- USED-FOR -- speech recognition"
    ],
    "abstract": "this paper proposes a double layer <task_8> and <method_3> based on the analysis of the <metric_0> . for the lower layer , it uses standard <method_2> , followed by a <method_1> which provides us with the <otherscientificterm_6> of the <method_9> . in the second layer , these <otherscientificterm_6> are added to the regular set of <otherscientificterm_5> , building a new set of expanded <method_11> . using this expanded set of <method_11> for <task_8> a significant improvement in performance is achieved . next , we will use this new architecture for <task_7> in a '' second opinion '' framework . we will consign to the second layer evaluating the reliability of <method_10> using the <method_9> from the first layer . an outstanding improvement in performance versus a <method_4> has been achieved .",
    "abstract_og": "this paper proposes a double layer speech recognition and utterance verification system based on the analysis of the temporal evolution of hmm 's state scores . for the lower layer , it uses standard hmm-based acoustic modeling , followed by a viterbi grammar-free decoding step which provides us with the state scores of the acoustic models . in the second layer , these state scores are added to the regular set of acoustic parameters , building a new set of expanded hmms . using this expanded set of hmms for speech recognition a significant improvement in performance is achieved . next , we will use this new architecture for utterance verification in a '' second opinion '' framework . we will consign to the second layer evaluating the reliability of decoding using the acoustic models from the first layer . an outstanding improvement in performance versus a baseline verification algorithm has been achieved ."
  },
  {
    "title": "Residual Algorithms : Reinforcement Learning with Function Approximation .",
    "entities": [
      "direct and residual gradient algorithms",
      "mean squared bellman residual",
      "reinforcement learning algorithms",
      "residual gradient algorithms",
      "general function-approximation system",
      "sigmoidal multilayer perceptron",
      "memory-based learning system",
      "linear function-approximation system",
      "theoretical analysis",
      "lookup tables",
      "gradient descent",
      "direct algorithms",
      "value iteration",
      "radial-basis-function system",
      "residual gradient",
      "advantage learning",
      "q-learning"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "lookup tables -- USED-FOR -- reinforcement learning algorithms",
      "sigmoidal multilayer perceptron -- CONJUNCTION -- radial-basis-function system",
      "reinforcement learning algorithms -- USED-FOR -- gradient descent",
      "residual gradient -- CONJUNCTION -- q-learning",
      "radial-basis-function system -- CONJUNCTION -- linear function-approximation system",
      "q-learning -- CONJUNCTION -- advantage learning",
      "radial-basis-function system -- CONJUNCTION -- memory-based learning system",
      "direct and residual gradient algorithms -- USED-FOR -- residual gradient algorithms",
      "residual gradient algorithms -- USED-FOR -- gradient descent",
      "memory-based learning system -- CONJUNCTION -- linear function-approximation system"
    ],
    "abstract": "a number of <method_2> have been developed that are guaranteed to converge to the optimal solution when used with <otherscientificterm_9> . it is shown , however , that these <method_2> can easily become unstable when implemented directly with a <method_4> , such as a <method_5> , a <method_13> , a <method_6> , or even a <method_7> . a new class of <method_2> , <method_3> , is proposed , which perform <otherscientificterm_10> on the <otherscientificterm_1> , guaranteeing convergence . i shown , however , that they may learn very slowly in some cases . a larger class of <method_2> , <method_3> , is proposed that has the guaranteed convergence of the <method_3> , yet can retain the fast learning speed of <method_11> . in fact , both <method_0> are shown to be special cases of <method_3> , and it is shown that <method_3> can combine the advantages of each approach . the direct , <otherscientificterm_14> , and residual forms of <method_12> , <method_16> , and <method_15> are all presented . <method_8> is given explaining the properties these <method_2> have , and simulation results are given that demonstrate these properties .",
    "abstract_og": "a number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables . it is shown , however , that these reinforcement learning algorithms can easily become unstable when implemented directly with a general function-approximation system , such as a sigmoidal multilayer perceptron , a radial-basis-function system , a memory-based learning system , or even a linear function-approximation system . a new class of reinforcement learning algorithms , residual gradient algorithms , is proposed , which perform gradient descent on the mean squared bellman residual , guaranteeing convergence . i shown , however , that they may learn very slowly in some cases . a larger class of reinforcement learning algorithms , residual gradient algorithms , is proposed that has the guaranteed convergence of the residual gradient algorithms , yet can retain the fast learning speed of direct algorithms . in fact , both direct and residual gradient algorithms are shown to be special cases of residual gradient algorithms , and it is shown that residual gradient algorithms can combine the advantages of each approach . the direct , residual gradient , and residual forms of value iteration , q-learning , and advantage learning are all presented . theoretical analysis is given explaining the properties these reinforcement learning algorithms have , and simulation results are given that demonstrate these properties ."
  },
  {
    "title": "A local intensity adaptive structural similarity index .",
    "entities": [
      "structural similarity index",
      "ssim index",
      "local intensities",
      "weighting factors",
      "luminance comparison",
      "ssim index"
    ],
    "types": "<otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "weighting factors -- USED-FOR -- ssim index",
      "ssim index -- USED-FOR -- local intensities"
    ],
    "abstract": "existing <otherscientificterm_0> comprises of one term on <otherscientificterm_4> and the other term on contrast and structure comparison . in this paper , the <material_5> is first improved by introducing three <otherscientificterm_3> to the second term such that <material_5> is adaptive to <otherscientificterm_2> of two images to be compared . the improved <metric_1> is further extended for two images with possibly different exposures . experimental results show that the proposed indices are more robust to large intensity changes of two images from the same scene and more sensitive to two images from different scenes than the existing <material_5> .",
    "abstract_og": "existing structural similarity index comprises of one term on luminance comparison and the other term on contrast and structure comparison . in this paper , the ssim index is first improved by introducing three weighting factors to the second term such that ssim index is adaptive to local intensities of two images to be compared . the improved ssim index is further extended for two images with possibly different exposures . experimental results show that the proposed indices are more robust to large intensity changes of two images from the same scene and more sensitive to two images from different scenes than the existing ssim index ."
  },
  {
    "title": "A sliding-window online fast variational sparse Bayesian learning algorithm .",
    "entities": [
      "variational sparse bayesian learning method",
      "automatic relevance determination",
      "inclusion or deletion of basis functions",
      "sliding window estimator",
      "online learning algorithm",
      "adaptive non-linear filtering",
      "sequential decision rule",
      "basis functions",
      "online fashion",
      "noise variance"
    ],
    "types": "<method> <method> <task> <method> <method> <task> <method> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "sliding window estimator -- USED-FOR -- online learning algorithm",
      "automatic relevance determination -- USED-FOR -- online learning algorithm",
      "online learning algorithm -- USED-FOR -- noise variance"
    ],
    "abstract": "in this work a new <method_4> that uses <method_1> is proposed for fast <task_5> . a <method_6> for <task_2> is obtained by applying a recently proposed fast <method_0> . the proposed <method_4> uses a <method_3> to process the data in an <material_8> . the <otherscientificterm_9> can be implicitly estimated by the <method_4> . it is shown that the described <method_4> has better mean square error -lrb- mse -rrb- performance than a state of the art kernel re-cursive least squares -lrb- kernel-rls -rrb- <method_4> when using the same number of <otherscientificterm_7> .",
    "abstract_og": "in this work a new online learning algorithm that uses automatic relevance determination is proposed for fast adaptive non-linear filtering . a sequential decision rule for inclusion or deletion of basis functions is obtained by applying a recently proposed fast variational sparse bayesian learning method . the proposed online learning algorithm uses a sliding window estimator to process the data in an online fashion . the noise variance can be implicitly estimated by the online learning algorithm . it is shown that the described online learning algorithm has better mean square error -lrb- mse -rrb- performance than a state of the art kernel re-cursive least squares -lrb- kernel-rls -rrb- online learning algorithm when using the same number of basis functions ."
  },
  {
    "title": "Multi-resolution motion estimation .",
    "entities": [
      "fine-to-coarse motion estimation",
      "spatial multi-resolution video sequences",
      "multi-resolution video coding",
      "coding schemes",
      "visual quality",
      "coarsest resolution"
    ],
    "types": "<method> <material> <task> <method> <metric> <otherscientificterm>",
    "relations": [
      "fine-to-coarse motion estimation -- USED-FOR -- coding schemes",
      "fine-to-coarse motion estimation -- USED-FOR -- multi-resolution video coding",
      "fine-to-coarse motion estimation -- USED-FOR -- coarsest resolution"
    ],
    "abstract": "spatial multi-resolution video sequences provide video at multiple frame sizes , allowing extraction of only the resolution or bit rate required by the user . this paper proposes <method_0> for <task_2> . while <method_0> , used in previously proposed <method_3> , can provide a better estimate at the <otherscientificterm_5> , <method_0> is outperformed by <method_0> at finer resolutions due to the inability of <method_0> to accurately track motion at finer resolutions . at the finest resolution , <method_0> provides a psnr improvement of up to 1 db , for the sequences tested , and better <metric_4> at all resolutions . in addition , <method_0> provides more accurate and thus more compressible motion estimates .",
    "abstract_og": "spatial multi-resolution video sequences provide video at multiple frame sizes , allowing extraction of only the resolution or bit rate required by the user . this paper proposes fine-to-coarse motion estimation for multi-resolution video coding . while fine-to-coarse motion estimation , used in previously proposed coding schemes , can provide a better estimate at the coarsest resolution , fine-to-coarse motion estimation is outperformed by fine-to-coarse motion estimation at finer resolutions due to the inability of fine-to-coarse motion estimation to accurately track motion at finer resolutions . at the finest resolution , fine-to-coarse motion estimation provides a psnr improvement of up to 1 db , for the sequences tested , and better visual quality at all resolutions . in addition , fine-to-coarse motion estimation provides more accurate and thus more compressible motion estimates ."
  },
  {
    "title": "Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification .",
    "entities": [
      "cross-domain sentiment classification methods",
      "labeled and unlabeled data",
      "sentiment classification method",
      "multiple source domains",
      "sentiment sensitive thesaurus",
      "amazon user reviews",
      "binary classifier",
      "feature vectors"
    ],
    "types": "<method> <material> <method> <material> <otherscientificterm> <material> <method> <otherscientificterm>",
    "relations": [
      "sentiment classification method -- COMPARE -- cross-domain sentiment classification methods",
      "sentiment sensitive thesaurus -- USED-FOR -- binary classifier",
      "feature vectors -- USED-FOR -- binary classifier",
      "sentiment sensitive thesaurus -- USED-FOR -- feature vectors"
    ],
    "abstract": "we describe a <method_2> that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains , designated as the source domains . we automatically create a <otherscientificterm_4> using both <material_1> from <material_3> to find the association between words that express similar sentiments in different domains . the created thesaurus is then used to expand <otherscientificterm_7> to train a <method_6> . unlike previous <method_0> , our <method_2> can efficiently learn from <material_3> . our <method_2> significantly outperforms numerous baselines and returns results that are better than or comparable to previous <method_0> on a benchmark dataset containing <material_5> for different types of products .",
    "abstract_og": "we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains , designated as the source domains . we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains . the created thesaurus is then used to expand feature vectors to train a binary classifier . unlike previous cross-domain sentiment classification methods , our sentiment classification method can efficiently learn from multiple source domains . our sentiment classification method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing amazon user reviews for different types of products ."
  },
  {
    "title": "Recovering dropped pronouns from Chinese text messages .",
    "entities": [
      "lexical , contextual and syntactic information",
      "machine learning algorithms",
      "chinese text messages",
      "sms data",
      "text messages",
      "dropped pronouns",
      "sms files",
      "chi-nese sentences",
      "informal data",
      "features",
      "pronouns"
    ],
    "types": "<otherscientificterm> <method> <material> <material> <material> <otherscientificterm> <material> <material> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "lexical , contextual and syntactic information -- USED-FOR -- features",
      "dropped pronouns -- PART-OF -- sms data",
      "text messages -- HYPONYM-OF -- informal data",
      "lexical , contextual and syntactic information -- USED-FOR -- machine learning algorithms"
    ],
    "abstract": "pronouns are frequently dropped in <material_7> , especially in <material_8> such as <material_4> . in this work we propose a solution to recover <otherscientificterm_5> in <material_3> . we manually annotate <otherscientificterm_5> in 684 <material_6> and apply <method_1> to recover them , leveraging <otherscientificterm_0> as <otherscientificterm_9> . we believe this is the first work on recovering <otherscientificterm_5> in <material_2> .",
    "abstract_og": "pronouns are frequently dropped in chi-nese sentences , especially in informal data such as text messages . in this work we propose a solution to recover dropped pronouns in sms data . we manually annotate dropped pronouns in 684 sms files and apply machine learning algorithms to recover them , leveraging lexical , contextual and syntactic information as features . we believe this is the first work on recovering dropped pronouns in chinese text messages ."
  },
  {
    "title": "Pre-steering derivative constraints for robust broadband antenna arrays .",
    "entities": [
      "lcmv -lrb- linearly constrained minimum variance -rrb- problem",
      "pre-steered front end",
      "pre-steering derivative constraints",
      "look direction",
      "lcmv processor",
      "processor robustness",
      "broadband processor",
      "linear equations",
      "objective function",
      "positional errors",
      "quantization errors",
      "constraints space",
      "mismatches",
      "constraints"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "pre-steering derivative constraints -- HYPONYM-OF -- constraints"
    ],
    "abstract": "the weights of an optimum pb -lrb- pre-steered broadband -rrb- antenna array processor are often obtained by solving a <task_0> . the <otherscientificterm_8> is the mean output power -lrb- variance -rrb- and the <otherscientificterm_11> is a set of <otherscientificterm_7> which ensure a constant gain in a fixed direction known as the <otherscientificterm_3> . however , errors in a practical scenario could degrade the performance of the <method_4> significantly , namely , <otherscientificterm_12> between the <otherscientificterm_3> and the actual doa -lrb- direction of arrival -rrb- of the desired signal , <otherscientificterm_9> in the sensors and <otherscientificterm_10> in the <otherscientificterm_1> of the <method_6> . the main contribution of this paper is the derivation of a new set of <otherscientificterm_13> , referred to as the <otherscientificterm_2> , which is able to maintain the <metric_5> in the general 3d -lrb- three dimensional -rrb- space scenario with all the errors mentioned above .",
    "abstract_og": "the weights of an optimum pb -lrb- pre-steered broadband -rrb- antenna array processor are often obtained by solving a lcmv -lrb- linearly constrained minimum variance -rrb- problem . the objective function is the mean output power -lrb- variance -rrb- and the constraints space is a set of linear equations which ensure a constant gain in a fixed direction known as the look direction . however , errors in a practical scenario could degrade the performance of the lcmv processor significantly , namely , mismatches between the look direction and the actual doa -lrb- direction of arrival -rrb- of the desired signal , positional errors in the sensors and quantization errors in the pre-steered front end of the broadband processor . the main contribution of this paper is the derivation of a new set of constraints , referred to as the pre-steering derivative constraints , which is able to maintain the processor robustness in the general 3d -lrb- three dimensional -rrb- space scenario with all the errors mentioned above ."
  },
  {
    "title": "Learning to Branch in Mixed Integer Programming .",
    "entities": [
      "machine learning framework",
      "mixed integer programming",
      "strong branching",
      "small search trees",
      "variable branching",
      "ranking function",
      "branch-and-bound search",
      "surrogate function",
      "benchmark instances",
      "parameter settings",
      "learning-to-rank problem",
      "commercial solver",
      "parameter tuning",
      "heterogeneous testbed",
      "time-consuming strategy",
      "search trees",
      "offline experimentation",
      "heuristics",
      "ml",
      "learning",
      "features",
      "branching"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <task> <otherscientificterm> <task>",
    "relations": [
      "ranking function -- USED-FOR -- branching",
      "time-consuming strategy -- USED-FOR -- small search trees",
      "heuristics -- COMPARE -- commercial solver",
      "machine learning framework -- COMPARE -- commercial solver",
      "machine learning framework -- COMPARE -- heuristics",
      "parameter tuning -- CONJUNCTION -- offline experimentation",
      "benchmark instances -- EVALUATE-FOR -- machine learning framework"
    ],
    "abstract": "the design of strategies for <task_21> in <task_1> is guided by cycles of <method_12> and <otherscientificterm_16> on an extremely <otherscientificterm_13> , using the average performance . once devised , these strategies -lrb- and their <otherscientificterm_9> -rrb- are essentially input-agnostic . to address these issues , we propose a <method_0> for <task_4> in mip . our <method_0> observes the decisions made by <method_2> , a <method_14> that produces <otherscientificterm_3> , collecting <otherscientificterm_20> that characterize the candidate <task_21> variables at each node of the tree . based on the collected data , we learn an easy-to-evaluate <otherscientificterm_7> that mimics the <method_2> , by means of solving a <task_10> , common in <task_18> . the learned <otherscientificterm_5> is then used for <task_21> . the <task_19> is instance-specific , and is performed on-the-fly while executing a <method_6> to solve the instance . experiments on <material_8> indicate that our <method_0> produces significantly smaller <otherscientificterm_15> than existing <method_17> , and is competitive with a state-of-the-art <method_11> .",
    "abstract_og": "the design of strategies for branching in mixed integer programming is guided by cycles of parameter tuning and offline experimentation on an extremely heterogeneous testbed , using the average performance . once devised , these strategies -lrb- and their parameter settings -rrb- are essentially input-agnostic . to address these issues , we propose a machine learning framework for variable branching in mip . our machine learning framework observes the decisions made by strong branching , a time-consuming strategy that produces small search trees , collecting features that characterize the candidate branching variables at each node of the tree . based on the collected data , we learn an easy-to-evaluate surrogate function that mimics the strong branching , by means of solving a learning-to-rank problem , common in ml . the learned ranking function is then used for branching . the learning is instance-specific , and is performed on-the-fly while executing a branch-and-bound search to solve the instance . experiments on benchmark instances indicate that our machine learning framework produces significantly smaller search trees than existing heuristics , and is competitive with a state-of-the-art commercial solver ."
  },
  {
    "title": "Improved Chinese broadcast news transcription by language modeling with temporally consistent training corpora and iterative phrase extraction .",
    "entities": [
      "iterative chinese new phrase extraction method",
      "chinese language model enhancement framework",
      "chinese broadcast news transcription",
      "adaptation corpora",
      "lexicon expansion",
      "temporal consistency"
    ],
    "types": "<method> <method> <material> <material> <task> <metric>",
    "relations": [
      "lexicon expansion -- PART-OF -- chinese language model enhancement framework"
    ],
    "abstract": "in this paper an <method_0> based on the intra-phrase association and context variation statistics is proposed . a <method_1> including <task_4> is then developed . extensive experiments for <material_2> were then performed to explore the achievable improvements with respect to the degree of <metric_5> for the <material_3> . very encouraging results were obtained and detailed analysis discussed .",
    "abstract_og": "in this paper an iterative chinese new phrase extraction method based on the intra-phrase association and context variation statistics is proposed . a chinese language model enhancement framework including lexicon expansion is then developed . extensive experiments for chinese broadcast news transcription were then performed to explore the achievable improvements with respect to the degree of temporal consistency for the adaptation corpora . very encouraging results were obtained and detailed analysis discussed ."
  },
  {
    "title": "Exploiting the baseband phase structure of the voiced speech for speech enhancement .",
    "entities": [
      "noise dominant frequency bins",
      "baseband stft domain",
      "speech enhancement techniques",
      "babble noise",
      "voiced frames",
      "voiced segment",
      "phase spectra",
      "log-mmse stsa",
      "spectral subtraction",
      "pitch estimation",
      "noisy speech",
      "voiced speech"
    ],
    "types": "<otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <task> <material> <material>",
    "relations": [
      "noisy speech -- USED-FOR -- pitch estimation",
      "spectral subtraction -- CONJUNCTION -- log-mmse stsa",
      "voiced speech -- PART-OF -- baseband stft domain",
      "spectral subtraction -- HYPONYM-OF -- speech enhancement techniques",
      "baseband stft domain -- FEATURE-OF -- voiced speech"
    ],
    "abstract": "performance of traditional <method_2> like <otherscientificterm_8> and log-minimum mean squared error short time spectral amplitude -lrb- <metric_7> -rrb- estimation degrades in presence of highly non-stationary noises like <otherscientificterm_3> . this is mainly due to inaccurate noise estimation during the <otherscientificterm_5> of the speech signal . in this paper , we propose to exploit the fine structure of the <otherscientificterm_6> of the <material_11> in the <material_1> . this phase structure is used to detect the <otherscientificterm_0> in the <otherscientificterm_4> . this information is used to achieve better non-stationary noise power spectral density -lrb- psd -rrb- estimation . using this estimation , performance of <otherscientificterm_8> and <metric_7> is improved overall by 0.3 and 0.2 , respectively , in terms of perceptual evaluation of speech quality -lrb- pesq -rrb- measure over the original algorithms when <material_10> is used for <task_9> . we also present the combination of these two algorithms -lrb- <otherscientificterm_8> and <metric_7> -rrb- to achieve the overall pesq improvement of 0.5 over standard <metric_7> when accurate <task_9> is available .",
    "abstract_og": "performance of traditional speech enhancement techniques like spectral subtraction and log-minimum mean squared error short time spectral amplitude -lrb- log-mmse stsa -rrb- estimation degrades in presence of highly non-stationary noises like babble noise . this is mainly due to inaccurate noise estimation during the voiced segment of the speech signal . in this paper , we propose to exploit the fine structure of the phase spectra of the voiced speech in the baseband stft domain . this phase structure is used to detect the noise dominant frequency bins in the voiced frames . this information is used to achieve better non-stationary noise power spectral density -lrb- psd -rrb- estimation . using this estimation , performance of spectral subtraction and log-mmse stsa is improved overall by 0.3 and 0.2 , respectively , in terms of perceptual evaluation of speech quality -lrb- pesq -rrb- measure over the original algorithms when noisy speech is used for pitch estimation . we also present the combination of these two algorithms -lrb- spectral subtraction and log-mmse stsa -rrb- to achieve the overall pesq improvement of 0.5 over standard log-mmse stsa when accurate pitch estimation is available ."
  },
  {
    "title": "Dialogue act compression via pitch contour preservation .",
    "entities": [
      "automatically compressing dialogue acts",
      "random baseline approach",
      "text compression method",
      "pitch contour",
      "human-authored gold-standard",
      "human an-notators",
      "prosodic approach",
      "meeting speech",
      "edit distance",
      "prosody"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method>",
    "relations": [
      "prosodic approach -- COMPARE -- text compression method",
      "prosodic approach -- COMPARE -- random baseline approach",
      "random baseline approach -- COMPARE -- text compression method",
      "human-authored gold-standard -- USED-FOR -- edit distance"
    ],
    "abstract": "this paper explores the usefulness of <method_9> in <task_0> from <material_7> . specifically , this work attempts to compress utterances by preserving the <otherscientificterm_3> of the original whole utterance . two methods of doing this are described in detail and are evaluated subjectively using <otherscientificterm_5> and objectively using <otherscientificterm_8> with a <otherscientificterm_4> . both metrics show that such a <method_6> is much better than the <method_1> and significantly better than a simple <method_2> .",
    "abstract_og": "this paper explores the usefulness of prosody in automatically compressing dialogue acts from meeting speech . specifically , this work attempts to compress utterances by preserving the pitch contour of the original whole utterance . two methods of doing this are described in detail and are evaluated subjectively using human an-notators and objectively using edit distance with a human-authored gold-standard . both metrics show that such a prosodic approach is much better than the random baseline approach and significantly better than a simple text compression method ."
  },
  {
    "title": "Prediction of Pronunciation Variations for Speech Synthesis : A Data-Driven Approach .",
    "entities": [
      "full and reduced pronunciations",
      "unit selection speech synthesis",
      "pronunciation variation prediction model",
      "speaker 's pronunciation distribution",
      "automatically categorized data",
      "machine learning techniques",
      "human labeled data",
      "acoustic modeling techniques",
      "automatic pronunciation labels",
      "utterance text",
      "speech recognition"
    ],
    "types": "<otherscientificterm> <task> <method> <otherscientificterm> <material> <method> <material> <method> <material> <material> <task>",
    "relations": [
      "machine learning techniques -- USED-FOR -- automatically categorized data",
      "acoustic modeling techniques -- USED-FOR -- full and reduced pronunciations",
      "speaker 's pronunciation distribution -- USED-FOR -- unit selection speech synthesis",
      "machine learning techniques -- USED-FOR -- pronunciation variation prediction model",
      "speech recognition -- USED-FOR -- acoustic modeling techniques"
    ],
    "abstract": "the fact that speakers vary pronunciations of the same word within their own speech is well known , but little has been done to automatically categorize and predict a <otherscientificterm_3> for <task_1> . recent work demonstrated how to automatically identify a speaker 's choice between <otherscientificterm_0> using <method_7> from <task_10> . here , we extend this approach and show how its results can be used to predict a speaker 's choice of pronunciations for synthesis . we apply <method_5> to the <material_4> to produce a <method_2> given only the <material_9> -- allowing the system to synthesize novel phrases with variations like those the speaker would make . empirical studies emphasize that we can improve <material_8> and successfully utilize the results for prediction of future synthesized examples . the prediction results based on these <material_8> are very similar to those trained from <material_6> -- allowing us to reduce manual effort while still achieving comparable results .",
    "abstract_og": "the fact that speakers vary pronunciations of the same word within their own speech is well known , but little has been done to automatically categorize and predict a speaker 's pronunciation distribution for unit selection speech synthesis . recent work demonstrated how to automatically identify a speaker 's choice between full and reduced pronunciations using acoustic modeling techniques from speech recognition . here , we extend this approach and show how its results can be used to predict a speaker 's choice of pronunciations for synthesis . we apply machine learning techniques to the automatically categorized data to produce a pronunciation variation prediction model given only the utterance text -- allowing the system to synthesize novel phrases with variations like those the speaker would make . empirical studies emphasize that we can improve automatic pronunciation labels and successfully utilize the results for prediction of future synthesized examples . the prediction results based on these automatic pronunciation labels are very similar to those trained from human labeled data -- allowing us to reduce manual effort while still achieving comparable results ."
  },
  {
    "title": "Item Bidding for Combinatorial Public Projects .",
    "entities": [
      "combina-torial public project problem",
      "coordinated deviations of subsets of agents",
      "combi-natorial preferences -lrb- valuation functions",
      "truthful approximation mechanisms",
      "optimum social welfare",
      "tight worst-case bounds",
      "vcg-based payment rule",
      "item bidding interface",
      "multi-agent environments",
      "autonomous agents",
      "social welfare",
      "abstract model",
      "computational hardness",
      "valuation functions",
      "decision making",
      "natural simplicity"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <metric>",
    "relations": [
      "abstract model -- USED-FOR -- autonomous agents",
      "combina-torial public project problem -- USED-FOR -- decision making",
      "social welfare -- USED-FOR -- autonomous agents",
      "abstract model -- USED-FOR -- decision making"
    ],
    "abstract": "we present and analyze a mechanism for the <task_0> . the problem asks to select k out of m available items , so as to maximize the <otherscientificterm_10> for <method_9> with <otherscientificterm_2> -rrb- over subsets of items . the <task_0> constitutes an <method_11> for <task_14> by <method_9> and has been shown to present severe <otherscientificterm_12> , in the design of <method_3> . we study a non-truthful mechanism that is , however , practically relevant to <otherscientificterm_8> , by virtue of its <metric_15> . <task_0> employs an <method_7> , wherein every agent issues a separate bid for the inclusion of each distinct item in the outcome ; the k items with the highest sums of bids are chosen and agents are charged according to a <otherscientificterm_6> . for fairly expressive classes of the agents ' <otherscientificterm_13> , we establish existence of socially optimal pure nash and strong equilibria , that are resilient to <otherscientificterm_1> . subsequently we derive <otherscientificterm_5> on the approximation of the <otherscientificterm_4> achieved in equilibrium . we show that the mechanism 's performance improves with the number of agents that can coordinate , and reaches half of the optimum welfare at strong equilibrium .",
    "abstract_og": "we present and analyze a mechanism for the combina-torial public project problem . the problem asks to select k out of m available items , so as to maximize the social welfare for autonomous agents with combi-natorial preferences -lrb- valuation functions -rrb- over subsets of items . the combina-torial public project problem constitutes an abstract model for decision making by autonomous agents and has been shown to present severe computational hardness , in the design of truthful approximation mechanisms . we study a non-truthful mechanism that is , however , practically relevant to multi-agent environments , by virtue of its natural simplicity . combina-torial public project problem employs an item bidding interface , wherein every agent issues a separate bid for the inclusion of each distinct item in the outcome ; the k items with the highest sums of bids are chosen and agents are charged according to a vcg-based payment rule . for fairly expressive classes of the agents ' valuation functions , we establish existence of socially optimal pure nash and strong equilibria , that are resilient to coordinated deviations of subsets of agents . subsequently we derive tight worst-case bounds on the approximation of the optimum social welfare achieved in equilibrium . we show that the mechanism 's performance improves with the number of agents that can coordinate , and reaches half of the optimum welfare at strong equilibrium ."
  },
  {
    "title": "Harmonic phase estimation in single-channel speech enhancement using von mises distribution and prior SNR .",
    "entities": [
      "single-channel speech enhancement",
      "noisy spectral phase",
      "speech enhancement",
      "signal reconstruction",
      "bayesian estimator",
      "noisy signal",
      "noisy speech",
      "spectral amplitude",
      "signal-to-noise ratio",
      "upper-bound",
      "intelligibility",
      "snrs",
      "quality"
    ],
    "types": "<task> <otherscientificterm> <task> <task> <method> <otherscientificterm> <material> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <metric>",
    "relations": [
      "spectral amplitude -- FEATURE-OF -- single-channel speech enhancement",
      "signal-to-noise ratio -- EVALUATE-FOR -- bayesian estimator",
      "intelligibility -- EVALUATE-FOR -- bayesian estimator",
      "spectral amplitude -- FEATURE-OF -- noisy signal",
      "noisy spectral phase -- USED-FOR -- signal reconstruction"
    ],
    "abstract": "in <task_0> the <otherscientificterm_7> of the <otherscientificterm_5> is often modified while the <otherscientificterm_1> is directly employed for <task_3> . recently , additional improvement in <task_2> performance has been reported when the <otherscientificterm_1> is modified . in this work , we propose a <method_4> for phase of harmonics given the <material_6> . the proposed <method_4> relies on the fundamental frequency and the <metric_8> at harmonics . throughout our experiments , we evaluate the performance of the proposed <task_0> in comparison with the <otherscientificterm_1> , a benchmark and the clean phase as the <otherscientificterm_9> . the proposed <method_4> leads to joint improvement in <metric_12> and <metric_10> at different <otherscientificterm_11> and noise types .",
    "abstract_og": "in single-channel speech enhancement the spectral amplitude of the noisy signal is often modified while the noisy spectral phase is directly employed for signal reconstruction . recently , additional improvement in speech enhancement performance has been reported when the noisy spectral phase is modified . in this work , we propose a bayesian estimator for phase of harmonics given the noisy speech . the proposed bayesian estimator relies on the fundamental frequency and the signal-to-noise ratio at harmonics . throughout our experiments , we evaluate the performance of the proposed single-channel speech enhancement in comparison with the noisy spectral phase , a benchmark and the clean phase as the upper-bound . the proposed bayesian estimator leads to joint improvement in quality and intelligibility at different snrs and noise types ."
  },
  {
    "title": "Preference Elicitation and Interview Minimization in Stable Matchings .",
    "entities": [
      "knowledge of agent preferences",
      "pure interview minimization algorithm",
      "stable matching problems",
      "coarse preference queries",
      "agent preferences",
      "stable matching"
    ],
    "types": "<otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "coarse preference queries -- USED-FOR -- knowledge of agent preferences"
    ],
    "abstract": "while <task_2> are widely studied , little work has investigated schemes for effectively eliciting <otherscientificterm_4> using either preference -lrb- e.g. , comparison -rrb- queries or interviews -lrb- to form such comparisons -rrb- ; and no work has addressed how to combine both . we develop a new model for representing and assessing <otherscientificterm_4> that accommodates both forms of information and -lrb- heuristically -rrb- minimizes the number of queries and interviews required to determine a <method_5> . our refine-then-interview -lrb- rti -rrb- scheme uses <otherscientificterm_3> to refine <otherscientificterm_0> and relies on interviews only to assess comparisons of relatively '' close '' options . empirical results show that rti compares favorably to a recent <method_1> , and that the number of interviews it requires is generally independent of the size of the market .",
    "abstract_og": "while stable matching problems are widely studied , little work has investigated schemes for effectively eliciting agent preferences using either preference -lrb- e.g. , comparison -rrb- queries or interviews -lrb- to form such comparisons -rrb- ; and no work has addressed how to combine both . we develop a new model for representing and assessing agent preferences that accommodates both forms of information and -lrb- heuristically -rrb- minimizes the number of queries and interviews required to determine a stable matching . our refine-then-interview -lrb- rti -rrb- scheme uses coarse preference queries to refine knowledge of agent preferences and relies on interviews only to assess comparisons of relatively '' close '' options . empirical results show that rti compares favorably to a recent pure interview minimization algorithm , and that the number of interviews it requires is generally independent of the size of the market ."
  },
  {
    "title": "Supervaluation Semantics for an Inland Water Feature Ontology .",
    "entities": [
      "snapshots of river networks",
      "formal concept analysis",
      "inland water features",
      "threshold parameters",
      "supervaluation semantics",
      "su-pervaluation semantics"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "formal concept analysis -- CONJUNCTION -- su-pervaluation semantics"
    ],
    "abstract": "this paper describes an ontology for <task_2> built using <method_1> and <otherscientificterm_5> . the first is used to generate a complete lattice of the water domain , whereas <otherscientificterm_4> is used to model the variability of the concepts in terms of <otherscientificterm_3> . we also present an algorithm for a mechanism of individuation and classification of water features , from <method_0> , according to the proposed ontology .",
    "abstract_og": "this paper describes an ontology for inland water features built using formal concept analysis and su-pervaluation semantics . the first is used to generate a complete lattice of the water domain , whereas supervaluation semantics is used to model the variability of the concepts in terms of threshold parameters . we also present an algorithm for a mechanism of individuation and classification of water features , from snapshots of river networks , according to the proposed ontology ."
  },
  {
    "title": "An application of system theory to stochastic models for first order chemical reactions .",
    "entities": [
      "closed and open reaction environments",
      "coupled first-order chemical reactions",
      "computation of probability distributions",
      "first-order reaction chain",
      "enzyme catalyst reaction",
      "complicated reaction system",
      "chemical species",
      "block diagrams",
      "coupled reactions",
      "transfer functions",
      "chemical reactions",
      "system theory",
      "reaction topology",
      "analytic solution",
      "molecule species",
      "molecule population",
      "vice versa"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "transfer functions -- USED-FOR -- coupled reactions",
      "block diagrams -- USED-FOR -- complicated reaction system",
      "coupled reactions -- USED-FOR -- closed and open reaction environments",
      "computation of probability distributions -- USED-FOR -- coupled first-order chemical reactions",
      "first-order reaction chain -- HYPONYM-OF -- reaction topology"
    ],
    "abstract": "a new approach for the <task_2> for <otherscientificterm_1> is introduced . the approach is based on <method_11> , where the <method_11> states are <otherscientificterm_6> and the signals are probabilities . the <otherscientificterm_9> of the so defined systems for <otherscientificterm_8> can be applied to both <otherscientificterm_0> . the use of <method_7> offers a clear , visual , and convenient way to decompose a <method_5> into simpler subsystems and <otherscientificterm_16> . since the state of the <method_11> is defined as a <otherscientificterm_14> instead of <otherscientificterm_15> , with this method one can study <otherscientificterm_10> involving any number of molecules . such analysis is shown on an <otherscientificterm_4> . in addition , a special form of <otherscientificterm_12> , the <method_3> , is studied , and the <method_13> for its distribution is derived .",
    "abstract_og": "a new approach for the computation of probability distributions for coupled first-order chemical reactions is introduced . the approach is based on system theory , where the system theory states are chemical species and the signals are probabilities . the transfer functions of the so defined systems for coupled reactions can be applied to both closed and open reaction environments . the use of block diagrams offers a clear , visual , and convenient way to decompose a complicated reaction system into simpler subsystems and vice versa . since the state of the system theory is defined as a molecule species instead of molecule population , with this method one can study chemical reactions involving any number of molecules . such analysis is shown on an enzyme catalyst reaction . in addition , a special form of reaction topology , the first-order reaction chain , is studied , and the analytic solution for its distribution is derived ."
  },
  {
    "title": "Rescaling , thinning or complementing ? On goodness-of-fit procedures for point process models and Generalized Linear Models .",
    "entities": [
      "checking model adequacy of point processes",
      "generalized linear models",
      "stochastic point processes",
      "neural spike trains",
      "neural firing rates",
      "surrogate point processes",
      "complementing point processes",
      "time-rescaling theorem",
      "coarse discretization",
      "time-series observations",
      "discretized models",
      "point-process theory",
      "thinning"
    ],
    "types": "<task> <method> <method> <task> <metric> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "time-rescaling theorem -- HYPONYM-OF -- point-process theory",
      "neural firing rates -- CONJUNCTION -- coarse discretization",
      "time-series observations -- USED-FOR -- surrogate point processes",
      "point-process theory -- USED-FOR -- generalized linear models"
    ],
    "abstract": "generalized linear models -lrb- <method_1> -rrb- are an increasingly popular framework for modeling <task_3> . they have been linked to the theory of <method_2> and researchers have used this relation to assess goodness-of-fit using methods from <method_11> , e.g. the <method_7> . however , high <metric_4> or <otherscientificterm_8> lead to a breakdown of the assumptions necessary for this connection . here , we show how goodness-of-fit tests from <method_11> can still be applied to <method_1> by constructing equivalent <method_5> out of <otherscientificterm_9> . furthermore , two additional tests based on <otherscientificterm_12> and <method_6> are introduced . they augment the instruments available for <task_0> as well as <method_10> .",
    "abstract_og": "generalized linear models -lrb- generalized linear models -rrb- are an increasingly popular framework for modeling neural spike trains . they have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory , e.g. the time-rescaling theorem . however , high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection . here , we show how goodness-of-fit tests from point-process theory can still be applied to generalized linear models by constructing equivalent surrogate point processes out of time-series observations . furthermore , two additional tests based on thinning and complementing point processes are introduced . they augment the instruments available for checking model adequacy of point processes as well as discretized models ."
  },
  {
    "title": "A Topographic Support Vector Machine : Classification Using Local Label Configurations .",
    "entities": [
      "cell image segmentation task",
      "measured vectorial feature information",
      "2d regular rectangular grid",
      "classification of objects",
      "recurrent neural network",
      "topo-graphical relationship",
      "image segmentation",
      "to-pographic neighborhood",
      "classification method",
      "label configuration",
      "topographic kernel",
      "label assignment",
      "collective classification",
      "self-consistent solution",
      "topography",
      "svm"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "topographic kernel -- CONJUNCTION -- self-consistent solution",
      "measured vectorial feature information -- USED-FOR -- classification method",
      "cell image segmentation task -- EVALUATE-FOR -- svm"
    ],
    "abstract": "the standard approach to the <task_3> is to consider the examples as independent and identically distributed -lrb- iid -rrb- . in many real world settings , however , this assumption is not valid , because a <otherscientificterm_5> exists between the objects . in this contribution we consider the special case of <task_6> , where the objects are pixels and where the underlying <otherscientificterm_14> is a <otherscientificterm_2> . we introduce a <method_8> which not only uses <otherscientificterm_1> but also the <otherscientificterm_9> within a <otherscientificterm_7> . due to the resulting dependence between the labels of neighboring pixels , a <method_12> of a set of pixels becomes necessary . we propose a new method called ` topographic support vector machine ' -lrb- tsvm -rrb- , which is based on a <method_10> and a <method_13> to the <task_11> shown to be equivalent to a <method_4> . the performance of the algorithm is compared to a conventional <method_15> on a <task_0> .",
    "abstract_og": "the standard approach to the classification of objects is to consider the examples as independent and identically distributed -lrb- iid -rrb- . in many real world settings , however , this assumption is not valid , because a topo-graphical relationship exists between the objects . in this contribution we consider the special case of image segmentation , where the objects are pixels and where the underlying topography is a 2d regular rectangular grid . we introduce a classification method which not only uses measured vectorial feature information but also the label configuration within a to-pographic neighborhood . due to the resulting dependence between the labels of neighboring pixels , a collective classification of a set of pixels becomes necessary . we propose a new method called ` topographic support vector machine ' -lrb- tsvm -rrb- , which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network . the performance of the algorithm is compared to a conventional svm on a cell image segmentation task ."
  },
  {
    "title": "Signal Independent Wideband Activity Detection Features for Microphone Arrays .",
    "entities": [
      "tolerable detection errors",
      "direction of arrival",
      "detection of activity",
      "detection methods",
      "time delays",
      "array system",
      "activity features",
      "loudspeaker experiment",
      "spatial data",
      "microphone arrays"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "time delays -- CONJUNCTION -- direction of arrival",
      "detection of activity -- USED-FOR -- microphone arrays",
      "spatial data -- USED-FOR -- activity features",
      "loudspeaker experiment -- USED-FOR -- activity features"
    ],
    "abstract": "detection of activity is a key capability for <otherscientificterm_9> . an <method_5> should tell when a source of interest is present and evaluate the usability of the computed spatial estimates . this work proposes <otherscientificterm_6> that are computed from <material_8> only , using <otherscientificterm_4> and <otherscientificterm_1> . the <otherscientificterm_6> are validated with a <method_7> . results show that the <otherscientificterm_6> are effective : <otherscientificterm_0> are achievable with simple <method_3> . in addition , <otherscientificterm_1> estimation error is reduced down to one third when unreliable estimates are discarded .",
    "abstract_og": "detection of activity is a key capability for microphone arrays . an array system should tell when a source of interest is present and evaluate the usability of the computed spatial estimates . this work proposes activity features that are computed from spatial data only , using time delays and direction of arrival . the activity features are validated with a loudspeaker experiment . results show that the activity features are effective : tolerable detection errors are achievable with simple detection methods . in addition , direction of arrival estimation error is reduced down to one third when unreliable estimates are discarded ."
  },
  {
    "title": "Meta-Learning with Memory-Augmented Neural Networks .",
    "entities": [
      "neural turing machines",
      "memory location-based focusing mechanisms",
      "memory-augmented neu-ral network",
      "augmented memory capacities",
      "deep neural networks",
      "one-shot learning",
      "external memory",
      "iterative training",
      "memory content",
      "gradient-based networks",
      "catastrophic interference",
      "architec-tures"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <material> <method> <otherscientificterm> <method>",
    "relations": [
      "neural turing machines -- HYPONYM-OF -- augmented memory capacities"
    ],
    "abstract": "despite recent breakthroughs in the applications of <method_4> , one setting that presents a persistent challenge is that of '' <task_5> . '' traditional <method_9> require a lot of data to learn , often through extensive <method_7> . when new data is encountered , the models must inefficiently relearn their parameters to adequately incorporate the new information without <otherscientificterm_10> . <method_11> with <otherscientificterm_3> , such as <method_0> , offer the ability to quickly encode and retrieve new information , and hence can potentially obviate the down-sides of conventional models . here , we demonstrate the ability of a <method_2> to rapidly assimilate new data , and leverage this data to make accurate predictions after only a few samples . we also introduce a new method for accessing an <otherscientificterm_6> that focuses on <material_8> , unlike previous methods that additionally use <method_1> .",
    "abstract_og": "despite recent breakthroughs in the applications of deep neural networks , one setting that presents a persistent challenge is that of '' one-shot learning . '' traditional gradient-based networks require a lot of data to learn , often through extensive iterative training . when new data is encountered , the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference . architec-tures with augmented memory capacities , such as neural turing machines , offer the ability to quickly encode and retrieve new information , and hence can potentially obviate the down-sides of conventional models . here , we demonstrate the ability of a memory-augmented neu-ral network to rapidly assimilate new data , and leverage this data to make accurate predictions after only a few samples . we also introduce a new method for accessing an external memory that focuses on memory content , unlike previous methods that additionally use memory location-based focusing mechanisms ."
  },
  {
    "title": "Mining Parallel Documents Using Low Bandwidth and High Precision CLIR from the Heterogeneous Web .",
    "entities": [
      "search query relevance score",
      "cross lingual information retrieval",
      "mined parallel material",
      "pure clir approach",
      "parallel documents",
      "mined documents",
      "parallel resources",
      "content-based approach",
      "mining recall",
      "url matching",
      "batch mode",
      "computational cost",
      "non-parallel sites",
      "local machines",
      "smt performance",
      "bleu score",
      "mining precision",
      "search engines",
      "web"
    ],
    "types": "<method> <method> <material> <method> <material> <material> <material> <method> <task> <method> <otherscientificterm> <metric> <material> <otherscientificterm> <metric> <metric> <metric> <method> <material>",
    "relations": [
      "cross lingual information retrieval -- USED-FOR -- content-based approach",
      "content-based approach -- USED-FOR -- parallel documents",
      "search engines -- USED-FOR -- content-based approach",
      "content-based approach -- USED-FOR -- mining recall",
      "content-based approach -- USED-FOR -- parallel resources",
      "url matching -- USED-FOR -- parallel documents",
      "computational cost -- CONJUNCTION -- local machines"
    ],
    "abstract": "we propose a <method_7> to mine <material_6> from the entire <material_18> using <method_1> with <method_0> . our <method_7> improves <task_8> by going beyond <method_9> to find <material_4> from <material_12> . we introduce <method_0> to improve the precision of mining . our <method_7> makes use of <method_17> to query for target document given each source document and therefore does not require downloading target language documents in <otherscientificterm_10> , reducing <metric_11> on the <otherscientificterm_13> and bandwidth consumption . we obtained a very high <metric_16> -lrb- 88 % -rrb- on the <material_4> by the <method_3> . after extracting parallel sentences from the <material_5> and using them to train an <method_1> , we found that the <metric_14> , with 29.88 <metric_15> , is comparable to that obtained with high quality manually translated parallel sentences with 29.54 <metric_15> , illustrating the excellent quality of the <material_2> .",
    "abstract_og": "we propose a content-based approach to mine parallel resources from the entire web using cross lingual information retrieval with search query relevance score . our content-based approach improves mining recall by going beyond url matching to find parallel documents from non-parallel sites . we introduce search query relevance score to improve the precision of mining . our content-based approach makes use of search engines to query for target document given each source document and therefore does not require downloading target language documents in batch mode , reducing computational cost on the local machines and bandwidth consumption . we obtained a very high mining precision -lrb- 88 % -rrb- on the parallel documents by the pure clir approach . after extracting parallel sentences from the mined documents and using them to train an cross lingual information retrieval , we found that the smt performance , with 29.88 bleu score , is comparable to that obtained with high quality manually translated parallel sentences with 29.54 bleu score , illustrating the excellent quality of the mined parallel material ."
  },
  {
    "title": "Influence of temporal discretization schemes on formant frequencies and bandwidths in time domain simulations of the vocal tract system .",
    "entities": [
      "piece-wise constant area function",
      "temporal finite-difference approximation",
      "frequency domain simulations",
      "transmission line model",
      "trapezoid rule",
      "finite-difference scheme",
      "acoustic propagation",
      "differential equations",
      "finite-difference schemes",
      "parameter \u03b8",
      "formant frequencies",
      "sampling rates",
      "lumped elements",
      "sampling rate",
      "vocal tract",
      "simulated vowels",
      "spatial discretization",
      "\u03b8"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <material> <otherscientificterm> <method>",
    "relations": [
      "piece-wise constant area function -- USED-FOR -- spatial discretization",
      "trapezoid rule -- PART-OF -- finite-difference scheme",
      "sampling rate -- EVALUATE-FOR -- \u03b8",
      "lumped elements -- FEATURE-OF -- vocal tract",
      "parameter \u03b8 -- USED-FOR -- finite-difference scheme",
      "acoustic propagation -- PART-OF -- vocal tract"
    ],
    "abstract": "a time domain simulation of <task_6> in the <otherscientificterm_14> requires the spatial and temporal discretization of the equations of motion and continuity . in the classic <method_3> of the <otherscientificterm_14> with <otherscientificterm_12> , the <otherscientificterm_16> is provided by the <otherscientificterm_0> . the <method_1> of the <otherscientificterm_7> can , however , vary from one implementation to the other -lrb- e.g. , -lsb- 4 -rsb- vs. -lsb- 5 -rsb- -rrb- . in this study , we have adopted a general <method_5> that depends on a <method_9> where 0 \u2264 <method_17> \u2264 1 . as special cases , this general <method_5> includes the <otherscientificterm_4> -lrb- <method_17> = 0.5 -rrb- as well as the implicit -lrb- <method_17> = 1 -rrb- and explicit -lrb- <method_17> = 0 -rrb- <otherscientificterm_8> . we have examined how <otherscientificterm_10> and bandwidths of <material_15> are effected by the choice of <method_17> . the experiments were conducted for the <metric_11> of 44.1 khz and 88.2 khz and compared with the accurate and thus desirable frequencies and bandwidths measured in <method_2> of the <otherscientificterm_14> . it can be shown that optimal values for <method_17> are slightly above 0.5 depending on the <metric_13> .",
    "abstract_og": "a time domain simulation of acoustic propagation in the vocal tract requires the spatial and temporal discretization of the equations of motion and continuity . in the classic transmission line model of the vocal tract with lumped elements , the spatial discretization is provided by the piece-wise constant area function . the temporal finite-difference approximation of the differential equations can , however , vary from one implementation to the other -lrb- e.g. , -lsb- 4 -rsb- vs. -lsb- 5 -rsb- -rrb- . in this study , we have adopted a general finite-difference scheme that depends on a parameter \u03b8 where 0 \u2264 \u03b8 \u2264 1 . as special cases , this general finite-difference scheme includes the trapezoid rule -lrb- \u03b8 = 0.5 -rrb- as well as the implicit -lrb- \u03b8 = 1 -rrb- and explicit -lrb- \u03b8 = 0 -rrb- finite-difference schemes . we have examined how formant frequencies and bandwidths of simulated vowels are effected by the choice of \u03b8 . the experiments were conducted for the sampling rates of 44.1 khz and 88.2 khz and compared with the accurate and thus desirable frequencies and bandwidths measured in frequency domain simulations of the vocal tract . it can be shown that optimal values for \u03b8 are slightly above 0.5 depending on the sampling rate ."
  },
  {
    "title": "Algorithms for Non-negative Matrix Factorization .",
    "entities": [
      "non-negative matrix factorization",
      "diagonally rescaled gradient descent",
      "generalized kullback-leibler divergence",
      "rescaling factor",
      "multi-plicative algorithms",
      "expectation-maximization algorithm",
      "multivariate data",
      "auxiliary function",
      "update rules",
      "multiplicative factor",
      "monotonic convergence",
      "decomposition"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "multiplicative factor -- USED-FOR -- update rules",
      "auxiliary function -- USED-FOR -- monotonic convergence",
      "multi-plicative algorithms -- USED-FOR -- non-negative matrix factorization"
    ],
    "abstract": "non-negative matrix factorization -lrb- <method_0> -rrb- has previously been shown to be a useful <method_11> for <material_6> . two different <method_4> for <method_0> are analyzed . <method_0> differ only slightly in the <otherscientificterm_9> used in the <otherscientificterm_8> . one algorithm can be shown to minimize the conventional least squares error while the other minimizes the <otherscientificterm_2> . the <otherscientificterm_10> of both algorithms can be proven using an <otherscientificterm_7> analogous to that used for proving convergence of the <method_5> . the algorithms can also be interpreted as <otherscientificterm_1> , where the <method_3> is optimally chosen to ensure convergence .",
    "abstract_og": "non-negative matrix factorization -lrb- non-negative matrix factorization -rrb- has previously been shown to be a useful decomposition for multivariate data . two different multi-plicative algorithms for non-negative matrix factorization are analyzed . non-negative matrix factorization differ only slightly in the multiplicative factor used in the update rules . one algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized kullback-leibler divergence . the monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the expectation-maximization algorithm . the algorithms can also be interpreted as diagonally rescaled gradient descent , where the rescaling factor is optimally chosen to ensure convergence ."
  },
  {
    "title": "Human Re-identification by Matching Compositional Template with Cluster Sampling .",
    "entities": [
      "markov chain monte carlo mechanism",
      "large human appearance variability",
      "matching solution",
      "cluster sampling",
      "visual surveillance",
      "candidacy graph",
      "partial matches",
      "public databases",
      "body information",
      "reference template",
      "re-identifying people",
      "matching algorithm",
      "surrounding clutters",
      "poses/views",
      "occlusions",
      "illumination"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "poses/views -- CONJUNCTION -- illumination"
    ],
    "abstract": "this paper aims at a newly raising task in <task_4> : <task_10> at a distance by matching <otherscientificterm_8> , given several reference examples . most of existing works solve this task by matching a <otherscientificterm_9> with the target individual , but often suffer from <otherscientificterm_1> -lrb- e.g. different <otherscientificterm_13> , <otherscientificterm_15> -rrb- and high false positives in matching caused by conjunctions , <otherscientificterm_14> or <otherscientificterm_12> . addressing these problems , we construct a simple yet expressive template from a few reference images of a certain individual , which represents the body as an articulated assembly of compositional and alternative parts , and propose an effective <method_11> with <method_3> . this algorithm is designed within a <method_5> whose vertices are matching candidates -lrb- i.e. a pair of source and target body parts -rrb- , and iterates in two steps for convergence . -lrb- i -rrb- it generates possible <otherscientificterm_6> based on compatible and competitive relations among body parts . -lrb- ii -rrb- it confirms the <otherscientificterm_6> to generate a new <method_2> , which is accepted by the <method_0> . in the experiments , we demonstrate the superior performance of our approach on three <material_7> compared to existing methods .",
    "abstract_og": "this paper aims at a newly raising task in visual surveillance : re-identifying people at a distance by matching body information , given several reference examples . most of existing works solve this task by matching a reference template with the target individual , but often suffer from large human appearance variability -lrb- e.g. different poses/views , illumination -rrb- and high false positives in matching caused by conjunctions , occlusions or surrounding clutters . addressing these problems , we construct a simple yet expressive template from a few reference images of a certain individual , which represents the body as an articulated assembly of compositional and alternative parts , and propose an effective matching algorithm with cluster sampling . this algorithm is designed within a candidacy graph whose vertices are matching candidates -lrb- i.e. a pair of source and target body parts -rrb- , and iterates in two steps for convergence . -lrb- i -rrb- it generates possible partial matches based on compatible and competitive relations among body parts . -lrb- ii -rrb- it confirms the partial matches to generate a new matching solution , which is accepted by the markov chain monte carlo mechanism . in the experiments , we demonstrate the superior performance of our approach on three public databases compared to existing methods ."
  },
  {
    "title": "On the Hardness of Approximate Reasoning .",
    "entities": [
      "counting satisfying assignments of propositional languages",
      "restricted classes of propositional formulae",
      "linear time satissability algorithms",
      "horn and monotone formulae",
      "bayesian belief networks",
      "counting satisfying assignments",
      "knowledge compilation",
      "approximate reasoning",
      "ai problems",
      "constraint satisfaction",
      "propositional domain",
      "propo-sitional expression",
      "eecient algorithms",
      "deductive reasoning",
      "model-counting problems",
      "reasoning techniques",
      "horn theories",
      "binary clauses",
      "computational diiculties",
      "approximation"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <method> <task> <task> <method> <task> <method> <material> <otherscientificterm> <method> <method> <task> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "constraint satisfaction -- CONJUNCTION -- bayesian belief networks",
      "bayesian belief networks -- HYPONYM-OF -- approximate reasoning",
      "constraint satisfaction -- CONJUNCTION -- knowledge compilation",
      "eecient algorithms -- USED-FOR -- counting satisfying assignments",
      "knowledge compilation -- CONJUNCTION -- bayesian belief networks",
      "knowledge compilation -- HYPONYM-OF -- reasoning techniques",
      "knowledge compilation -- HYPONYM-OF -- approximate reasoning",
      "constraint satisfaction -- HYPONYM-OF -- reasoning techniques",
      "constraint satisfaction -- HYPONYM-OF -- approximate reasoning"
    ],
    "abstract": "many <task_8> , when formalized , reduce to evaluating the probability that a <otherscientificterm_11> is true . in this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an <method_19> to this probability . we consider various methods used in <method_7> such as computing degree of belief and <method_4> , as well as <method_15> such as <method_9> and <task_6> , that use <method_19> to avoid <otherscientificterm_18> , and reduce them to <task_14> over a <material_10> . we prove that <task_0> is intractable even for <otherscientificterm_3> , and even when the size of clauses and number of occurrences of the variables are extremely limited . this should be contrasted with the case of <method_13> , where <method_16> and theories with <otherscientificterm_17> are distinguished by the existence of <method_2> . what is even more surprising is that , as we show , even approximating the number of satisfying assignments -lrb- i.e. , \\ approximating '' <method_7> -rrb- , is intractable for most of these restricted theories . we also identify some <method_1> for which <method_12> for <task_5> can be given .",
    "abstract_og": "many ai problems , when formalized , reduce to evaluating the probability that a propo-sitional expression is true . in this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an approximation to this probability . we consider various methods used in approximate reasoning such as computing degree of belief and bayesian belief networks , as well as reasoning techniques such as constraint satisfaction and knowledge compilation , that use approximation to avoid computational diiculties , and reduce them to model-counting problems over a propositional domain . we prove that counting satisfying assignments of propositional languages is intractable even for horn and monotone formulae , and even when the size of clauses and number of occurrences of the variables are extremely limited . this should be contrasted with the case of deductive reasoning , where horn theories and theories with binary clauses are distinguished by the existence of linear time satissability algorithms . what is even more surprising is that , as we show , even approximating the number of satisfying assignments -lrb- i.e. , \\ approximating '' approximate reasoning -rrb- , is intractable for most of these restricted theories . we also identify some restricted classes of propositional formulae for which eecient algorithms for counting satisfying assignments can be given ."
  },
  {
    "title": "Learning attentional policies for tracking and recognition in video with deep networks .",
    "entities": [
      "simultaneous object tracking and recognition",
      "dorsal pathway models",
      "human perceptual system",
      "ventral and dorsal",
      "restricted boltzmann machines",
      "particle filtering",
      "posterior distribution",
      "attentional mechanism",
      "video sequences",
      "retinal images",
      "dorsal pathway",
      "tracking uncertainty",
      "gaze data",
      "attentional model",
      "decaying resolution",
      "orientation",
      "scale",
      "location"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <material> <material> <otherscientificterm> <task> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "location -- CONJUNCTION -- scale",
      "location -- CONJUNCTION -- orientation",
      "attentional model -- USED-FOR -- simultaneous object tracking and recognition",
      "attentional mechanism -- USED-FOR -- tracking uncertainty",
      "orientation -- CONJUNCTION -- scale",
      "particle filtering -- USED-FOR -- posterior distribution"
    ],
    "abstract": "we propose a novel <method_13> for <task_0> that is driven by <material_12> . motivated by theories of the <method_2> , the <method_13> consists of two interacting pathways : <otherscientificterm_3> . the ventral pathway models object appearance and classification using deep -lrb- factored -rrb- - <otherscientificterm_4> . at each point in time , the observations consist of <material_9> , with <otherscientificterm_14> toward the periphery of the gaze . the <method_1> the <otherscientificterm_17> , <otherscientificterm_15> , <otherscientificterm_16> and speed of the attended object . the <otherscientificterm_6> of these states is estimated with <method_5> . deeper in the <otherscientificterm_10> , we encounter an <method_7> that learns to control gazes so as to minimize <task_11> . the <method_13> is modular -lrb- with each module easily replaceable with more sophisticated algorithms -rrb- , straightforward to implement , practically efficient , and works well in simple <material_8> .",
    "abstract_og": "we propose a novel attentional model for simultaneous object tracking and recognition that is driven by gaze data . motivated by theories of the human perceptual system , the attentional model consists of two interacting pathways : ventral and dorsal . the ventral pathway models object appearance and classification using deep -lrb- factored -rrb- - restricted boltzmann machines . at each point in time , the observations consist of retinal images , with decaying resolution toward the periphery of the gaze . the dorsal pathway models the location , orientation , scale and speed of the attended object . the posterior distribution of these states is estimated with particle filtering . deeper in the dorsal pathway , we encounter an attentional mechanism that learns to control gazes so as to minimize tracking uncertainty . the attentional model is modular -lrb- with each module easily replaceable with more sophisticated algorithms -rrb- , straightforward to implement , practically efficient , and works well in simple video sequences ."
  },
  {
    "title": "Common Pattern Discovery Using Earth Mover 's Distance and Local Flow Maximization .",
    "entities": [
      "earth movers distance framework",
      "unary and adaptive neighborhood color similarity",
      "local flow maximization approach",
      "reduced spatial space",
      "common pattern discovery",
      "iterative optimization",
      "segmentation-insensitive approach",
      "search space",
      "location"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "reduced spatial space -- USED-FOR -- common pattern discovery",
      "earth movers distance framework -- CONJUNCTION -- unary and adaptive neighborhood color similarity",
      "earth movers distance framework -- USED-FOR -- segmentation-insensitive approach"
    ],
    "abstract": "in this paper , we present a novel <method_6> for mining common patterns from 2 images . we develop an <method_6> using the <method_0> , <otherscientificterm_1> . we then propose a novel <method_2> to provide the best estimation of <otherscientificterm_8> and scale of the common pattern . this is achieved by performing an <method_5> in search of the most stable flows ' centroid . common pattern discovery is difficult owing to the huge <otherscientificterm_7> and problem domain . we intend to solve this problem by reducing the <otherscientificterm_7> through identifying the <otherscientificterm_8> and a <otherscientificterm_3> for <task_4> . experimental results justify the effectiveness and the potential of the <method_2> .",
    "abstract_og": "in this paper , we present a novel segmentation-insensitive approach for mining common patterns from 2 images . we develop an segmentation-insensitive approach using the earth movers distance framework , unary and adaptive neighborhood color similarity . we then propose a novel local flow maximization approach to provide the best estimation of location and scale of the common pattern . this is achieved by performing an iterative optimization in search of the most stable flows ' centroid . common pattern discovery is difficult owing to the huge search space and problem domain . we intend to solve this problem by reducing the search space through identifying the location and a reduced spatial space for common pattern discovery . experimental results justify the effectiveness and the potential of the local flow maximization approach ."
  },
  {
    "title": "A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining .",
    "entities": [
      "interpolation of translitera-tion and non-transliteration sub-models",
      "news 2010 shared task data",
      "unsupervised and semi-supervised settings",
      "transliteration pairs",
      "transliter-ation mining",
      "parallel corpora"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <task> <material>",
    "relations": [
      "parallel corpora -- USED-FOR -- transliteration pairs",
      "interpolation of translitera-tion and non-transliteration sub-models -- USED-FOR -- transliter-ation mining"
    ],
    "abstract": "we propose a novel model to automatically extract <otherscientificterm_3> from <material_5> . our model is efficient , language pair independent and mines <otherscientificterm_3> in a consistent fashion in both <otherscientificterm_2> . we model <task_4> as an <method_0> . we evaluate on <material_1> and on <material_5> with competitive results .",
    "abstract_og": "we propose a novel model to automatically extract transliteration pairs from parallel corpora . our model is efficient , language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings . we model transliter-ation mining as an interpolation of translitera-tion and non-transliteration sub-models . we evaluate on news 2010 shared task data and on parallel corpora with competitive results ."
  },
  {
    "title": "Extending Decidable Cases for Rules with Existential Variables .",
    "entities": [
      "map of known decidable cases",
      "tgd -lrb- tuple-generating dependencies -rrb-",
      "backward and forward chaining schemes",
      "existen-tially quantified variables",
      "conceptual graph rules",
      "reasoning tasks",
      "logical form",
      "deduction",
      "graph"
    ],
    "types": "<task> <material> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "existen-tially quantified variables -- USED-FOR -- reasoning tasks"
    ],
    "abstract": "in \u2200 \u2203 - rules , the conclusion may contain <otherscientificterm_3> , which makes <task_5> -lrb- as <otherscientificterm_7> -rrb- non-decidable . these rules have the same <otherscientificterm_6> as <material_1> in databases and as <otherscientificterm_4> . we extend known decidable cases by combining <method_2> , in association with a <otherscientificterm_8> that captures exactly the notion of dependency between rules . finally , we draw a <task_0> , including an extension obtained by combining our approach with very recent results on <material_1> .",
    "abstract_og": "in \u2200 \u2203 - rules , the conclusion may contain existen-tially quantified variables , which makes reasoning tasks -lrb- as deduction -rrb- non-decidable . these rules have the same logical form as tgd -lrb- tuple-generating dependencies -rrb- in databases and as conceptual graph rules . we extend known decidable cases by combining backward and forward chaining schemes , in association with a graph that captures exactly the notion of dependency between rules . finally , we draw a map of known decidable cases , including an extension obtained by combining our approach with very recent results on tgd -lrb- tuple-generating dependencies -rrb- ."
  },
  {
    "title": "Efficient Large-Scale Similarity Search Using Matrix Factorization .",
    "entities": [
      "high-dimensional global image descriptors",
      "group testing formulation",
      "locality sensitive hashing",
      "image search benchmarks",
      "small-to-medium sized problems",
      "image retrieval problem",
      "search complexity",
      "decoding architecture",
      "large-scale scenarios",
      "global descriptors",
      "dictionary learning",
      "-rrb- accuracy",
      "ya-hoo100m dataset",
      "dimen-sionality reduction",
      "vector operations",
      "query image",
      "memory",
      "images",
      "eigendecomposition",
      "accuracy"
    ],
    "types": "<otherscientificterm> <method> <method> <material> <task> <task> <metric> <method> <task> <otherscientificterm> <method> <metric> <material> <method> <task> <material> <otherscientificterm> <material> <method> <metric>",
    "relations": [
      "group testing formulation -- USED-FOR -- decoding architecture",
      "dictionary learning -- CONJUNCTION -- eigendecomposition",
      "eigendecomposition -- USED-FOR -- decoding architecture",
      "dimen-sionality reduction -- CONJUNCTION -- locality sensitive hashing",
      "group testing formulation -- USED-FOR -- small-to-medium sized problems",
      "dictionary learning -- PART-OF -- large-scale scenarios",
      "ya-hoo100m dataset -- HYPONYM-OF -- image search benchmarks",
      "dictionary learning -- USED-FOR -- decoding architecture"
    ],
    "abstract": "we consider the <task_5> of finding the <material_17> in a dataset that are most similar to a <material_15> . our goal is to reduce the number of <task_14> and <otherscientificterm_16> for performing a search without sacrificing <metric_19> of the returned <material_17> . we adopt a <method_1> and design the <method_7> using either <method_10> or <method_18> . the latter is a plausible option for <task_4> with <otherscientificterm_0> , whereas <method_10> is applicable in <task_8> . we evaluate our approach for <otherscientificterm_9> obtained from both sift and cnn features . experiments with standard <material_3> , including the <material_12> comprising 100 million <material_17> , show that our method gives comparable -lrb- and sometimes superior <metric_11> compared to exhaustive search while requiring only 10 % of the <task_14> and <otherscientificterm_16> . moreover , for the same <metric_6> , our method gives significantly better <metric_19> compared to approaches based on <method_13> or <method_2> .",
    "abstract_og": "we consider the image retrieval problem of finding the images in a dataset that are most similar to a query image . our goal is to reduce the number of vector operations and memory for performing a search without sacrificing accuracy of the returned images . we adopt a group testing formulation and design the decoding architecture using either dictionary learning or eigendecomposition . the latter is a plausible option for small-to-medium sized problems with high-dimensional global image descriptors , whereas dictionary learning is applicable in large-scale scenarios . we evaluate our approach for global descriptors obtained from both sift and cnn features . experiments with standard image search benchmarks , including the ya-hoo100m dataset comprising 100 million images , show that our method gives comparable -lrb- and sometimes superior -rrb- accuracy compared to exhaustive search while requiring only 10 % of the vector operations and memory . moreover , for the same search complexity , our method gives significantly better accuracy compared to approaches based on dimen-sionality reduction or locality sensitive hashing ."
  },
  {
    "title": "Finite data record performance analysis of rapid synchronization and combined demodulation algorithms .",
    "entities": [
      "wireless direct-sequence code-division-multiple-access communication environments",
      "short-data-record sample-matrix-inversion implementations",
      "self-synchronized receivers -lrb- integrated synchronizers/demodulators -rrb-",
      "minimum-variance-distortionless-response-type linear self-synchronized receivers",
      "computationally demanding performance evaluation",
      "asynchronous direct-sequence code-division-multiple-access communications",
      "finite data record adaptation",
      "filter order p",
      "adaptive short-data-record designs",
      "coarse synchronization error",
      "short data records",
      "system adaptivity",
      "matured discipline",
      "communication environments",
      "coarse synchronization",
      "computational complexity",
      "analytic expressions",
      "introduction"
    ],
    "types": "<task> <method> <method> <method> <task> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <task> <metric> <otherscientificterm> <method>",
    "relations": [
      "short data records -- USED-FOR -- system adaptivity",
      "short-data-record sample-matrix-inversion implementations -- USED-FOR -- coarse synchronization",
      "minimum-variance-distortionless-response-type linear self-synchronized receivers -- USED-FOR -- asynchronous direct-sequence code-division-multiple-access communications"
    ],
    "abstract": "we investigate the <task_14> performance of matched-filter-type -lrb- mf -rrb- and <method_3> for <task_5> under <otherscientificterm_6> . <otherscientificterm_16> are derived that approximate closely the probability of <otherscientificterm_9> and provide low-cost highly-accurate alternatives to the <task_4> through simulations . the expressions are explicit functions of the data record size n and the <otherscientificterm_7> and reveal the effect of <method_1> on the <task_14> performance . 1 . <method_17> the effectiveness of a receiver designed for rapidly changing <task_0> depends on establishing a successful tradeoff among the following three design objectives : -lrb- z -rrb- low <metric_15> , -lrb- 22 -rrb- multiple-access-interference -lrb- mai -rrb- near-far resistance , and -lrb- iii -rrb- system adaptivity with superior performance under limited data support . <method_8> appear as the natural next step -lsb- l -rsb- to a <otherscientificterm_12> that has extensively addressed the first two design objectives in ideal setups -lrb- perfectly known or asymptotically estimated statistical properties -rrb- -lsb- 2 -rsb- . <task_11> based on <material_10> is necessary for the development of receivers that exhibit superior bit-error-rate performance when <task_11> operate in rapidly changing <otherscientificterm_13> that limit substantially the available data support . in -lsb- 3 -rsb- , we considered <method_2> and we presented",
    "abstract_og": "we investigate the coarse synchronization performance of matched-filter-type -lrb- mf -rrb- and minimum-variance-distortionless-response-type linear self-synchronized receivers for asynchronous direct-sequence code-division-multiple-access communications under finite data record adaptation . analytic expressions are derived that approximate closely the probability of coarse synchronization error and provide low-cost highly-accurate alternatives to the computationally demanding performance evaluation through simulations . the expressions are explicit functions of the data record size n and the filter order p and reveal the effect of short-data-record sample-matrix-inversion implementations on the coarse synchronization performance . 1 . introduction the effectiveness of a receiver designed for rapidly changing wireless direct-sequence code-division-multiple-access communication environments depends on establishing a successful tradeoff among the following three design objectives : -lrb- z -rrb- low computational complexity , -lrb- 22 -rrb- multiple-access-interference -lrb- mai -rrb- near-far resistance , and -lrb- iii -rrb- system adaptivity with superior performance under limited data support . adaptive short-data-record designs appear as the natural next step -lsb- l -rsb- to a matured discipline that has extensively addressed the first two design objectives in ideal setups -lrb- perfectly known or asymptotically estimated statistical properties -rrb- -lsb- 2 -rsb- . system adaptivity based on short data records is necessary for the development of receivers that exhibit superior bit-error-rate performance when system adaptivity operate in rapidly changing communication environments that limit substantially the available data support . in -lsb- 3 -rsb- , we considered self-synchronized receivers -lrb- integrated synchronizers/demodulators -rrb- and we presented"
  },
  {
    "title": "Active Face Tracking and Pose Estimation in an Interactive Room .",
    "entities": [
      "spatial location of a user 's head",
      "foveated images of the face",
      "closed loop feedback",
      "unconstrained ooce environment",
      "active foveated camera",
      "real-time face tracking",
      "pose estimation",
      "vision routines",
      "interactive environment",
      "foveated view",
      "eigenspaces",
      "faces"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "active foveated camera -- FEATURE-OF -- unconstrained ooce environment",
      "vision routines -- USED-FOR -- interactive environment"
    ],
    "abstract": "we demonstrate <task_5> and <task_6> in an <otherscientificterm_3> with an <otherscientificterm_4> . using <method_7> previously implemented for an <otherscientificterm_8> , we determine the <otherscientificterm_0> and guide an active camera to obtain <material_1> . <otherscientificterm_11> are analyzed using a set of <otherscientificterm_10> indexed over both pose and world location . <otherscientificterm_2> from the estimated facial location is used to guide the camera when a face is present in the <otherscientificterm_9> . our system can detect the head pose of an unconstrained user in real-time as he or she moves about an open room .",
    "abstract_og": "we demonstrate real-time face tracking and pose estimation in an unconstrained ooce environment with an active foveated camera . using vision routines previously implemented for an interactive environment , we determine the spatial location of a user 's head and guide an active camera to obtain foveated images of the face . faces are analyzed using a set of eigenspaces indexed over both pose and world location . closed loop feedback from the estimated facial location is used to guide the camera when a face is present in the foveated view . our system can detect the head pose of an unconstrained user in real-time as he or she moves about an open room ."
  },
  {
    "title": "Third-Order Edge Statistics : Contour Continuation , Curvature , and Cortical Connections .",
    "entities": [
      "pairwise statistics of edges",
      "suf-ficiency of pairwise statistics",
      "human contour grouping",
      "probabilistic spectral embedding",
      "association field models",
      "curvature-dependent components",
      "association fields",
      "natural scenes",
      "order structure",
      "v1"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "association field models -- USED-FOR -- human contour grouping"
    ],
    "abstract": "association field models have attempted to explain <task_2> performance , and to explain the mean frequency of long-range horizontal connections across cortical columns in <otherscientificterm_9> . however , <otherscientificterm_6> only depend on the <otherscientificterm_0> in <material_7> . we develop a spectral test of the <otherscientificterm_1> and show there is significant higher <otherscientificterm_8> . an analysis using a <method_3> reveals <method_5> .",
    "abstract_og": "association field models have attempted to explain human contour grouping performance , and to explain the mean frequency of long-range horizontal connections across cortical columns in v1 . however , association fields only depend on the pairwise statistics of edges in natural scenes . we develop a spectral test of the suf-ficiency of pairwise statistics and show there is significant higher order structure . an analysis using a probabilistic spectral embedding reveals curvature-dependent components ."
  },
  {
    "title": "Combining recurrent neural networks and factored language models during decoding of code-Switching speech .",
    "entities": [
      "recurrent neu-ral network language models",
      "syntactic and semantic features",
      "unconverted recurrent neural network",
      "mixed error rate reduction",
      "backoff language models",
      "seame evaluation set",
      "factored language models",
      "language modeling process",
      "brown word clusters",
      "language models",
      "part-of-speech tags",
      "language modeling",
      "text material",
      "language identifiers",
      "error analysis",
      "code-switching speech",
      "perplexity",
      "decoding",
      "code-switching"
    ],
    "types": "<method> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <material> <otherscientificterm> <method> <material> <metric> <task> <task>",
    "relations": [
      "part-of-speech tags -- CONJUNCTION -- language identifiers",
      "recurrent neu-ral network language models -- PART-OF -- backoff language models",
      "language models -- USED-FOR -- decoding",
      "syntactic and semantic features -- PART-OF -- language modeling process",
      "language identifiers -- CONJUNCTION -- brown word clusters",
      "backoff language models -- USED-FOR -- decoding",
      "error analysis -- EVALUATE-FOR -- language models",
      "text material -- USED-FOR -- code-switching speech",
      "language modeling -- USED-FOR -- code-switching"
    ],
    "abstract": "in this paper , we present our latest investigations of <task_11> for <task_18> . since there is only little <material_12> for <material_15> available , we integrate <otherscientificterm_1> into the <method_7> . in particular , we use <otherscientificterm_10> , <otherscientificterm_13> , <otherscientificterm_8> and clusters of open class words . we develop <method_6> and convert <method_0> into <method_4> for an efficient usage during <task_17> . a detailed <method_14> reveals the strengths and weaknesses of the different <method_9> . when we interpolate the <method_4> linearly , we reduce the <metric_16> by 15.6 % relative on the <otherscientificterm_5> . this is even slightly better than the result of the <method_2> . we also combine the <method_9> during <task_17> and obtain a <metric_3> of 4.4 % relative on the <otherscientificterm_5> .",
    "abstract_og": "in this paper , we present our latest investigations of language modeling for code-switching . since there is only little text material for code-switching speech available , we integrate syntactic and semantic features into the language modeling process . in particular , we use part-of-speech tags , language identifiers , brown word clusters and clusters of open class words . we develop factored language models and convert recurrent neu-ral network language models into backoff language models for an efficient usage during decoding . a detailed error analysis reveals the strengths and weaknesses of the different language models . when we interpolate the backoff language models linearly , we reduce the perplexity by 15.6 % relative on the seame evaluation set . this is even slightly better than the result of the unconverted recurrent neural network . we also combine the language models during decoding and obtain a mixed error rate reduction of 4.4 % relative on the seame evaluation set ."
  },
  {
    "title": "A Color-based Particle Filter for Joint Detection and Tracking of Multiple Objects .",
    "entities": [
      "hybrid valued sequential state estimation algorithm",
      "external target detection algorithm",
      "particle filter-based solution",
      "real-world video sequences",
      "color particle filter",
      "tracking deformable objects",
      "observation feature",
      "image sequences",
      "complex backgrounds",
      "color description",
      "track initialization",
      "particle filter",
      "color"
    ],
    "types": "<method> <method> <method> <material> <method> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "track initialization -- USED-FOR -- particle filter",
      "color -- USED-FOR -- particle filter",
      "particle filter -- USED-FOR -- tracking deformable objects",
      "particle filter-based solution -- USED-FOR -- color particle filter",
      "observation feature -- USED-FOR -- particle filter",
      "complex backgrounds -- FEATURE-OF -- image sequences",
      "real-world video sequences -- EVALUATE-FOR -- hybrid valued sequential state estimation algorithm"
    ],
    "abstract": "recent works have shown that the <method_11> using <otherscientificterm_12> as <otherscientificterm_6> is a powerful technique for <task_5> in <material_7> with <otherscientificterm_8> . this paper presents a <method_0> , and its <method_2> , that extends the standard <method_4> in two ways . firstly , <task_10> is embedded in the <method_11> without relying on an <method_1> . secondly , the <method_0> is able to track multiple objects sharing the same <otherscientificterm_9> . we evaluate the performance of the proposed <method_0> on various <material_3> with appearing and disappearing targets .",
    "abstract_og": "recent works have shown that the particle filter using color as observation feature is a powerful technique for tracking deformable objects in image sequences with complex backgrounds . this paper presents a hybrid valued sequential state estimation algorithm , and its particle filter-based solution , that extends the standard color particle filter in two ways . firstly , track initialization is embedded in the particle filter without relying on an external target detection algorithm . secondly , the hybrid valued sequential state estimation algorithm is able to track multiple objects sharing the same color description . we evaluate the performance of the proposed hybrid valued sequential state estimation algorithm on various real-world video sequences with appearing and disappearing targets ."
  },
  {
    "title": "A constrained optimal data association for multiple target tracking .",
    "entities": [
      "heuris-tic adjustments of the parameters",
      "multiple target tracking",
      "false alarm errors",
      "map estimation method",
      "optimal data association",
      "adaptive mechanism",
      "natural constraints",
      "energy function",
      "parameter updation",
      "missed detection",
      "position errors",
      "clutter environment",
      "noisy measurements",
      "nnf",
      "pda"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "map estimation method -- USED-FOR -- optimal data association",
      "energy function -- USED-FOR -- map estimation method",
      "pda -- CONJUNCTION -- nnf",
      "adaptive mechanism -- USED-FOR -- parameter updation",
      "missed detection -- CONJUNCTION -- false alarm errors"
    ],
    "abstract": "one of the major problems in <task_1> is to obtain an accurate association between targets and <otherscientificterm_12> . we introduce a new scheme , called constrained optimal data association -lrb- coda -rrb- , that finds the <task_4> by a <method_3> and uses a new <method_7> . in this scheme , the <otherscientificterm_6> between targets and measurements are defined so that <otherscientificterm_6> may contain <otherscientificterm_9> and <otherscientificterm_2> . most current algorithms involve many <otherscientificterm_0> . instead , this paper suggests an <method_5> for such <task_8> . in this manner , the system automatically adapts to the <otherscientificterm_11> as it continuously changes in time and space , resulting in better association . experimental results , using <method_14> , <method_13> , and coda , show that the new approach reduces <otherscientificterm_10> in crossing trajecto-ries by 13.9 % on average compared to <method_13> .",
    "abstract_og": "one of the major problems in multiple target tracking is to obtain an accurate association between targets and noisy measurements . we introduce a new scheme , called constrained optimal data association -lrb- coda -rrb- , that finds the optimal data association by a map estimation method and uses a new energy function . in this scheme , the natural constraints between targets and measurements are defined so that natural constraints may contain missed detection and false alarm errors . most current algorithms involve many heuris-tic adjustments of the parameters . instead , this paper suggests an adaptive mechanism for such parameter updation . in this manner , the system automatically adapts to the clutter environment as it continuously changes in time and space , resulting in better association . experimental results , using pda , nnf , and coda , show that the new approach reduces position errors in crossing trajecto-ries by 13.9 % on average compared to nnf ."
  },
  {
    "title": "Unit fusion for concatenative speech synthesis .",
    "entities": [
      "sinusoidal + all-pole analysis of speech",
      "spectral trajectories of the concatenation units",
      "inventory of diphones",
      "modified speech units",
      "sonorant speech units",
      "initial spectral trajectories",
      "fusion units",
      "dynamic constraints",
      "residual-excited lpc",
      "concatenative synthesis",
      "synthesis algorithm",
      "concatenated units",
      "concatenation mismatch",
      "fusion units",
      "con-catenation units",
      "spectral information",
      "concatenation artifacts",
      "spectral discontinuities"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "synthesis algorithm -- USED-FOR -- modified speech units",
      "spectral trajectories of the concatenation units -- USED-FOR -- dynamic constraints",
      "dynamic constraints -- USED-FOR -- con-catenation units",
      "sinusoidal + all-pole analysis of speech -- USED-FOR -- synthesis algorithm"
    ],
    "abstract": "an important problem in <task_9> is the occurence of <otherscientificterm_17> or '' <task_12> '' between <otherscientificterm_4> . in this paper , we present an approach to reduce <task_12> by combining <otherscientificterm_15> from two sequences of speech units selected in parallel . <method_14> , on one hand , define <otherscientificterm_5> for a target utterance . <method_6> , on the other hand , define the desired transitions between <otherscientificterm_11> . the two <method_14> are '' fused '' by imposing <otherscientificterm_7> defined by the <otherscientificterm_13> on the <otherscientificterm_1> . to regenerate the <otherscientificterm_3> , we use a <method_10> based on <material_0> , which overcomes the limitations of <method_8> . results from a perceptual test show that our method is highly successful at removing <otherscientificterm_16> in speech generated from an <otherscientificterm_2> .",
    "abstract_og": "an important problem in concatenative synthesis is the occurence of spectral discontinuities or '' concatenation mismatch '' between sonorant speech units . in this paper , we present an approach to reduce concatenation mismatch by combining spectral information from two sequences of speech units selected in parallel . con-catenation units , on one hand , define initial spectral trajectories for a target utterance . fusion units , on the other hand , define the desired transitions between concatenated units . the two con-catenation units are '' fused '' by imposing dynamic constraints defined by the fusion units on the spectral trajectories of the concatenation units . to regenerate the modified speech units , we use a synthesis algorithm based on sinusoidal + all-pole analysis of speech , which overcomes the limitations of residual-excited lpc . results from a perceptual test show that our method is highly successful at removing concatenation artifacts in speech generated from an inventory of diphones ."
  },
  {
    "title": "Unsupervised speaker diarization using riemannian manifold clustering .",
    "entities": [
      "rieman-nian locally linear embedding algorithm",
      "nist 2010 speaker recognition evaluation set",
      "riemannian property of gaussian pdfs",
      "robust un-supervised speaker diarization",
      "riemannian manifold clustering problem",
      "speaker-homogeneous segment",
      "single-gaussian modeling",
      "speaker clustering",
      "der"
    ],
    "types": "<method> <material> <method> <task> <task> <method> <method> <task> <metric>",
    "relations": [
      "speaker clustering -- USED-FOR -- robust un-supervised speaker diarization"
    ],
    "abstract": "we address the problem of <task_7> for <task_3> . we model each <method_5> as one single full multivariate gaussian probability density function -lrb- pdf -rrb- and take into consideration the <method_2> . by assuming that segments from different speakers lie on different -lrb- possibly intersected -rrb- sub-manifolds of the manifold of gaussian pdfs , we formulate the original problem as a <task_4> . to apply the computationally simple <method_0> , we impose a constraint on the length of each segment so as to ensure the fitness of <method_6> and to increase the chance that all k-nearest neighbors of a pdf are from the same sub-manifold -lrb- speaker -rrb- . experiments on the microphone-recorded conversational interviews from <material_1> demonstrate promising results of less than 1 % <metric_8> .",
    "abstract_og": "we address the problem of speaker clustering for robust un-supervised speaker diarization . we model each speaker-homogeneous segment as one single full multivariate gaussian probability density function -lrb- pdf -rrb- and take into consideration the riemannian property of gaussian pdfs . by assuming that segments from different speakers lie on different -lrb- possibly intersected -rrb- sub-manifolds of the manifold of gaussian pdfs , we formulate the original problem as a riemannian manifold clustering problem . to apply the computationally simple rieman-nian locally linear embedding algorithm , we impose a constraint on the length of each segment so as to ensure the fitness of single-gaussian modeling and to increase the chance that all k-nearest neighbors of a pdf are from the same sub-manifold -lrb- speaker -rrb- . experiments on the microphone-recorded conversational interviews from nist 2010 speaker recognition evaluation set demonstrate promising results of less than 1 % der ."
  },
  {
    "title": "Variational Inference for Stick-Breaking Beta Process Priors .",
    "entities": [
      "variational bayesian inference algorithm",
      "matrix factorization problems",
      "non-negative factorization model",
      "truncated beta process",
      "variational inference",
      "linear-gaussian model",
      "beta process",
      "infinite counterpart"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm>",
    "relations": [
      "non-negative factorization model -- CONJUNCTION -- linear-gaussian model",
      "variational bayesian inference algorithm -- USED-FOR -- beta process"
    ],
    "abstract": "we present a <method_0> for the stick-breaking construction of the <task_6> . we derive an alternate representation of the <task_6> that is amenable to <otherscientificterm_4> , and present a bound relating the <otherscientificterm_3> to its <otherscientificterm_7> . we assess performance on two <task_1> , using a <method_2> and a <method_5> .",
    "abstract_og": "we present a variational bayesian inference algorithm for the stick-breaking construction of the beta process . we derive an alternate representation of the beta process that is amenable to variational inference , and present a bound relating the truncated beta process to its infinite counterpart . we assess performance on two matrix factorization problems , using a non-negative factorization model and a linear-gaussian model ."
  },
  {
    "title": "Music algorithm to localize sources with unknown directivity in acoustic imaging .",
    "entities": [
      "multiple signal classification algorithm",
      "pure monopolar , dipolar and quadrupolar sources",
      "fixed functional form",
      "arbitrary directional characteristics",
      "undirectional radiation pattern",
      "acoustic imaging",
      "hamiltonian matrix",
      "mathematical problem"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "multiple signal classification algorithm -- USED-FOR -- acoustic imaging"
    ],
    "abstract": "the <method_0> for <task_5> most commonly assumes that all sources have <otherscientificterm_4> . we here propose a modification of this <method_0> such that the concept is applicable for <otherscientificterm_3> of the sources . this is accomplished by fitting for each frequency the real valued amplitudes of the <task_5> rather than assuming a <otherscientificterm_2> . the <task_7> can be solved analytically resulting in an eigenvalue problem of a real valued <otherscientificterm_6> . the performance is illustrated in simulations using <material_1> .",
    "abstract_og": "the multiple signal classification algorithm for acoustic imaging most commonly assumes that all sources have undirectional radiation pattern . we here propose a modification of this multiple signal classification algorithm such that the concept is applicable for arbitrary directional characteristics of the sources . this is accomplished by fitting for each frequency the real valued amplitudes of the acoustic imaging rather than assuming a fixed functional form . the mathematical problem can be solved analytically resulting in an eigenvalue problem of a real valued hamiltonian matrix . the performance is illustrated in simulations using pure monopolar , dipolar and quadrupolar sources ."
  },
  {
    "title": "On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators .",
    "entities": [
      "stochastic gradient mcmc algorithms",
      "stochas-tic gradient langevin dynamics",
      "stochastic gradient hamiltonian mcmc",
      "mean square error",
      "synthetic and real datasets",
      "2nd-order symmetric splitting integrator",
      "1st-order euler integrators",
      "stochastic gradient thermostat",
      "1st-order euler integrator",
      "finite-time convergence properties",
      "asymptotic invariant measures",
      "high-order integrators",
      "higher-order integrators",
      "convergence rates",
      "bayesian learning",
      "posterior average",
      "large-scale data",
      "convergence rate",
      "fixed-step-size counterparts",
      "invariant measures"
    ],
    "types": "<method> <method> <method> <metric> <material> <method> <method> <method> <method> <otherscientificterm> <method> <method> <method> <metric> <task> <otherscientificterm> <material> <metric> <otherscientificterm> <metric>",
    "relations": [
      "large-scale data -- USED-FOR -- bayesian learning",
      "stochastic gradient mcmc algorithms -- CONJUNCTION -- stochas-tic gradient langevin dynamics",
      "finite-time convergence properties -- CONJUNCTION -- asymptotic invariant measures",
      "stochastic gradient hamiltonian mcmc -- CONJUNCTION -- stochastic gradient thermostat",
      "invariant measures -- USED-FOR -- stochastic gradient mcmc algorithms",
      "large-scale data -- USED-FOR -- stochastic gradient mcmc algorithms",
      "stochas-tic gradient langevin dynamics -- CONJUNCTION -- stochastic gradient hamiltonian mcmc",
      "1st-order euler integrator -- USED-FOR -- finite-time convergence properties",
      "stochas-tic gradient langevin dynamics -- HYPONYM-OF -- stochastic gradient mcmc algorithms"
    ],
    "abstract": "recent advances in <task_14> with <material_16> have witnessed emergence of <method_0> , such as <method_1> , <method_2> , and the <method_7> . while <otherscientificterm_9> of the <method_1> with a <method_8> have recently been studied , corresponding theory for general <method_0> has not been explored . in this paper we consider general <method_0> with <method_11> , and develop theory to analyze <otherscientificterm_9> and their <method_10> . our theoretical results show faster <metric_13> and more accurate <metric_19> for <method_0> with <method_12> . for example , with the proposed efficient <method_5> , the <metric_3> of the <otherscientificterm_15> for the <method_0> achieves an optimal <metric_17> of l \u2212 4/5 at l iterations , compared to l \u2212 2/3 for the <method_0> and <method_1> with <method_6> . furthermore , convergence results of <method_0> are also developed , with the same <metric_13> as their <otherscientificterm_18> for a specific decreasing sequence . experiments on both <material_4> verify our theory , and show advantages of the proposed method in two large-scale real applications .",
    "abstract_og": "recent advances in bayesian learning with large-scale data have witnessed emergence of stochastic gradient mcmc algorithms , such as stochas-tic gradient langevin dynamics , stochastic gradient hamiltonian mcmc , and the stochastic gradient thermostat . while finite-time convergence properties of the stochas-tic gradient langevin dynamics with a 1st-order euler integrator have recently been studied , corresponding theory for general stochastic gradient mcmc algorithms has not been explored . in this paper we consider general stochastic gradient mcmc algorithms with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures . our theoretical results show faster convergence rates and more accurate invariant measures for stochastic gradient mcmc algorithms with higher-order integrators . for example , with the proposed efficient 2nd-order symmetric splitting integrator , the mean square error of the posterior average for the stochastic gradient mcmc algorithms achieves an optimal convergence rate of l \u2212 4/5 at l iterations , compared to l \u2212 2/3 for the stochastic gradient mcmc algorithms and stochas-tic gradient langevin dynamics with 1st-order euler integrators . furthermore , convergence results of stochastic gradient mcmc algorithms are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence . experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications ."
  },
  {
    "title": "Flexible Guidance Generation Using User Model in Spoken Dialogue Systems .",
    "entities": [
      "kyoto city bus information system",
      "real dialogue data",
      "spoken dialogue systems",
      "user 's knowledge",
      "decision tree learning",
      "classification accuracy",
      "cooperative responses",
      "user modeling",
      "dialogue strategies",
      "dialogue duration"
    ],
    "types": "<method> <material> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "user modeling -- PART-OF -- spoken dialogue systems",
      "user modeling -- USED-FOR -- cooperative responses",
      "decision tree learning -- USED-FOR -- user modeling",
      "user modeling -- USED-FOR -- dialogue strategies",
      "real dialogue data -- USED-FOR -- decision tree learning",
      "dialogue strategies -- USED-FOR -- kyoto city bus information system"
    ],
    "abstract": "we address appropriate <method_7> in order to generate <otherscientificterm_6> to each user in <method_2> . unlike previous studies that focus on <otherscientificterm_3> or typical kinds of users , the <method_7> we propose is more comprehensive . specifically , we set up three dimensions of <method_7> : skill level to the system , knowledge level on the target domain and the degree of hastiness . moreover , the <method_7> are automatically derived by <method_4> using <material_1> collected by the system . we obtained reasonable <metric_5> for all dimensions . <method_8> based on the <method_7> are implemented in <method_0> that has been developed at our laboratory . experimental evaluation shows that the <otherscientificterm_6> adaptive to individual users serve as good guidance for novice users without increasing the <otherscientificterm_9> for skilled users .",
    "abstract_og": "we address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems . unlike previous studies that focus on user 's knowledge or typical kinds of users , the user modeling we propose is more comprehensive . specifically , we set up three dimensions of user modeling : skill level to the system , knowledge level on the target domain and the degree of hastiness . moreover , the user modeling are automatically derived by decision tree learning using real dialogue data collected by the system . we obtained reasonable classification accuracy for all dimensions . dialogue strategies based on the user modeling are implemented in kyoto city bus information system that has been developed at our laboratory . experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users ."
  },
  {
    "title": "Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy .",
    "entities": [
      "unknown probability distributions",
      "bayes error rate",
      "shannon entropy approximation",
      "unbounded variables",
      "continuous variables",
      "shannon entropy",
      "bayesian networks",
      "clas-sifier"
    ],
    "types": "<otherscientificterm> <metric> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "bayes error rate -- CONJUNCTION -- shannon entropy"
    ],
    "abstract": "we provide a method that approximates the <metric_1> and the <method_5> with high probability . the <metric_1> approximation makes possible to build a <method_7> that polynomially approaches <metric_1> . the <method_2> provides provable performance guarantees for learning trees and <method_6> from <otherscientificterm_4> . our results rely on some reasonable regularity conditions of the <otherscientificterm_0> , and apply to bounded as well as <otherscientificterm_3> .",
    "abstract_og": "we provide a method that approximates the bayes error rate and the shannon entropy with high probability . the bayes error rate approximation makes possible to build a clas-sifier that polynomially approaches bayes error rate . the shannon entropy approximation provides provable performance guarantees for learning trees and bayesian networks from continuous variables . our results rely on some reasonable regularity conditions of the unknown probability distributions , and apply to bounded as well as unbounded variables ."
  },
  {
    "title": "Using cross-decoder co-occurrences of phone n-grams in SVM-based phonotactic language recognition .",
    "entities": [
      "cross-decoder co-occurrences of phone n-grams",
      "cross-decoder co-occurrences of phones",
      "nist lre2007 database",
      "phonotactic language recognition",
      "baseline phonotactic recognizers",
      "time alignment",
      "phone n-grams",
      "frame level",
      "fused system",
      "phone decoders",
      "language modeling",
      "cross-decoder dependencies",
      "co-occurrence statistics",
      "decodings",
      "eer"
    ],
    "types": "<method> <otherscientificterm> <material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <metric>",
    "relations": [
      "fused system -- COMPARE -- baseline phonotactic recognizers",
      "eer -- EVALUATE-FOR -- fused system",
      "eer -- EVALUATE-FOR -- baseline phonotactic recognizers",
      "cross-decoder dependencies -- USED-FOR -- language modeling",
      "co-occurrence statistics -- USED-FOR -- baseline phonotactic recognizers"
    ],
    "abstract": "most common approaches to <task_3> deal with several independent <method_9> . <method_13> are processed and scored in a fully uncoupled way , their <otherscientificterm_5> -lrb- and the information that may be extracted from it -rrb- being completely lost . recently , we have presented a new approach to <task_3> which takes into account <otherscientificterm_5> information , by considering <otherscientificterm_1> or <otherscientificterm_6> at the <otherscientificterm_7> . experiments on the <material_2> demonstrated that using <otherscientificterm_12> could improve the performance of <method_4> . in this work , the approach based on <method_0> is further developed and evaluated . systems were built by means of open software -lrb- brno university of technology phone de-coders , liblinear and focal -rrb- and experiments were carried out on the <material_2> . a system based on co-occurrences of <otherscientificterm_6> -lrb- up to 4-grams -rrb- outperformed the <method_4> , yielding around 8 % relative improvement in terms of <metric_14> . the best <method_8> attained 1,90 % <metric_14> -lrb- a 16 % improvement with regard to the <method_4> -rrb- , which supports the use of <otherscientificterm_11> for improved <task_10> .",
    "abstract_og": "most common approaches to phonotactic language recognition deal with several independent phone decoders . decodings are processed and scored in a fully uncoupled way , their time alignment -lrb- and the information that may be extracted from it -rrb- being completely lost . recently , we have presented a new approach to phonotactic language recognition which takes into account time alignment information , by considering cross-decoder co-occurrences of phones or phone n-grams at the frame level . experiments on the nist lre2007 database demonstrated that using co-occurrence statistics could improve the performance of baseline phonotactic recognizers . in this work , the approach based on cross-decoder co-occurrences of phone n-grams is further developed and evaluated . systems were built by means of open software -lrb- brno university of technology phone de-coders , liblinear and focal -rrb- and experiments were carried out on the nist lre2007 database . a system based on co-occurrences of phone n-grams -lrb- up to 4-grams -rrb- outperformed the baseline phonotactic recognizers , yielding around 8 % relative improvement in terms of eer . the best fused system attained 1,90 % eer -lrb- a 16 % improvement with regard to the baseline phonotactic recognizers -rrb- , which supports the use of cross-decoder dependencies for improved language modeling ."
  },
  {
    "title": "Improving the suitability of imperfect transcriptions for information retrieval from spoken documents .",
    "entities": [
      "speech recognition engines",
      "word error probability",
      "word transcription error",
      "retrieval effectiveness",
      "multimedia databases",
      "hypothesized texts",
      "multimedia indexing",
      "transcriber error",
      "spoken documents",
      "reference texts",
      "information retrieval",
      "word graphs",
      "speech"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <metric> <material> <material> <task> <otherscientificterm> <material> <material> <task> <otherscientificterm> <material>",
    "relations": [
      "word graphs -- USED-FOR -- speech recognition engines",
      "speech -- USED-FOR -- multimedia indexing",
      "reference texts -- CONJUNCTION -- hypothesized texts",
      "information retrieval -- USED-FOR -- multimedia databases",
      "spoken documents -- USED-FOR -- information retrieval"
    ],
    "abstract": "recently there has been a considerable focus on <task_10> for <material_4> . when <material_12> is used as the source material for <task_6> , the effect of <otherscientificterm_7> on <metric_3> must be considered . this paper describes a method for measuring the relevance of documents to queries when information about the probability of <otherscientificterm_2> is available . to support the use of this technique , a method is presented for estimating <otherscientificterm_1> in <method_0> that use <otherscientificterm_11> -lrb- lattices -rrb- . an <task_10> experiment using this technique on a large corpus of <material_8> is discussed . the method was able to reduce the difference in <metric_3> between <material_9> and <material_5> by 13 % -38 % depending on the size of the document set .",
    "abstract_og": "recently there has been a considerable focus on information retrieval for multimedia databases . when speech is used as the source material for multimedia indexing , the effect of transcriber error on retrieval effectiveness must be considered . this paper describes a method for measuring the relevance of documents to queries when information about the probability of word transcription error is available . to support the use of this technique , a method is presented for estimating word error probability in speech recognition engines that use word graphs -lrb- lattices -rrb- . an information retrieval experiment using this technique on a large corpus of spoken documents is discussed . the method was able to reduce the difference in retrieval effectiveness between reference texts and hypothesized texts by 13 % -38 % depending on the size of the document set ."
  },
  {
    "title": "Polarization and Phase-Shifting for 3D Scanning of Translucent Objects .",
    "entities": [
      "structured light 3d scanning techniques",
      "polarization direction of the illumination",
      "optical 3d scanning techniques",
      "scanning real-world translucent objects",
      "polarization direction of light",
      "descattering methods",
      "descattered reflectance",
      "3d coordinates",
      "polarization-difference imaging",
      "descattering technique",
      "subsurface scattering",
      "subsurface scattering",
      "range estimation",
      "surface reflectance",
      "translucent objects",
      "translucent objects",
      "structured light",
      "signal-to-noise ratio",
      "phase-shifting"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "surface reflectance -- USED-FOR -- polarization direction of the illumination",
      "optical 3d scanning techniques -- USED-FOR -- descattered reflectance",
      "phase-shifting -- USED-FOR -- scanning real-world translucent objects",
      "subsurface scattering -- USED-FOR -- range estimation",
      "descattered reflectance -- USED-FOR -- 3d coordinates",
      "optical 3d scanning techniques -- USED-FOR -- 3d coordinates",
      "structured light -- HYPONYM-OF -- optical 3d scanning techniques",
      "descattering methods -- USED-FOR -- 3d coordinates",
      "3d coordinates -- USED-FOR -- translucent objects"
    ],
    "abstract": "translucent objects pose a difficult problem for traditional <method_0> . <task_10> corrupts the <task_12> in two ways : by drastically reducing the <otherscientificterm_17> and by shifting the intensity peak beneath the surface to a point which does not coincide with the point of incidence . in this paper we analyze and compare two <method_5> in order to obtain reliable <otherscientificterm_7> for <otherscientificterm_15> . by using <method_8> , <method_11> can be filtered out because multiple scattering ran-domizes the <otherscientificterm_4> while the <otherscientificterm_13> partially keeps the <otherscientificterm_1> . the <otherscientificterm_6> can be used for reliable <otherscientificterm_7> using traditional <method_2> , such as <otherscientificterm_16> . <method_18> is another effective <method_9> if the frequency of the projected pattern is sufficiently high . we demonstrate the performance of these two techniques and the combination of <method_18> on <task_3> .",
    "abstract_og": "translucent objects pose a difficult problem for traditional structured light 3d scanning techniques . subsurface scattering corrupts the range estimation in two ways : by drastically reducing the signal-to-noise ratio and by shifting the intensity peak beneath the surface to a point which does not coincide with the point of incidence . in this paper we analyze and compare two descattering methods in order to obtain reliable 3d coordinates for translucent objects . by using polarization-difference imaging , subsurface scattering can be filtered out because multiple scattering ran-domizes the polarization direction of light while the surface reflectance partially keeps the polarization direction of the illumination . the descattered reflectance can be used for reliable 3d coordinates using traditional optical 3d scanning techniques , such as structured light . phase-shifting is another effective descattering technique if the frequency of the projected pattern is sufficiently high . we demonstrate the performance of these two techniques and the combination of phase-shifting on scanning real-world translucent objects ."
  },
  {
    "title": "Discriminative template learning in group-convolutional networks for invariant speech representations .",
    "entities": [
      "perturbations of the vocal tract length",
      "invariant sensory signal representations",
      "wall street journal datasets",
      "filter weight sharing",
      "densely connected layers",
      "convolutional neural networks",
      "theoretical invariance guarantees",
      "discriminative template selection",
      "locally compact groups",
      "max pooling",
      "discrimina-tive approach",
      "deep network",
      "speech variability",
      "frame classification",
      "convolution-pooling layers",
      "speech sounds",
      "local translations",
      "convolutional networks",
      "frequency transpositions",
      "template signals",
      "data-specific templates",
      "group-generalized convolutions",
      "signature",
      "dnns",
      "templates",
      "translation-cnns"
    ],
    "types": "<otherscientificterm> <task> <material> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "group-generalized convolutions -- PART-OF -- convolutional networks",
      "deep network -- USED-FOR -- data-specific templates",
      "filter weight sharing -- FEATURE-OF -- convolutional neural networks",
      "convolutional networks -- USED-FOR -- data-specific templates",
      "signature -- USED-FOR -- speech sounds",
      "discriminative template selection -- CONJUNCTION -- translation-cnns",
      "filter weight sharing -- CONJUNCTION -- max pooling",
      "translation-cnns -- CONJUNCTION -- dnns",
      "theoretical invariance guarantees -- CONJUNCTION -- translation-cnns",
      "group-generalized convolutions -- CONJUNCTION -- discriminative template selection",
      "densely connected layers -- FEATURE-OF -- deep network",
      "frequency transpositions -- CONJUNCTION -- perturbations of the vocal tract length",
      "theoretical invariance guarantees -- CONJUNCTION -- discriminative template selection",
      "convolution-pooling layers -- CONJUNCTION -- densely connected layers",
      "convolution-pooling layers -- FEATURE-OF -- deep network",
      "group-generalized convolutions -- CONJUNCTION -- theoretical invariance guarantees"
    ],
    "abstract": "in the framework of a theory for <task_1> , a <otherscientificterm_22> which is invariant and selective for <material_15> can be obtained through projections in <material_19> and pooling over their transformations under a group . for <otherscientificterm_8> , e.g. , translations , the theory explains the resilience of <method_5> with <method_3> and <method_9> across their <otherscientificterm_16> in frequency or time . in this paper we propose a <method_10> for learning an optimum set of <otherscientificterm_24> , under a family of transformations , namely <otherscientificterm_18> and <otherscientificterm_0> , which are among the primary sources of <otherscientificterm_12> . implicitly , we generalize <method_17> to transformations other than translations , and derive <otherscientificterm_20> by training a <method_11> with <otherscientificterm_14> and <otherscientificterm_4> . we demonstrate that such a <method_17> , combining <otherscientificterm_21> , <otherscientificterm_6> and <method_7> , improves <task_13> performance over standard <method_25> and <method_23> on timit and <material_2> .",
    "abstract_og": "in the framework of a theory for invariant sensory signal representations , a signature which is invariant and selective for speech sounds can be obtained through projections in template signals and pooling over their transformations under a group . for locally compact groups , e.g. , translations , the theory explains the resilience of convolutional neural networks with filter weight sharing and max pooling across their local translations in frequency or time . in this paper we propose a discrimina-tive approach for learning an optimum set of templates , under a family of transformations , namely frequency transpositions and perturbations of the vocal tract length , which are among the primary sources of speech variability . implicitly , we generalize convolutional networks to transformations other than translations , and derive data-specific templates by training a deep network with convolution-pooling layers and densely connected layers . we demonstrate that such a convolutional networks , combining group-generalized convolutions , theoretical invariance guarantees and discriminative template selection , improves frame classification performance over standard translation-cnns and dnns on timit and wall street journal datasets ."
  },
  {
    "title": "Noise estimation for efficient speech enhancement and robust speech recognition .",
    "entities": [
      "distributed speech recognition",
      "minima tracking based noise estimation algorithms",
      "aurora 2 evaluation databases",
      "noise suppression algorithm",
      "speech recognition system",
      "noise estimation techniques",
      "estimated noise",
      "algorithmic delay"
    ],
    "types": "<task> <method> <material> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "noise suppression algorithm -- USED-FOR -- speech recognition system",
      "aurora 2 evaluation databases -- EVALUATE-FOR -- noise suppression algorithm"
    ],
    "abstract": "different approaches of <method_1> are compared and modifications increasing their efficiency are proposed . <otherscientificterm_6> is used by <method_3> that is a part of <method_4> . moreover , the <method_3> are developed to be applied in feature extraction of <task_0> . therefore we propose such modifications to the <method_5> that are quickly adaptable on varying noise and do not need so much information from past segments . we also minimized the <otherscientificterm_7> . the robustness of proposed <method_3> were tested under several noisy conditions on five speech-dat car -lrb- sdc -rrb- and <material_2> .",
    "abstract_og": "different approaches of minima tracking based noise estimation algorithms are compared and modifications increasing their efficiency are proposed . estimated noise is used by noise suppression algorithm that is a part of speech recognition system . moreover , the noise suppression algorithm are developed to be applied in feature extraction of distributed speech recognition . therefore we propose such modifications to the noise estimation techniques that are quickly adaptable on varying noise and do not need so much information from past segments . we also minimized the algorithmic delay . the robustness of proposed noise suppression algorithm were tested under several noisy conditions on five speech-dat car -lrb- sdc -rrb- and aurora 2 evaluation databases ."
  },
  {
    "title": "Perception of disfluency : language differences and listener bias .",
    "entities": [
      "non-recognition of the acoustic material",
      "crosslinguistic disfluency perception experiment",
      "recognizability of pause fillers",
      "phonetic crosslinguistic cues",
      "german accuracy rates",
      "fluent utterances",
      "pause fillers",
      "partial words",
      "accuracy rates",
      "dis-fluent speech",
      "conservative bias",
      "english",
      "german",
      "mandarin"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <metric> <material> <otherscientificterm> <otherscientificterm> <metric> <material> <otherscientificterm> <material> <material> <material>",
    "relations": [
      "german -- CONJUNCTION -- mandarin",
      "english -- CONJUNCTION -- german",
      "recognizability of pause fillers -- CONJUNCTION -- partial words",
      "pause fillers -- COMPARE -- partial words"
    ],
    "abstract": "this paper describes a <otherscientificterm_1> . we tested the <task_2> and <otherscientificterm_7> in <material_11> , <material_12> and <material_13> . subjects were speakers of <material_11> with no knowledge of <material_13> or ger-man . we found that subjects could identify disfluent from <material_5> at a level above chance . <otherscientificterm_6> were easier to identify than <otherscientificterm_7> . <metric_8> were highest for <material_11> , followed by <material_12> and then <material_13> . although <metric_4> were higher than those for <material_13> , discriminability analysis suggests that this is due to <otherscientificterm_10> towards false negatives rather than <otherscientificterm_0> . the fact that subjects could identify <material_9> in languages they did not know shows that there are real <otherscientificterm_3> to disfluency .",
    "abstract_og": "this paper describes a crosslinguistic disfluency perception experiment . we tested the recognizability of pause fillers and partial words in english , german and mandarin . subjects were speakers of english with no knowledge of mandarin or ger-man . we found that subjects could identify disfluent from fluent utterances at a level above chance . pause fillers were easier to identify than partial words . accuracy rates were highest for english , followed by german and then mandarin . although german accuracy rates were higher than those for mandarin , discriminability analysis suggests that this is due to conservative bias towards false negatives rather than non-recognition of the acoustic material . the fact that subjects could identify dis-fluent speech in languages they did not know shows that there are real phonetic crosslinguistic cues to disfluency ."
  },
  {
    "title": "LOCUS : Learning Object Classes with Unsupervised Segmentation .",
    "entities": [
      "locus -lrb- learning object classes",
      "bottom-up cues of color",
      "top-down cues of shape",
      "unsupervised segmentation -rrb-",
      "object class models",
      "generative probabilistic model",
      "object class model",
      "unsupervised object discovery",
      "object appearance",
      "supervised methods",
      "unannotated images",
      "object segmentations",
      "within-class variation",
      "unlabeled images",
      "object classes",
      "segmentation accuracies",
      "simultaneous recognition",
      "size",
      "segmentation",
      "edge"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <task> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "unannotated images -- USED-FOR -- object segmentations",
      "object class models -- CONJUNCTION -- object segmentations",
      "generative probabilistic model -- USED-FOR -- locus -lrb- learning object classes",
      "unlabeled images -- USED-FOR -- object class model",
      "size -- CONJUNCTION -- segmentation",
      "unsupervised object discovery -- HYPONYM-OF -- object classes",
      "locus -lrb- learning object classes -- USED-FOR -- object class model"
    ],
    "abstract": "we address the problem of learning <method_4> and <otherscientificterm_11> from <material_10> . we introduce <method_0> with <method_3> which uses a <method_5> to combine <otherscientificterm_1> and <otherscientificterm_19> with <otherscientificterm_2> and pose . a key aspect of this <method_5> is that the <otherscientificterm_8> is allowed to vary from image to image , allowing for significant <otherscientificterm_12> . by iteratively updating the belief in the object 's position , <otherscientificterm_17> , <otherscientificterm_18> and pose , <method_0> avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage . we show that <method_0> successfully learns an <method_6> from <material_13> , whilst also giving <method_15> that rival existing <method_9> . finally , we demonstrate <task_16> and <otherscientificterm_18> in novel images using the learned models for a number of <otherscientificterm_14> , as well as <task_7> and tracking in video .",
    "abstract_og": "we address the problem of learning object class models and object segmentations from unannotated images . we introduce locus -lrb- learning object classes with unsupervised segmentation -rrb- which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose . a key aspect of this generative probabilistic model is that the object appearance is allowed to vary from image to image , allowing for significant within-class variation . by iteratively updating the belief in the object 's position , size , segmentation and pose , locus -lrb- learning object classes avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage . we show that locus -lrb- learning object classes successfully learns an object class model from unlabeled images , whilst also giving segmentation accuracies that rival existing supervised methods . finally , we demonstrate simultaneous recognition and segmentation in novel images using the learned models for a number of object classes , as well as unsupervised object discovery and tracking in video ."
  },
  {
    "title": "A fixed dimension and perceptually based dynamic sinusoidal model of speech .",
    "entities": [
      "fixed-and low-dimensional , perceptually based dynamic sinusoidal model",
      "pdm -lrb- perceptual dynamic model -rrb-",
      "maximum spectral amplitude",
      "modulated noise component",
      "dynamic sinusoidal component",
      "sinusoidal components",
      "listening test",
      "sinusoidal model",
      "sinu-soids"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <method> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "sinusoidal components -- USED-FOR -- sinusoidal model"
    ],
    "abstract": "this paper presents a <method_0> of speech referred to as <method_1> . to decrease and fix the number of <method_5> typically used in the standard <method_7> , we propose to use only one <method_4> per critical band . for each band , the si-nusoid with the <otherscientificterm_2> is selected and associated with the centre frequency of that critical band . the model is expanded at low frequencies by incorporating <otherscientificterm_8> at the boundaries of the corresponding bands while at the higher frequencies a <method_3> is used . a <method_6> is conducted to compare speech reconstructed with <method_1> and state-of-the-art models of speech , where all models are constrained to use an equal number of parameters . the results show that <method_1> is clearly preferred in terms of quality over the other systems .",
    "abstract_og": "this paper presents a fixed-and low-dimensional , perceptually based dynamic sinusoidal model of speech referred to as pdm -lrb- perceptual dynamic model -rrb- . to decrease and fix the number of sinusoidal components typically used in the standard sinusoidal model , we propose to use only one dynamic sinusoidal component per critical band . for each band , the si-nusoid with the maximum spectral amplitude is selected and associated with the centre frequency of that critical band . the model is expanded at low frequencies by incorporating sinu-soids at the boundaries of the corresponding bands while at the higher frequencies a modulated noise component is used . a listening test is conducted to compare speech reconstructed with pdm -lrb- perceptual dynamic model -rrb- and state-of-the-art models of speech , where all models are constrained to use an equal number of parameters . the results show that pdm -lrb- perceptual dynamic model -rrb- is clearly preferred in terms of quality over the other systems ."
  },
  {
    "title": "A comparison of supervised and unsupervised cross-lingual speaker adaptation approaches for HMM-based speech synthesis .",
    "entities": [
      "objective and subjective evaluations",
      "unsupervised and cross-lingual cases",
      "unsupervised cross-lingual speaker adaptation",
      "phoneme error rate",
      "hmm state mapping",
      "decision tree marginalization",
      "speaker adaptation systems",
      "personalized speech-to-speech translator",
      "emime scenario",
      "spoken input",
      "spoken output",
      "spectrum adaptation",
      "supervised case"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <metric> <method> <task> <method> <method> <otherscientificterm> <material> <material> <task> <task>",
    "relations": [
      "supervised case -- USED-FOR -- emime scenario",
      "unsupervised cross-lingual speaker adaptation -- COMPARE -- supervised case",
      "spectrum adaptation -- PART-OF -- emime scenario",
      "objective and subjective evaluations -- EVALUATE-FOR -- speaker adaptation systems",
      "decision tree marginalization -- CONJUNCTION -- hmm state mapping",
      "spoken input -- USED-FOR -- spoken output"
    ],
    "abstract": "the emime project aims to build a <method_7> , such that <material_9> of a user in one language is used to produce <material_10> that still sounds like the user 's voice however in another language . this distinctiveness makes <task_2> one key to the project 's success . so far , research has been conducted into <otherscientificterm_1> separately by means of <task_5> and <method_4> respectively . in this paper we combine the two techniques to perform <task_2> . the performance of eight <method_6> -lrb- supervised vs. unsupervised , intra-lingual vs. cross-lingual -rrb- is compared using <otherscientificterm_0> . experimental results show the performance of <task_2> is comparable to that of the <task_12> in terms of <task_11> in the <otherscientificterm_8> , even though automatically obtained transcriptions have a very high <metric_3> .",
    "abstract_og": "the emime project aims to build a personalized speech-to-speech translator , such that spoken input of a user in one language is used to produce spoken output that still sounds like the user 's voice however in another language . this distinctiveness makes unsupervised cross-lingual speaker adaptation one key to the project 's success . so far , research has been conducted into unsupervised and cross-lingual cases separately by means of decision tree marginalization and hmm state mapping respectively . in this paper we combine the two techniques to perform unsupervised cross-lingual speaker adaptation . the performance of eight speaker adaptation systems -lrb- supervised vs. unsupervised , intra-lingual vs. cross-lingual -rrb- is compared using objective and subjective evaluations . experimental results show the performance of unsupervised cross-lingual speaker adaptation is comparable to that of the supervised case in terms of spectrum adaptation in the emime scenario , even though automatically obtained transcriptions have a very high phoneme error rate ."
  },
  {
    "title": "Competence Driven Case-Base Mining .",
    "entities": [
      "smyth and keane 's case-deletion policy",
      "nonlin-ear transformation of the data set",
      "high-quality case base",
      "statistical distribution",
      "case-based reasoning",
      "case-base sizes",
      "data sets",
      "case base",
      "raw data",
      "case-base competence",
      "deletion-based algorithm",
      "features"
    ],
    "types": "<method> <otherscientificterm> <material> <method> <method> <otherscientificterm> <material> <material> <material> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "statistical distribution -- USED-FOR -- case base"
    ],
    "abstract": "we present a novel algorithm for extracting a <material_2> from <material_8> while preserving and sometimes improving the competence of <method_4> . we extend the framework of <method_0> with two additional <otherscientificterm_11> . first , we build a <material_7> using a <method_3> that is mined from the input data so that the <otherscientificterm_9> can be preserved or even increased for future problems . second , we introduce a <otherscientificterm_1> so that the <otherscientificterm_5> can be further reduced while ensuring that the competence be preserved and even increased . we show that smyth and keane 's <method_10> is sensitive to noisy cases , and that our <method_10> solves this problem more satisfactorily . we show the theoretical foundation and empirical evaluation on several <material_6> .",
    "abstract_og": "we present a novel algorithm for extracting a high-quality case base from raw data while preserving and sometimes improving the competence of case-based reasoning . we extend the framework of smyth and keane 's case-deletion policy with two additional features . first , we build a case base using a statistical distribution that is mined from the input data so that the case-base competence can be preserved or even increased for future problems . second , we introduce a nonlin-ear transformation of the data set so that the case-base sizes can be further reduced while ensuring that the competence be preserved and even increased . we show that smyth and keane 's deletion-based algorithm is sensitive to noisy cases , and that our deletion-based algorithm solves this problem more satisfactorily . we show the theoretical foundation and empirical evaluation on several data sets ."
  },
  {
    "title": "What Do You Do ? Occupation Recognition in a Photo via Social Context .",
    "entities": [
      "well-labeled occupation database",
      "spatial configuration model",
      "computer vision",
      "structure svm",
      "representative occupations",
      "arbitrary poses",
      "occupation recognition",
      "co-occurrence"
    ],
    "types": "<material> <method> <task> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "co-occurrence -- CONJUNCTION -- spatial configuration model",
      "representative occupations -- FEATURE-OF -- well-labeled occupation database",
      "occupation recognition -- USED-FOR -- computer vision"
    ],
    "abstract": "in this paper , we investigate the problem of recognizing occupations of multiple people with <otherscientificterm_5> in a photo . previous work utilizing single person 's nearly frontal clothing information and fore/background context preliminarily proves that <task_6> is com-putationally feasible in <task_2> . however , in practice , multiple people with <otherscientificterm_5> are common in a photo , and recognizing their occupations is even more challenging . we argue that with appropriately built visual attributes , <otherscientificterm_7> , and <method_1> that is learned through <method_3> , we can recognize multiple people 's occupations in a photo simultaneously . to evaluate our method 's performance , we conduct extensive experiments on a new <material_0> with 14 <otherscientificterm_4> and over 7k images . results on this database validate our method 's effectiveness and show that <task_6> is solv-able in a more general case .",
    "abstract_og": "in this paper , we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo . previous work utilizing single person 's nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is com-putationally feasible in computer vision . however , in practice , multiple people with arbitrary poses are common in a photo , and recognizing their occupations is even more challenging . we argue that with appropriately built visual attributes , co-occurrence , and spatial configuration model that is learned through structure svm , we can recognize multiple people 's occupations in a photo simultaneously . to evaluate our method 's performance , we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7k images . results on this database validate our method 's effectiveness and show that occupation recognition is solv-able in a more general case ."
  },
  {
    "title": "The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System .",
    "entities": [
      "multi-state time delay neural network",
      "curvature or writing direction",
      "npen + +",
      "word recognition rates",
      "neural network architecture",
      "dynamic writing information",
      "robust input representation",
      "language models",
      "temporal sequences",
      "local features",
      "vocabulary sizes",
      "coordinate sequence",
      "ms-tdnn architecture",
      "connectionist system",
      "preprocessing",
      "segmen-tation",
      "rec.ognition"
    ],
    "types": "<method> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <method> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "npen + + -- HYPONYM-OF -- connectionist system",
      "coordinate sequence -- PART-OF -- preprocessing",
      "multi-state time delay neural network -- HYPONYM-OF -- neural network architecture",
      "ms-tdnn architecture -- USED-FOR -- temporal sequences",
      "robust input representation -- USED-FOR -- dynamic writing information",
      "neural network architecture -- PART-OF -- npen + +",
      "npen + + -- USED-FOR -- dynamic writing information"
    ],
    "abstract": "in this paper we present <method_2> , a <method_13> for writer independent , large vocabulary on-line cursive handwriting recognition . this <method_2> combines a <method_6> , which preserves the <otherscientificterm_5> , with a <method_4> , a so called <method_0> , which integrates <otherscientificterm_16> and <otherscientificterm_15> in a single framework . our <method_14> transforms the original <otherscientificterm_11> into a -lrb- still temporal -rrb- sequence offea-ture vectors , which combine strictly <otherscientificterm_9> , like <otherscientificterm_1> , with a bitmap-like representation of the co-ordinate 's proximity . the <method_12> is well suited for handling <material_8> as provided by this <method_6> . our <method_2> is tested both on writer dependent and writer independent tasks with <otherscientificterm_10> ranging from 400 up to 20,000 words . for example , on a 20,000 word vocabulary we achieve <metric_3> up to 88.9 % -lrb- writer dependent -rrb- and 84.1 % -lrb- writer independent -rrb- without using any <method_7> .",
    "abstract_og": "in this paper we present npen + + , a connectionist system for writer independent , large vocabulary on-line cursive handwriting recognition . this npen + + combines a robust input representation , which preserves the dynamic writing information , with a neural network architecture , a so called multi-state time delay neural network , which integrates rec.ognition and segmen-tation in a single framework . our preprocessing transforms the original coordinate sequence into a -lrb- still temporal -rrb- sequence offea-ture vectors , which combine strictly local features , like curvature or writing direction , with a bitmap-like representation of the co-ordinate 's proximity . the ms-tdnn architecture is well suited for handling temporal sequences as provided by this robust input representation . our npen + + is tested both on writer dependent and writer independent tasks with vocabulary sizes ranging from 400 up to 20,000 words . for example , on a 20,000 word vocabulary we achieve word recognition rates up to 88.9 % -lrb- writer dependent -rrb- and 84.1 % -lrb- writer independent -rrb- without using any language models ."
  },
  {
    "title": "Lexical embedding in spoken dutch .",
    "entities": [
      "corpora of spoken dutch",
      "polysyllabic word tokens",
      "dictionary of dutch",
      "monosyllabic word tokens",
      "speech processing",
      "temporary ambiguity",
      "word-initial embedding",
      "lexical embedding"
    ],
    "types": "<material> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "word-initial embedding -- FEATURE-OF -- polysyllabic word tokens"
    ],
    "abstract": "a stretch of speech is often consistent with multiple words , e.g. , the sequence / haem / is consistent with ` ham ' but also with the first syllable of ` hamster ' , resulting in <otherscientificterm_5> . however , to what degree does this <otherscientificterm_7> occur ? analyses on two <material_0> showed that 11.9 % -19.5 % of <otherscientificterm_1> have <otherscientificterm_6> , while 4.1 % -7.5 % of <otherscientificterm_3> can appear word-initially embedded . this is much lower than suggested by an analysis of a large <material_2> . <task_4> thus appears to be simpler than one might expect on the basis of statistics on a dictionary .",
    "abstract_og": "a stretch of speech is often consistent with multiple words , e.g. , the sequence / haem / is consistent with ` ham ' but also with the first syllable of ` hamster ' , resulting in temporary ambiguity . however , to what degree does this lexical embedding occur ? analyses on two corpora of spoken dutch showed that 11.9 % -19.5 % of polysyllabic word tokens have word-initial embedding , while 4.1 % -7.5 % of monosyllabic word tokens can appear word-initially embedded . this is much lower than suggested by an analysis of a large dictionary of dutch . speech processing thus appears to be simpler than one might expect on the basis of statistics on a dictionary ."
  },
  {
    "title": "Decoding of Neuronal Signals in Visual Pattern Recognition .",
    "entities": [
      "inferior temporal cortex",
      "spatial patterns of the stimuli",
      "pattern matching task",
      "temporally encoded information",
      "measured neuronal signal",
      "neuronal response waveforms",
      "back-propagation networks",
      "behavioral context",
      "stimulus conditions"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "back-propagation networks -- USED-FOR -- neuronal response waveforms"
    ],
    "abstract": "we have investigated the properties of neurons in <otherscientificterm_0> in monkeys performing a <task_2> . simple <method_6> were trained to discriminate the various <otherscientificterm_8> on the basis of the <otherscientificterm_4> . we also trained <method_6> to predict the <otherscientificterm_5> from the <otherscientificterm_1> . the results indicate t.hat it neurons convey <otherscientificterm_3> about both current and remembered patterns , as well as about their <otherscientificterm_7> .",
    "abstract_og": "we have investigated the properties of neurons in inferior temporal cortex in monkeys performing a pattern matching task . simple back-propagation networks were trained to discriminate the various stimulus conditions on the basis of the measured neuronal signal . we also trained back-propagation networks to predict the neuronal response waveforms from the spatial patterns of the stimuli . the results indicate t.hat it neurons convey temporally encoded information about both current and remembered patterns , as well as about their behavioral context ."
  },
  {
    "title": "A Simple Additive Re-weighting Strategy for Improving Margins .",
    "entities": [
      "state of the art algorithms",
      "vector quantization algorithm",
      "tangent distance models",
      "sample re-weighting scheme",
      "tangent models",
      "margin theory",
      "input distribution",
      "generalization power",
      "1-nn classifier",
      "svm"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <metric> <method> <method>",
    "relations": [
      "vector quantization algorithm -- USED-FOR -- 1-nn classifier",
      "vector quantization algorithm -- USED-FOR -- tangent distance models"
    ],
    "abstract": "we present a <method_3> inspired by recent results in <method_5> . the basic idea is to add to the training set replicas of samples which are not classified with a sufficient margin . we prove the convergence of the <otherscientificterm_6> obtained in this way . as study case , we consider an instance of the <method_3> involving a <method_8> implementing a <method_1> that accommodates <method_2> . the <method_2> created in this way have shown a significant improvement in <metric_7> with respect to the standard <method_4> . moreover , the obtained models were able to outperform <method_0> , such as <method_9> .",
    "abstract_og": "we present a sample re-weighting scheme inspired by recent results in margin theory . the basic idea is to add to the training set replicas of samples which are not classified with a sufficient margin . we prove the convergence of the input distribution obtained in this way . as study case , we consider an instance of the sample re-weighting scheme involving a 1-nn classifier implementing a vector quantization algorithm that accommodates tangent distance models . the tangent distance models created in this way have shown a significant improvement in generalization power with respect to the standard tangent models . moreover , the obtained models were able to outperform state of the art algorithms , such as svm ."
  },
  {
    "title": "Optimizing bottle-neck features for lvcsr .",
    "entities": [
      "phoneme mlp training targets",
      "english conversational telephone speech",
      "neural net input representations",
      "arabic broadcast news",
      "probabilistic mlp features",
      "mlp training targets",
      "arbitrary feature size",
      "bottleneck feature extraction",
      "bottleneck features",
      "mlp topology",
      "lvcsr tasks",
      "dimensionality reduction",
      "delta features",
      "optimized features",
      "english meetings",
      "cepstral features",
      "five-layers mlp",
      "asr"
    ],
    "types": "<task> <material> <method> <material> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <task>",
    "relations": [
      "english meetings -- HYPONYM-OF -- lvcsr tasks",
      "english conversational telephone speech -- CONJUNCTION -- english meetings",
      "english conversational telephone speech -- HYPONYM-OF -- lvcsr tasks",
      "five-layers mlp -- USED-FOR -- bottleneck feature extraction",
      "probabilistic mlp features -- USED-FOR -- neural net input representations",
      "arabic broadcast news -- CONJUNCTION -- english conversational telephone speech",
      "arabic broadcast news -- HYPONYM-OF -- lvcsr tasks",
      "cepstral features -- CONJUNCTION -- probabilistic mlp features",
      "bottleneck features -- USED-FOR -- asr"
    ],
    "abstract": "this work continues in development of the recently proposed <method_8> for <task_17> . a <method_16> used in <task_7> allows to obtain <otherscientificterm_6> without <otherscientificterm_11> by transforms , independently on the <otherscientificterm_5> . the <otherscientificterm_9> -- number and sizes of layers , suitable training targets , the impact of output feature transforms , the need of <otherscientificterm_12> , and the dimensionality of the final feature vector are studied with respect to the best <task_17> result . <otherscientificterm_13> are employed in three <task_10> : <material_3> , <material_1> and <material_14> . improvements over standard <otherscientificterm_15> and <method_4> are shown for different tasks and different <method_2> . a significant improvement is observed when <task_0> are replaced by phoneme states and when <otherscientificterm_12> are added .",
    "abstract_og": "this work continues in development of the recently proposed bottleneck features for asr . a five-layers mlp used in bottleneck feature extraction allows to obtain arbitrary feature size without dimensionality reduction by transforms , independently on the mlp training targets . the mlp topology -- number and sizes of layers , suitable training targets , the impact of output feature transforms , the need of delta features , and the dimensionality of the final feature vector are studied with respect to the best asr result . optimized features are employed in three lvcsr tasks : arabic broadcast news , english conversational telephone speech and english meetings . improvements over standard cepstral features and probabilistic mlp features are shown for different tasks and different neural net input representations . a significant improvement is observed when phoneme mlp training targets are replaced by phoneme states and when delta features are added ."
  },
  {
    "title": "Multi-channel high resolution blind image restoration .",
    "entities": [
      "mutually referen-ced equalizers algorithm",
      "blind multi-input multi-output deconvolution",
      "artificial and photographics images",
      "blind source separation algorithms",
      "mixed polyphase components",
      "digital communications",
      "polyphase components",
      "unknown response",
      "fir filters",
      "blind equalization",
      "bandlimited signal",
      "fir channels",
      "undersampled measurements"
    ],
    "types": "<method> <method> <material> <method> <method> <task> <method> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "unknown response -- FEATURE-OF -- undersampled measurements",
      "mutually referen-ced equalizers algorithm -- USED-FOR -- blind equalization",
      "polyphase components -- PART-OF -- bandlimited signal",
      "mixed polyphase components -- PART-OF -- bandlimited signal",
      "digital communications -- USED-FOR -- blind equalization",
      "unknown response -- FEATURE-OF -- fir channels"
    ],
    "abstract": "we address the reconstruction problem of a high resolution image from its <material_12> accross multiple <otherscientificterm_11> with <otherscientificterm_7> . our method consists of two stages : <method_1> using <method_8> and blind separation of <method_4> . the proposed deconvolution method is based on the <method_0> previously developed for <task_9> in <task_5> . for sources separation , a method is proposed for separating <method_4> of a <otherscientificterm_10> . the existing <method_3> assume that the source signals are either independent or uncorrelated , which is not the case when the sources are <method_6> of a <otherscientificterm_10> . simulation results on <material_2> are given .",
    "abstract_og": "we address the reconstruction problem of a high resolution image from its undersampled measurements accross multiple fir channels with unknown response . our method consists of two stages : blind multi-input multi-output deconvolution using fir filters and blind separation of mixed polyphase components . the proposed deconvolution method is based on the mutually referen-ced equalizers algorithm previously developed for blind equalization in digital communications . for sources separation , a method is proposed for separating mixed polyphase components of a bandlimited signal . the existing blind source separation algorithms assume that the source signals are either independent or uncorrelated , which is not the case when the sources are polyphase components of a bandlimited signal . simulation results on artificial and photographics images are given ."
  },
  {
    "title": "Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing .",
    "entities": [
      "average attachment score",
      "unsupervised dependency parsers",
      "dependency model",
      "reducibility principle",
      "stop-probabilities",
      "valence"
    ],
    "types": "<metric> <method> <method> <method> <method> <method>",
    "relations": [
      "average attachment score -- EVALUATE-FOR -- dependency model",
      "average attachment score -- EVALUATE-FOR -- valence"
    ],
    "abstract": "even though the quality of <method_1> grows , <method_1> often fail in recognition of very basic dependencies . in this paper , we exploit a prior knowledge of <method_4> -lrb- whether a given word has any children in a given direction -rrb- , which is obtained from a large raw corpus using the <method_3> . by incorporating this knowledge into <method_2> with <method_5> , we managed to considerably outperform the state-of-the-art results in terms of <metric_0> over 20 treebanks from conll 2006 and 2007 shared tasks .",
    "abstract_og": "even though the quality of unsupervised dependency parsers grows , unsupervised dependency parsers often fail in recognition of very basic dependencies . in this paper , we exploit a prior knowledge of stop-probabilities -lrb- whether a given word has any children in a given direction -rrb- , which is obtained from a large raw corpus using the reducibility principle . by incorporating this knowledge into dependency model with valence , we managed to considerably outperform the state-of-the-art results in terms of average attachment score over 20 treebanks from conll 2006 and 2007 shared tasks ."
  },
  {
    "title": "Generalizing Image Captions for Image-Text Parallel Corpus .",
    "entities": [
      "image-text parallel corpus",
      "visually-guided sentence compression",
      "dynamic beam search",
      "image caption generalization",
      "natural language processing",
      "image caption transfer",
      "generalized captions",
      "computer vision",
      "image content",
      "integrative models",
      "web images",
      "image-caption pairs",
      "dependency-based constraints",
      "extrinsic utility",
      "intrinsic quality",
      "complexity",
      "noise"
    ],
    "types": "<material> <task> <method> <task> <task> <task> <otherscientificterm> <task> <material> <method> <material> <otherscientificterm> <otherscientificterm> <metric> <metric> <metric> <otherscientificterm>",
    "relations": [
      "integrative models -- USED-FOR -- computer vision",
      "integrative models -- USED-FOR -- natural language processing",
      "image-caption pairs -- FEATURE-OF -- image-text parallel corpus",
      "natural language processing -- CONJUNCTION -- computer vision",
      "complexity -- CONJUNCTION -- noise",
      "visually-guided sentence compression -- USED-FOR -- image caption generalization",
      "dependency-based constraints -- FEATURE-OF -- dynamic beam search"
    ],
    "abstract": "the ever growing amount of <material_10> and their associated texts offers new opportunities for <method_9> bridging <task_4> and <task_7> . however , the potential benefits of such data are yet to be fully realized due to the <metric_15> and <otherscientificterm_16> in the alignment between <material_8> and text . we address this challenge with contributions in two folds : first , we introduce the new task of <task_3> , formulated as <task_1> , and present an efficient algorithm based on <method_2> with <otherscientificterm_12> . second , we release a new <material_0> with 1 million <otherscientificterm_11> achieving tighter content alignment between images and text . evaluation results show the <metric_14> of the <otherscientificterm_6> and the <metric_13> of the new <material_0> with respect to a concrete application of <task_5> .",
    "abstract_og": "the ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision . however , the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text . we address this challenge with contributions in two folds : first , we introduce the new task of image caption generalization , formulated as visually-guided sentence compression , and present an efficient algorithm based on dynamic beam search with dependency-based constraints . second , we release a new image-text parallel corpus with 1 million image-caption pairs achieving tighter content alignment between images and text . evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new image-text parallel corpus with respect to a concrete application of image caption transfer ."
  },
  {
    "title": "Discriminative learning for differing training and test distributions .",
    "entities": [
      "kernel logistic regression classifier",
      "integrated optimization problem",
      "covariate shift",
      "test distribution",
      "classification problems",
      "distribution",
      "classification"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "integrated optimization problem -- USED-FOR -- covariate shift"
    ],
    "abstract": "we address <task_4> for which the training instances are governed by a <otherscientificterm_5> that is allowed to differ arbitrarily from the <otherscientificterm_3> -- problems also referred to as <task_6> under <otherscientificterm_2> . we derive a solution that is purely discriminative : neither training nor <otherscientificterm_3> are modeled explicitly . we formulate the general problem of learning under <otherscientificterm_2> as an <task_1> . we derive a <method_0> for differing training and test distributions .",
    "abstract_og": "we address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution -- problems also referred to as classification under covariate shift . we derive a solution that is purely discriminative : neither training nor test distribution are modeled explicitly . we formulate the general problem of learning under covariate shift as an integrated optimization problem . we derive a kernel logistic regression classifier for differing training and test distributions ."
  },
  {
    "title": "Non-intrusive Iris Image Capturing System Using Light Stripe Projection and Pan-Tilt-Zoom Camera .",
    "entities": [
      "non-intrusive iris image capturing system",
      "exact zoom and focus position",
      "light stripe projection",
      "user 's position",
      "2d face search",
      "narrow search range",
      "user 's position",
      "adaboost-based face detection",
      "1d face search",
      "pan-tilt-zoom camera",
      "tilt angle",
      "iris image",
      "zoom",
      "panning"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method>",
    "relations": [
      "1d face search -- USED-FOR -- 2d face search",
      "non-intrusive iris image capturing system -- USED-FOR -- iris image",
      "user 's position -- COMPARE -- 2d face search",
      "adaboost-based face detection -- USED-FOR -- tilt angle"
    ],
    "abstract": "this paper proposes <method_0> , which consists of <otherscientificterm_9> and light stripe projection . <task_2> provides the position of user . after <method_13> according to <otherscientificterm_6> , <task_7> finds <otherscientificterm_10> . with <otherscientificterm_6> and <otherscientificterm_10> , <otherscientificterm_12> and focus position are initialized . <otherscientificterm_3> replaces <method_4> with <method_8> . <otherscientificterm_1> enable fast control and <otherscientificterm_5> . consequently , experimental results show that proposed <method_0> can capture <material_11> within acceptable time .",
    "abstract_og": "this paper proposes non-intrusive iris image capturing system , which consists of pan-tilt-zoom camera and light stripe projection . light stripe projection provides the position of user . after panning according to user 's position , adaboost-based face detection finds tilt angle . with user 's position and tilt angle , zoom and focus position are initialized . user 's position replaces 2d face search with 1d face search . exact zoom and focus position enable fast control and narrow search range . consequently , experimental results show that proposed non-intrusive iris image capturing system can capture iris image within acceptable time ."
  },
  {
    "title": "Exponentially Decaying Bag-of-Words Input Features for Feed-Forward Neural Network in Statistical Machine Translation .",
    "entities": [
      "one-hot encoded input vectors of words",
      "feed-forward neural network translation models",
      "neural network translation model",
      "bidirectional lstm translation model",
      "phrase-based state-of-the-art system",
      "neural network models",
      "statistical machine translation",
      "decay rates",
      "translation tasks",
      "bag-of-words model",
      "weight parameters",
      "ter",
      "bleu"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <method> <method> <task> <otherscientificterm> <task> <method> <otherscientificterm> <metric> <metric>",
    "relations": [
      "neural network translation model -- EVALUATE-FOR -- phrase-based state-of-the-art system",
      "bleu -- CONJUNCTION -- ter",
      "translation tasks -- EVALUATE-FOR -- phrase-based state-of-the-art system",
      "bag-of-words model -- USED-FOR -- phrase-based state-of-the-art system",
      "one-hot encoded input vectors of words -- USED-FOR -- neural network models",
      "neural network models -- USED-FOR -- statistical machine translation"
    ],
    "abstract": "recently , <method_5> have achieved consistent improvements in <task_6> . however , most <method_5> only use <otherscientificterm_0> as their input . in this work , we investigated the exponentially decaying bag-of-words input features for <method_1> and proposed to train the <otherscientificterm_7> along with other <otherscientificterm_10> . this novel <method_9> improved our <method_4> , which already includes a <method_2> , by up to 0.5 % <metric_12> and 0.6 % <metric_11> on three different <task_8> and even achieved a similar performance to the <method_3> .",
    "abstract_og": "recently , neural network models have achieved consistent improvements in statistical machine translation . however , most neural network models only use one-hot encoded input vectors of words as their input . in this work , we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters . this novel bag-of-words model improved our phrase-based state-of-the-art system , which already includes a neural network translation model , by up to 0.5 % bleu and 0.6 % ter on three different translation tasks and even achieved a similar performance to the bidirectional lstm translation model ."
  },
  {
    "title": "Non-Ideal Sampling and Adapted Reconstruction Using the Stochastic Matern Model .",
    "entities": [
      "mmse reconstruction of stochastic mat\u00e9rn signals",
      "generalized , anisotropic version",
      "lter-based reconstruction method",
      "riesz basis",
      "correlation structure",
      "natural images",
      "mat\u00e9rn function",
      "reconstruction space",
      "multi-integer shifts",
      "autocorrelation functions",
      "mat\u00e9rn class",
      "measured data",
      "geostatistics"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "riesz basis -- FEATURE-OF -- mat\u00e9rn function",
      "autocorrelation functions -- USED-FOR -- geostatistics"
    ],
    "abstract": "the <otherscientificterm_10> is a parametric family of <otherscientificterm_9> that is commonly used in <otherscientificterm_12> . we argue that a <method_1> of this model is suitable for capturing the <otherscientificterm_4> of a variety of <material_5> . we specify the optimal space for the <task_0> from their uniformly-sampled noisy measurements -lrb- generalized sampling problem -rrb- . we prove that the optimal <otherscientificterm_7> is generated by the <otherscientificterm_8> of a <otherscientificterm_6> which form a <otherscientificterm_3> . based on this representation , we propose a practical <method_2> that relies on the prior identi cation of the mat\u00e9rn parameters from the <material_11> . we present experimental results to justify the use of the mat\u00e9rn model and to demonstrate the performance of our <method_2> .",
    "abstract_og": "the mat\u00e9rn class is a parametric family of autocorrelation functions that is commonly used in geostatistics . we argue that a generalized , anisotropic version of this model is suitable for capturing the correlation structure of a variety of natural images . we specify the optimal space for the mmse reconstruction of stochastic mat\u00e9rn signals from their uniformly-sampled noisy measurements -lrb- generalized sampling problem -rrb- . we prove that the optimal reconstruction space is generated by the multi-integer shifts of a mat\u00e9rn function which form a riesz basis . based on this representation , we propose a practical lter-based reconstruction method that relies on the prior identi cation of the mat\u00e9rn parameters from the measured data . we present experimental results to justify the use of the mat\u00e9rn model and to demonstrate the performance of our lter-based reconstruction method ."
  },
  {
    "title": "BIG & QUIC : Sparse Inverse Covariance Estimation for a Million Variables .",
    "entities": [
      "1-regularized gaussian maximum likelihood estimator",
      "super-linear or even quadratic convergence rates",
      "sparse inverse covari-ance matrix",
      "non-smooth log-determinant program",
      "block-coordinate descent method",
      "clustering scheme",
      "statistical guarantees",
      "bounded memory",
      "high-dimensional settings",
      "gaussian variables",
      "repeated computations"
    ],
    "types": "<method> <metric> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "1-regularized gaussian maximum likelihood estimator -- USED-FOR -- sparse inverse covari-ance matrix",
      "clustering scheme -- USED-FOR -- block-coordinate descent method"
    ],
    "abstract": "the <method_0> has been shown to have strong <otherscientificterm_6> in recovering a <otherscientificterm_2> even under <otherscientificterm_8> . however , it requires solving a difficult <method_3> with number of parameters scaling quadratically with the number of <otherscientificterm_9> . state-of-the-art methods thus do not scale to problems with more than 20 , 000 variables . in this paper , we develop an algorithm bigquic , which can solve 1 million dimensional 1-regularized gaussian mle problems -lrb- which would thus have 1000 billion parameters -rrb- using a single machine , with <otherscientificterm_7> . in order to do so , we carefully exploit the underlying structure of the problem . our innovations include a novel <method_4> with the blocks chosen via a <method_5> to minimize <otherscientificterm_10> ; and allowing for inexact computation of specific components . in spite of these modifications , we are able to theoretically analyze our <method_4> and show that bigquic can achieve <metric_1> .",
    "abstract_og": "the 1-regularized gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covari-ance matrix even under high-dimensional settings . however , it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of gaussian variables . state-of-the-art methods thus do not scale to problems with more than 20 , 000 variables . in this paper , we develop an algorithm bigquic , which can solve 1 million dimensional 1-regularized gaussian mle problems -lrb- which would thus have 1000 billion parameters -rrb- using a single machine , with bounded memory . in order to do so , we carefully exploit the underlying structure of the problem . our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations ; and allowing for inexact computation of specific components . in spite of these modifications , we are able to theoretically analyze our block-coordinate descent method and show that bigquic can achieve super-linear or even quadratic convergence rates ."
  },
  {
    "title": "Better Synchronous Binarization for Machine Translation .",
    "entities": [
      "scfg parsing based machine translation systems",
      "string-to-tree statistical machine translations system",
      "nist machine translation evaluation tasks",
      "left-heavy binary scfg",
      "synchronous bina-rization method",
      "polynomial time complexity",
      "binary scfgs",
      "decoding"
    ],
    "types": "<task> <method> <task> <method> <method> <metric> <method> <task>",
    "relations": [
      "decoding -- USED-FOR -- scfg parsing based machine translation systems",
      "synchronous bina-rization method -- USED-FOR -- nist machine translation evaluation tasks",
      "nist machine translation evaluation tasks -- EVALUATE-FOR -- string-to-tree statistical machine translations system"
    ],
    "abstract": "binarization of synchronous context free grammars -lrb- scfg -rrb- is essential for achieving <metric_5> of <task_7> for <task_0> . in this paper , we first investigate the excess edge competition issue caused by a <method_3> derived with the method of zhang et al. -lrb- 2006 -rrb- . then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent <method_6> . we present an algorithm that iteratively improves the resulting <method_6> , and empirically show that our method can improve a <method_1> based on the <method_4> in zhang et al. -lrb- 2006 -rrb- on the <task_2> .",
    "abstract_og": "binarization of synchronous context free grammars -lrb- scfg -rrb- is essential for achieving polynomial time complexity of decoding for scfg parsing based machine translation systems . in this paper , we first investigate the excess edge competition issue caused by a left-heavy binary scfg derived with the method of zhang et al. -lrb- 2006 -rrb- . then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary scfgs . we present an algorithm that iteratively improves the resulting binary scfgs , and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous bina-rization method in zhang et al. -lrb- 2006 -rrb- on the nist machine translation evaluation tasks ."
  },
  {
    "title": "Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods .",
    "entities": [
      "stochastic gradient methods",
      "noisy function values",
      "information-theoretic lower bounds",
      "finite-sample convergence rates",
      "minimax convergence rate",
      "stochastic optimization problems",
      "random perturbations",
      "function values",
      "convergence rate",
      "gradient estimates",
      "problem-dependent quantities",
      "algorithmic development",
      "derivative-free algorithms",
      "gradients"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <metric> <metric> <task> <otherscientificterm> <otherscientificterm> <metric> <method> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "derivative-free algorithms -- USED-FOR -- stochastic optimization problems",
      "noisy function values -- USED-FOR -- derivative-free algorithms",
      "information-theoretic lower bounds -- USED-FOR -- algorithmic development",
      "random perturbations -- USED-FOR -- gradient estimates"
    ],
    "abstract": "we consider <method_12> for <task_5> that use only <otherscientificterm_1> rather than <otherscientificterm_13> , analyzing their <metric_3> . we show that if pairs of <otherscientificterm_7> are available , algorithms that use <method_9> based on <otherscientificterm_6> suffer a factor of at most \u221a d in <metric_8> over traditional <method_0> , where d is the problem dimension . we complement our <method_11> with <otherscientificterm_2> on the <metric_4> of such problems , which show that our bounds are sharp with respect to all <otherscientificterm_10> : they can not be improved by more than constant factors .",
    "abstract_og": "we consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients , analyzing their finite-sample convergence rates . we show that if pairs of function values are available , algorithms that use gradient estimates based on random perturbations suffer a factor of at most \u221a d in convergence rate over traditional stochastic gradient methods , where d is the problem dimension . we complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems , which show that our bounds are sharp with respect to all problem-dependent quantities : they can not be improved by more than constant factors ."
  },
  {
    "title": "Learning Hybrid Representations to Retrieve Semantically Equivalent Questions .",
    "entities": [
      "convolutional neural network",
      "bow based information retrieval methods",
      "bag-of-words representation",
      "semantically equivalent question retrieval",
      "online q&a community sites",
      "distributed vector representation",
      "neural network architecture",
      "long texts",
      "bow-cnn",
      "tfidf"
    ],
    "types": "<method> <method> <method> <task> <material> <method> <method> <material> <method> <method>",
    "relations": [
      "bag-of-words representation -- CONJUNCTION -- distributed vector representation",
      "convolutional neural network -- USED-FOR -- distributed vector representation",
      "neural network architecture -- USED-FOR -- semantically equivalent question retrieval",
      "bow-cnn -- COMPARE -- bow based information retrieval methods"
    ],
    "abstract": "retrieving similar questions in <material_4> is a difficult task because different users may formulate the same question in a variety of ways , using different vocabulary and structure . in this work , we propose a new <method_6> to perform the task of <task_3> . the proposed <method_6> , which we call <method_8> , combines a <method_2> with a <method_5> created by a <method_0> . we perform experiments using data collected from two stack exchange communities . our experimental results evidence that : -lrb- 1 -rrb- <method_8> is more effective than <method_1> such as <method_9> ; -lrb- 2 -rrb- <method_8> is more robust than the pure cnn for <material_7> .",
    "abstract_og": "retrieving similar questions in online q&a community sites is a difficult task because different users may formulate the same question in a variety of ways , using different vocabulary and structure . in this work , we propose a new neural network architecture to perform the task of semantically equivalent question retrieval . the proposed neural network architecture , which we call bow-cnn , combines a bag-of-words representation with a distributed vector representation created by a convolutional neural network . we perform experiments using data collected from two stack exchange communities . our experimental results evidence that : -lrb- 1 -rrb- bow-cnn is more effective than bow based information retrieval methods such as tfidf ; -lrb- 2 -rrb- bow-cnn is more robust than the pure cnn for long texts ."
  },
  {
    "title": "NF-Features - No-Feature-Features for Representing Non-textured Regions .",
    "entities": [
      "description of non-textured areas",
      "regular interest point detectors",
      "correspondences of regular features",
      "nf recall rates",
      "regular feature detection",
      "local image statistics",
      "recall rates",
      "image description",
      "nf descriptors",
      "textured regions",
      "non-textured regions",
      "regular features",
      "unchanged precision",
      "once-detected nf-features",
      "nf descriptor",
      "affine transformations",
      "image perturbation",
      "description approaches",
      "precision",
      "recall",
      "sift",
      "nf-features",
      "points",
      "blobs",
      "surf",
      "illumination"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <metric> <task> <material> <metric> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric> <metric> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "blobs -- HYPONYM-OF -- textured regions",
      "nf descriptor -- USED-FOR -- non-textured regions",
      "nf-features -- USED-FOR -- description approaches",
      "nf-features -- USED-FOR -- regular feature detection",
      "points -- CONJUNCTION -- blobs",
      "points -- HYPONYM-OF -- textured regions",
      "regular feature detection -- CONJUNCTION -- description approaches"
    ],
    "abstract": "in order to achieve a complete <task_7> , we introduce no-feature-features -lrb- <method_21> -rrb- representing object regions where <otherscientificterm_1> do not detect features . as these regions are usually non-textured , stable re-localization in different images with conventional methods is not possible . therefore , a technique is presented which re-localizes <otherscientificterm_13> using <otherscientificterm_2> . furthermore , a distinctive <otherscientificterm_14> for <otherscientificterm_10> is derived which has invariance towards <otherscientificterm_15> and changes in <otherscientificterm_25> . for the matching of <method_8> , an approach is introduced that is based on <material_5> . <method_21> can be used complementary to all kinds of <task_4> and <method_17> that focus on <otherscientificterm_9> , i.e. <otherscientificterm_22> , <otherscientificterm_23> or contours . using <method_20> , mser , hessian-affine or <method_24> as regular detectors , we demonstrate that our approach is not only suitable for the <task_0> but that <metric_18> and <metric_19> of the <method_21> is significantly superior to those of <otherscientificterm_11> . in experiments with high variation of the perspective or <otherscientificterm_16> , at <metric_12> we achieve <metric_3> which are better by more than a factor of two compared to <metric_6> of <otherscientificterm_11> .",
    "abstract_og": "in order to achieve a complete image description , we introduce no-feature-features -lrb- nf-features -rrb- representing object regions where regular interest point detectors do not detect features . as these regions are usually non-textured , stable re-localization in different images with conventional methods is not possible . therefore , a technique is presented which re-localizes once-detected nf-features using correspondences of regular features . furthermore , a distinctive nf descriptor for non-textured regions is derived which has invariance towards affine transformations and changes in illumination . for the matching of nf descriptors , an approach is introduced that is based on local image statistics . nf-features can be used complementary to all kinds of regular feature detection and description approaches that focus on textured regions , i.e. points , blobs or contours . using sift , mser , hessian-affine or surf as regular detectors , we demonstrate that our approach is not only suitable for the description of non-textured areas but that precision and recall of the nf-features is significantly superior to those of regular features . in experiments with high variation of the perspective or image perturbation , at unchanged precision we achieve nf recall rates which are better by more than a factor of two compared to recall rates of regular features ."
  },
  {
    "title": "Relighting objects from image collections .",
    "entities": [
      "distant , unknown illumination",
      "all-frequency relighting framework",
      "synthetic test cases",
      "per-surface point reflectance",
      "per-image incident illumination",
      "multi-view stereo reconstruction",
      "reflection models",
      "laboratory conditions",
      "known geometry",
      "geometry",
      "images",
      "illumination"
    ],
    "types": "<otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "per-image incident illumination -- CONJUNCTION -- per-surface point reflectance"
    ],
    "abstract": "we present an approach for recovering the reflectance of a static scene with <otherscientificterm_8> from a collection of <material_10> taken under <otherscientificterm_0> . in contrast to previous work , we allow the <otherscientificterm_11> to vary between the <material_10> , which greatly increases the applicability of the approach . using an <method_1> based on wavelets , we are able to simultaneously estimate the <otherscientificterm_4> and the <otherscientificterm_3> . the wavelet framework allows for incorporating various <method_6> . we demonstrate the quality of our results for <task_2> as well as for several datasets captured under <otherscientificterm_7> . combined with <method_5> , we are even able to recover the <otherscientificterm_9> and reflectance of a scene solely using <material_10> collected from the internet .",
    "abstract_og": "we present an approach for recovering the reflectance of a static scene with known geometry from a collection of images taken under distant , unknown illumination . in contrast to previous work , we allow the illumination to vary between the images , which greatly increases the applicability of the approach . using an all-frequency relighting framework based on wavelets , we are able to simultaneously estimate the per-image incident illumination and the per-surface point reflectance . the wavelet framework allows for incorporating various reflection models . we demonstrate the quality of our results for synthetic test cases as well as for several datasets captured under laboratory conditions . combined with multi-view stereo reconstruction , we are even able to recover the geometry and reflectance of a scene solely using images collected from the internet ."
  },
  {
    "title": "Automated Variational Inference for Gaussian Process Models .",
    "entities": [
      "gaus-sian process models",
      "automated variational method",
      "model-specific inference algorithms",
      "mcmc sampling approaches",
      "univari-ate gaussian distributions",
      "variational distribution",
      "gradient-based optimization",
      "black-box manner",
      "approximate inference",
      "gp hyperparameters",
      "model likelihood",
      "variational objective",
      "covariance matrices",
      "gaussians",
      "posteriors",
      "sampling"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "covariance matrices -- FEATURE-OF -- variational distribution",
      "automated variational method -- USED-FOR -- approximate inference"
    ],
    "abstract": "we develop an <method_1> for <task_8> in <method_0> whose <otherscientificterm_14> are often intractable . using a mixture of <method_13> as the <otherscientificterm_5> , we show that -lrb- i -rrb- the <otherscientificterm_11> and its gradients can be approximated efficiently via <method_15> from <otherscientificterm_4> and -lrb- ii -rrb- the gradients wrt the <method_9> can be obtained analytically regardless of the <otherscientificterm_10> . we further propose two instances of the <otherscientificterm_5> whose <otherscientificterm_12> can be parametrized linearly in the number of observations . these results allow <method_6> to be done efficiently in a <method_7> . our <method_1> is thoroughly verified on five models using six benchmark datasets , performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative <method_3> . our <method_1> can be a valuable <method_1> for practitioners and researchers to investigate new models with minimal effort in deriving <method_2> .",
    "abstract_og": "we develop an automated variational method for approximate inference in gaus-sian process models whose posteriors are often intractable . using a mixture of gaussians as the variational distribution , we show that -lrb- i -rrb- the variational objective and its gradients can be approximated efficiently via sampling from univari-ate gaussian distributions and -lrb- ii -rrb- the gradients wrt the gp hyperparameters can be obtained analytically regardless of the model likelihood . we further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations . these results allow gradient-based optimization to be done efficiently in a black-box manner . our automated variational method is thoroughly verified on five models using six benchmark datasets , performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative mcmc sampling approaches . our automated variational method can be a valuable automated variational method for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms ."
  },
  {
    "title": "Automatically learning speaker-independent acoustic subword units .",
    "entities": [
      "unsupervised learning of sub-word acoustic units",
      "maximum likelihood successive state splitting algorithm",
      "speaker-dependent and cross-speaker correspondence",
      "hidden markov model",
      "viterbi state sequence",
      "unsupervised adaptation",
      "mllr",
      "accuracy",
      "speech"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <material> <method> <method> <metric> <material>",
    "relations": [
      "maximum likelihood successive state splitting algorithm -- USED-FOR -- hidden markov model",
      "mllr -- USED-FOR -- unsupervised adaptation",
      "accuracy -- EVALUATE-FOR -- unsupervised adaptation"
    ],
    "abstract": "we investigate methods for <task_0> of a language directly from <material_8> . we demonstrate that states of a <method_3> '' grown '' using a novel modification of the <method_1> correspond very well with the phones of the language . in particular , the correspondence between the <material_4> for unseen <material_8> from the training speaker and the phone transcription of the <material_8> is over 85 % , and generalizes to a large extent -lrb- \u223c 63 % -rrb- to <material_8> from a different speaker . furthermore , we are able to bridge more than half the gap between the <otherscientificterm_2> of the automatically learned units to phones -lrb- \u223c 75 % <metric_7> -rrb- by <method_5> via <method_6> .",
    "abstract_og": "we investigate methods for unsupervised learning of sub-word acoustic units of a language directly from speech . we demonstrate that states of a hidden markov model '' grown '' using a novel modification of the maximum likelihood successive state splitting algorithm correspond very well with the phones of the language . in particular , the correspondence between the viterbi state sequence for unseen speech from the training speaker and the phone transcription of the speech is over 85 % , and generalizes to a large extent -lrb- \u223c 63 % -rrb- to speech from a different speaker . furthermore , we are able to bridge more than half the gap between the speaker-dependent and cross-speaker correspondence of the automatically learned units to phones -lrb- \u223c 75 % accuracy -rrb- by unsupervised adaptation via mllr ."
  },
  {
    "title": "Pixel Recurrent Neural Networks .",
    "entities": [
      "fast two-dimensional recurrent layers",
      "deep recurrent networks",
      "deep neural network",
      "raw pixel values",
      "residual connections",
      "spatial dimensions",
      "natural images",
      "architectural novelties",
      "unsupervised learning",
      "image model",
      "log-likelihood scores",
      "imagenet dataset",
      "image",
      "pixels"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "residual connections -- PART-OF -- deep recurrent networks",
      "deep neural network -- USED-FOR -- pixels",
      "fast two-dimensional recurrent layers -- HYPONYM-OF -- architectural novelties",
      "residual connections -- USED-FOR -- architectural novelties",
      "fast two-dimensional recurrent layers -- CONJUNCTION -- residual connections"
    ],
    "abstract": "modeling the distribution of <material_6> is a landmark problem in <task_8> . this task requires an <method_9> that is at once expressive , tractable and scalable . we present a <method_2> that sequentially predicts the <otherscientificterm_13> in an <otherscientificterm_12> along the two <otherscientificterm_5> . our <method_2> models the discrete probability of the <otherscientificterm_3> and encodes the complete set of dependencies in the <otherscientificterm_12> . <otherscientificterm_7> include <otherscientificterm_0> and an effective use of <otherscientificterm_4> in <method_1> . we achieve <otherscientificterm_10> on <material_6> that are considerably better than the previous state of the art . our main results also provide benchmarks on the diverse <material_11> . samples generated from the <method_2> appear crisp , varied and globally coherent .",
    "abstract_og": "modeling the distribution of natural images is a landmark problem in unsupervised learning . this task requires an image model that is at once expressive , tractable and scalable . we present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions . our deep neural network models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image . architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks . we achieve log-likelihood scores on natural images that are considerably better than the previous state of the art . our main results also provide benchmarks on the diverse imagenet dataset . samples generated from the deep neural network appear crisp , varied and globally coherent ."
  },
  {
    "title": "Analytical Assessment of Capacity Vs. Robustness Trade-Offs in Systems with Selective Multi-User Diversity .",
    "entities": [
      "spatial vs. multiuser diversity tradeoffs",
      "siso and stbc transmission schemes",
      "short-term snr fluctuations",
      "cellular system",
      "feedback channel",
      "design trade-offs",
      "bandwidth restrictions",
      "transmission schemes",
      "closed-form expressions",
      "selective feedback",
      "robustness",
      "terminals"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "selective feedback -- PART-OF -- cellular system",
      "spatial vs. multiuser diversity tradeoffs -- PART-OF -- cellular system"
    ],
    "abstract": "in this paper , we explore <otherscientificterm_0> in a <method_3> with <otherscientificterm_9> . we first derive <otherscientificterm_8> of the average system capacity for both <otherscientificterm_1> in order to analytically assess the impact of the number of <otherscientificterm_11> and <otherscientificterm_6> in the <otherscientificterm_4> . next , we analyze several <otherscientificterm_5> in terms of increased average -lrb- long term -rrb- system capacity vs. <metric_10> to <otherscientificterm_2> for both <method_7> under consideration .",
    "abstract_og": "in this paper , we explore spatial vs. multiuser diversity tradeoffs in a cellular system with selective feedback . we first derive closed-form expressions of the average system capacity for both siso and stbc transmission schemes in order to analytically assess the impact of the number of terminals and bandwidth restrictions in the feedback channel . next , we analyze several design trade-offs in terms of increased average -lrb- long term -rrb- system capacity vs. robustness to short-term snr fluctuations for both transmission schemes under consideration ."
  },
  {
    "title": "Context-based error recovery technique for GSM AMR speech codec .",
    "entities": [
      "internet and mobile networks",
      "context-based error recovery technique",
      "gsm amr speech codec",
      "recovery of erased frames",
      "random bit errors",
      "output speech quality",
      "segmental itakura-saito measure",
      "error concealment techniques",
      "celp-based speech codec",
      "contextual information",
      "mos scores",
      "frame erasures",
      "erasure spells",
      "parameter estimation",
      "codec parameters",
      "robustness"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <metric> <metric> <method> <task> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <metric>",
    "relations": [
      "contextual information -- USED-FOR -- context-based error recovery technique",
      "frame erasures -- CONJUNCTION -- random bit errors",
      "context-based error recovery technique -- USED-FOR -- celp-based speech codec",
      "gsm amr speech codec -- USED-FOR -- internet and mobile networks",
      "segmental itakura-saito measure -- CONJUNCTION -- mos scores",
      "context-based error recovery technique -- USED-FOR -- recovery of erased frames"
    ],
    "abstract": "gsm amr speech codec being used for both <method_0> , <metric_15> to both <otherscientificterm_11> and <otherscientificterm_4> assumes significance . this paper proposes a new <method_1> for the <task_8> accomplishing <task_3> , updating decoder state during <otherscientificterm_12> and reliable estimation of <otherscientificterm_14> in case of bit errors . previous <method_7> do not adequately make use of the context in which concealment is being done . the proposed <method_1> is intended to retrieve and use <otherscientificterm_9> for better performance . the <method_1> is solely receiver based , uses no look ahead , makes use of implicitly available <otherscientificterm_14> and buffers for <task_13> and is hence computationally efficient . <metric_6> and <metric_10> are used to compare the <metric_5> of the proposed technique with those of the basic techniques as recommended by the standard .",
    "abstract_og": "gsm amr speech codec being used for both internet and mobile networks , robustness to both frame erasures and random bit errors assumes significance . this paper proposes a new context-based error recovery technique for the celp-based speech codec accomplishing recovery of erased frames , updating decoder state during erasure spells and reliable estimation of codec parameters in case of bit errors . previous error concealment techniques do not adequately make use of the context in which concealment is being done . the proposed context-based error recovery technique is intended to retrieve and use contextual information for better performance . the context-based error recovery technique is solely receiver based , uses no look ahead , makes use of implicitly available codec parameters and buffers for parameter estimation and is hence computationally efficient . segmental itakura-saito measure and mos scores are used to compare the output speech quality of the proposed technique with those of the basic techniques as recommended by the standard ."
  },
  {
    "title": "Antenna selection for space time coding over frequency-selective fading channels .",
    "entities": [
      "space-time code",
      "signal-to-noise-ratio",
      "frequency-selective fading channel -rrb-",
      "frequency-selective fading channels",
      "space-time coded systems",
      "receiver side",
      "antenna selection"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "antenna selection -- USED-FOR -- receiver side",
      "receiver side -- FEATURE-OF -- space-time coded systems",
      "signal-to-noise-ratio -- USED-FOR -- antenna selection"
    ],
    "abstract": "in this paper , we deal with <task_6> at the <otherscientificterm_5> for <method_4> over <otherscientificterm_3> . we reveal that introducing <task_6> based on the <method_1> observed can still achieve the full diversity available , if the underlying <otherscientificterm_0> is full-rank -lrb- i.e. , if <otherscientificterm_0> achieves full diversity without <task_6> over the <otherscientificterm_2> . we also argue that if the <otherscientificterm_0> is not full-rank , <task_6> results in a loss in the diversity of the system .",
    "abstract_og": "in this paper , we deal with antenna selection at the receiver side for space-time coded systems over frequency-selective fading channels . we reveal that introducing antenna selection based on the signal-to-noise-ratio observed can still achieve the full diversity available , if the underlying space-time code is full-rank -lrb- i.e. , if space-time code achieves full diversity without antenna selection over the frequency-selective fading channel -rrb- . we also argue that if the space-time code is not full-rank , antenna selection results in a loss in the diversity of the system ."
  },
  {
    "title": "Lateen EM : Unsupervised Training with Multiple Objectives , Applied to Dependency Grammar Induction .",
    "entities": [
      "expectation max-imization algorithms",
      "unsu-pervised dependency parsing tasks",
      "en-glish dependency grammar induction",
      "lateen em",
      "switching objectives",
      "local optima",
      "training methods",
      "unsupervised training",
      "em",
      "disagreements",
      "accuracy"
    ],
    "types": "<method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <metric>",
    "relations": [
      "training methods -- COMPARE -- expectation max-imization algorithms",
      "training methods -- USED-FOR -- local optima"
    ],
    "abstract": "we present new <method_6> that aim to mitigate <otherscientificterm_5> and slow convergence in <method_7> by using additional imperfect objectives . in its simplest form , <method_3> alternates between the two objectives of ordinary '' soft '' and '' hard '' <method_0> . <otherscientificterm_4> when stuck can help escape <otherscientificterm_5> . we find that applying a single such alternation already yields state-of-the-art results for <task_2> . more elaborate <method_6> track both objectives , with each validating the moves proposed by the other . <otherscientificterm_9> can signal earlier opportunities to switch or terminate , saving iterations . de-emphasizing fixed points in these ways eliminates some guesswork from tuning <method_8> . an evaluation against a suite of <task_1> , for a variety of languages , showed that <method_6> significantly speed up training of both <method_0> , and improve <metric_10> for <otherscientificterm_5> .",
    "abstract_og": "we present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives . in its simplest form , lateen em alternates between the two objectives of ordinary '' soft '' and '' hard '' expectation max-imization algorithms . switching objectives when stuck can help escape local optima . we find that applying a single such alternation already yields state-of-the-art results for en-glish dependency grammar induction . more elaborate training methods track both objectives , with each validating the moves proposed by the other . disagreements can signal earlier opportunities to switch or terminate , saving iterations . de-emphasizing fixed points in these ways eliminates some guesswork from tuning em . an evaluation against a suite of unsu-pervised dependency parsing tasks , for a variety of languages , showed that training methods significantly speed up training of both expectation max-imization algorithms , and improve accuracy for local optima ."
  },
  {
    "title": "Fast Parameter Sensitivity Analysis of PDE-Based Image Processing Methods .",
    "entities": [
      "discontinuity-preserving optical flow computation",
      "fast parameter sensitivity analysis",
      "pde-based image processing operators",
      "stochastic finite elements",
      "image processing operators",
      "full sensitivity analysis",
      "parameter sensitivity analysis",
      "polynomial chaos expansion",
      "perona-malik diffusion",
      "ambrosio-tortorelli segmentation",
      "uncertainty quantification",
      "sampling strategy"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <method> <task> <method> <method> <task> <task> <method> <method>",
    "relations": [
      "sampling strategy -- USED-FOR -- fast parameter sensitivity analysis",
      "polynomial chaos expansion -- CONJUNCTION -- stochastic finite elements",
      "parameter sensitivity analysis -- USED-FOR -- perona-malik diffusion",
      "polynomial chaos expansion -- CONJUNCTION -- pde-based image processing operators",
      "uncertainty quantification -- USED-FOR -- fast parameter sensitivity analysis",
      "uncertainty quantification -- CONJUNCTION -- image processing operators"
    ],
    "abstract": "we present a <method_1> by combining recent developments from <method_10> with <method_4> . the <method_1> is not based on a <method_11> , instead we combine the <method_7> and <otherscientificterm_3> with <method_2> . with our <method_1> and a moderate number of parameters in the models the <task_5> is obtained at the cost of a few monte carlo runs . to demonstrate the efficiency and simplicity of the <method_1> we show a <method_6> for <task_8> , random walker and <task_9> , and <task_0> .",
    "abstract_og": "we present a fast parameter sensitivity analysis by combining recent developments from uncertainty quantification with image processing operators . the fast parameter sensitivity analysis is not based on a sampling strategy , instead we combine the polynomial chaos expansion and stochastic finite elements with pde-based image processing operators . with our fast parameter sensitivity analysis and a moderate number of parameters in the models the full sensitivity analysis is obtained at the cost of a few monte carlo runs . to demonstrate the efficiency and simplicity of the fast parameter sensitivity analysis we show a parameter sensitivity analysis for perona-malik diffusion , random walker and ambrosio-tortorelli segmentation , and discontinuity-preserving optical flow computation ."
  },
  {
    "title": "Revisiting probabilistic models for clustering with pair-wise constraints .",
    "entities": [
      "undesirable local behaviors",
      "chunklet model",
      "constraint violation",
      "probabilistic clustering",
      "learning technique",
      "pair-wise constraints",
      "soft constraints",
      "misspecified constraints",
      "robustness"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "robustness -- FEATURE-OF -- misspecified constraints",
      "chunklet model -- USED-FOR -- soft constraints"
    ],
    "abstract": "we revisit recently proposed algorithms for <task_3> with <otherscientificterm_5> between data points . we evaluate and compare existing techniques in terms of <metric_8> to <otherscientificterm_7> . we show that the technique that strictly enforces the given constraints , namely the <method_1> , produces poor results even under a small number of <otherscientificterm_7> . we further show that methods that penalize <otherscientificterm_2> are more robust to <otherscientificterm_7> but have <otherscientificterm_0> . based on this evaluation , we propose a new <method_4> , extending the <method_1> to allow <otherscientificterm_6> represented by an intuitive measure of confidence in the constraint .",
    "abstract_og": "we revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points . we evaluate and compare existing techniques in terms of robustness to misspecified constraints . we show that the technique that strictly enforces the given constraints , namely the chunklet model , produces poor results even under a small number of misspecified constraints . we further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors . based on this evaluation , we propose a new learning technique , extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint ."
  },
  {
    "title": "Impact of an Energy Normalization Transform on the Performance of the LF-ASD Brain Computer Interface .",
    "entities": [
      "active and idle eeg data",
      "lf-asd brain-computer interface",
      "energy normalization transform",
      "signal amplitude variability",
      "class separation",
      "system errors",
      "non-normalized accuracy",
      "lf-asd"
    ],
    "types": "<material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <method>",
    "relations": [
      "active and idle eeg data -- USED-FOR -- class separation"
    ],
    "abstract": "this paper presents an <method_2> as a method to reduce <otherscientificterm_5> in the <task_1> . the <method_2> has two major benefits to the system performance . first , it can increase <otherscientificterm_4> between the <material_0> . second , it can desensitize the system to the <otherscientificterm_3> . for four subjects in the study , the benefits resulted in the performance improvement of the <method_7> in the range from 7.7 % to 18.9 % , while for the fifth subject , who had the highest <metric_6> of 90.5 % , the performance did not change notably with normalization .",
    "abstract_og": "this paper presents an energy normalization transform as a method to reduce system errors in the lf-asd brain-computer interface . the energy normalization transform has two major benefits to the system performance . first , it can increase class separation between the active and idle eeg data . second , it can desensitize the system to the signal amplitude variability . for four subjects in the study , the benefits resulted in the performance improvement of the lf-asd in the range from 7.7 % to 18.9 % , while for the fifth subject , who had the highest non-normalized accuracy of 90.5 % , the performance did not change notably with normalization ."
  },
  {
    "title": "A study on domain recognition of spoken dialogue systems .",
    "entities": [
      "parallel computation of speech-recognition engines",
      "multi-domain spoken dialogue system",
      "information retrieval nature",
      "human-machine interaction",
      "scoring method",
      "weather report",
      "news query",
      "in-car usage",
      "recognition accuracy",
      "speech recognizer"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <metric> <method>",
    "relations": [
      "weather report -- CONJUNCTION -- news query"
    ],
    "abstract": "in this paper , we present a <method_1> equipped with the capability of <method_0> that are assigned to each domain . the experimental <method_1> is set up to handle three different domains -lrb- restaurant information , <material_5> , and <otherscientificterm_6> -rrb- in an <otherscientificterm_7> . all of these tasks are of <otherscientificterm_2> . the domain of a particular utterance is determined based on the likelihood of each <method_9> . in addition to the <otherscientificterm_3> , synthesized voice of the route subsystem interrupts the dialogue frequently . experimental evaluation has yielded 95 percent <metric_8> in selecting the task domain based on a specially designed <method_4> .",
    "abstract_og": "in this paper , we present a multi-domain spoken dialogue system equipped with the capability of parallel computation of speech-recognition engines that are assigned to each domain . the experimental multi-domain spoken dialogue system is set up to handle three different domains -lrb- restaurant information , weather report , and news query -rrb- in an in-car usage . all of these tasks are of information retrieval nature . the domain of a particular utterance is determined based on the likelihood of each speech recognizer . in addition to the human-machine interaction , synthesized voice of the route subsystem interrupts the dialogue frequently . experimental evaluation has yielded 95 percent recognition accuracy in selecting the task domain based on a specially designed scoring method ."
  },
  {
    "title": "Repulsive Mixtures .",
    "entities": [
      "markov chain monte carlo sampling algorithm",
      "fully supervised multi-task learning",
      "iris data set",
      "infinite mixtures",
      "redundant components",
      "low separation",
      "dirichlet processes",
      "finite mixtures",
      "bayesian approach",
      "posterior computation",
      "discrete mixtures",
      "computational problems",
      "discrete mixtures",
      "supervised settings",
      "unsupervised learning",
      "synthetic examples",
      "repulsive process",
      "unsupervised settings",
      "repulsive prior",
      "redundancy",
      "redundancy",
      "clusters",
      "penalty"
    ],
    "types": "<method> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <task> <otherscientificterm> <material> <method> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "markov chain monte carlo sampling algorithm -- USED-FOR -- posterior computation",
      "synthetic examples -- CONJUNCTION -- iris data set",
      "finite mixtures -- CONJUNCTION -- infinite mixtures"
    ],
    "abstract": "discrete mixtures are used routinely in broad sweeping applications ranging from <method_17> to <task_1> . indeed , <otherscientificterm_7> and <otherscientificterm_3> , relying on <method_6> and modifications , have become a standard tool . one important issue that arises in using <otherscientificterm_12> is <otherscientificterm_5> in the components ; in particular , different components can be introduced that are very similar and hence redundant . such <otherscientificterm_20> leads to too many <otherscientificterm_21> that are too similar , degrading performance in <method_14> and leading to <task_11> and an unnecessarily complex model in <material_13> . <otherscientificterm_19> can arise in the absence of a <otherscientificterm_22> on components placed close together even when a <method_8> is used to learn the number of components . to solve this problem , we propose a novel prior that generates components from a <otherscientificterm_16> , automatically penalizing <otherscientificterm_4> . we characterize this <otherscientificterm_18> theoretically and propose a <method_0> for <task_9> . the methods are illustrated using <material_15> and an <material_2> .",
    "abstract_og": "discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning . indeed , finite mixtures and infinite mixtures , relying on dirichlet processes and modifications , have become a standard tool . one important issue that arises in using discrete mixtures is low separation in the components ; in particular , different components can be introduced that are very similar and hence redundant . such redundancy leads to too many clusters that are too similar , degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings . redundancy can arise in the absence of a penalty on components placed close together even when a bayesian approach is used to learn the number of components . to solve this problem , we propose a novel prior that generates components from a repulsive process , automatically penalizing redundant components . we characterize this repulsive prior theoretically and propose a markov chain monte carlo sampling algorithm for posterior computation . the methods are illustrated using synthetic examples and an iris data set ."
  },
  {
    "title": "Estimation of cortical connectivity from E/MEG using nonlinear state-space models .",
    "entities": [
      "radial basis function kernels",
      "nonlinear multivariate au-toregressive model",
      "maximum likelihood estimates",
      "nonlinear dynamics of the cortical signals",
      "magnetoencephalographic data",
      "expectation-maximization algorithm",
      "spatially extended cortical sources",
      "nonlinear state-space model parameters",
      "nonlinear state-space model",
      "parsimonious spatial bases",
      "estimating cortical connectivity",
      "measuring cortical connectivity",
      "e/meg data",
      "observation equation",
      "system identification",
      "granger causality",
      "cortical signals",
      "decoupled approach",
      "inverse problem",
      "mvar model",
      "state equation",
      "observed data",
      "roi"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <material> <method> <material> <otherscientificterm> <method> <otherscientificterm> <task> <task> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <task> <method> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "nonlinear multivariate au-toregressive model -- USED-FOR -- state equation",
      "radial basis function kernels -- USED-FOR -- nonlinear multivariate au-toregressive model",
      "system identification -- USED-FOR -- estimating cortical connectivity",
      "cortical signals -- USED-FOR -- observation equation",
      "parsimonious spatial bases -- USED-FOR -- spatially extended cortical sources",
      "magnetoencephalographic data -- USED-FOR -- system identification",
      "parsimonious spatial bases -- USED-FOR -- e/meg data",
      "nonlinear state-space model -- USED-FOR -- system identification",
      "expectation-maximization algorithm -- USED-FOR -- maximum likelihood estimates",
      "cortical signals -- COMPARE -- decoupled approach",
      "parsimonious spatial bases -- USED-FOR -- cortical signals",
      "radial basis function kernels -- USED-FOR -- nonlinear dynamics of the cortical signals",
      "magnetoencephalographic data -- USED-FOR -- estimating cortical connectivity",
      "cortical signals -- FEATURE-OF -- e/meg data"
    ],
    "abstract": "we present the problem of <task_10> between different regions of the cortex from scalp electroen-cephalographic -lrb- eeg -rrb- or <material_4> as <task_14> of a <method_8> . the <otherscientificterm_20> is based on a <method_1> with <otherscientificterm_0> . the <otherscientificterm_0> capture the <otherscientificterm_3> and provide a framework for measuring interactions between cortical regions of interest -lrb- rois -rrb- based on the definition of <otherscientificterm_15> . the <otherscientificterm_13> relates the <otherscientificterm_16> associated with each <otherscientificterm_22> to the observed <material_12> using a set of <otherscientificterm_9> to represent <material_6> . an <method_5> is derived to obtain <method_2> of the <otherscientificterm_7> directly from the <material_21> . we show that this integrated approach for <task_11> performs significantly better than the conventional <method_17> in which <otherscientificterm_16> are first estimated by solving the <task_18> followed by fitting a <method_19> .",
    "abstract_og": "we present the problem of estimating cortical connectivity between different regions of the cortex from scalp electroen-cephalographic -lrb- eeg -rrb- or magnetoencephalographic data as system identification of a nonlinear state-space model . the state equation is based on a nonlinear multivariate au-toregressive model with radial basis function kernels . the radial basis function kernels capture the nonlinear dynamics of the cortical signals and provide a framework for measuring interactions between cortical regions of interest -lrb- rois -rrb- based on the definition of granger causality . the observation equation relates the cortical signals associated with each roi to the observed e/meg data using a set of parsimonious spatial bases to represent spatially extended cortical sources . an expectation-maximization algorithm is derived to obtain maximum likelihood estimates of the nonlinear state-space model parameters directly from the observed data . we show that this integrated approach for measuring cortical connectivity performs significantly better than the conventional decoupled approach in which cortical signals are first estimated by solving the inverse problem followed by fitting a mvar model ."
  },
  {
    "title": "Prediction error based feedback for downlink transmit beamforming .",
    "entities": [
      "joint maximum-likelihood and set-membership filtering algorithm",
      "downlink transmit beamforming scheme",
      "adaptive channel estimation",
      "uplink capacity analysis",
      "mobile decision errors",
      "adaptive channel estimator",
      "prediction error",
      "incorrect feedback",
      "robustness",
      "base-station",
      "power"
    ],
    "types": "<method> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "joint maximum-likelihood and set-membership filtering algorithm -- USED-FOR -- adaptive channel estimation",
      "uplink capacity analysis -- USED-FOR -- joint maximum-likelihood and set-membership filtering algorithm"
    ],
    "abstract": "this paper examines a <method_1> recently proposed in -lsb- 1 -rsb- . the idea is based on the use of an <method_5> at the <otherscientificterm_9> and requires the mobile to selectively feed back the value of the <otherscientificterm_6> . this paper proposes a <method_0> for <task_2> that provides <metric_8> against <otherscientificterm_7> due to <otherscientificterm_4> . the amount of <otherscientificterm_10> and bandwidth-saving possible with this <method_0> is quantified via an <method_3> .",
    "abstract_og": "this paper examines a downlink transmit beamforming scheme recently proposed in -lsb- 1 -rsb- . the idea is based on the use of an adaptive channel estimator at the base-station and requires the mobile to selectively feed back the value of the prediction error . this paper proposes a joint maximum-likelihood and set-membership filtering algorithm for adaptive channel estimation that provides robustness against incorrect feedback due to mobile decision errors . the amount of power and bandwidth-saving possible with this joint maximum-likelihood and set-membership filtering algorithm is quantified via an uplink capacity analysis ."
  },
  {
    "title": "Sparse variable reduced rank regression via Stiefel optimization .",
    "entities": [
      "sparse variable reduced rank regression",
      "reduced rank regression",
      "rank regression",
      "vector l1 penalty",
      "estimation algorithm",
      "stiefel manifold",
      "signal processing",
      "optimization"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <method>",
    "relations": [
      "stiefel manifold -- USED-FOR -- estimation algorithm",
      "optimization -- USED-FOR -- estimation algorithm",
      "rank regression -- USED-FOR -- signal processing"
    ],
    "abstract": "reduced <method_2> has found application in various fields of <task_6> . in this paper we propose a novel extension of the <method_2> which we call <otherscientificterm_0> . by using a <otherscientificterm_3> we remove variables completely from the <method_2> . the proposed <method_4> involves <method_7> on the <otherscientificterm_5> and we illustrate <method_4> both on a simulated and a real functional magnetic resonance imaging -lrb- fmri -rrb- data set .",
    "abstract_og": "reduced rank regression has found application in various fields of signal processing . in this paper we propose a novel extension of the rank regression which we call sparse variable reduced rank regression . by using a vector l1 penalty we remove variables completely from the rank regression . the proposed estimation algorithm involves optimization on the stiefel manifold and we illustrate estimation algorithm both on a simulated and a real functional magnetic resonance imaging -lrb- fmri -rrb- data set ."
  },
  {
    "title": "Enhanced multidimensional spatial functions for unambiguous localization of multiple sparse acoustic sources .",
    "entities": [
      "global coherence field",
      "detection of multiple competing sources",
      "spatial and multidimensional tdoa domains",
      "single source localiza-tion case",
      "discrete time-frequency domain",
      "acoustic source localization",
      "multiple source case",
      "steered response power",
      "spatial function",
      "simulated data",
      "computational inexpensiveness",
      "cross-power spectrum",
      "multidimensional metric",
      "ambiguous locations",
      "mid-high reverberation",
      "higher norms",
      "source sparseness",
      "partitioned representation",
      "multidimensional kernel",
      "l1 norm",
      "phat",
      "robustness"
    ],
    "types": "<method> <task> <material> <task> <otherscientificterm> <task> <material> <method> <otherscientificterm> <material> <metric> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <metric>",
    "relations": [
      "multidimensional metric -- USED-FOR -- steered response power",
      "cross-power spectrum -- USED-FOR -- multidimensional kernel",
      "steered response power -- USED-FOR -- acoustic source localization",
      "computational inexpensiveness -- CONJUNCTION -- mid-high reverberation",
      "simulated data -- EVALUATE-FOR -- spatial function",
      "spatial function -- USED-FOR -- spatial and multidimensional tdoa domains",
      "partitioned representation -- USED-FOR -- multidimensional kernel",
      "global coherence field -- USED-FOR -- acoustic source localization",
      "computational inexpensiveness -- CONJUNCTION -- robustness",
      "steered response power -- CONJUNCTION -- global coherence field"
    ],
    "abstract": "the <method_7> with <method_20> transform -lrb- srp-phat -rrb- or <method_0> , has become a standard method for <task_5> , thanks to their simplicity , <metric_10> and <metric_21> against <otherscientificterm_14> . however , originally formulated for the <task_3> , <method_7> does not apply satisfactorily to the <material_6> . in this paper , we analyze the structure of the <otherscientificterm_8> and reshape <method_7> according to a generic <method_12> . we show that traditional functions are based on the <otherscientificterm_19> which is prone to generate <otherscientificterm_13> with high likelihood -lrb- i.e. ghosts -rrb- . a more generic <method_18> based on <otherscientificterm_15> and on a <method_17> of the <otherscientificterm_11> is introduced , which better exploits the <otherscientificterm_16> in the <otherscientificterm_4> . evaluation results over <material_9> show that the new <otherscientificterm_8> considerably improve the <task_1> in both <material_2> .",
    "abstract_og": "the steered response power with phat transform -lrb- srp-phat -rrb- or global coherence field , has become a standard method for acoustic source localization , thanks to their simplicity , computational inexpensiveness and robustness against mid-high reverberation . however , originally formulated for the single source localiza-tion case , steered response power does not apply satisfactorily to the multiple source case . in this paper , we analyze the structure of the spatial function and reshape steered response power according to a generic multidimensional metric . we show that traditional functions are based on the l1 norm which is prone to generate ambiguous locations with high likelihood -lrb- i.e. ghosts -rrb- . a more generic multidimensional kernel based on higher norms and on a partitioned representation of the cross-power spectrum is introduced , which better exploits the source sparseness in the discrete time-frequency domain . evaluation results over simulated data show that the new spatial function considerably improve the detection of multiple competing sources in both spatial and multidimensional tdoa domains ."
  },
  {
    "title": "Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs .",
    "entities": [
      "forest and hyperforest polytopes",
      "convex optimization problem",
      "combi-natorial optimization problem",
      "undirected graphical models",
      "local search techniques",
      "maximum likelihood framework",
      "run-time complexity",
      "synthetic datasets",
      "classical benchmarks",
      "convex approach",
      "supergradient method",
      "bounded treewidth",
      "np-hard problem"
    ],
    "types": "<otherscientificterm> <task> <task> <method> <method> <method> <metric> <material> <material> <method> <method> <otherscientificterm> <task>",
    "relations": [
      "synthetic datasets -- EVALUATE-FOR -- supergradient method",
      "convex optimization problem -- USED-FOR -- forest and hyperforest polytopes",
      "bounded treewidth -- FEATURE-OF -- undirected graphical models"
    ],
    "abstract": "we consider the problem of learning the structure of <method_3> with <otherscientificterm_11> , within the <method_5> . this is an <task_12> and most approaches consider <method_4> . in this paper , we pose <task_12> as a <task_2> , which is then relaxed to a <task_1> that involves searching over the <otherscientificterm_0> with special structures , independently . a <method_10> is used to solve the dual problem , with a <metric_6> of o -lrb- k 3 n k +2 log n -rrb- for each iteration , where n is the number of variables and k is a bound on the treewidth . we compare our <method_10> to state-of-the-art methods on <material_7> and <material_8> , showing the gains of the novel <method_9> .",
    "abstract_og": "we consider the problem of learning the structure of undirected graphical models with bounded treewidth , within the maximum likelihood framework . this is an np-hard problem and most approaches consider local search techniques . in this paper , we pose np-hard problem as a combi-natorial optimization problem , which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures , independently . a supergradient method is used to solve the dual problem , with a run-time complexity of o -lrb- k 3 n k +2 log n -rrb- for each iteration , where n is the number of variables and k is a bound on the treewidth . we compare our supergradient method to state-of-the-art methods on synthetic datasets and classical benchmarks , showing the gains of the novel convex approach ."
  },
  {
    "title": "Bayesian MCMC nonlinear time series prediction .",
    "entities": [
      "mcmc -lrb- markov chain monte carlo -rrb- algorithm",
      "nonlinear time series prediction",
      "hierarchical bayesian framework",
      "predictive distributions",
      "quadratic approximations",
      "time series"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "mcmc -lrb- markov chain monte carlo -rrb- algorithm -- USED-FOR -- nonlinear time series prediction",
      "mcmc -lrb- markov chain monte carlo -rrb- algorithm -- COMPARE -- quadratic approximations"
    ],
    "abstract": "an <method_0> is proposed for <task_1> with <method_2> . the <method_0> computes predictive mean and error bar by drawing samples from <otherscientificterm_3> . the <method_0> is tested against <otherscientificterm_5> generated by -lrb- chaotic -rrb- r\u00f6ssler system and <method_0> outperforms <method_4> previously proposed by the authors .",
    "abstract_og": "an mcmc -lrb- markov chain monte carlo -rrb- algorithm is proposed for nonlinear time series prediction with hierarchical bayesian framework . the mcmc -lrb- markov chain monte carlo -rrb- algorithm computes predictive mean and error bar by drawing samples from predictive distributions . the mcmc -lrb- markov chain monte carlo -rrb- algorithm is tested against time series generated by -lrb- chaotic -rrb- r\u00f6ssler system and mcmc -lrb- markov chain monte carlo -rrb- algorithm outperforms quadratic approximations previously proposed by the authors ."
  },
  {
    "title": "Activity-Based Scheduling of Science Campaigns for the Rosetta Orbiter .",
    "entities": [
      "science ground segment",
      "automated and semi-automated scheduling software",
      "science planning and scheduling system",
      "incremental planning process",
      "medium term planning",
      "automated scheduling capability",
      "pre-landing operations phase",
      "long term planning",
      "short term planning",
      "science mission plan",
      "rosetta orbiter",
      "skeleton planning",
      "comet nucleus",
      "rosetta orbiter",
      "philae lander"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <task> <method> <method> <method> <task> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "medium term planning -- CONJUNCTION -- short term planning",
      "science mission plan -- USED-FOR -- rosetta orbiter",
      "long term planning -- CONJUNCTION -- medium term planning"
    ],
    "abstract": "rosetta is an esa cornerstone mission that will reach the comet 67p/churyumov-gerasimenko in august 2014 and will escort the comet for a 1.5 year nominal mission offering the most detailed study of a comet ever undertaken by humankind . the <method_13> has 11 scientific instruments -lrb- 4 remote sensing -rrb- and the <method_14> to make complementary measurements of the <otherscientificterm_12> , coma -lrb- gas and dust -rrb- , and surrounding environment . the esa rosetta science ground segment has developed a <method_2> that includes an <method_5> to assist in developing science plans for the <task_10> . while <method_5> is a small portion of the overall <method_0> as well as the overall scheduling system , this paper focuses on the <method_1> -lrb- called aspen-rssc -rrb- and how this software is used . specifically , the <method_13> uses an <method_3> of successive refinement of the <method_9> beginning with <method_11> , <method_7> , <method_4> , and <method_8> . these phases represent the evolution of the <method_9> from one year before execution running through just before execution . we also report on aspen-rssc experience and usage during the <task_6> thus far .",
    "abstract_og": "rosetta is an esa cornerstone mission that will reach the comet 67p/churyumov-gerasimenko in august 2014 and will escort the comet for a 1.5 year nominal mission offering the most detailed study of a comet ever undertaken by humankind . the rosetta orbiter has 11 scientific instruments -lrb- 4 remote sensing -rrb- and the philae lander to make complementary measurements of the comet nucleus , coma -lrb- gas and dust -rrb- , and surrounding environment . the esa rosetta science ground segment has developed a science planning and scheduling system that includes an automated scheduling capability to assist in developing science plans for the rosetta orbiter . while automated scheduling capability is a small portion of the overall science ground segment as well as the overall scheduling system , this paper focuses on the automated and semi-automated scheduling software -lrb- called aspen-rssc -rrb- and how this software is used . specifically , the rosetta orbiter uses an incremental planning process of successive refinement of the science mission plan beginning with skeleton planning , long term planning , medium term planning , and short term planning . these phases represent the evolution of the science mission plan from one year before execution running through just before execution . we also report on aspen-rssc experience and usage during the pre-landing operations phase thus far ."
  },
  {
    "title": "Bounded Gaussian fingerprints and the gradient collusion attack -LSB- multimedia fingerprinting applications -RSB- .",
    "entities": [
      "direct-sequence and uniformly distributed spread spectrum fingerprints",
      "analog hole '' problem",
      "audio or video signals",
      "digital rights management system",
      "bounded gaussian fingerprints",
      "multimedia file sharing",
      "analog hole",
      "object size",
      "cryptographic primitives",
      "gradient attack",
      "spread-spectrum fingerprints",
      "collusion procedure",
      "multimedia fingerprinting",
      "robustness",
      "encryption",
      "scrambling"
    ],
    "types": "<otherscientificterm> <task> <material> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <metric> <method> <method>",
    "relations": [
      "robustness -- FEATURE-OF -- gradient attack",
      "encryption -- CONJUNCTION -- scrambling",
      "encryption -- HYPONYM-OF -- cryptographic primitives",
      "collusion procedure -- USED-FOR -- spread-spectrum fingerprints",
      "scrambling -- HYPONYM-OF -- cryptographic primitives",
      "bounded gaussian fingerprints -- USED-FOR -- gradient attack"
    ],
    "abstract": "the difficulty of building an effective <method_3> stems from the fact that traditional <otherscientificterm_8> such as <method_14> or <method_15> do not protect <material_2> once they are played in plain-text . this fact , commonly referred to as '' the <otherscientificterm_6> , '' has been responsible for the popularity of <task_5> which can not be controlled , at least technically , by content 's copyright owners . in this paper , we explore a specific issue in <task_12> as an answer to '' the <task_1> . we analyze the collusion resistance of three large classes of <otherscientificterm_10> using a recently introduced <method_11> , the <method_9> . surprisingly , we show that the collusion resistance of <otherscientificterm_0> is a small constant that does not depend on the <otherscientificterm_7> , whereas <otherscientificterm_4> demonstrate significantly better <metric_13> to the <method_9> .",
    "abstract_og": "the difficulty of building an effective digital rights management system stems from the fact that traditional cryptographic primitives such as encryption or scrambling do not protect audio or video signals once they are played in plain-text . this fact , commonly referred to as '' the analog hole , '' has been responsible for the popularity of multimedia file sharing which can not be controlled , at least technically , by content 's copyright owners . in this paper , we explore a specific issue in multimedia fingerprinting as an answer to '' the analog hole '' problem . we analyze the collusion resistance of three large classes of spread-spectrum fingerprints using a recently introduced collusion procedure , the gradient attack . surprisingly , we show that the collusion resistance of direct-sequence and uniformly distributed spread spectrum fingerprints is a small constant that does not depend on the object size , whereas bounded gaussian fingerprints demonstrate significantly better robustness to the gradient attack ."
  },
  {
    "title": "Full Diversity Detection in MIMO Systems with a Fixed-Complexity Sphere Decoder .",
    "entities": [
      "\u017fxed-complexity sphere decoder",
      "maximum likelihood detector",
      "noise ratio regime",
      "sphere decoder",
      "signi\u017fcantly lower complexity",
      "hardware implementation",
      "sequential structure",
      "coding loss",
      "variable complexity",
      "diversity order",
      "mld"
    ],
    "types": "<method> <method> <metric> <method> <metric> <task> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method>",
    "relations": [
      "variable complexity -- CONJUNCTION -- sequential structure",
      "\u017fxed-complexity sphere decoder -- USED-FOR -- hardware implementation"
    ],
    "abstract": "the <method_0> has been previously proposed for multiple input-multiple output -lrb- mimo -rrb- detection to overcome the two main drawbacks of the original <method_3> , namely its <metric_8> and <otherscientificterm_6> . as such , the <method_0> is highly suitable for <task_5> and has shown remarkable performance through simulations . herein , we explore the theoretical aspects of the algorithm and prove that the <method_0> achieves the same <otherscientificterm_9> as the <method_1> . further , we show that the <otherscientificterm_7> can be made negligible in the high signal to <metric_2> with a <metric_4> than that of the <method_10> .",
    "abstract_og": "the \u017fxed-complexity sphere decoder has been previously proposed for multiple input-multiple output -lrb- mimo -rrb- detection to overcome the two main drawbacks of the original sphere decoder , namely its variable complexity and sequential structure . as such , the \u017fxed-complexity sphere decoder is highly suitable for hardware implementation and has shown remarkable performance through simulations . herein , we explore the theoretical aspects of the algorithm and prove that the \u017fxed-complexity sphere decoder achieves the same diversity order as the maximum likelihood detector . further , we show that the coding loss can be made negligible in the high signal to noise ratio regime with a signi\u017fcantly lower complexity than that of the mld ."
  },
  {
    "title": "Sparse covariance estimation under Kronecker product structure .",
    "entities": [
      "sparse kronecker-decomposable covariance matrix",
      "sparse covariance estimation method",
      "large-sample statistical consistency",
      "1-penalized log-likelihood function",
      "block coordinate-descent approach",
      "high dimensional setting",
      "penalized maximum-likelihood approach",
      "iterative algorithm",
      "glasso algorithm",
      "local maximum",
      "covariance matrix",
      "mild assumptions",
      "objective function",
      "convergence bound",
      "kronecker product",
      "ffp",
      "gaussian"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "glasso algorithm -- USED-FOR -- sparse kronecker-decomposable covariance matrix",
      "ffp -- COMPARE -- glasso algorithm",
      "iterative algorithm -- USED-FOR -- sparse covariance estimation method",
      "sparse covariance estimation method -- USED-FOR -- high dimensional setting"
    ],
    "abstract": "we introduce a <method_1> for the <otherscientificterm_5> when the <otherscientificterm_10> decomposes as a <otherscientificterm_14> , i.e. , \u03c3 0 = a 0 \u2297 b 0 , and the observations are <method_16> . we propose an 1 <method_6> to solve this problem . the <method_1> motivates an <method_7> -lrb- penalized flip-flop ; <method_15> -rrb- based on a <method_4> . although the <otherscientificterm_3> -lrb- <otherscientificterm_12> -rrb- is non-convex in general and non-smooth , we show that <method_15> converges to a <otherscientificterm_9> under relatively <otherscientificterm_11> . for the fixed dimension case , <otherscientificterm_2> is proved and a rate of <otherscientificterm_13> is derived . simulations show that <method_15> outperforms its non-penalized counterpart and the naive <method_8> for <otherscientificterm_0> .",
    "abstract_og": "we introduce a sparse covariance estimation method for the high dimensional setting when the covariance matrix decomposes as a kronecker product , i.e. , \u03c3 0 = a 0 \u2297 b 0 , and the observations are gaussian . we propose an 1 penalized maximum-likelihood approach to solve this problem . the sparse covariance estimation method motivates an iterative algorithm -lrb- penalized flip-flop ; ffp -rrb- based on a block coordinate-descent approach . although the 1-penalized log-likelihood function -lrb- objective function -rrb- is non-convex in general and non-smooth , we show that ffp converges to a local maximum under relatively mild assumptions . for the fixed dimension case , large-sample statistical consistency is proved and a rate of convergence bound is derived . simulations show that ffp outperforms its non-penalized counterpart and the naive glasso algorithm for sparse kronecker-decomposable covariance matrix ."
  },
  {
    "title": "Learning feed-forward one-shot learners .",
    "entities": [
      "visual object tracking benchmark",
      "feed-forward one-shot learner",
      "tracking visual objects",
      "one-shot classification objective",
      "deep network",
      "one-shot learning",
      "pupil network",
      "learning scenarios",
      "generative models",
      "deep model",
      "one-shot learning",
      "discriminative methods",
      "discriminative embeddings",
      "deep learning",
      "omniglot"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <method> <task> <method> <task> <method> <method> <task> <method> <method> <method> <material>",
    "relations": [
      "discriminative methods -- USED-FOR -- learning scenarios",
      "generative models -- CONJUNCTION -- discriminative embeddings",
      "discriminative embeddings -- USED-FOR -- one-shot learning",
      "deep learning -- USED-FOR -- discriminative methods",
      "visual object tracking benchmark -- USED-FOR -- tracking visual objects",
      "generative models -- USED-FOR -- one-shot learning"
    ],
    "abstract": "one-shot learning is usually tackled by using <method_8> or <method_12> . <method_11> based on <method_13> , which are very effective in other <task_7> , are ill-suited for <task_5> as <method_11> need large amounts of training data . in this paper , we propose a method to learn the parameters of a <method_9> in one shot . we construct the learner as a second <method_4> , called a learnet , which predicts the parameters of a <method_6> from a single exemplar . in this manner we obtain an efficient <method_1> , trained end-to-end by minimizing a <otherscientificterm_3> in a learning to learn formulation . in order to make the construction feasible , we propose a number of factorizations of the parameters of the <method_6> . we demonstrate encouraging results by learning characters from single exemplars in <material_14> , and by <task_2> from a single initial exemplar in the <task_0> .",
    "abstract_og": "one-shot learning is usually tackled by using generative models or discriminative embeddings . discriminative methods based on deep learning , which are very effective in other learning scenarios , are ill-suited for one-shot learning as discriminative methods need large amounts of training data . in this paper , we propose a method to learn the parameters of a deep model in one shot . we construct the learner as a second deep network , called a learnet , which predicts the parameters of a pupil network from a single exemplar . in this manner we obtain an efficient feed-forward one-shot learner , trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation . in order to make the construction feasible , we propose a number of factorizations of the parameters of the pupil network . we demonstrate encouraging results by learning characters from single exemplars in omniglot , and by tracking visual objects from a single initial exemplar in the visual object tracking benchmark ."
  },
  {
    "title": "On-demand new word learning using world wide web .",
    "entities": [
      "out-of-vocabulary words",
      "global semantic features",
      "oov word retrieval",
      "local decoding pass",
      "locally-augmented lexicons",
      "hypothesis words",
      "lexicon augmenting",
      "local context",
      "transcription process",
      "oov words",
      "web-based methods",
      "part-of-speech models",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <method> <metric>",
    "relations": [
      "web-based methods -- USED-FOR -- lexicon augmenting",
      "locally-augmented lexicons -- USED-FOR -- local decoding pass",
      "part-of-speech models -- USED-FOR -- transcription process"
    ],
    "abstract": "most of the <method_10> for <task_6> consist in capturing <otherscientificterm_1> of the targeted domain in order to collect relevant documents from the web . we suggest that the <otherscientificterm_7> of the <otherscientificterm_0> contains relevant information on the <otherscientificterm_9> . with this information , we propose to use the web to build <otherscientificterm_4> which are used in a final <method_3> . we first demonstrate the relevance of the web for the <task_2> . then , different methods are proposed to retrieve the <otherscientificterm_5> . finally we present the integration of new words in the <method_8> based on <method_11> . this technique allows to recover 7.7 % of the significant <otherscientificterm_9> and the <metric_12> of the system is slightly improved .",
    "abstract_og": "most of the web-based methods for lexicon augmenting consist in capturing global semantic features of the targeted domain in order to collect relevant documents from the web . we suggest that the local context of the out-of-vocabulary words contains relevant information on the oov words . with this information , we propose to use the web to build locally-augmented lexicons which are used in a final local decoding pass . we first demonstrate the relevance of the web for the oov word retrieval . then , different methods are proposed to retrieve the hypothesis words . finally we present the integration of new words in the transcription process based on part-of-speech models . this technique allows to recover 7.7 % of the significant oov words and the accuracy of the system is slightly improved ."
  },
  {
    "title": "Scalable Semantic Retrieval through Summarization and Refinement .",
    "entities": [
      "processing of instance retrieval queries",
      "expressive description logic shin",
      "querying of ontologies",
      "secondary storage",
      "query processing",
      "owl-dl ontologies",
      "instance retrieval",
      "summarization",
      "owl-dl",
      "abox",
      "datatypes"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <task> <material> <task> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "instance retrieval -- PART-OF -- abox",
      "summarization -- USED-FOR -- instance retrieval"
    ],
    "abstract": "query processing of <material_5> is intractable in the worst case , but we present a novel technique that in practice allows for efficient <task_2> with large aboxes in <otherscientificterm_3> . we focus on the <task_0> , i.e. , queries that retrieve individuals in the <otherscientificterm_9> which are instances of a given concept c . our technique uses <method_7> and refinement to reduce <task_6> to a small relevant subset of the original <otherscientificterm_9> . we demonstrate the effectiveness of this technique in aboxes with up to 7 million assertions . our results are applicable to the very <method_1> , which corresponds to <method_8> minus nominals and <method_10> .",
    "abstract_og": "query processing of owl-dl ontologies is intractable in the worst case , but we present a novel technique that in practice allows for efficient querying of ontologies with large aboxes in secondary storage . we focus on the processing of instance retrieval queries , i.e. , queries that retrieve individuals in the abox which are instances of a given concept c . our technique uses summarization and refinement to reduce instance retrieval to a small relevant subset of the original abox . we demonstrate the effectiveness of this technique in aboxes with up to 7 million assertions . our results are applicable to the very expressive description logic shin , which corresponds to owl-dl minus nominals and datatypes ."
  },
  {
    "title": "Adaptive Singleton-Based Consistencies .",
    "entities": [
      "singleton arc consistency",
      "adaptive partition-one-ac",
      "adaptive variants",
      "singleton-based consistency",
      "singleton tests",
      "arc consistency",
      "singleton-based consistencies",
      "constraint solvers",
      "fixpoint",
      "partition-one-ac",
      "pruning"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method> <otherscientificterm> <method> <task>",
    "relations": [
      "singleton-based consistencies -- USED-FOR -- constraint solvers",
      "arc consistency -- CONJUNCTION -- partition-one-ac"
    ],
    "abstract": "singleton-based consistencies have been shown to dramatically improve the performance of <method_7> on some difficult instances . however , they are in general too expensive to be applied exhaustively during the whole search . in this paper , we focus on <method_9> , a <otherscientificterm_3> which , as opposed to <otherscientificterm_0> , is able to prune values on all variables when <method_9> performs <otherscientificterm_4> on one of them . we propose <method_2> of <method_9> that do not necessarily run until having proved the <otherscientificterm_8> . the <task_10> can be weaker than the full version but the computational effort can be significantly reduced . our experiments show that <method_1> can obtain significant speed-ups over <metric_5> and over the full version of <method_9> .",
    "abstract_og": "singleton-based consistencies have been shown to dramatically improve the performance of constraint solvers on some difficult instances . however , they are in general too expensive to be applied exhaustively during the whole search . in this paper , we focus on partition-one-ac , a singleton-based consistency which , as opposed to singleton arc consistency , is able to prune values on all variables when partition-one-ac performs singleton tests on one of them . we propose adaptive variants of partition-one-ac that do not necessarily run until having proved the fixpoint . the pruning can be weaker than the full version but the computational effort can be significantly reduced . our experiments show that adaptive partition-one-ac can obtain significant speed-ups over arc consistency and over the full version of partition-one-ac ."
  },
  {
    "title": "K-SVD dictionary-learning for the analysis sparse model .",
    "entities": [
      "greedy tailored pursuit algorithms",
      "synthesis-based sparse representation model",
      "signal of interest",
      "dictionary update stage",
      "synthesis model",
      "analysis dictionary",
      "signal examples",
      "analysis dictionary",
      "analysis-based model",
      "k-svd algorithm",
      "penalty function",
      "linear combination"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <material> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "greedy tailored pursuit algorithms -- CONJUNCTION -- penalty function",
      "penalty function -- USED-FOR -- dictionary update stage",
      "greedy tailored pursuit algorithms -- HYPONYM-OF -- analysis-based model",
      "signal examples -- USED-FOR -- analysis dictionary",
      "penalty function -- PART-OF -- analysis-based model"
    ],
    "abstract": "the <method_1> for signals has drawn a considerable interest in the past decade . such a <method_1> assumes that the <otherscientificterm_2> can be decomposed as a <method_11> of a few atoms from a given dictionary . in this paper we concentrate on an alternative , <method_8> , where an <method_7> multiplies the signal , leading to a sparse outcome . our goal is to learn the <method_5> from a set of <material_6> , and the <method_8> taken is parallel and similar to the one adopted by the <method_9> that serves the corresponding problem in the <method_4> . we present the development of the <method_8> , which include two <method_0> and a <otherscientificterm_10> for the <otherscientificterm_3> . we demonstrate its effectiveness in several experiments , showing a successful and meaningful recovery of the <method_5> .",
    "abstract_og": "the synthesis-based sparse representation model for signals has drawn a considerable interest in the past decade . such a synthesis-based sparse representation model assumes that the signal of interest can be decomposed as a linear combination of a few atoms from a given dictionary . in this paper we concentrate on an alternative , analysis-based model , where an analysis dictionary multiplies the signal , leading to a sparse outcome . our goal is to learn the analysis dictionary from a set of signal examples , and the analysis-based model taken is parallel and similar to the one adopted by the k-svd algorithm that serves the corresponding problem in the synthesis model . we present the development of the analysis-based model , which include two greedy tailored pursuit algorithms and a penalty function for the dictionary update stage . we demonstrate its effectiveness in several experiments , showing a successful and meaningful recovery of the analysis dictionary ."
  },
  {
    "title": "Frequency-domain single-channel inverse filtering for speech dereverberation : Theory and practice .",
    "entities": [
      "approximate time-domain inverse filtering techniques",
      "single-channel least-squares",
      "single-channel speech enhancement scheme",
      "frequency-domain inverse filtering technique",
      "acoustic transfer function",
      "single-channel inverse filtering",
      "rir inaccuracies",
      "theoretical analysis",
      "inverse filter",
      "scls technique",
      "computational complexity",
      "frequency-domain",
      "dereverberation",
      "estimate",
      "regularization"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <metric> <otherscientificterm> <task> <task> <otherscientificterm>",
    "relations": [
      "acoustic transfer function -- PART-OF -- frequency-domain",
      "computational complexity -- EVALUATE-FOR -- frequency-domain inverse filtering technique",
      "frequency-domain inverse filtering technique -- COMPARE -- scls technique",
      "single-channel inverse filtering -- USED-FOR -- inverse filter",
      "computational complexity -- EVALUATE-FOR -- scls technique",
      "single-channel least-squares -- HYPONYM-OF -- approximate time-domain inverse filtering techniques",
      "regularization -- CONJUNCTION -- single-channel speech enhancement scheme"
    ],
    "abstract": "the objective of <method_5> is to design an <method_8> that achieves <task_12> while being robust to an inaccurate room impulse response -lrb- rir -rrb- measurement or <task_13> . since a stable and causal <method_8> typically does not exist , <method_0> such as <otherscientificterm_1> have been proposed . however , besides being computationally expensive and often infeasible , <otherscientificterm_1> generally leads to distortions in the output signal in the presence of <otherscientificterm_6> . in this paper , a <method_7> is initially provided , showing that the direct inversion of the <otherscientificterm_4> in the <otherscientificterm_11> generally yields instability and acausality issues . in order to resolve these issues , a novel <method_3> is proposed that incorporates <otherscientificterm_14> and uses a <method_2> . experimental results demonstrate that the proposed <method_3> yields a higher dere-verberation performance and has a significantly lower <metric_10> compared to the <method_9> .",
    "abstract_og": "the objective of single-channel inverse filtering is to design an inverse filter that achieves dereverberation while being robust to an inaccurate room impulse response -lrb- rir -rrb- measurement or estimate . since a stable and causal inverse filter typically does not exist , approximate time-domain inverse filtering techniques such as single-channel least-squares have been proposed . however , besides being computationally expensive and often infeasible , single-channel least-squares generally leads to distortions in the output signal in the presence of rir inaccuracies . in this paper , a theoretical analysis is initially provided , showing that the direct inversion of the acoustic transfer function in the frequency-domain generally yields instability and acausality issues . in order to resolve these issues , a novel frequency-domain inverse filtering technique is proposed that incorporates regularization and uses a single-channel speech enhancement scheme . experimental results demonstrate that the proposed frequency-domain inverse filtering technique yields a higher dere-verberation performance and has a significantly lower computational complexity compared to the scls technique ."
  },
  {
    "title": "ReNoun : Fact Extraction for Nominal Attributes .",
    "entities": [
      "open information extraction system",
      "open information extraction",
      "renoun 's approach",
      "noun-based relations",
      "verb phrases",
      "search engines",
      "nominal attributes",
      "text corpus",
      "verb-based techniques",
      "knowledge bases",
      "user queries",
      "precision",
      "renoun"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <method> <otherscientificterm> <material> <metric> <method>",
    "relations": [
      "text corpus -- CONJUNCTION -- user queries",
      "renoun -- HYPONYM-OF -- open information extraction system"
    ],
    "abstract": "search engines are increasingly relying on large <otherscientificterm_9> of facts to provide direct answers to users ' queries . however , the construction of these <otherscientificterm_9> is largely manual and does not scale to the long and heavy tail of facts . <task_1> tries to address this challenge , but typically assumes that facts are expressed with <otherscientificterm_4> , and therefore has had difficulty extracting facts for <otherscientificterm_3> . we describe <method_12> , an <method_0> that complements previous efforts by focusing on <otherscientificterm_6> and on the long tail . <method_2> is based on leveraging a large on-tology of noun attributes mined from a <material_7> and from <material_10> . <method_12> creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontol-ogy . <method_12> then generalizes from this seed set to produce a much larger set of extractions that are then scored . we describe experiments that show that we extract facts with high <metric_11> and for attributes that can not be extracted with <method_8> .",
    "abstract_og": "search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users ' queries . however , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts . open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations . we describe renoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail . renoun 's approach is based on leveraging a large on-tology of noun attributes mined from a text corpus and from user queries . renoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontol-ogy . renoun then generalizes from this seed set to produce a much larger set of extractions that are then scored . we describe experiments that show that we extract facts with high precision and for attributes that can not be extracted with verb-based techniques ."
  },
  {
    "title": "Incremental Learning in SwiftFile .",
    "entities": [
      "user 's mail-filing habits",
      "swiftfile 's classifier",
      "incremental learning algorithms",
      "learning systems",
      "intelligent assistant",
      "text classifier",
      "incremen-tal learning",
      "shortcut buttons",
      "swiftfile"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <method> <method> <task> <otherscientificterm> <method>",
    "relations": [
      "incremental learning algorithms -- USED-FOR -- swiftfile 's classifier",
      "incremen-tal learning -- USED-FOR -- swiftfile",
      "text classifier -- USED-FOR -- swiftfile"
    ],
    "abstract": "swiftfile is an <method_4> that helps users organize their e-mail into folders . <method_8> uses a <method_5> to predict where each new message is likely to be filed by the user and provides <otherscientificterm_7> to quickly file messages into one of its predicted folders . one of the challenges faced by <method_8> is that the <otherscientificterm_0> are constantly changing -- users are frequently creating , deleting and rearranging folders to meet their current filing needs . in this paper , we discuss the importance of <task_6> in <method_8> . we present several criteria for judging how well <method_2> adapt to quickly changing data and evaluate <method_1> using these criteria . we find that <method_1> is surprisingly responsive and does not require the extensive training that is often assumed in most <method_3> .",
    "abstract_og": "swiftfile is an intelligent assistant that helps users organize their e-mail into folders . swiftfile uses a text classifier to predict where each new message is likely to be filed by the user and provides shortcut buttons to quickly file messages into one of its predicted folders . one of the challenges faced by swiftfile is that the user 's mail-filing habits are constantly changing -- users are frequently creating , deleting and rearranging folders to meet their current filing needs . in this paper , we discuss the importance of incremen-tal learning in swiftfile . we present several criteria for judging how well incremental learning algorithms adapt to quickly changing data and evaluate swiftfile 's classifier using these criteria . we find that swiftfile 's classifier is surprisingly responsive and does not require the extensive training that is often assumed in most learning systems ."
  },
  {
    "title": "AOSO-LogitBoost : Adaptive One-Vs-One LogitBoost for Multi-Class Problem .",
    "entities": [
      "tractable model learning algorithm",
      "adaptive block coordinate descent",
      "node value fittings",
      "dense hessian matrices",
      "additive tree regression",
      "public data sets",
      "classification accuracy",
      "node values",
      "convergence rates",
      "node value",
      "log-itboost setting",
      "sum-to-zero constraint",
      "vector tree",
      "statistical view",
      "multi-class logitboost",
      "model learning",
      "abc-logitboost implementations",
      "dense hessian",
      "logitboost",
      "lat-ter",
      "classification"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <material> <metric> <otherscientificterm> <metric> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <method> <task>",
    "relations": [
      "adaptive block coordinate descent -- USED-FOR -- dense hessian",
      "multi-class logitboost -- USED-FOR -- model learning",
      "vector tree -- USED-FOR -- log-itboost setting",
      "multi-class logitboost -- USED-FOR -- classification"
    ],
    "abstract": "this paper presents an improvement to <task_15> when using <method_14> for <task_20> . motivated by the <otherscientificterm_13> , <method_18> can be seen as <method_4> . two important factors in this setting are : 1 -rrb- coupled classifier output due to a <otherscientificterm_11> , and 2 -rrb- the <otherscientificterm_3> that arise when computing tree node split gain and <otherscientificterm_2> . in general , this setting is too complicated for a <method_0> . however , too aggressive simplification of the setting may lead to degraded performance . for example , the original <method_18> is outper-formed by <method_18> due to the <method_19> 's more careful treatment of the above two factors . in this paper we propose techniques to address the two main difficulties of the <method_10> : 1 -rrb- we adopt a <method_12> -lrb- i.e. , each <otherscientificterm_9> is vector -rrb- that enforces a <otherscientificterm_11> , and 2 -rrb- we use an <method_1> that exploits the <otherscientificterm_17> when computing tree split gain and <otherscientificterm_7> . higher <metric_6> and faster <metric_8> are observed for a range of <material_5> when compared to both the original and the <method_16> .",
    "abstract_og": "this paper presents an improvement to model learning when using multi-class logitboost for classification . motivated by the statistical view , logitboost can be seen as additive tree regression . two important factors in this setting are : 1 -rrb- coupled classifier output due to a sum-to-zero constraint , and 2 -rrb- the dense hessian matrices that arise when computing tree node split gain and node value fittings . in general , this setting is too complicated for a tractable model learning algorithm . however , too aggressive simplification of the setting may lead to degraded performance . for example , the original logitboost is outper-formed by logitboost due to the lat-ter 's more careful treatment of the above two factors . in this paper we propose techniques to address the two main difficulties of the log-itboost setting : 1 -rrb- we adopt a vector tree -lrb- i.e. , each node value is vector -rrb- that enforces a sum-to-zero constraint , and 2 -rrb- we use an adaptive block coordinate descent that exploits the dense hessian when computing tree split gain and node values . higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the abc-logitboost implementations ."
  },
  {
    "title": "Automatic Non-rigid 3D Modeling from Video .",
    "entities": [
      "estimating non-rigid 3d shape and motion",
      "rigid and non-rigid shape reconstruction",
      "input video sequence",
      "3d time-varying shape",
      "video sequences",
      "user-tuned parameters",
      "non-rigid deformations",
      "system parameters",
      "user-specified region",
      "variable illumination",
      "image stream",
      "pdf",
      "initialization",
      "outliers",
      "occlusion"
    ],
    "types": "<task> <task> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "occlusion -- CONJUNCTION -- variable illumination",
      "non-rigid deformations -- PART-OF -- system parameters"
    ],
    "abstract": "we present a robust framework for <task_0> in <material_4> . given an <material_2> , and a <otherscientificterm_8> to reconstruct , the algorithm automatically solves for the <otherscientificterm_3> and motion of the object , and estimates which pixels are <otherscientificterm_13> , while learning all <otherscientificterm_7> , including a <otherscientificterm_11> over <otherscientificterm_6> . there are no <otherscientificterm_5> -lrb- other than <otherscientificterm_12> -rrb- ; all parameters are learned by maximizing the likelihood of the entire <otherscientificterm_10> . we apply our method to both <task_1> , and demonstrate it in challenging cases of <otherscientificterm_14> and <otherscientificterm_9> .",
    "abstract_og": "we present a robust framework for estimating non-rigid 3d shape and motion in video sequences . given an input video sequence , and a user-specified region to reconstruct , the algorithm automatically solves for the 3d time-varying shape and motion of the object , and estimates which pixels are outliers , while learning all system parameters , including a pdf over non-rigid deformations . there are no user-tuned parameters -lrb- other than initialization -rrb- ; all parameters are learned by maximizing the likelihood of the entire image stream . we apply our method to both rigid and non-rigid shape reconstruction , and demonstrate it in challenging cases of occlusion and variable illumination ."
  },
  {
    "title": "Joint Acoustic-Video Fingerprinting of Vehicles , Part I.",
    "entities": [
      "envelope shape components",
      "intuitive discriminatory feature space",
      "acoustic and video sensors",
      "passive acoustic sensor",
      "vehicle speed estimation",
      "vehicle profile vector",
      "silhouette extraction",
      "vehicle classification",
      "field data",
      "mensuration problems",
      "video sensor",
      "wheel detection",
      "acoustic wave-pattern",
      "shape",
      "mensuration",
      "classification"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <task> <task> <material> <task> <method> <task> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "passive acoustic sensor -- USED-FOR -- acoustic wave-pattern",
      "silhouette extraction -- CONJUNCTION -- wheel detection",
      "envelope shape components -- USED-FOR -- acoustic wave-pattern",
      "vehicle classification -- CONJUNCTION -- mensuration"
    ],
    "abstract": "we address <task_7> and <task_9> using <otherscientificterm_2> . in this paper , we show how to estimate a vehicle 's speed , width , and length by jointly estimating its <otherscientificterm_12> using a single <method_3> that records the vehicle 's drive-by noise . the <otherscientificterm_12> is approximated using three <method_0> , which approximate the <otherscientificterm_13> of the received signal 's power envelope . we incorporate the parameters of the <method_0> along with estimates of the vehicle engine rpm and number of cylinders to create a <otherscientificterm_5> that forms an <otherscientificterm_1> . in the companion paper , we discuss <task_7> and <task_14> based on <task_6> and <task_11> , using a <method_10> . <task_4> and <task_15> results are provided using <material_8> .",
    "abstract_og": "we address vehicle classification and mensuration problems using acoustic and video sensors . in this paper , we show how to estimate a vehicle 's speed , width , and length by jointly estimating its acoustic wave-pattern using a single passive acoustic sensor that records the vehicle 's drive-by noise . the acoustic wave-pattern is approximated using three envelope shape components , which approximate the shape of the received signal 's power envelope . we incorporate the parameters of the envelope shape components along with estimates of the vehicle engine rpm and number of cylinders to create a vehicle profile vector that forms an intuitive discriminatory feature space . in the companion paper , we discuss vehicle classification and mensuration based on silhouette extraction and wheel detection , using a video sensor . vehicle speed estimation and classification results are provided using field data ."
  },
  {
    "title": "Coupled dictionary training for exemplar-based speech enhancement .",
    "entities": [
      "signal-to-distortion ratio",
      "speech and noise estimates",
      "exemplar-based speech enhancement systems",
      "lower dimensional features",
      "exemplar-based speech enhancement",
      "full-scale dft features",
      "noisy dft enhancement",
      "modulation envelope features",
      "activations of exemplars",
      "wiener-like filter",
      "mel resolution",
      "wiener filter",
      "dft space",
      "sub-optimal filter",
      "feature space",
      "computational complexity"
    ],
    "types": "<metric> <material> <method> <otherscientificterm> <task> <otherscientificterm> <task> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "activations of exemplars -- USED-FOR -- wiener filter",
      "feature space -- CONJUNCTION -- dft space",
      "modulation envelope features -- USED-FOR -- exemplar-based speech enhancement",
      "feature space -- FEATURE-OF -- speech and noise estimates",
      "wiener-like filter -- USED-FOR -- noisy dft enhancement",
      "mel resolution -- USED-FOR -- activations of exemplars"
    ],
    "abstract": "in <method_2> , <otherscientificterm_3> are preferred over the <otherscientificterm_5> for their reduced <metric_15> and the ability to better generalize for the unseen cases . but in order to obtain the <method_9> for <task_6> , the <material_1> obtained in the <otherscientificterm_14> need to be mapped to the <otherscientificterm_12> , which yield a low-rank approximation of the estimates resulting in a <otherscientificterm_13> . this paper proposes a novel method using coupled dictionaries where the exemplars for the required <otherscientificterm_14> and the <otherscientificterm_12> are jointly extracted and the estimates are directly obtained in the <otherscientificterm_12> following the decomposition in the chosen <otherscientificterm_14> . simulation experiments revealed that the proposed approach , where the <otherscientificterm_8> calculated using the <task_10> are directly used to obtain the <method_11> in the <otherscientificterm_12> , results in improved <metric_0> when compared to the system without coupled dictionaries . to further motivate the use of coupled dictionaries , the paper also investigates the use of <method_7> for the <task_4> .",
    "abstract_og": "in exemplar-based speech enhancement systems , lower dimensional features are preferred over the full-scale dft features for their reduced computational complexity and the ability to better generalize for the unseen cases . but in order to obtain the wiener-like filter for noisy dft enhancement , the speech and noise estimates obtained in the feature space need to be mapped to the dft space , which yield a low-rank approximation of the estimates resulting in a sub-optimal filter . this paper proposes a novel method using coupled dictionaries where the exemplars for the required feature space and the dft space are jointly extracted and the estimates are directly obtained in the dft space following the decomposition in the chosen feature space . simulation experiments revealed that the proposed approach , where the activations of exemplars calculated using the mel resolution are directly used to obtain the wiener filter in the dft space , results in improved signal-to-distortion ratio when compared to the system without coupled dictionaries . to further motivate the use of coupled dictionaries , the paper also investigates the use of modulation envelope features for the exemplar-based speech enhancement ."
  },
  {
    "title": "Cost-sensitive multi-class classification from probability estimates .",
    "entities": [
      "naive bayes and quadratic discriminant analysis",
      "class probability estimates",
      "roc curve analysis",
      "multiclass probability simplex",
      "partition matrix",
      "two-class classification",
      "cost matrix",
      "multiclass classification",
      "cost matrices",
      "benchmark datasets",
      "systematic errors",
      "class partitioning",
      "threshold"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <task> <metric> <material> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "class probability estimates -- CONJUNCTION -- cost matrices",
      "benchmark datasets -- EVALUATE-FOR -- partition matrix",
      "roc curve analysis -- USED-FOR -- threshold"
    ],
    "abstract": "for <task_5> , it is common to classify by setting a <otherscientificterm_12> on <otherscientificterm_1> , where the <otherscientificterm_12> is determined by <method_2> . an analog for <task_5> is learning a new <task_11> of the <otherscientificterm_3> to minimize empirical misclassification costs . we analyze the interplay between <otherscientificterm_10> in the <otherscientificterm_1> and <metric_8> for <task_7> . we explore the effect on the <task_11> of five different transformations of the <otherscientificterm_6> . experiments on <material_9> with <method_0> show the effectiveness of learning a new <method_4> compared to previously proposed methods .",
    "abstract_og": "for two-class classification , it is common to classify by setting a threshold on class probability estimates , where the threshold is determined by roc curve analysis . an analog for two-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs . we analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification . we explore the effect on the class partitioning of five different transformations of the cost matrix . experiments on benchmark datasets with naive bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods ."
  },
  {
    "title": "An efficient approximation of the forward-backward algorithm to deal with packet loss , with applications to remote speech recognition .",
    "entities": [
      "hidden markov models",
      "tree-structure mapping of quantizer centroids",
      "forward-backward algorithm",
      "remote speech recognition",
      "estimating missing features",
      "lower resolution quantizers",
      "word recognition accuracy",
      "downsampling statistical models",
      "error concealment",
      "aurora-2 database",
      "fb approximation",
      "computational load",
      "estimation process",
      "hmms"
    ],
    "types": "<method> <method> <method> <task> <task> <method> <metric> <method> <task> <material> <method> <otherscientificterm> <task> <method>",
    "relations": [
      "fb approximation -- COMPARE -- forward-backward algorithm",
      "tree-structure mapping of quantizer centroids -- USED-FOR -- hmms",
      "hidden markov models -- USED-FOR -- estimation process",
      "hmms -- USED-FOR -- forward-backward algorithm",
      "word recognition accuracy -- EVALUATE-FOR -- fb approximation",
      "forward-backward algorithm -- USED-FOR -- estimating missing features",
      "lower resolution quantizers -- USED-FOR -- hmms",
      "word recognition accuracy -- EVALUATE-FOR -- forward-backward algorithm",
      "error concealment -- USED-FOR -- remote speech recognition",
      "forward-backward algorithm -- USED-FOR -- remote speech recognition",
      "forward-backward algorithm -- USED-FOR -- error concealment"
    ],
    "abstract": "this paper proposes an efficient approximation of the <method_2> , for the purpose of <task_4> , based on <method_7> . the paper discusses the role of <method_0> in the <task_12> , and presents an approximation to the <method_2> by developing <method_13> based on <method_5> , which are obtained through a <method_1> . to illustrate the effectiveness of the proposed <method_2> , we apply <method_2> to the problem of <task_8> in <task_3> , using the <material_9> . the <method_10> provides comparable <metric_6> results relative to the standard <method_2> , while reducing the <otherscientificterm_11> by a large factor -lrb- > 250 in this case -rrb- .",
    "abstract_og": "this paper proposes an efficient approximation of the forward-backward algorithm , for the purpose of estimating missing features , based on downsampling statistical models . the paper discusses the role of hidden markov models in the estimation process , and presents an approximation to the forward-backward algorithm by developing hmms based on lower resolution quantizers , which are obtained through a tree-structure mapping of quantizer centroids . to illustrate the effectiveness of the proposed forward-backward algorithm , we apply forward-backward algorithm to the problem of error concealment in remote speech recognition , using the aurora-2 database . the fb approximation provides comparable word recognition accuracy results relative to the standard forward-backward algorithm , while reducing the computational load by a large factor -lrb- > 250 in this case -rrb- ."
  },
  {
    "title": "Slice sampling covariance hyperparameters of latent Gaussian models .",
    "entities": [
      "markov chain monte carlo sampling",
      "gaussian process",
      "hyperparameter sampling approaches",
      "slice sampling approach",
      "strong-and weak-data regimes",
      "probabilistic model",
      "bayesian framework",
      "unknown hyperparameters",
      "covariance structure",
      "random variables"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "unknown hyperparameters -- USED-FOR -- covariance structure",
      "bayesian framework -- USED-FOR -- covariance structure",
      "unknown hyperparameters -- USED-FOR -- bayesian framework"
    ],
    "abstract": "the <method_1> is a popular way to specify dependencies between <otherscientificterm_9> in a <method_5> . in the <method_6> the <otherscientificterm_8> can be specified using <otherscientificterm_7> . integrating over these <otherscientificterm_7> considers different possible explanations for the data when making predictions . this integration is often performed using <method_0> . however , with non-gaussian observations standard <method_2> require careful tuning and may converge slowly . in this paper we present a <method_3> that requires little tuning while mixing well in both <otherscientificterm_4> .",
    "abstract_og": "the gaussian process is a popular way to specify dependencies between random variables in a probabilistic model . in the bayesian framework the covariance structure can be specified using unknown hyperparameters . integrating over these unknown hyperparameters considers different possible explanations for the data when making predictions . this integration is often performed using markov chain monte carlo sampling . however , with non-gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly . in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong-and weak-data regimes ."
  },
  {
    "title": "Subspace projection of multichannel audio data for automatic control of motion-platform-based multimedia display systems .",
    "entities": [
      "haptic and vestibular sensations",
      "motion platform control",
      "multimedia display systems",
      "motion-platform-based multimedia display",
      "multichannel audio data",
      "motion data",
      "audiovisual content",
      "displayed environment",
      "motion platforms",
      "subspace projection"
    ],
    "types": "<otherscientificterm> <task> <method> <material> <material> <material> <material> <otherscientificterm> <method> <method>",
    "relations": [
      "multichannel audio data -- USED-FOR -- motion platform control",
      "multichannel audio data -- USED-FOR -- motion data",
      "motion data -- USED-FOR -- motion platform control",
      "motion platforms -- USED-FOR -- multimedia display systems",
      "multimedia display systems -- USED-FOR -- haptic and vestibular sensations",
      "audiovisual content -- USED-FOR -- motion data"
    ],
    "abstract": "this paper addresses a practical problem associated with <method_2> that utilize <method_8> to create for users both <otherscientificterm_0> associated with their movement through a <otherscientificterm_7> . given <material_6> for which <material_5> is not available , the <material_5> that is required for <task_1> can be generated automatically from <material_4> such as that distributed on dvds presenting popular movie titles . this paper presents initial results of a study designed to test the effectiveness of a <method_9> of <material_4> for automatic control of <material_3> .",
    "abstract_og": "this paper addresses a practical problem associated with multimedia display systems that utilize motion platforms to create for users both haptic and vestibular sensations associated with their movement through a displayed environment . given audiovisual content for which motion data is not available , the motion data that is required for motion platform control can be generated automatically from multichannel audio data such as that distributed on dvds presenting popular movie titles . this paper presents initial results of a study designed to test the effectiveness of a subspace projection of multichannel audio data for automatic control of motion-platform-based multimedia display ."
  },
  {
    "title": "Random Indexing using Statistical Weight Functions .",
    "entities": [
      "vector space technique",
      "distributional similarity problems",
      "small data sets",
      "high frequency attributes",
      "random indexing",
      "weighting functions",
      "weight functions"
    ],
    "types": "<method> <task> <material> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "random indexing -- HYPONYM-OF -- vector space technique",
      "weighting functions -- USED-FOR -- random indexing",
      "random indexing -- USED-FOR -- small data sets",
      "vector space technique -- USED-FOR -- distributional similarity problems"
    ],
    "abstract": "random indexing is a <method_0> that provides an efficient and scal-able approximation to <task_1> . we present experiments showing <task_4> to be poor at handling large volumes of data and evaluate the use of <method_5> for improving the performance of <task_4> . we find that <task_4> is robust for <material_2> , but performance degrades because of the influence of <otherscientificterm_3> in large data sets . the use of appropriate <otherscientificterm_6> improves this significantly .",
    "abstract_og": "random indexing is a vector space technique that provides an efficient and scal-able approximation to distributional similarity problems . we present experiments showing random indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of random indexing . we find that random indexing is robust for small data sets , but performance degrades because of the influence of high frequency attributes in large data sets . the use of appropriate weight functions improves this significantly ."
  },
  {
    "title": "Recursive Inversion Models for Permutations .",
    "entities": [
      "mallows and generalized mal-lows models",
      "exponential family probabilistic model",
      "class of models",
      "structure search",
      "parameter estimation",
      "hierarchical structure"
    ],
    "types": "<method> <method> <method> <task> <task> <otherscientificterm>",
    "relations": [
      "structure search -- USED-FOR -- class of models",
      "exponential family probabilistic model -- USED-FOR -- hierarchical structure"
    ],
    "abstract": "we develop a new <method_1> for permutations that can capture <otherscientificterm_5> and that has the <method_0> as subclasses . we describe how to do <task_4> and propose an approach to <task_3> for this <method_2> . we provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations .",
    "abstract_og": "we develop a new exponential family probabilistic model for permutations that can capture hierarchical structure and that has the mallows and generalized mal-lows models as subclasses . we describe how to do parameter estimation and propose an approach to structure search for this class of models . we provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations ."
  },
  {
    "title": "Role of phase estimation in speech enhancement .",
    "entities": [
      "short-time fourier transform phase spectrum",
      "stft analysis-modification-synthesis framework",
      "speech enhancement algorithms",
      "stft phase spectra",
      "speech quality measures",
      "noise reduction",
      "speech enhancement",
      "fourier domain",
      "phase component",
      "magnitude component",
      "ham-ming window",
      "window function",
      "dynamic range",
      "spec-trogram plots"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <metric> <task> <task> <material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "window function -- USED-FOR -- short-time fourier transform phase spectrum",
      "window function -- USED-FOR -- stft phase spectra",
      "stft analysis-modification-synthesis framework -- USED-FOR -- noise reduction",
      "dynamic range -- FEATURE-OF -- window function",
      "window function -- USED-FOR -- noise reduction",
      "fourier domain -- FEATURE-OF -- speech enhancement algorithms"
    ],
    "abstract": "typical <method_2> that operate in the <material_7> only modify the <method_9> . it is commonly understood that the <method_8> is perceptually unimportant , and thus , <method_8> is passed directly to the output . in recent intelligibility experiments , <method_8> has been reported that the <method_0> can provide significant intelligibility when estimated using a <otherscientificterm_11> lower in <otherscientificterm_12> than the typical <otherscientificterm_10> . motivated by this , we investigate the role of the <otherscientificterm_11> for <method_0> in relation to <task_6> . using a modified <method_1> , we show that <task_5> can be achieved by modifying the <otherscientificterm_11> used to estimate the <otherscientificterm_3> . we demonstrate this through <material_13> and results from two objective <metric_4> .",
    "abstract_og": "typical speech enhancement algorithms that operate in the fourier domain only modify the magnitude component . it is commonly understood that the phase component is perceptually unimportant , and thus , phase component is passed directly to the output . in recent intelligibility experiments , phase component has been reported that the short-time fourier transform phase spectrum can provide significant intelligibility when estimated using a window function lower in dynamic range than the typical ham-ming window . motivated by this , we investigate the role of the window function for short-time fourier transform phase spectrum in relation to speech enhancement . using a modified stft analysis-modification-synthesis framework , we show that noise reduction can be achieved by modifying the window function used to estimate the stft phase spectra . we demonstrate this through spec-trogram plots and results from two objective speech quality measures ."
  },
  {
    "title": "Direct construction of compact context-dependency transducers from data .",
    "entities": [
      "phonetic decision tree growing",
      "finite-state transducer-based asr decoders",
      "compact context-dependency transducers",
      "context sizes",
      "context-dependency transducer",
      "fst compilation",
      "regularization term",
      "recognition accuracy",
      "phonetic context",
      "n-phone orders",
      "phonetic features",
      "transducer construction",
      "features"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "recognition accuracy -- EVALUATE-FOR -- context-dependency transducer",
      "context sizes -- CONJUNCTION -- features",
      "regularization term -- USED-FOR -- compact context-dependency transducers",
      "phonetic context -- PART-OF -- transducer construction",
      "compact context-dependency transducers -- USED-FOR -- finite-state transducer-based asr decoders"
    ],
    "abstract": "this paper describes a new method for building <method_2> for <task_1> . instead of the conventional <method_0> followed by <method_5> , this approach incorporates the <otherscientificterm_8> splitting directly into the <task_11> . the objective function of the <method_2> is augmented with a <otherscientificterm_6> that measures the number of transducer states introduced by a split . we give results on a large spoken-query task for various <otherscientificterm_9> and other <otherscientificterm_10> that show this method can greatly reduce the size of the resulting <method_4> with no significant impact on <metric_7> . this permits using <otherscientificterm_3> and <otherscientificterm_12> that might otherwise be unmanageable .",
    "abstract_og": "this paper describes a new method for building compact context-dependency transducers for finite-state transducer-based asr decoders . instead of the conventional phonetic decision tree growing followed by fst compilation , this approach incorporates the phonetic context splitting directly into the transducer construction . the objective function of the compact context-dependency transducers is augmented with a regularization term that measures the number of transducer states introduced by a split . we give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy . this permits using context sizes and features that might otherwise be unmanageable ."
  },
  {
    "title": "PLIS : a Probabilistic Lexical Inference System .",
    "entities": [
      "open source probabilistic lexical inference system",
      "lexical inference processes",
      "online interactive viewer",
      "text processing applications",
      "lexical inference knowledge",
      "textual inferences",
      "lexical knowledge",
      "integrated knowledge",
      "system customiza-tion",
      "diverse resources",
      "plis"
    ],
    "types": "<method> <task> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <method>",
    "relations": [
      "plis -- USED-FOR -- text processing applications",
      "plis -- HYPONYM-OF -- open source probabilistic lexical inference system",
      "plis -- USED-FOR -- lexical inference processes",
      "online interactive viewer -- PART-OF -- plis"
    ],
    "abstract": "this paper presents <method_10> , an <method_0> which combines two functionalities : -lrb- i -rrb- a tool for integrating <otherscientificterm_4> from <material_9> , and -lrb- ii -rrb- a framework for scoring <otherscientificterm_5> based on the <otherscientificterm_7> . we provide <method_10> with two probabilistic implementation of this framework . <method_10> is available for download and developers of <task_3> can use <method_10> as an off-the-shelf component for injecting <otherscientificterm_6> into their applications . <method_10> is easily configurable , components can be extended or replaced with user generated ones to enable <method_8> and further research . <method_10> includes an <method_2> , which is a powerful tool for investigating <task_1> .",
    "abstract_og": "this paper presents plis , an open source probabilistic lexical inference system which combines two functionalities : -lrb- i -rrb- a tool for integrating lexical inference knowledge from diverse resources , and -lrb- ii -rrb- a framework for scoring textual inferences based on the integrated knowledge . we provide plis with two probabilistic implementation of this framework . plis is available for download and developers of text processing applications can use plis as an off-the-shelf component for injecting lexical knowledge into their applications . plis is easily configurable , components can be extended or replaced with user generated ones to enable system customiza-tion and further research . plis includes an online interactive viewer , which is a powerful tool for investigating lexical inference processes ."
  },
  {
    "title": "Optimal Incremental Parsing via Best-First Dynamic Programming .",
    "entities": [
      "polynomial time dynamic programming algorithm",
      "probablistic best-first shift-reduce parser",
      "best-first shift-reduce parsing",
      "best-first parser",
      "complexity",
      "search"
    ],
    "types": "<method> <method> <task> <method> <metric> <method>",
    "relations": [
      "polynomial time dynamic programming algorithm -- USED-FOR -- best-first shift-reduce parsing",
      "probablistic best-first shift-reduce parser -- EVALUATE-FOR -- polynomial time dynamic programming algorithm"
    ],
    "abstract": "we present the first provably optimal <method_0> for <task_2> , which applies the <method_0> idea of huang and sagae -lrb- 2010 -rrb- to the <method_3> of sagae and lavie -lrb- 2006 -rrb- in a non-trivial way , reducing the <metric_4> of the latter from exponential to polynomial . we prove the correctness of our <method_0> rigorously . experiments confirm that <method_0> leads to a significant speedup on a <method_1> , and makes exact <method_5> under such a model tractable for the first time .",
    "abstract_og": "we present the first provably optimal polynomial time dynamic programming algorithm for best-first shift-reduce parsing , which applies the polynomial time dynamic programming algorithm idea of huang and sagae -lrb- 2010 -rrb- to the best-first parser of sagae and lavie -lrb- 2006 -rrb- in a non-trivial way , reducing the complexity of the latter from exponential to polynomial . we prove the correctness of our polynomial time dynamic programming algorithm rigorously . experiments confirm that polynomial time dynamic programming algorithm leads to a significant speedup on a probablistic best-first shift-reduce parser , and makes exact search under such a model tractable for the first time ."
  },
  {
    "title": "Building Reconstruction from Optical and Range Images .",
    "entities": [
      "3d nature of the buildings",
      "reconstructions of diier-ent buildings classes",
      "top-down robust surface ttting",
      "diierent range sensor types",
      "registered range image",
      "attentional focus stage",
      "monocular optical image",
      "noisy range data",
      "range data",
      "focus-of-attention area",
      "optical image",
      "model indexing"
    ],
    "types": "<otherscientificterm> <task> <task> <otherscientificterm> <material> <otherscientificterm> <material> <material> <material> <otherscientificterm> <material> <method>",
    "relations": [
      "monocular optical image -- USED-FOR -- focus-of-attention area",
      "top-down robust surface ttting -- USED-FOR -- 3d nature of the buildings",
      "model indexing -- USED-FOR -- attentional focus stage"
    ],
    "abstract": "a technique is introduced for extracting and reconstructing a wide class of building types from a <material_4> and <material_10> . an <otherscientificterm_5> , followed by <method_11> , allows <task_2> to reconstruct the <otherscientificterm_0> in the data . because of the effectiveness of <method_11> , top-down processing of <material_7> still succeeds and the algorithm is capable of detecting and reconstructing several diierent building roof classes , including at single level , at multi-leveled , peaked , and curved rooftops . the algorithm is applicable to <material_8> that may have been collected from several <otherscientificterm_3> . we demonstrate <task_1> in the presence of large amounts of noise . our results underline the usefuless of <material_8> when processed in the context of a <otherscientificterm_9> derived from the <material_6> .",
    "abstract_og": "a technique is introduced for extracting and reconstructing a wide class of building types from a registered range image and optical image . an attentional focus stage , followed by model indexing , allows top-down robust surface ttting to reconstruct the 3d nature of the buildings in the data . because of the effectiveness of model indexing , top-down processing of noisy range data still succeeds and the algorithm is capable of detecting and reconstructing several diierent building roof classes , including at single level , at multi-leveled , peaked , and curved rooftops . the algorithm is applicable to range data that may have been collected from several diierent range sensor types . we demonstrate reconstructions of diier-ent buildings classes in the presence of large amounts of noise . our results underline the usefuless of range data when processed in the context of a focus-of-attention area derived from the monocular optical image ."
  },
  {
    "title": "Readability Indices for Automatic Evaluation of Text Simplification Systems : A Feasibility Study for Spanish .",
    "entities": [
      "automatic evaluation of text simplification systems",
      "corpus of original news texts",
      "measuring syntactic complexity",
      "linguistically motivated features",
      "already-existing readability for-mulae",
      "cognitive disabilities",
      "readability indices",
      "read-ability indices",
      "lexical complexity",
      "manual simplifications",
      "spanish"
    ],
    "types": "<task> <material> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <material>",
    "relations": [
      "lexical complexity -- FEATURE-OF -- read-ability indices",
      "cognitive disabilities -- FEATURE-OF -- manual simplifications"
    ],
    "abstract": "this paper addresses the problem of <task_0> for <material_10> . we test whether <otherscientificterm_4> would be suitable for this task . we adapt three existing <method_6> -lrb- two measuring <metric_8> and one <metric_2> -rrb- to be computed automatically , which are then applied to a <material_1> and their <method_9> aimed at people with <otherscientificterm_5> . we show that there is a significant correlation between each of the three <method_6> and several <otherscientificterm_3> which might be seen as reading obstacles for various target populations . furthermore , we show that there is a significant correlation between the two <otherscientificterm_7> which measure <metric_8> .",
    "abstract_og": "this paper addresses the problem of automatic evaluation of text simplification systems for spanish . we test whether already-existing readability for-mulae would be suitable for this task . we adapt three existing readability indices -lrb- two measuring lexical complexity and one measuring syntactic complexity -rrb- to be computed automatically , which are then applied to a corpus of original news texts and their manual simplifications aimed at people with cognitive disabilities . we show that there is a significant correlation between each of the three readability indices and several linguistically motivated features which might be seen as reading obstacles for various target populations . furthermore , we show that there is a significant correlation between the two read-ability indices which measure lexical complexity ."
  },
  {
    "title": "3D Reconstruction of Dynamic Scenes with Multiple Handheld Cameras .",
    "entities": [
      "accurate dense 3d reconstruction of dynamic scenes",
      "separation of static and dynamic points",
      "segmentation of static and dynamic points",
      "unified energy minimization framework",
      "dense depth estimation method",
      "data capturing setup",
      "synchronized video sequences",
      "fixed camera arrays",
      "fixed cameras",
      "natural images",
      "known background",
      "handheld cameras",
      "bilayer segmentation",
      "spatio-temporal constraints",
      "depth optimization"
    ],
    "types": "<task> <otherscientificterm> <task> <method> <method> <method> <material> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "depth optimization -- CONJUNCTION -- segmentation of static and dynamic points",
      "spatio-temporal constraints -- FEATURE-OF -- unified energy minimization framework",
      "dense depth estimation method -- USED-FOR -- unified energy minimization framework",
      "synchronized video sequences -- USED-FOR -- dense depth estimation method"
    ],
    "abstract": "accurate dense 3d reconstruction of dynamic scenes from <material_9> is still very challenging . most previous methods rely on a large number of <otherscientificterm_8> to obtain good results . some of these methods further require <otherscientificterm_1> , which are usually restricted to scenes with <otherscientificterm_10> . we propose a novel <method_4> which can automatically recover accurate and consistent depth maps from the <material_6> taken by a few <otherscientificterm_11> . unlike <otherscientificterm_7> , our <method_5> is much more flexible and easier to use . our <method_4> simultaneously solves <task_12> and depth estimation in a <method_3> , which combines different <otherscientificterm_13> for effective <task_14> and <task_2> . a variety of examples demonstrate the effectiveness of the proposed <method_4> .",
    "abstract_og": "accurate dense 3d reconstruction of dynamic scenes from natural images is still very challenging . most previous methods rely on a large number of fixed cameras to obtain good results . some of these methods further require separation of static and dynamic points , which are usually restricted to scenes with known background . we propose a novel dense depth estimation method which can automatically recover accurate and consistent depth maps from the synchronized video sequences taken by a few handheld cameras . unlike fixed camera arrays , our data capturing setup is much more flexible and easier to use . our dense depth estimation method simultaneously solves bilayer segmentation and depth estimation in a unified energy minimization framework , which combines different spatio-temporal constraints for effective depth optimization and segmentation of static and dynamic points . a variety of examples demonstrate the effectiveness of the proposed dense depth estimation method ."
  },
  {
    "title": "Effects of frequency shifts on perceived naturalness and gender information in speech .",
    "entities": [
      "geometric mean formant frequencies",
      "low formant frequencies",
      "natural voices",
      "gender ambiguity",
      "perceived naturalness",
      "frequency-shifted versions",
      "scale factors",
      "natural speech",
      "frequency-shifted sentences",
      "moderate correlation",
      "frequency-shifted sentences",
      "perceived gender",
      "formant frequencies"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "scale factors -- USED-FOR -- frequency-shifted versions"
    ],
    "abstract": "in <material_7> , there is a <otherscientificterm_9> between the fundamental frequency and <otherscientificterm_12> across talkers . the present study used a high-quality vocoder to manipulate these properties and determine their contribution to <otherscientificterm_4> and voice gender . the stimuli were re-synthesized sentences spoken by two adult males and two adult females . <otherscientificterm_6> were chosen for each sentence and for each talker to produce <method_5> with a specified mean fundamental frequency -lrb- f 0 -rrb- ranging from 60 hz to 450 hz in 10 steps , paired with 10 steps in <otherscientificterm_0> ranging from 850 hz to 2500 hz . listeners judged <material_8> as more natural when f 0 and <otherscientificterm_12> followed the co-variation of f 0 and <otherscientificterm_12> in <material_2> . sentences with low f 0 s and <otherscientificterm_1> were perceived as masculine , while sentences with high f 0 and high <otherscientificterm_12> were assigned high ratings of femininity . sentences with '' mismatched '' f 0 and <otherscientificterm_12> were assigned ratings near the midpoint of the range , indicating <otherscientificterm_3> . <material_10> derived from male talkers received consistently higher ratings of masculinity than those derived from females , while sentences from female talkers received higher ratings of femininity , even when assigned scale factors appropriate for the opposite gender , indicating that factors other than f 0 and mean <otherscientificterm_12> contribute to <otherscientificterm_11> .",
    "abstract_og": "in natural speech , there is a moderate correlation between the fundamental frequency and formant frequencies across talkers . the present study used a high-quality vocoder to manipulate these properties and determine their contribution to perceived naturalness and voice gender . the stimuli were re-synthesized sentences spoken by two adult males and two adult females . scale factors were chosen for each sentence and for each talker to produce frequency-shifted versions with a specified mean fundamental frequency -lrb- f 0 -rrb- ranging from 60 hz to 450 hz in 10 steps , paired with 10 steps in geometric mean formant frequencies ranging from 850 hz to 2500 hz . listeners judged frequency-shifted sentences as more natural when f 0 and formant frequencies followed the co-variation of f 0 and formant frequencies in natural voices . sentences with low f 0 s and low formant frequencies were perceived as masculine , while sentences with high f 0 and high formant frequencies were assigned high ratings of femininity . sentences with '' mismatched '' f 0 and formant frequencies were assigned ratings near the midpoint of the range , indicating gender ambiguity . frequency-shifted sentences derived from male talkers received consistently higher ratings of masculinity than those derived from females , while sentences from female talkers received higher ratings of femininity , even when assigned scale factors appropriate for the opposite gender , indicating that factors other than f 0 and mean formant frequencies contribute to perceived gender ."
  },
  {
    "title": "Spherical Random Features for Polynomial Kernels .",
    "entities": [
      "spherical random fourier features",
      "compact explicit feature maps",
      "low approximation error",
      "random fourier features",
      "non-linear classification",
      "classification accuracy",
      "kernel methods",
      "unit sphere",
      "polynomial kernels",
      "higher-order polynomials",
      "affirmative answer",
      "large-scale learning",
      "approximation paradigm",
      "kernel approximation"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <task> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "compact explicit feature maps -- USED-FOR -- large-scale learning",
      "compact explicit feature maps -- USED-FOR -- kernel methods",
      "kernel approximation -- USED-FOR -- higher-order polynomials",
      "kernel methods -- USED-FOR -- large-scale learning",
      "random fourier features -- USED-FOR -- affirmative answer",
      "polynomial kernels -- USED-FOR -- unit sphere"
    ],
    "abstract": "compact explicit feature <method_1> provide a practical framework to scale <method_6> to <task_11> , but deriving such <method_1> for many types of kernels remains a challenging open problem . among the commonly used kernels for <task_4> are <otherscientificterm_8> , for which <otherscientificterm_2> has thus far necessitated explicit feature <method_1> of large dimensionality , especially for <otherscientificterm_9> . meanwhile , because <otherscientificterm_8> are unbounded , <otherscientificterm_8> are frequently applied to data that has been normalized to unit 2 norm . the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ? we show that a putative <otherscientificterm_10> to this question based on <material_3> is impossible in this setting , and introduce a new <method_12> , <method_0> , which circumvents these issues and delivers a compact approximation to <otherscientificterm_8> for data on the <otherscientificterm_7> . compared to prior work , <method_0> are less rank-deficient , more compact , and achieve better <method_13> , especially for <otherscientificterm_9> . the resulting predictions have lower variance and typically yield better <metric_5> .",
    "abstract_og": "compact explicit feature compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such compact explicit feature maps for many types of kernels remains a challenging open problem . among the commonly used kernels for non-linear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature compact explicit feature maps of large dimensionality , especially for higher-order polynomials . meanwhile , because polynomial kernels are unbounded , polynomial kernels are frequently applied to data that has been normalized to unit 2 norm . the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ? we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere . compared to prior work , spherical random fourier features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials . the resulting predictions have lower variance and typically yield better classification accuracy ."
  },
  {
    "title": "Trailer Generation via a Point Process-Based Visual Attractiveness Model .",
    "entities": [
      "self-correcting point process-based attractiveness model",
      "graph-based trailer generation algorithm",
      "automatic trailer generation",
      "max-attractiveness trailer",
      "training trailers",
      "attractiveness model",
      "trailer generation",
      "video summarization",
      "trailer generators",
      "quality"
    ],
    "types": "<method> <method> <task> <method> <method> <method> <task> <method> <method> <metric>",
    "relations": [
      "attractiveness model -- USED-FOR -- graph-based trailer generation algorithm",
      "graph-based trailer generation algorithm -- USED-FOR -- max-attractiveness trailer",
      "graph-based trailer generation algorithm -- COMPARE -- trailer generators"
    ],
    "abstract": "producing attractive trailers for videos needs human expertise and creativity , and hence is challenging and costly . different from <method_7> that focuses on capturing storylines or important scenes , <task_6> aims at producing trailers that are attractive so that viewers will be eager to watch the original video . in this work , we study the problem of <task_2> , in which an attractive trailer is produced given a video and a piece of music . we propose a sur-rogate measure of video attractiveness named fix-ation variance , and learn a novel <method_0> that can effectively describe the dynamics of attractiveness of a video . furthermore , based on the <method_5> learned from existing <method_4> , we propose an efficient <method_1> to produce a <method_3> . experiments demonstrate that our <method_1> outper-forms the state-of-the-art <method_8> in terms of both <metric_9> and efficiency .",
    "abstract_og": "producing attractive trailers for videos needs human expertise and creativity , and hence is challenging and costly . different from video summarization that focuses on capturing storylines or important scenes , trailer generation aims at producing trailers that are attractive so that viewers will be eager to watch the original video . in this work , we study the problem of automatic trailer generation , in which an attractive trailer is produced given a video and a piece of music . we propose a sur-rogate measure of video attractiveness named fix-ation variance , and learn a novel self-correcting point process-based attractiveness model that can effectively describe the dynamics of attractiveness of a video . furthermore , based on the attractiveness model learned from existing training trailers , we propose an efficient graph-based trailer generation algorithm to produce a max-attractiveness trailer . experiments demonstrate that our graph-based trailer generation algorithm outper-forms the state-of-the-art trailer generators in terms of both quality and efficiency ."
  },
  {
    "title": "Improving Users ' Demographic Prediction via the Videos They Talk about .",
    "entities": [
      "direct and indirect relationships",
      "demographic predictive ability",
      "actor names",
      "video names",
      "video keywords",
      "bayesian method",
      "video websites",
      "indirect relationship",
      "real-world dataset"
    ],
    "types": "<otherscientificterm> <metric> <otherscientificterm> <material> <otherscientificterm> <method> <material> <otherscientificterm> <material>",
    "relations": [
      "video names -- CONJUNCTION -- actor names",
      "actor names -- CONJUNCTION -- video keywords"
    ],
    "abstract": "in this paper , we improve microblog users ' demographic prediction via the videos they talk about . first , we collect the describing words of currently popular videos , including <material_3> , <otherscientificterm_2> and <otherscientificterm_4> , from <material_6> . next , we search these describing words in users ' microblogs , and build the direct relationships between users and the appeared words . after that , we propose a <method_5> to calculate the probability of connections between users and video describing words . if the probability is beyond a threshold , an <otherscientificterm_7> is founded . last , two models are built to predict users ' demographics with the obtained <otherscientificterm_0> . based on a large <material_8> , experiment results show that our <method_5> can significantly improve these words ' <metric_1> by more than 15 % on average .",
    "abstract_og": "in this paper , we improve microblog users ' demographic prediction via the videos they talk about . first , we collect the describing words of currently popular videos , including video names , actor names and video keywords , from video websites . next , we search these describing words in users ' microblogs , and build the direct relationships between users and the appeared words . after that , we propose a bayesian method to calculate the probability of connections between users and video describing words . if the probability is beyond a threshold , an indirect relationship is founded . last , two models are built to predict users ' demographics with the obtained direct and indirect relationships . based on a large real-world dataset , experiment results show that our bayesian method can significantly improve these words ' demographic predictive ability by more than 15 % on average ."
  },
  {
    "title": "Domain adaptation with clustered language models .",
    "entities": [
      "` fillup ' method",
      "modied optimi-sation criterion",
      "ord error rate",
      "clustered language models",
      "n-gram models",
      "adaptation data",
      "domain adaptation",
      "clustering algorithm"
    ],
    "types": "<method> <otherscientificterm> <metric> <method> <method> <material> <method> <method>",
    "relations": [
      "modied optimi-sation criterion -- USED-FOR -- clustering algorithm",
      "` fillup ' method -- USED-FOR -- n-gram models",
      "domain adaptation -- USED-FOR -- clustered language models"
    ],
    "abstract": "in this paper , a method of <method_6> for <method_3> is developed . it is based on a previously developed <method_7> , but with a <otherscientificterm_1> . the results are shown to be slightly superior to the previously published <method_0> , which can be used to adapt standard <method_4> . however , the improvement both methods give compared to models built from scratch on the <material_5> is quite small -lrb- less than 11 % relative improvement i n w <metric_2> -rrb- . this suggests that both methods are still unsatisfactory from a practical point of view .",
    "abstract_og": "in this paper , a method of domain adaptation for clustered language models is developed . it is based on a previously developed clustering algorithm , but with a modied optimi-sation criterion . the results are shown to be slightly superior to the previously published ` fillup ' method , which can be used to adapt standard n-gram models . however , the improvement both methods give compared to models built from scratch on the adaptation data is quite small -lrb- less than 11 % relative improvement i n w ord error rate -rrb- . this suggests that both methods are still unsatisfactory from a practical point of view ."
  },
  {
    "title": "Relaxed matching kernels for robust image comparison .",
    "entities": [
      "baseline bag-of-features algorithm",
      "information-compressed features",
      "object recognition",
      "bag-of-features representation",
      "features",
      "rmks"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <otherscientificterm> <method>",
    "relations": [
      "bag-of-features representation -- USED-FOR -- object recognition"
    ],
    "abstract": "the popular <method_3> for <task_2> collects signatures of local image patches and discards spatial information . some have recently attempted to at least partially overcome this limitation , for instance by '' spatial pyramids '' and '' proximity '' kernels . we introduce the general formalism of '' relaxed matching kernels '' -lrb- <method_5> -rrb- that includes such approaches as special cases , allow us to derive useful general properties of these kernels , and to introduce new ones . as an example , we introduce a kernel based on matching graphs of <otherscientificterm_4> and one based on matching <otherscientificterm_1> . we show that all <method_5> are competitive and outperform in several cases recently published state-of-the-art results on standard datasets . however , we also show that a proper implementation of a <method_0> can be extremely competitive , and outperform the other methods in some cases .",
    "abstract_og": "the popular bag-of-features representation for object recognition collects signatures of local image patches and discards spatial information . some have recently attempted to at least partially overcome this limitation , for instance by '' spatial pyramids '' and '' proximity '' kernels . we introduce the general formalism of '' relaxed matching kernels '' -lrb- rmks -rrb- that includes such approaches as special cases , allow us to derive useful general properties of these kernels , and to introduce new ones . as an example , we introduce a kernel based on matching graphs of features and one based on matching information-compressed features . we show that all rmks are competitive and outperform in several cases recently published state-of-the-art results on standard datasets . however , we also show that a proper implementation of a baseline bag-of-features algorithm can be extremely competitive , and outperform the other methods in some cases ."
  },
  {
    "title": "Solving Generalized Semi-Markov Decision Processes Using Continuous Phase-Type Distributions .",
    "entities": [
      "generalized semi-markov decision process",
      "smdp solution techniques",
      "stochastic decision processes",
      "phase-type distributions",
      "discrete-time mdp",
      "mdp techniques",
      "continuous-time mdps",
      "arbitrary gsmdp",
      "asynchronous events",
      "uniformization"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "phase-type distributions -- CONJUNCTION -- uniformization",
      "mdp techniques -- USED-FOR -- discrete-time mdp",
      "mdp techniques -- USED-FOR -- generalized semi-markov decision process",
      "discrete-time mdp -- USED-FOR -- generalized semi-markov decision process"
    ],
    "abstract": "we introduce the <method_0> as an extension of <method_6> and semi-markov decision processes -lrb- <method_0> -rrb- for modeling <method_2> with <otherscientificterm_8> and actions . using <otherscientificterm_3> and <method_9> , we show how an <method_7> can be approximated by a <method_4> , which can then be solved using existing <method_5> . the techniques we present can also be seen as an alternative approach for solving <method_0> , and we demonstrate that the introduction of phases allows us to generate higher quality policies than those obtained by standard <method_1> .",
    "abstract_og": "we introduce the generalized semi-markov decision process as an extension of continuous-time mdps and semi-markov decision processes -lrb- generalized semi-markov decision process -rrb- for modeling stochastic decision processes with asynchronous events and actions . using phase-type distributions and uniformization , we show how an arbitrary gsmdp can be approximated by a discrete-time mdp , which can then be solved using existing mdp techniques . the techniques we present can also be seen as an alternative approach for solving generalized semi-markov decision process , and we demonstrate that the introduction of phases allows us to generate higher quality policies than those obtained by standard smdp solution techniques ."
  },
  {
    "title": "On tuning of self-quotient \u03b5-filter and support vector machine and its application to noise robust human detection .",
    "entities": [
      "histograms of oriented gradients",
      "support vector machine",
      "self-quotient \u03b5-filter",
      "noise robust human detection",
      "noise corrupted images",
      "manual parameter setting",
      "local intensity gradients",
      "human detection",
      "self-quotient \u03b5-filter",
      "tuning algorithm",
      "noise",
      "images"
    ],
    "types": "<method> <method> <method> <task> <material> <material> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <material>",
    "relations": [
      "histograms of oriented gradients -- USED-FOR -- human detection",
      "self-quotient \u03b5-filter -- CONJUNCTION -- histograms of oriented gradients",
      "histograms of oriented gradients -- CONJUNCTION -- self-quotient \u03b5-filter",
      "self-quotient \u03b5-filter -- CONJUNCTION -- support vector machine",
      "self-quotient \u03b5-filter -- USED-FOR -- human detection",
      "self-quotient \u03b5-filter -- USED-FOR -- self-quotient \u03b5-filter",
      "human detection -- USED-FOR -- noise robust human detection",
      "images -- USED-FOR -- self-quotient \u03b5-filter",
      "self-quotient \u03b5-filter -- CONJUNCTION -- self-quotient \u03b5-filter",
      "manual parameter setting -- USED-FOR -- self-quotient \u03b5-filter",
      "self-quotient \u03b5-filter -- USED-FOR -- noise robust human detection",
      "histograms of oriented gradients -- USED-FOR -- noise robust human detection",
      "tuning algorithm -- USED-FOR -- noise robust human detection"
    ],
    "abstract": "this paper introduces a <method_9> of <method_2> and <method_1> , and its application to <task_3> combining <method_2> , <method_0> , and <method_2> . although <task_7> combining <method_0> and <method_2> is a powerful approach , as it uses <otherscientificterm_6> , it is difficult to handle <material_4> . on the other hand , although <task_7> combining <method_2> , <method_0> and <method_2> can realize <task_3> , <method_2> requires <material_5> . our aim is not only to set the parameter of <otherscientificterm_8> but also to train <method_2> by using numerous <material_11> without <otherscientificterm_10> and a small amount of <material_11> with <otherscientificterm_10> .",
    "abstract_og": "this paper introduces a tuning algorithm of self-quotient \u03b5-filter and support vector machine , and its application to noise robust human detection combining self-quotient \u03b5-filter , histograms of oriented gradients , and self-quotient \u03b5-filter . although human detection combining histograms of oriented gradients and self-quotient \u03b5-filter is a powerful approach , as it uses local intensity gradients , it is difficult to handle noise corrupted images . on the other hand , although human detection combining self-quotient \u03b5-filter , histograms of oriented gradients and self-quotient \u03b5-filter can realize noise robust human detection , self-quotient \u03b5-filter requires manual parameter setting . our aim is not only to set the parameter of self-quotient \u03b5-filter but also to train self-quotient \u03b5-filter by using numerous images without noise and a small amount of images with noise ."
  },
  {
    "title": "Context-Based Pedestrian Path Prediction .",
    "entities": [
      "switching linear dynamical system",
      "spatial layout of the environment",
      "intelligent vehicle domain",
      "stereo vision data",
      "pedestrian head orientation",
      "dynamic bayesian network",
      "pedestrian path prediction",
      "pedestrian situational awareness",
      "computer vision",
      "crossing pedestrian",
      "situational awareness",
      "pedestrian dynamics",
      "path prediction",
      "spatial layout",
      "latent states",
      "situation criticality",
      "slds",
      "curbside"
    ],
    "types": "<method> <otherscientificterm> <task> <material> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "intelligent vehicle domain -- FEATURE-OF -- pedestrian path prediction",
      "pedestrian situational awareness -- CONJUNCTION -- situation criticality",
      "pedestrian situational awareness -- PART-OF -- dynamic bayesian network",
      "situation criticality -- CONJUNCTION -- spatial layout of the environment",
      "situation criticality -- PART-OF -- dynamic bayesian network",
      "dynamic bayesian network -- USED-FOR -- pedestrian path prediction"
    ],
    "abstract": "we present a novel <method_5> for <task_6> in the <task_2> . the <method_5> incorporates the <otherscientificterm_7> , <otherscientificterm_15> and <otherscientificterm_1> as <otherscientificterm_14> on top of a <method_0> to anticipate changes in the <otherscientificterm_11> . using <method_8> , <otherscientificterm_10> is assessed by the <otherscientificterm_4> , <otherscientificterm_15> by the distance between vehicle and pedestrian at the expected point of closest <method_5> , and <otherscientificterm_13> by the distance of the pedestrian to the <otherscientificterm_17> . our particular scenario is that of a <otherscientificterm_9> , who might stop or continue walking at the curb . in experiments using <material_3> obtained from a vehicle , we demonstrate that the proposed <method_5> results in more accurate <task_12> than only <method_16> , at the relevant short time horizon -lrb- 1 s -rrb- , and slightly outperforms a computationally more demanding state-of-the-art method .",
    "abstract_og": "we present a novel dynamic bayesian network for pedestrian path prediction in the intelligent vehicle domain . the dynamic bayesian network incorporates the pedestrian situational awareness , situation criticality and spatial layout of the environment as latent states on top of a switching linear dynamical system to anticipate changes in the pedestrian dynamics . using computer vision , situational awareness is assessed by the pedestrian head orientation , situation criticality by the distance between vehicle and pedestrian at the expected point of closest dynamic bayesian network , and spatial layout by the distance of the pedestrian to the curbside . our particular scenario is that of a crossing pedestrian , who might stop or continue walking at the curb . in experiments using stereo vision data obtained from a vehicle , we demonstrate that the proposed dynamic bayesian network results in more accurate path prediction than only slds , at the relevant short time horizon -lrb- 1 s -rrb- , and slightly outperforms a computationally more demanding state-of-the-art method ."
  },
  {
    "title": "On analyzing video with very small motions .",
    "entities": [
      "scene-specific non-parametric motion basis",
      "linear appearance variations",
      "dense flow estimates",
      "long-term point tracking",
      "appearance variations",
      "small motions",
      "pca decomposition",
      "motion segmentation",
      "scene motions",
      "scenes"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <material>",
    "relations": [
      "motion segmentation -- CONJUNCTION -- long-term point tracking",
      "small motions -- FEATURE-OF -- scenes"
    ],
    "abstract": "we characterize a class of videos consisting of very small but potentially complicated motions . we find that in these <material_9> , <otherscientificterm_1> have a direct relationship to <otherscientificterm_8> . we show how to interpret <otherscientificterm_4> captured through a <method_6> of the image set as a <otherscientificterm_0> . we propose fast , robust tools for <task_2> that are effective in <material_9> with <otherscientificterm_5> and potentially large image noise . we show example results in a variety of applications , including <task_7> and <task_3> .",
    "abstract_og": "we characterize a class of videos consisting of very small but potentially complicated motions . we find that in these scenes , linear appearance variations have a direct relationship to scene motions . we show how to interpret appearance variations captured through a pca decomposition of the image set as a scene-specific non-parametric motion basis . we propose fast , robust tools for dense flow estimates that are effective in scenes with small motions and potentially large image noise . we show example results in a variety of applications , including motion segmentation and long-term point tracking ."
  },
  {
    "title": "Symmetric Sub-Pixel Stereo Matching .",
    "entities": [
      "continuous disparity space image",
      "textureless and occluded areas",
      "reconstructed image signals",
      "minimal smoothness assumptions",
      "global smoothness assumptions",
      "symmetric matching process",
      "stereo algorithm design",
      "integer disparities",
      "sub-pixel information",
      "stereo images",
      "matching criterion",
      "visibility constraints",
      "stereo algorithm",
      "matching"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <method> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "matching criterion -- PART-OF -- stereo algorithm design",
      "visibility constraints -- USED-FOR -- symmetric matching process"
    ],
    "abstract": "two central issues in <task_6> are the <otherscientificterm_10> and the underlying smoothness assumptions . in this paper we propose a new <method_12> with novel approaches to both issues . we start with a careful analysis of the properties of the <otherscientificterm_0> , and derive a new <task_13> cost based on the <material_2> . we then use a <method_5> that employs <otherscientificterm_11> to assign disparities to a large fraction of pixels with <otherscientificterm_3> . while the <task_13> operates on <otherscientificterm_7> , <otherscientificterm_8> is maintained throughout the process . <task_4> are delayed until a later stage in which disparities are assigned in <otherscientificterm_1> . we validate our <method_12> with experimental results on <material_9> with ground truth .",
    "abstract_og": "two central issues in stereo algorithm design are the matching criterion and the underlying smoothness assumptions . in this paper we propose a new stereo algorithm with novel approaches to both issues . we start with a careful analysis of the properties of the continuous disparity space image , and derive a new matching cost based on the reconstructed image signals . we then use a symmetric matching process that employs visibility constraints to assign disparities to a large fraction of pixels with minimal smoothness assumptions . while the matching operates on integer disparities , sub-pixel information is maintained throughout the process . global smoothness assumptions are delayed until a later stage in which disparities are assigned in textureless and occluded areas . we validate our stereo algorithm with experimental results on stereo images with ground truth ."
  },
  {
    "title": "Reduced Complexity and Scaling for Asynchronous HMMS in a Bimodal Input Fusion Application .",
    "entities": [
      "bimodal speech and gesture user input fusion task",
      "asynchronous hidden markov model",
      "absolute recognition performance",
      "audiovisual recognition tasks",
      "scaling procedure",
      "fusion hmm",
      "ahmm characteristics",
      "decoding complexity",
      "numerical values",
      "complexity"
    ],
    "types": "<task> <method> <metric> <task> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <metric>",
    "relations": [
      "asynchronous hidden markov model -- USED-FOR -- audiovisual recognition tasks",
      "scaling procedure -- USED-FOR -- numerical values"
    ],
    "abstract": "the <method_1> can <method_1> the joint likelihood of two observation sequences , even if the streams are not synchronised . previously this <method_1> has been applied to <task_3> . the main drawback of the concept is its rather high training and <metric_7> . in this work we show how the <metric_9> can be reduced significantly with advanced running indices for the calculations . yet , the <otherscientificterm_6> and its advantages are preserved . the improvement also allows a <method_4> to keep <otherscientificterm_8> in a reasonable range . in an experimental section we compare the <metric_9> of the original and the improved concept and validate the theoretical results . then the <method_1> is tested on a <task_0> : compared to a late <method_5> an improvement of more than 10 % <metric_2> has been achieved .",
    "abstract_og": "the asynchronous hidden markov model can asynchronous hidden markov model the joint likelihood of two observation sequences , even if the streams are not synchronised . previously this asynchronous hidden markov model has been applied to audiovisual recognition tasks . the main drawback of the concept is its rather high training and decoding complexity . in this work we show how the complexity can be reduced significantly with advanced running indices for the calculations . yet , the ahmm characteristics and its advantages are preserved . the improvement also allows a scaling procedure to keep numerical values in a reasonable range . in an experimental section we compare the complexity of the original and the improved concept and validate the theoretical results . then the asynchronous hidden markov model is tested on a bimodal speech and gesture user input fusion task : compared to a late fusion hmm an improvement of more than 10 % absolute recognition performance has been achieved ."
  },
  {
    "title": "Training audio events detectors with a sound effects corpus .",
    "entities": [
      "detection of non-voice sounds",
      "one-against-all svm classi-fiers",
      "audio event detection",
      "audio events",
      "birds",
      "traffic",
      "machines"
    ],
    "types": "<task> <method> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "birds -- HYPONYM-OF -- detection of non-voice sounds",
      "birds -- CONJUNCTION -- machines",
      "machines -- HYPONYM-OF -- detection of non-voice sounds",
      "machines -- CONJUNCTION -- traffic",
      "birds -- CONJUNCTION -- traffic"
    ],
    "abstract": "this paper describes the work done in the framework of the vidivideo european project in terms of <task_2> . our first experiments concerned the <task_0> , such as <otherscientificterm_4> , <otherscientificterm_6> , <otherscientificterm_5> , water and steps . given the unavailability of a corpus labelled in terms of <material_3> , we used a relatively small sound effect corpus for training . our initial experiments with <method_1> for these 5 classes showed us the feasibility of using this type of data for training , thus avoiding the extremely morose task of manual labelling of a very high number of <material_3> . preliminary integration experiments are quite promising .",
    "abstract_og": "this paper describes the work done in the framework of the vidivideo european project in terms of audio event detection . our first experiments concerned the detection of non-voice sounds , such as birds , machines , traffic , water and steps . given the unavailability of a corpus labelled in terms of audio events , we used a relatively small sound effect corpus for training . our initial experiments with one-against-all svm classi-fiers for these 5 classes showed us the feasibility of using this type of data for training , thus avoiding the extremely morose task of manual labelling of a very high number of audio events . preliminary integration experiments are quite promising ."
  },
  {
    "title": "Robust Tracking of Multiple Sound Sources by Spatial Integration of Room And Robot Microphone Arrays .",
    "entities": [
      "2d sound source localization",
      "weighted delay-and-sum beamforming method",
      "particle filter based integration",
      "2d sound source tracking",
      "sound source tracking",
      "multiple sound sources",
      "room microphone array",
      "sound source tracking",
      "robot microphone array",
      "robot head",
      "localization errors",
      "sound event",
      "environmental sounds",
      "sound sources",
      "particle filter",
      "robot",
      "azimuth",
      "room"
    ],
    "types": "<method> <method> <task> <task> <task> <material> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "weighted delay-and-sum beamforming method -- USED-FOR -- 2d sound source localization",
      "particle filter based integration -- USED-FOR -- 2d sound source tracking",
      "environmental sounds -- HYPONYM-OF -- sound event",
      "sound source tracking -- USED-FOR -- robot",
      "particle filter -- USED-FOR -- multiple sound sources"
    ],
    "abstract": "sound source tracking is an important function for a <otherscientificterm_15> operating in a daily environment , because the <otherscientificterm_15> should recognize where a <otherscientificterm_11> such as speech , music and other <otherscientificterm_12> originates from . this paper addresses <task_4> by integrating a <otherscientificterm_17> and a <method_8> . the <otherscientificterm_6> consists of 64 microphones attached to the walls . it provides <method_0> based on a <method_1> . the <method_8> consists of eight microphones installed on a <otherscientificterm_9> , and localizes <material_5> in <otherscientificterm_16> . the lo-calization results are integrated to track <material_13> by using a <method_14> for <material_5> . the experimental results show that <task_2> reduces <otherscientificterm_10> and provides accurate and robust <task_3> .",
    "abstract_og": "sound source tracking is an important function for a robot operating in a daily environment , because the robot should recognize where a sound event such as speech , music and other environmental sounds originates from . this paper addresses sound source tracking by integrating a room and a robot microphone array . the room microphone array consists of 64 microphones attached to the walls . it provides 2d sound source localization based on a weighted delay-and-sum beamforming method . the robot microphone array consists of eight microphones installed on a robot head , and localizes multiple sound sources in azimuth . the lo-calization results are integrated to track sound sources by using a particle filter for multiple sound sources . the experimental results show that particle filter based integration reduces localization errors and provides accurate and robust 2d sound source tracking ."
  },
  {
    "title": "Reasoning About General Games Described in GDL-II .",
    "entities": [
      "game description language",
      "sound and complete reasoning method",
      "game description language",
      "arbitrary games",
      "reasoning challenge",
      "situation calculus",
      "action formalisms",
      "human intervention",
      "in-complete/imperfect information",
      "learning"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "game description language -- USED-FOR -- situation calculus",
      "situation calculus -- HYPONYM-OF -- action formalisms"
    ],
    "abstract": "recently the general <method_0> has been extended so as to cover <otherscientificterm_3> with <otherscientificterm_8> . <method_9> -- without <otherscientificterm_7> -- to play such games poses a <task_4> for general game-playing systems that is much more intricate than in case of complete information games . <method_6> like the <method_5> have been developed for precisely this purpose . in this paper we present a full embedding of the <method_2> into the <method_5> -lrb- with scherl and levesque 's knowledge fluent -rrb- . we formally prove that this provides a <method_1> for players ' knowledge about game states as well as about the knowledge of the other players .",
    "abstract_og": "recently the general game description language has been extended so as to cover arbitrary games with in-complete/imperfect information . learning -- without human intervention -- to play such games poses a reasoning challenge for general game-playing systems that is much more intricate than in case of complete information games . action formalisms like the situation calculus have been developed for precisely this purpose . in this paper we present a full embedding of the game description language into the situation calculus -lrb- with scherl and levesque 's knowledge fluent -rrb- . we formally prove that this provides a sound and complete reasoning method for players ' knowledge about game states as well as about the knowledge of the other players ."
  },
  {
    "title": "Revisiting the security of speaker verification systems against imposture using synthetic speech .",
    "entities": [
      "speaker verification",
      "false acceptance rate",
      "hmm-based speech synthesizer",
      "human speech",
      "synthesized speech",
      "sv systems",
      "two-step process",
      "synthetic speech",
      "background model",
      "speech synthesis",
      "svm-based",
      "imposture",
      "accuracy",
      "gmm-ubm-based"
    ],
    "types": "<task> <metric> <method> <material> <material> <method> <method> <material> <method> <task> <method> <otherscientificterm> <metric> <method>",
    "relations": [
      "gmm-ubm-based -- HYPONYM-OF -- sv systems",
      "gmm-ubm-based -- CONJUNCTION -- svm-based",
      "synthetic speech -- USED-FOR -- imposture",
      "speaker verification -- CONJUNCTION -- speech synthesis",
      "hmm-based speech synthesizer -- USED-FOR -- synthetic speech",
      "svm-based -- HYPONYM-OF -- sv systems"
    ],
    "abstract": "in this paper , we investigate <otherscientificterm_11> using <material_7> . although this problem was first examined over a decade ago , dramatic improvements in both <task_0> and <task_9> have renewed interest in this problem . we use a <method_2> which creates <material_7> for a targeted speaker through adaptation of a <method_8> . we use two <method_5> : standard <method_13> and a newer <method_10> . our results show when the systems are tested with <material_3> , there are zero false acceptances and zero false rejections . however , when the systems are tested with <material_4> , all claims for the targeted speaker are accepted while all other claims are rejected . we propose a <method_6> for detection of <material_4> in order to prevent this <otherscientificterm_11> . overall , while <method_5> have impressive <metric_12> , even with the proposed detector , high-quality <material_7> will lead to an unacceptably high <metric_1> .",
    "abstract_og": "in this paper , we investigate imposture using synthetic speech . although this problem was first examined over a decade ago , dramatic improvements in both speaker verification and speech synthesis have renewed interest in this problem . we use a hmm-based speech synthesizer which creates synthetic speech for a targeted speaker through adaptation of a background model . we use two sv systems : standard gmm-ubm-based and a newer svm-based . our results show when the systems are tested with human speech , there are zero false acceptances and zero false rejections . however , when the systems are tested with synthesized speech , all claims for the targeted speaker are accepted while all other claims are rejected . we propose a two-step process for detection of synthesized speech in order to prevent this imposture . overall , while sv systems have impressive accuracy , even with the proposed detector , high-quality synthetic speech will lead to an unacceptably high false acceptance rate ."
  },
  {
    "title": "On the relationship between phonetic modeling precision and phonetic speaker recognition accuracy .",
    "entities": [
      "nist 2005 speaker recognition evaluation",
      "phonetic speaker recognition accuracy",
      "phonetic speaker recognition",
      "statistical language models",
      "speaker recognition techniques",
      "phonetic modeling precision",
      "phonetic decodings",
      "speaker recognition",
      "accuracy"
    ],
    "types": "<task> <metric> <task> <method> <method> <metric> <method> <task> <metric>",
    "relations": [
      "accuracy -- EVALUATE-FOR -- phonetic speaker recognition",
      "phonetic decodings -- USED-FOR -- phonetic speaker recognition",
      "phonetic modeling precision -- CONJUNCTION -- phonetic speaker recognition accuracy",
      "phonetic modeling precision -- CONJUNCTION -- accuracy",
      "phonetic modeling precision -- CONJUNCTION -- phonetic speaker recognition"
    ],
    "abstract": "speaker recognition techniques have traditionally relied on purely acoustic features and models . during the last few years , however , the field of <task_7> has started to show interest in the use of higher level features . in particular , <method_6> modeled with <method_3> -lrb- n-grams -rrb- have already shown its effectiveness in several research works . however , the relationship between <metric_5> and the <metric_8> of <task_2> has not yet been sufficiently analyzed . as part of our preparation for the <task_0> , we have performed a number of experiments that show that there is a negligible correlation between <metric_5> and <metric_1> . furthermore , our experimental results show that <task_2> results may even be better when using <method_6> in languages different from that of the speech .",
    "abstract_og": "speaker recognition techniques have traditionally relied on purely acoustic features and models . during the last few years , however , the field of speaker recognition has started to show interest in the use of higher level features . in particular , phonetic decodings modeled with statistical language models -lrb- n-grams -rrb- have already shown its effectiveness in several research works . however , the relationship between phonetic modeling precision and the accuracy of phonetic speaker recognition has not yet been sufficiently analyzed . as part of our preparation for the nist 2005 speaker recognition evaluation , we have performed a number of experiments that show that there is a negligible correlation between phonetic modeling precision and phonetic speaker recognition accuracy . furthermore , our experimental results show that phonetic speaker recognition results may even be better when using phonetic decodings in languages different from that of the speech ."
  },
  {
    "title": "Adaptation of Precision Matrix Models on Large Vocabulary Continuous Speech Recognition .",
    "entities": [
      "minimum phone error discriminative training",
      "diagonal covariance matrix models",
      "structured precision matrix models",
      "precision and mean models",
      "expectation maximisation framework",
      "row-by-row iterative formulae",
      "broadcast news",
      "adaptation techniques",
      "adaptation data"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <material> <method> <material>",
    "relations": [
      "row-by-row iterative formulae -- PART-OF -- expectation maximisation framework",
      "structured precision matrix models -- USED-FOR -- diagonal covariance matrix models",
      "adaptation techniques -- USED-FOR -- diagonal covariance matrix models"
    ],
    "abstract": "recently , <method_2> were found to outper-form the conventional <method_1> . <method_0> of these <method_1> gave very good unadapted performance on large vocabulary continuous speech recognition systems . to obtain state-of-the-art performance , it is important to apply <method_7> efficiently to these <method_1> . in this paper , simple <method_5> are described for both mllr mean and constrained mllr transform estimations of these <method_1> . these update formulae are derived within the standard <method_4> and are guaranteed to increase the likelihood of the <material_8> . efficient approximate schemes for these <method_7> are also investigated to further reduce the computation . experimental results are presented based on the mpe trained subspace for <method_3> , evaluated on both <material_6> and conversational telephone speech english tasks .",
    "abstract_og": "recently , structured precision matrix models were found to outper-form the conventional diagonal covariance matrix models . minimum phone error discriminative training of these diagonal covariance matrix models gave very good unadapted performance on large vocabulary continuous speech recognition systems . to obtain state-of-the-art performance , it is important to apply adaptation techniques efficiently to these diagonal covariance matrix models . in this paper , simple row-by-row iterative formulae are described for both mllr mean and constrained mllr transform estimations of these diagonal covariance matrix models . these update formulae are derived within the standard expectation maximisation framework and are guaranteed to increase the likelihood of the adaptation data . efficient approximate schemes for these adaptation techniques are also investigated to further reduce the computation . experimental results are presented based on the mpe trained subspace for precision and mean models , evaluated on both broadcast news and conversational telephone speech english tasks ."
  },
  {
    "title": "Minivectors : an improved GMM-SVM approach for speaker verification .",
    "entities": [
      "kharroubi 's system",
      "speaker verification algorithms",
      "speaker verification systems",
      "run time",
      "supervectors algorithm",
      "accuracy levels",
      "real-life applications",
      "speaker models"
    ],
    "types": "<method> <method> <method> <metric> <method> <metric> <task> <method>",
    "relations": [
      "accuracy levels -- EVALUATE-FOR -- speaker verification systems"
    ],
    "abstract": "the <metric_5> achieved by state-of-the-art <method_2> are high enough for the technology to be used in <task_6> . unfortunately , the transfer from the lab to the field is not as straightforward as could be : the best performing systems can be computationally expensive to run and need large speaker model footprints . in this paper , we compare two <method_1> -lrb- gmm-svm su-pervectors and kharroubi 's gmm-svm vectors -rrb- and propose an improvement of <method_0> that : -lrb- a -rrb- achieves up to 17 % relative performance improvement when compared to the <method_4> ; -lrb- b -rrb- is 24 % faster in <metric_3> and -lrb- c -rrb- makes use of <method_7> that are 94 % smaller than those needed by the <method_4> .",
    "abstract_og": "the accuracy levels achieved by state-of-the-art speaker verification systems are high enough for the technology to be used in real-life applications . unfortunately , the transfer from the lab to the field is not as straightforward as could be : the best performing systems can be computationally expensive to run and need large speaker model footprints . in this paper , we compare two speaker verification algorithms -lrb- gmm-svm su-pervectors and kharroubi 's gmm-svm vectors -rrb- and propose an improvement of kharroubi 's system that : -lrb- a -rrb- achieves up to 17 % relative performance improvement when compared to the supervectors algorithm ; -lrb- b -rrb- is 24 % faster in run time and -lrb- c -rrb- makes use of speaker models that are 94 % smaller than those needed by the supervectors algorithm ."
  },
  {
    "title": "Speech recognition with phonological features : some issues to attend .",
    "entities": [
      "acoustic and the linguistic level",
      "articulatory feature driven system",
      "acoustic-phonetic or articulatory features",
      "acoustic and articulatory information",
      "articulatory driven asr system",
      "automatic speech recognition",
      "acoustic features",
      "articulatory features",
      "phonetic unit",
      "asr"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "acoustic and articulatory information -- USED-FOR -- asr",
      "acoustic features -- USED-FOR -- articulatory driven asr system",
      "acoustic-phonetic or articulatory features -- USED-FOR -- automatic speech recognition"
    ],
    "abstract": "it is often argued that <otherscientificterm_2> could be beneficial to <task_5> because <otherscientificterm_2> provide a convenient interface between the <otherscientificterm_0> . former research has shown that a combination of <otherscientificterm_3> can lead to improved <task_9> . however there exists no purely <method_4> that outper-forms state-of-the-art systems driven by <otherscientificterm_6> . in this paper we propose a novel method for improving <task_9> on the basis of <otherscientificterm_7> . it is designed to take account of -lrb- 1 -rrb- the correlations between <otherscientificterm_7> and -lrb- 2 -rrb- the fact that not all <otherscientificterm_7> are relevant for the description of a certain <otherscientificterm_8> . we also investigate to what extend an acoustic and an <method_1> make different errors .",
    "abstract_og": "it is often argued that acoustic-phonetic or articulatory features could be beneficial to automatic speech recognition because acoustic-phonetic or articulatory features provide a convenient interface between the acoustic and the linguistic level . former research has shown that a combination of acoustic and articulatory information can lead to improved asr . however there exists no purely articulatory driven asr system that outper-forms state-of-the-art systems driven by acoustic features . in this paper we propose a novel method for improving asr on the basis of articulatory features . it is designed to take account of -lrb- 1 -rrb- the correlations between articulatory features and -lrb- 2 -rrb- the fact that not all articulatory features are relevant for the description of a certain phonetic unit . we also investigate to what extend an acoustic and an articulatory feature driven system make different errors ."
  },
  {
    "title": "Warped Magnitude and Phase-Based Features for Language Identification .",
    "entities": [
      "modified group delay function coefficients",
      "gaussian mixture model based system",
      "regression-based shifted delta cepstrum",
      "identification of spoken languages",
      "language identification systems",
      "ogi ts corpus",
      "magnitude-based parameterization methods",
      "magnitude-based features",
      "sdc configuration",
      "feature warping",
      "modgdf",
      "mfcc",
      "accuracy"
    ],
    "types": "<method> <method> <material> <task> <method> <material> <method> <otherscientificterm> <method> <method> <method> <method> <metric>",
    "relations": [
      "regression-based shifted delta cepstrum -- HYPONYM-OF -- magnitude-based parameterization methods",
      "regression-based shifted delta cepstrum -- CONJUNCTION -- feature warping",
      "feature warping -- USED-FOR -- magnitude-based features",
      "mfcc -- HYPONYM-OF -- magnitude-based parameterization methods",
      "magnitude-based parameterization methods -- USED-FOR -- identification of spoken languages",
      "accuracy -- EVALUATE-FOR -- language identification systems",
      "modified group delay function coefficients -- COMPARE -- language identification systems",
      "feature warping -- CONJUNCTION -- modified group delay function coefficients",
      "ogi ts corpus -- EVALUATE-FOR -- modified group delay function coefficients",
      "ogi ts corpus -- EVALUATE-FOR -- language identification systems",
      "mfcc -- CONJUNCTION -- regression-based shifted delta cepstrum",
      "magnitude-based features -- PART-OF -- gaussian mixture model based system",
      "regression-based shifted delta cepstrum -- USED-FOR -- gaussian mixture model based system",
      "modified group delay function coefficients -- USED-FOR -- gaussian mixture model based system"
    ],
    "abstract": "to date , systems for the <task_3> have normally used <method_6> such as the <method_11> and <material_2> . this paper investigates the use of the recently proposed <method_0> in combination with traditional <otherscientificterm_7> in a <method_1> . we also examine the application of <method_9> to <otherscientificterm_7> and the <method_10> and find that it can offer a significant cumulative improvement . we find that the addition of a modified <material_2> further improves <method_1> performance beyond that obtained by a more standard <method_8> . the combination of <material_2> , <method_9> and the proposed <method_0> achieved an <metric_12> of 88.4 % in tests on 10 languages in the <material_5> , which compares very favourably with alternative <method_4> reported in the literature .",
    "abstract_og": "to date , systems for the identification of spoken languages have normally used magnitude-based parameterization methods such as the mfcc and regression-based shifted delta cepstrum . this paper investigates the use of the recently proposed modified group delay function coefficients in combination with traditional magnitude-based features in a gaussian mixture model based system . we also examine the application of feature warping to magnitude-based features and the modgdf and find that it can offer a significant cumulative improvement . we find that the addition of a modified regression-based shifted delta cepstrum further improves gaussian mixture model based system performance beyond that obtained by a more standard sdc configuration . the combination of regression-based shifted delta cepstrum , feature warping and the proposed modified group delay function coefficients achieved an accuracy of 88.4 % in tests on 10 languages in the ogi ts corpus , which compares very favourably with alternative language identification systems reported in the literature ."
  },
  {
    "title": "Fast Rates for Exp-concave Empirical Risk Minimization .",
    "entities": [
      "empirical risk minimization",
      "linear and logistic regression",
      "fast generalization rates",
      "general optimization framework",
      "learning svms",
      "learning problems",
      "online algorithms",
      "squared hinge-loss",
      "portfolio selection",
      "stochastic optimization"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <method> <task>",
    "relations": [
      "empirical risk minimization -- USED-FOR -- stochastic optimization",
      "squared hinge-loss -- CONJUNCTION -- portfolio selection",
      "linear and logistic regression -- HYPONYM-OF -- learning problems",
      "portfolio selection -- HYPONYM-OF -- learning problems",
      "portfolio selection -- CONJUNCTION -- learning svms",
      "learning svms -- HYPONYM-OF -- learning problems"
    ],
    "abstract": "we consider <method_0> in the context of <task_9> with exp-concave and smooth losses -- a <method_3> that captures several important <task_5> including <task_1> , <method_4> with the <otherscientificterm_7> , <method_8> and more . in this setting , we establish the first evidence that <method_0> is able to attain <otherscientificterm_2> , and show that the expected loss of the <method_0> in d dimensions converges to the optimal expected loss in a rate of d/n . this rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art , which is only known to be attained by an online-to-batch conversion of computationally expensive <method_6> .",
    "abstract_og": "we consider empirical risk minimization in the context of stochastic optimization with exp-concave and smooth losses -- a general optimization framework that captures several important learning problems including linear and logistic regression , learning svms with the squared hinge-loss , portfolio selection and more . in this setting , we establish the first evidence that empirical risk minimization is able to attain fast generalization rates , and show that the expected loss of the empirical risk minimization in d dimensions converges to the optimal expected loss in a rate of d/n . this rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art , which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms ."
  },
  {
    "title": "Symmetry-preserving reversible integer-to-integer wavelet transforms .",
    "entities": [
      "lifting-based families of symmetry-preserving reversible integer-to-integer wavelet transforms",
      "treatment of arbitrary length signals",
      "constant per-lifting-step extension",
      "rounding functions",
      "symmetric extension"
    ],
    "types": "<otherscientificterm> <task> <method> <otherscientificterm> <method>",
    "relations": [
      "symmetric extension -- COMPARE -- constant per-lifting-step extension"
    ],
    "abstract": "studied are two <otherscientificterm_0> . the transforms from both of these families are shown to be compatible with <method_4> , which permits the <task_1> in a nonexpansive manner . throughout this work , particularly close attention is paid to <otherscientificterm_3> , and the properties that they must possess in various instances . <method_4> is also shown to be equivalent to <method_2> in certain circumstances .",
    "abstract_og": "studied are two lifting-based families of symmetry-preserving reversible integer-to-integer wavelet transforms . the transforms from both of these families are shown to be compatible with symmetric extension , which permits the treatment of arbitrary length signals in a nonexpansive manner . throughout this work , particularly close attention is paid to rounding functions , and the properties that they must possess in various instances . symmetric extension is also shown to be equivalent to constant per-lifting-step extension in certain circumstances ."
  },
  {
    "title": "Identifying Protein-Protein Interaction Sites on a Genome-Wide Scale .",
    "entities": [
      "high-throughput protein interaction data",
      "drug and protein design",
      "short sequence motifs",
      "probabilistic relational model",
      "protein interactions",
      "branch-and-bound algorithm",
      "interacting proteins",
      "physical interaction",
      "protein-protein interactions",
      "em algorithm",
      "active motifs",
      "computational method",
      "approximate inference",
      "motifs",
      "e-step"
    ],
    "types": "<material> <task> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "em algorithm -- USED-FOR -- computational method",
      "computational method -- USED-FOR -- protein-protein interactions",
      "branch-and-bound algorithm -- USED-FOR -- computational method",
      "high-throughput protein interaction data -- CONJUNCTION -- short sequence motifs",
      "short sequence motifs -- USED-FOR -- computational method",
      "probabilistic relational model -- USED-FOR -- computational method",
      "high-throughput protein interaction data -- USED-FOR -- computational method",
      "computational method -- USED-FOR -- motifs",
      "approximate inference -- USED-FOR -- e-step",
      "high-throughput protein interaction data -- USED-FOR -- probabilistic relational model"
    ],
    "abstract": "protein interactions typically arise from a <otherscientificterm_7> of one or more small sites on the surface of the two proteins . identifying these sites is very important for <task_1> . in this paper , we propose a <method_11> based on <method_3> that attempts to address this task using <material_0> and a set of <material_2> . we learn the <method_11> using the <method_9> , with a <method_5> as an <otherscientificterm_12> for the <method_14> . our <method_11> searches for <otherscientificterm_13> whose presence in a pair of <otherscientificterm_6> can explain their observed interaction . it also tries to determine which motif pairs have high affinity , and can therefore lead to an interaction . we show that our <method_11> is more accurate than others at predicting new <otherscientificterm_8> . more importantly , by examining solved structures of protein complexes , we find that 2/3 of the predicted <otherscientificterm_10> correspond to actual interaction sites .",
    "abstract_og": "protein interactions typically arise from a physical interaction of one or more small sites on the surface of the two proteins . identifying these sites is very important for drug and protein design . in this paper , we propose a computational method based on probabilistic relational model that attempts to address this task using high-throughput protein interaction data and a set of short sequence motifs . we learn the computational method using the em algorithm , with a branch-and-bound algorithm as an approximate inference for the e-step . our computational method searches for motifs whose presence in a pair of interacting proteins can explain their observed interaction . it also tries to determine which motif pairs have high affinity , and can therefore lead to an interaction . we show that our computational method is more accurate than others at predicting new protein-protein interactions . more importantly , by examining solved structures of protein complexes , we find that 2/3 of the predicted active motifs correspond to actual interaction sites ."
  },
  {
    "title": "A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit .",
    "entities": [
      "large vocabulary continuous speech recognition",
      "graphics processing units",
      "personal desktop and laptop systems",
      "data parallel programming model",
      "communication-intensive graph traver-sal phase",
      "communication-intensive graph traversal phase",
      "speech data set",
      "close-to-peak execution efficiency",
      "speech inference engine",
      "intel core i7",
      "data parallel execution",
      "inference process",
      "redundant computation",
      "sequential implementation",
      "application-level trade-offs",
      "accuracy"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <task> <task> <material> <metric> <method> <material> <task> <method> <otherscientificterm> <method> <otherscientificterm> <metric>",
    "relations": [
      "application-level trade-offs -- USED-FOR -- data parallel execution"
    ],
    "abstract": "tremendous compute throughput is becoming available in <otherscientificterm_2> through the use of <method_1> . however , exploiting this resource requires re-architecting an application to fit a <method_3> . the complex graph traversal routines in the <method_11> for <task_0> have been considered by many as unsuitable for extensive parallelization . we explore and demonstrate a fully data parallel implementation of a <method_8> on nvidia 's gtx280 gpu . our implementation consists of two phases-compute-intensive observation probability computation phase and <task_4> . we take advantage of dynamic elimination of <otherscientificterm_12> in the compute-intensive phase while maintaining <metric_7> . we also demonstrate the importance of exploring <otherscientificterm_14> in the <task_5> to adapt the algorithm to <task_10> on gpus . on 3.1 hours of <material_6> , we achieve more than 11 \u00d7 speedup compared to a highly optimized <method_13> on <material_9> without sacrificing <metric_15> .",
    "abstract_og": "tremendous compute throughput is becoming available in personal desktop and laptop systems through the use of graphics processing units . however , exploiting this resource requires re-architecting an application to fit a data parallel programming model . the complex graph traversal routines in the inference process for large vocabulary continuous speech recognition have been considered by many as unsuitable for extensive parallelization . we explore and demonstrate a fully data parallel implementation of a speech inference engine on nvidia 's gtx280 gpu . our implementation consists of two phases-compute-intensive observation probability computation phase and communication-intensive graph traver-sal phase . we take advantage of dynamic elimination of redundant computation in the compute-intensive phase while maintaining close-to-peak execution efficiency . we also demonstrate the importance of exploring application-level trade-offs in the communication-intensive graph traversal phase to adapt the algorithm to data parallel execution on gpus . on 3.1 hours of speech data set , we achieve more than 11 \u00d7 speedup compared to a highly optimized sequential implementation on intel core i7 without sacrificing accuracy ."
  },
  {
    "title": "Correlated Bigram LSA for Unsupervised Language Model Adaptation .",
    "entities": [
      "relative character error rate reduction",
      "word error rate reduction",
      "bootstrapping of bigram lsa",
      "correlated bigram lsa approach",
      "mandarin rt04 test set",
      "automatic speech recognition",
      "background n-gram lm",
      "fractional kneser-ney smoothing",
      "unsupervised lm adaptation",
      "relative perplexity reduction",
      "variational em",
      "fractional counts",
      "linear interpolation",
      "unigram lsa",
      "bigram lsa",
      "lm adaptation",
      "marginal adaptation",
      "large-scale evaluation",
      "arabic",
      "unigram"
    ],
    "types": "<metric> <metric> <method> <method> <material> <task> <method> <method> <task> <metric> <method> <otherscientificterm> <method> <method> <method> <task> <method> <task> <material> <otherscientificterm>",
    "relations": [
      "correlated bigram lsa approach -- USED-FOR -- automatic speech recognition",
      "fractional kneser-ney smoothing -- USED-FOR -- fractional counts",
      "variational em -- USED-FOR -- correlated bigram lsa approach",
      "unigram -- CONJUNCTION -- bigram lsa",
      "relative character error rate reduction -- EVALUATE-FOR -- unigram lsa",
      "relative perplexity reduction -- CONJUNCTION -- relative character error rate reduction",
      "unigram lsa -- USED-FOR -- bootstrapping of bigram lsa",
      "fractional kneser-ney smoothing -- USED-FOR -- correlated bigram lsa approach",
      "marginal adaptation -- CONJUNCTION -- linear interpolation",
      "correlated bigram lsa approach -- USED-FOR -- unsupervised lm adaptation",
      "unsupervised lm adaptation -- USED-FOR -- automatic speech recognition"
    ],
    "abstract": "we present a <method_3> for <task_8> for <task_5> . the <method_3> is trained using efficient <method_10> and smoothed using the proposed <method_7> which handles <otherscientificterm_11> . we address the scalability issue to large training corpora via <method_2> from <method_13> . for <task_15> , <otherscientificterm_19> and <method_14> are integrated into the <method_6> via <method_16> and <method_12> respectively . experimental results on the <material_4> show that applying <otherscientificterm_19> and <method_14> together yields 6 % -- 8 % <metric_9> and 2.5 % <metric_0> which is statistically significant compared to applying only <method_13> . on the <task_17> on <material_18> , <metric_1> from <method_14> is statistically significant compared to the unadapted baseline .",
    "abstract_og": "we present a correlated bigram lsa approach for unsupervised lm adaptation for automatic speech recognition . the correlated bigram lsa approach is trained using efficient variational em and smoothed using the proposed fractional kneser-ney smoothing which handles fractional counts . we address the scalability issue to large training corpora via bootstrapping of bigram lsa from unigram lsa . for lm adaptation , unigram and bigram lsa are integrated into the background n-gram lm via marginal adaptation and linear interpolation respectively . experimental results on the mandarin rt04 test set show that applying unigram and bigram lsa together yields 6 % -- 8 % relative perplexity reduction and 2.5 % relative character error rate reduction which is statistically significant compared to applying only unigram lsa . on the large-scale evaluation on arabic , word error rate reduction from bigram lsa is statistically significant compared to the unadapted baseline ."
  },
  {
    "title": "On the Role of Domain Knowledge in Analogy-Based Story Generation .",
    "entities": [
      "analogy-based story generation",
      "case-based reasoning",
      "ai techniques",
      "knowledge container",
      "computational narrative",
      "story generation",
      "domain knowledge"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <task> <task> <otherscientificterm>",
    "relations": [
      "domain knowledge -- PART-OF -- analogy-based story generation",
      "domain knowledge -- USED-FOR -- story generation"
    ],
    "abstract": "computational narrative is a complex and interesting domain for exploring <method_2> that algo-rithmically analyze , understand , and most importantly , generate stories . this paper studies the importance of <otherscientificterm_6> in <task_5> , and particularly in <task_0> . based on the construct of <otherscientificterm_3> in <method_1> , we present a theoretical framework for incorporating <otherscientificterm_6> in <task_0> . we complement the framework with empirical results in our existing system riu .",
    "abstract_og": "computational narrative is a complex and interesting domain for exploring ai techniques that algo-rithmically analyze , understand , and most importantly , generate stories . this paper studies the importance of domain knowledge in story generation , and particularly in analogy-based story generation . based on the construct of knowledge container in case-based reasoning , we present a theoretical framework for incorporating domain knowledge in analogy-based story generation . we complement the framework with empirical results in our existing system riu ."
  },
  {
    "title": "Independent Vector Analysis using Non-Spherical Joint Densities for the Separation of Speech Signals .",
    "entities": [
      "vector representation of the source signal",
      "spherical joint density representations",
      "separation of convolved sources",
      "blind source separation approach",
      "non-spherical joint density model",
      "multidimensional joint densities",
      "frequency domain methods",
      "inherent signal dependencies",
      "convolved mixture problem",
      "uniform dependencies",
      "frequency bins",
      "speech signals",
      "permutation problem",
      "spherical distributions"
    ],
    "types": "<method> <method> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "blind source separation approach -- USED-FOR -- inherent signal dependencies",
      "frequency domain methods -- USED-FOR -- convolved mixture problem",
      "blind source separation approach -- COMPARE -- spherical joint density representations"
    ],
    "abstract": "we propose a new <method_3> that models the <otherscientificterm_7> such as those observed in <otherscientificterm_11> in order to solve the problem of separating convolved sources . the <method_6> for the <task_8> require a solution to the well-known <task_12> . our <method_3> is based on assuming a <method_0> where its <otherscientificterm_5> are non-spherical . <otherscientificterm_13> may be adequate for signals that exhibit <otherscientificterm_9> across frequencies but in case of <otherscientificterm_11> we can observe stronger dependencies for neighboring <otherscientificterm_10> and almost no dependency for <otherscientificterm_10> that are far apart . the <method_4> takes into account this phenomenon . for the <task_2> , the proposed <method_3> demonstrates consistent performance over previous methods and improved performance over the <method_1> .",
    "abstract_og": "we propose a new blind source separation approach that models the inherent signal dependencies such as those observed in speech signals in order to solve the problem of separating convolved sources . the frequency domain methods for the convolved mixture problem require a solution to the well-known permutation problem . our blind source separation approach is based on assuming a vector representation of the source signal where its multidimensional joint densities are non-spherical . spherical distributions may be adequate for signals that exhibit uniform dependencies across frequencies but in case of speech signals we can observe stronger dependencies for neighboring frequency bins and almost no dependency for frequency bins that are far apart . the non-spherical joint density model takes into account this phenomenon . for the separation of convolved sources , the proposed blind source separation approach demonstrates consistent performance over previous methods and improved performance over the spherical joint density representations ."
  },
  {
    "title": "Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization .",
    "entities": [
      "query focused multi-document summarization",
      "raw frequency",
      "summarization systems",
      "query-focused summarization",
      "log-likelihood ratio",
      "llr",
      "modules"
    ],
    "types": "<task> <material> <method> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "llr -- USED-FOR -- query-focused summarization"
    ],
    "abstract": "the increasing complexity of <method_2> makes <method_5> difficult to analyze exactly which <otherscientificterm_6> make a difference in performance . we carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of <task_0> : <material_1> -lrb- word probability -rrb- and <otherscientificterm_4> . we demonstrate that the advantages of <otherscientificterm_4> come from its known dis-tributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input . we also find that <method_5> is more suitable for <task_3> since , unlike <material_1> , <method_5> is more sensitive to the integration of the information need defined by the user .",
    "abstract_og": "the increasing complexity of summarization systems makes llr difficult to analyze exactly which modules make a difference in performance . we carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization : raw frequency -lrb- word probability -rrb- and log-likelihood ratio . we demonstrate that the advantages of log-likelihood ratio come from its known dis-tributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input . we also find that llr is more suitable for query-focused summarization since , unlike raw frequency , llr is more sensitive to the integration of the information need defined by the user ."
  },
  {
    "title": "Prime Compilation of Non-Clausal Formulae .",
    "entities": [
      "conjunctive/disjunctive normal form representation",
      "propositional satisfiability solving",
      "compilation of non-clausal formu-lae",
      "compilation of non-clausal formulae",
      "non-clausal formulae of size",
      "non-clausal form",
      "propositional formulae",
      "cnf formula",
      "formula compilation",
      "ai"
    ],
    "types": "<method> <method> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <task>",
    "relations": [
      "conjunctive/disjunctive normal form representation -- USED-FOR -- formula compilation"
    ],
    "abstract": "formula compilation by generation of prime implicates or implicants finds a wide range of applications in <task_9> . recent work on <task_8> by prime implicate/implicant generation often assumes a <method_0> . however , in many settings <otherscientificterm_6> are naturally expressed in <otherscientificterm_5> . despite a large body of work on <task_3> , in practice existing approaches can only be applied to fairly small formulae , containing at most a few hundred variables . this paper describes two novel approaches for the <task_2> either with prime implicants or implicates , that is based on <method_1> . these novel algorithms also find application when computing all prime implicates of a <method_7> . the proposed approach is shown to allow the <task_3> of size significantly larger than existing approaches .",
    "abstract_og": "formula compilation by generation of prime implicates or implicants finds a wide range of applications in ai . recent work on formula compilation by prime implicate/implicant generation often assumes a conjunctive/disjunctive normal form representation . however , in many settings propositional formulae are naturally expressed in non-clausal form . despite a large body of work on compilation of non-clausal formulae , in practice existing approaches can only be applied to fairly small formulae , containing at most a few hundred variables . this paper describes two novel approaches for the compilation of non-clausal formu-lae either with prime implicants or implicates , that is based on propositional satisfiability solving . these novel algorithms also find application when computing all prime implicates of a cnf formula . the proposed approach is shown to allow the compilation of non-clausal formulae of size significantly larger than existing approaches ."
  },
  {
    "title": "Hybrid Stochastic / Deterministic Optimization for Tracking Sports Players and Pedestrians .",
    "entities": [
      "baseline tracking-by-detection approaches",
      "missed detections",
      "` tracking-by-detection",
      "mcmcda techniques",
      "detection configurations",
      "object detectors",
      "object trajectories",
      "multi-target scenarios",
      "detection hypothesis",
      "deterministic computation",
      "stochastic search",
      "sampler",
      "rjmcmc"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <method>",
    "relations": [
      "rjmcmc -- USED-FOR -- stochastic search"
    ],
    "abstract": "although <method_2> ' is a popular approach when reliable <method_5> are available , <method_1> remain a difficult hurdle to overcome . we present a hybrid stochastic/deterministic optimization scheme that uses <method_12> to perform <task_10> over the space of <otherscientificterm_4> , interleaved with <method_9> of the optimal multi-frame data association for each proposed <method_8> . since <otherscientificterm_6> do not need to be estimated directly by the <method_11> , our approach is more efficient than traditional <method_3> . moreover , our holistic formulation is able to generate longer , more reliable trajectories than <method_0> in challenging <otherscientificterm_7> .",
    "abstract_og": "although ` tracking-by-detection ' is a popular approach when reliable object detectors are available , missed detections remain a difficult hurdle to overcome . we present a hybrid stochastic/deterministic optimization scheme that uses rjmcmc to perform stochastic search over the space of detection configurations , interleaved with deterministic computation of the optimal multi-frame data association for each proposed detection hypothesis . since object trajectories do not need to be estimated directly by the sampler , our approach is more efficient than traditional mcmcda techniques . moreover , our holistic formulation is able to generate longer , more reliable trajectories than baseline tracking-by-detection approaches in challenging multi-target scenarios ."
  },
  {
    "title": "Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions .",
    "entities": [
      "kernel embeddings of distributions",
      "multilingual wikipedia articles",
      "shared latent space",
      "multiset of features",
      "bag-of-words representation",
      "kernel-based method",
      "multilingual documents",
      "cross-domain relationships",
      "features",
      "documents",
      "embedding",
      "images"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <method> <method> <material> <otherscientificterm> <otherscientificterm> <material> <task> <material>",
    "relations": [
      "bag-of-words representation -- HYPONYM-OF -- multiset of features",
      "multilingual documents -- CONJUNCTION -- images",
      "bag-of-words representation -- USED-FOR -- documents"
    ],
    "abstract": "we propose a <method_5> for finding matching between instances across different domains , such as <material_6> and <material_11> with annotations . each instance is assumed to be represented as a <otherscientificterm_3> , e.g. , a <method_4> for <material_9> . the major difficulty in finding <otherscientificterm_7> is that the similarity between instances in different domains can not be directly measured . to overcome this difficulty , the proposed <method_5> embeds all the <otherscientificterm_8> of different domains in a <otherscientificterm_2> , and regards each instance as a distribution of its own <otherscientificterm_8> in the <otherscientificterm_2> . to represent the distributions efficiently and nonparametrically , we employ the framework of the <method_0> . the <task_10> is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart . in our experiments , we show that the proposed <method_5> can achieve high performance on finding correspondence between <material_1> , between <material_9> and tags , and between <material_11> and tags .",
    "abstract_og": "we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations . each instance is assumed to be represented as a multiset of features , e.g. , a bag-of-words representation for documents . the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured . to overcome this difficulty , the proposed kernel-based method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space . to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions . the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart . in our experiments , we show that the proposed kernel-based method can achieve high performance on finding correspondence between multilingual wikipedia articles , between documents and tags , and between images and tags ."
  },
  {
    "title": "On the success of network inference using a markov routing model .",
    "entities": [
      "network topology inference algorithm",
      "markov random walk model",
      "real network routing",
      "network co-occurrence measurements",
      "seeming model mismatch",
      "topology reconstruction",
      "routing"
    ],
    "types": "<method> <method> <task> <method> <otherscientificterm> <task> <task>",
    "relations": [
      "markov random walk model -- USED-FOR -- routing",
      "markov random walk model -- USED-FOR -- network topology inference algorithm",
      "routing -- USED-FOR -- topology reconstruction",
      "network co-occurrence measurements -- CONJUNCTION -- markov random walk model",
      "network topology inference algorithm -- USED-FOR -- topology reconstruction",
      "network co-occurrence measurements -- USED-FOR -- network topology inference algorithm",
      "network topology inference algorithm -- USED-FOR -- routing"
    ],
    "abstract": "in this paper we discuss why a simple <method_0> based on <method_3> and a <method_1> for <task_6> enables perfect <task_5> , despite the <otherscientificterm_4> to <task_2> .",
    "abstract_og": "in this paper we discuss why a simple network topology inference algorithm based on network co-occurrence measurements and a markov random walk model for routing enables perfect topology reconstruction , despite the seeming model mismatch to real network routing ."
  },
  {
    "title": "Computationally Efficient Nystr\u00f6m Approximation using Fast Transforms .",
    "entities": [
      "low-rank kernel matrix approximations",
      "structured landmark points",
      "kernel svm prediction",
      "computing kernel values",
      "linear svm prediction",
      "nystr\u00f6m method",
      "webspam data",
      "kernel machines",
      "prediction time",
      "time complexity",
      "fast transforms",
      "nystr\u00f6m approximation",
      "large-scale applications",
      "kernel values",
      "kernel approximation",
      "lib-svm",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <material> <method> <otherscientificterm> <metric> <method> <method> <task> <otherscientificterm> <task> <method> <metric>",
    "relations": [
      "lib-svm -- COMPARE -- linear svm prediction",
      "prediction time -- COMPARE -- linear svm prediction",
      "structured landmark points -- USED-FOR -- nystr\u00f6m approximation",
      "accuracy -- EVALUATE-FOR -- linear svm prediction",
      "fast transforms -- USED-FOR -- structured landmark points",
      "nystr\u00f6m approximation -- USED-FOR -- large-scale applications",
      "kernel approximation -- USED-FOR -- kernel machines"
    ],
    "abstract": "our goal is to improve the training and <otherscientificterm_8> of <method_5> , which is a widely-used technique for generating <otherscientificterm_0> . when applying the <method_11> for <task_12> , both training and <otherscientificterm_8> is dominated by <otherscientificterm_3> between a data point and all landmark points . with m landmark points , this computation requires \u03b8 -lrb- md -rrb- time -lrb- flops -rrb- , where d is the input dimension . in this paper , we propose the use of a family of <method_10> to generate <otherscientificterm_1> for <method_11> . by exploiting <method_10> , e.g. , haar transform and hadamard transform , our modified <method_5> requires only \u03b8 -lrb- m -rrb- or \u03b8 -lrb- m log d -rrb- time to compute the <otherscientificterm_13> between a given data point and m landmark points . this improvement in <metric_9> can significantly speed up <task_14> and benefit prediction speed in <method_7> . for instance , on the <material_6> -lrb- more than 300,000 data points -rrb- , our proposed algorithm enables <method_2> to deliver 98 % <metric_16> and the resulting <otherscientificterm_8> is 1000 times faster than <method_15> and only 10 times slower than <method_4> -lrb- which yields only 91 % <metric_16> -rrb- .",
    "abstract_og": "our goal is to improve the training and prediction time of nystr\u00f6m method , which is a widely-used technique for generating low-rank kernel matrix approximations . when applying the nystr\u00f6m approximation for large-scale applications , both training and prediction time is dominated by computing kernel values between a data point and all landmark points . with m landmark points , this computation requires \u03b8 -lrb- md -rrb- time -lrb- flops -rrb- , where d is the input dimension . in this paper , we propose the use of a family of fast transforms to generate structured landmark points for nystr\u00f6m approximation . by exploiting fast transforms , e.g. , haar transform and hadamard transform , our modified nystr\u00f6m method requires only \u03b8 -lrb- m -rrb- or \u03b8 -lrb- m log d -rrb- time to compute the kernel values between a given data point and m landmark points . this improvement in time complexity can significantly speed up kernel approximation and benefit prediction speed in kernel machines . for instance , on the webspam data -lrb- more than 300,000 data points -rrb- , our proposed algorithm enables kernel svm prediction to deliver 98 % accuracy and the resulting prediction time is 1000 times faster than lib-svm and only 10 times slower than linear svm prediction -lrb- which yields only 91 % accuracy -rrb- ."
  },
  {
    "title": "High Dimensional Structured Superposition Models .",
    "entities": [
      "gaussian widths of suitable sets",
      "high dimensional superposition models",
      "componentwise estimation error",
      "generic chaining",
      "geometric condition",
      "sample complexity",
      "statistical analysis",
      "superposition models",
      "empirical processes",
      "sparse matrices",
      "low rank"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "empirical processes -- CONJUNCTION -- generic chaining",
      "generic chaining -- USED-FOR -- statistical analysis"
    ],
    "abstract": "high dimensional <method_7> characterize observations using parameters which can be written as a sum of multiple component parameters , each with its own structure , e.g. , sum of <otherscientificterm_10> and <otherscientificterm_9> , sum of sparse and rotated sparse vectors , etc. . in this paper , we consider general <method_7> which allow sum of any number of component parameters , and each component structure can be characterized by any norm . we present a simple estimator for such models , give a <otherscientificterm_4> under which the components can be accurately estimated , characterize <metric_5> of the estimator , and give high probability non-asymptotic bounds on the <otherscientificterm_2> . we use tools from <method_8> and <method_3> for the <method_6> , and our results , which substantially generalize prior work on <method_7> , are in terms of <otherscientificterm_0> .",
    "abstract_og": "high dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters , each with its own structure , e.g. , sum of low rank and sparse matrices , sum of sparse and rotated sparse vectors , etc. . in this paper , we consider general superposition models which allow sum of any number of component parameters , and each component structure can be characterized by any norm . we present a simple estimator for such models , give a geometric condition under which the components can be accurately estimated , characterize sample complexity of the estimator , and give high probability non-asymptotic bounds on the componentwise estimation error . we use tools from empirical processes and generic chaining for the statistical analysis , and our results , which substantially generalize prior work on superposition models , are in terms of gaussian widths of suitable sets ."
  },
  {
    "title": "Detection of upper airway narrowing via classification of LPC coefficients : Implications for obstructive sleep apnea diagnosis .",
    "entities": [
      "low and high r au conditions",
      "ua resistance -lrb- r au -rrb-",
      "obstructive sleep apnea",
      "unvoiced speech sounds",
      "turbulent breath sounds",
      "turbulent inspiratory sounds",
      "breath sounds analysis",
      "breath sounds recording",
      "r au status",
      "linear prediction coding",
      "ua narrowing",
      "sound characteristics",
      "objective indicator",
      "distinct clusters"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <material> <otherscientificterm> <material> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "objective indicator -- CONJUNCTION -- ua narrowing",
      "turbulent inspiratory sounds -- USED-FOR -- linear prediction coding",
      "ua resistance -lrb- r au -rrb- -- USED-FOR -- breath sounds analysis",
      "objective indicator -- USED-FOR -- breath sounds analysis",
      "turbulent breath sounds -- USED-FOR -- sound characteristics",
      "unvoiced speech sounds -- CONJUNCTION -- turbulent breath sounds"
    ],
    "abstract": "the similarities between <material_3> and <otherscientificterm_4> were used to detect change in <otherscientificterm_11> caused by narrowing of the upper airway -lrb- ua -rrb- , similar to that occurring in <otherscientificterm_2> . in 18 awake subjects , <task_1> , an index of <method_10> , was measured simultaneously with <method_7> . <method_9> was applied on <material_5> drawn from <otherscientificterm_0> and k-means was used to cluster the resulting coefficients . the resulting 2 clusters were tested for agreement with the underlying <otherscientificterm_8> . <otherscientificterm_13> were formed when r ua increased relatively high but not in cases with lower rise in r ua -lrb- p < 0.01 for all indicators . -rrb- this is the first work to show the utility of <task_1> in <task_6> confirmed by an <method_12> or <method_10> .",
    "abstract_og": "the similarities between unvoiced speech sounds and turbulent breath sounds were used to detect change in sound characteristics caused by narrowing of the upper airway -lrb- ua -rrb- , similar to that occurring in obstructive sleep apnea . in 18 awake subjects , ua resistance -lrb- r au -rrb- , an index of ua narrowing , was measured simultaneously with breath sounds recording . linear prediction coding was applied on turbulent inspiratory sounds drawn from low and high r au conditions and k-means was used to cluster the resulting coefficients . the resulting 2 clusters were tested for agreement with the underlying r au status . distinct clusters were formed when r ua increased relatively high but not in cases with lower rise in r ua -lrb- p < 0.01 for all indicators . -rrb- this is the first work to show the utility of ua resistance -lrb- r au -rrb- in breath sounds analysis confirmed by an objective indicator or ua narrowing ."
  },
  {
    "title": "A unified approach to real time audio-to-score and audio-to-audio alignment using sequential Montecarlo inference techniques .",
    "entities": [
      "hidden markov model-dynamic time warping based systems",
      "real time alignment of music signals",
      "sequential montecarlo inference techniques",
      "audio-to-score and audio-to-audio alignment",
      "state tracking",
      "alignment problem",
      "hidden state",
      "dynamical system"
    ],
    "types": "<method> <task> <method> <task> <method> <task> <otherscientificterm> <method>",
    "relations": [
      "state tracking -- USED-FOR -- alignment problem",
      "dynamical system -- USED-FOR -- alignment problem"
    ],
    "abstract": "we present a methodology for the <task_1> using <method_2> . the <task_5> is formulated as the <method_4> of a <method_7> , and differs from traditional <method_0> in that the <otherscientificterm_6> is continuous rather than discrete . the major contribution of this paper is addressing both problems of <task_3> within the same framework in a real time setting . performances of the proposed methodology on both problems are then evaluated and discussed .",
    "abstract_og": "we present a methodology for the real time alignment of music signals using sequential montecarlo inference techniques . the alignment problem is formulated as the state tracking of a dynamical system , and differs from traditional hidden markov model-dynamic time warping based systems in that the hidden state is continuous rather than discrete . the major contribution of this paper is addressing both problems of audio-to-score and audio-to-audio alignment within the same framework in a real time setting . performances of the proposed methodology on both problems are then evaluated and discussed ."
  },
  {
    "title": "Duals , Invariants , and the Recognition of Smooth Objects from their Occlucing Contours .",
    "entities": [
      "planar slices of the dual surface",
      "recognizing complex 3-d objects",
      "object representation scheme",
      "weak perspective projection",
      "3-d surface",
      "image contours",
      "real images",
      "synthetic examples",
      "smooth surface",
      "viewing directions",
      "geometric relation",
      "occluding contours",
      "images"
    ],
    "types": "<otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "image contours -- USED-FOR -- object representation scheme",
      "smooth surface -- USED-FOR -- geometric relation",
      "synthetic examples -- USED-FOR -- object representation scheme",
      "weak perspective projection -- FEATURE-OF -- images",
      "synthetic examples -- CONJUNCTION -- image contours"
    ],
    "abstract": "this paper presents a new <otherscientificterm_10> between a solid bounded by a <otherscientificterm_8> and its silhouette in <material_12> formed under <otherscientificterm_3> . the relation has the potential to be used for <task_1> from a single image . objects are mod-eled by showing them to a camera without any knowledge of their motion . the main idea is to consider the dual of the <otherscientificterm_4> and the family of dual curves of the silhouettes over all <otherscientificterm_9> . <otherscientificterm_11> correspond to <otherscientificterm_0> . we introduce an affine-invariant representation of this surface that can constructed from a sequence of <material_12> and allows an object to be recognized from arbitrary <otherscientificterm_9> . we illustrate the proposed <method_2> through <material_7> and <otherscientificterm_5> detected in <material_6> .",
    "abstract_og": "this paper presents a new geometric relation between a solid bounded by a smooth surface and its silhouette in images formed under weak perspective projection . the relation has the potential to be used for recognizing complex 3-d objects from a single image . objects are mod-eled by showing them to a camera without any knowledge of their motion . the main idea is to consider the dual of the 3-d surface and the family of dual curves of the silhouettes over all viewing directions . occluding contours correspond to planar slices of the dual surface . we introduce an affine-invariant representation of this surface that can constructed from a sequence of images and allows an object to be recognized from arbitrary viewing directions . we illustrate the proposed object representation scheme through synthetic examples and image contours detected in real images ."
  },
  {
    "title": "Speechbuilder : facilitating spoken dialogue system development .",
    "entities": [
      "mixed-initiative spoken dialogue systems",
      "spoken dialogue interfaces",
      "human language interfaces",
      "human language technology",
      "linguistic information",
      "relational database",
      "transaction-based applications",
      "structured information",
      "speech-builder"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <task> <otherscientificterm> <method>",
    "relations": [
      "human language interfaces -- USED-FOR -- transaction-based applications"
    ],
    "abstract": "in this paper we report our attempts to facilitate the creation of <method_0> for both novice and experienced developers of <method_3> . our efforts have resulted in the creation of a utility called <method_8> , which allows developers to specify <otherscientificterm_4> about their domains , and rapidly create <otherscientificterm_1> to them . <method_8> has been used to create domains providing access to <otherscientificterm_7> contained in a <material_5> , as well as to provide <otherscientificterm_2> to control or <task_6> .",
    "abstract_og": "in this paper we report our attempts to facilitate the creation of mixed-initiative spoken dialogue systems for both novice and experienced developers of human language technology . our efforts have resulted in the creation of a utility called speech-builder , which allows developers to specify linguistic information about their domains , and rapidly create spoken dialogue interfaces to them . speech-builder has been used to create domains providing access to structured information contained in a relational database , as well as to provide human language interfaces to control or transaction-based applications ."
  },
  {
    "title": "Simultaneous Compaction and Factorization of Sparse Image Motion Matrices .",
    "entities": [
      "image coordinates of point features",
      "regularization of feature trajectories",
      "history-sensitive feature reinitialization method",
      "clean column merging",
      "temporary tracking failure",
      "matrix factorization methods",
      "missing matrix entries",
      "3d reconstruction",
      "image noise",
      "temporary occlusions",
      "tracking systems",
      "motion segmentation",
      "feature snapping",
      "matrix factorization",
      "sparse matrices",
      "object geometry",
      "compaction",
      "factorization",
      "pose",
      "lighting",
      "feature",
      "appearance",
      "trackers",
      "columns",
      "matrices"
    ],
    "types": "<otherscientificterm> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "temporary occlusions -- CONJUNCTION -- lighting",
      "compaction -- CONJUNCTION -- trackers",
      "compaction -- CONJUNCTION -- factorization",
      "matrix factorization methods -- USED-FOR -- motion segmentation",
      "3d reconstruction -- CONJUNCTION -- motion segmentation",
      "temporary occlusions -- CONJUNCTION -- image noise",
      "matrix factorization methods -- USED-FOR -- 3d reconstruction",
      "motion segmentation -- CONJUNCTION -- regularization of feature trajectories"
    ],
    "abstract": "matrices that collect the <otherscientificterm_0> tracked through video -- one column per <otherscientificterm_20> -- have often low rank , either exactly or approximately . this observation has led to many <method_5> for <task_7> , <task_11> , or <task_1> . however , <otherscientificterm_9> , <otherscientificterm_8> , and variations in <otherscientificterm_19> , <otherscientificterm_18> , or <otherscientificterm_15> often confound <method_22> . a <otherscientificterm_20> that reappears after a <otherscientificterm_4> -- whatever the cause -- is regarded as a new <otherscientificterm_20> by typical <method_10> , resulting in very <otherscientificterm_14> with many <otherscientificterm_23> and rendering <method_17> problematic . we propose a method to simultaneously factor and compact such a <method_5> by merging groups of <otherscientificterm_23> that correspond to the same <otherscientificterm_20> into single <otherscientificterm_23> . this combination of <otherscientificterm_16> and <method_17> makes <method_22> more resilient to changes in <otherscientificterm_21> and short occlusions . preliminary experiments show that imputation of <otherscientificterm_6> -- and therefore <method_13> -- becomes significantly more reliable as a result . <method_3> also required us to develop a <method_2> we call <method_12> that aligns merged <otherscientificterm_20> trajectory segments precisely to each other .",
    "abstract_og": "matrices that collect the image coordinates of point features tracked through video -- one column per feature -- have often low rank , either exactly or approximately . this observation has led to many matrix factorization methods for 3d reconstruction , motion segmentation , or regularization of feature trajectories . however , temporary occlusions , image noise , and variations in lighting , pose , or object geometry often confound trackers . a feature that reappears after a temporary tracking failure -- whatever the cause -- is regarded as a new feature by typical tracking systems , resulting in very sparse matrices with many columns and rendering factorization problematic . we propose a method to simultaneously factor and compact such a matrix factorization methods by merging groups of columns that correspond to the same feature into single columns . this combination of compaction and factorization makes trackers more resilient to changes in appearance and short occlusions . preliminary experiments show that imputation of missing matrix entries -- and therefore matrix factorization -- becomes significantly more reliable as a result . clean column merging also required us to develop a history-sensitive feature reinitialization method we call feature snapping that aligns merged feature trajectory segments precisely to each other ."
  },
  {
    "title": "Autocalibrated signal reconstruction from linear measurements using adaptive GAMP .",
    "entities": [
      "joint gain calibration",
      "underdetermined linear measurements",
      "1-based approach",
      "signal estimation",
      "sparse recovery",
      "adaptive gamp",
      "message-passing algorithm",
      "oracle gamp",
      "measurement system"
    ],
    "types": "<task> <otherscientificterm> <method> <task> <task> <method> <method> <method> <method>",
    "relations": [
      "adaptive gamp -- COMPARE -- 1-based approach",
      "adaptive gamp -- HYPONYM-OF -- message-passing algorithm",
      "adaptive gamp -- USED-FOR -- sparse recovery",
      "joint gain calibration -- CONJUNCTION -- signal estimation",
      "message-passing algorithm -- USED-FOR -- joint gain calibration"
    ],
    "abstract": "in this paper , we reconstruct signals from <otherscientificterm_1> where the componentwise gains of the <method_8> are unknown a priori . the reconstruction is performed through an adaptation of the <method_6> called <method_5> that enables <task_0> and <task_3> . to evaluate our <method_5> , we apply <method_5> to the problem of <task_4> and compare <method_5> against an <method_2> . we numerically show that <method_5> yields excellent results even for a moderate amount of data . <method_5> approaches the performance of <method_7> where the gains are perfectly known asymptot-ically .",
    "abstract_og": "in this paper , we reconstruct signals from underdetermined linear measurements where the componentwise gains of the measurement system are unknown a priori . the reconstruction is performed through an adaptation of the message-passing algorithm called adaptive gamp that enables joint gain calibration and signal estimation . to evaluate our adaptive gamp , we apply adaptive gamp to the problem of sparse recovery and compare adaptive gamp against an 1-based approach . we numerically show that adaptive gamp yields excellent results even for a moderate amount of data . adaptive gamp approaches the performance of oracle gamp where the gains are perfectly known asymptot-ically ."
  },
  {
    "title": "Spike sorting at sub-Nyquist rates .",
    "entities": [
      "temporal occurrence of action potentials",
      "sampling neural data",
      "sparse sampling",
      "spike sorting",
      "multi-channel recordings",
      "spike sorting",
      "neural information",
      "sub-nyquist rates"
    ],
    "types": "<otherscientificterm> <task> <task> <method> <material> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "multi-channel recordings -- USED-FOR -- spike sorting",
      "sampling neural data -- USED-FOR -- spike sorting",
      "spike sorting -- USED-FOR -- temporal occurrence of action potentials",
      "neural information -- USED-FOR -- sparse sampling"
    ],
    "abstract": "spike sorting relies on the ability to establish the <otherscientificterm_0> and their relation to specific neurons . <otherscientificterm_6> is intrinsically compressible and as such suitable for <task_2> . potentially , this should allow for the use of <material_4> , which is particularly advantageous to improve <task_5> . in this paper we propose a novel algorithm capable of <task_1> at <otherscientificterm_7> , yielding the same performance for <task_5> as traditional schemes .",
    "abstract_og": "spike sorting relies on the ability to establish the temporal occurrence of action potentials and their relation to specific neurons . neural information is intrinsically compressible and as such suitable for sparse sampling . potentially , this should allow for the use of multi-channel recordings , which is particularly advantageous to improve spike sorting . in this paper we propose a novel algorithm capable of sampling neural data at sub-nyquist rates , yielding the same performance for spike sorting as traditional schemes ."
  },
  {
    "title": "Top-down visual saliency via joint CRF and dictionary learning .",
    "entities": [
      "conditional random field",
      "graz-02 and pascal voc datasets",
      "structured output of crf layer",
      "stochastic gradient descent algorithm",
      "target object localization",
      "top-down saliency model",
      "top-down visual saliency",
      "human fixation prediction",
      "top-down saliency methods",
      "layered structure",
      "dictionary update",
      "crf parameters",
      "intermediate layer",
      "joint learning",
      "sparse coding",
      "sparse codes",
      "image patches",
      "visual dictionary",
      "supervised manner",
      "max-margin approach",
      "bottom",
      "features"
    ],
    "types": "<method> <material> <otherscientificterm> <method> <task> <method> <task> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "top-down saliency model -- COMPARE -- top-down saliency methods",
      "conditional random field -- CONJUNCTION -- sparse coding",
      "layered structure -- PART-OF -- top-down saliency model",
      "graz-02 and pascal voc datasets -- EVALUATE-FOR -- top-down saliency methods",
      "stochastic gradient descent algorithm -- USED-FOR -- max-margin approach",
      "target object localization -- CONJUNCTION -- dictionary update",
      "top-down saliency methods -- USED-FOR -- target object localization",
      "conditional random field -- HYPONYM-OF -- layered structure",
      "sparse coding -- USED-FOR -- crf parameters",
      "sparse coding -- CONJUNCTION -- image patches",
      "sparse codes -- USED-FOR -- conditional random field",
      "top-down saliency model -- COMPARE -- top-down saliency model",
      "top-down saliency model -- USED-FOR -- human fixation prediction",
      "dictionary update -- PART-OF -- top-down saliency model",
      "graz-02 and pascal voc datasets -- EVALUATE-FOR -- top-down saliency model",
      "sparse codes -- USED-FOR -- intermediate layer",
      "conditional random field -- CONJUNCTION -- visual dictionary"
    ],
    "abstract": "top-down visual saliency is an important module of visual attention . in this work , we propose a novel <method_5> that jointly learns a <method_0> and a <material_17> . the proposed <method_5> incorporates a <otherscientificterm_9> from top to <otherscientificterm_20> : <method_0> , <otherscientificterm_14> and <otherscientificterm_16> . by using <otherscientificterm_15> as <otherscientificterm_12> , we learn a <method_0> in a <method_18> with the <otherscientificterm_2> , and meanwhile learn the <otherscientificterm_11> with <otherscientificterm_14> as <otherscientificterm_21> . for efficient and effective <task_13> , we develop a <method_19> via a <method_3> . experimental results on the <material_1> show that our <method_5> performs favorably against the state-of-the-art <method_8> for <task_4> and the <method_10> significantly improves the performance of our <method_5> . in addition , we demonstrate the merits of the proposed <method_5> by applying <method_5> to <task_7> .",
    "abstract_og": "top-down visual saliency is an important module of visual attention . in this work , we propose a novel top-down saliency model that jointly learns a conditional random field and a visual dictionary . the proposed top-down saliency model incorporates a layered structure from top to bottom : conditional random field , sparse coding and image patches . by using sparse codes as intermediate layer , we learn a conditional random field in a supervised manner with the structured output of crf layer , and meanwhile learn the crf parameters with sparse coding as features . for efficient and effective joint learning , we develop a max-margin approach via a stochastic gradient descent algorithm . experimental results on the graz-02 and pascal voc datasets show that our top-down saliency model performs favorably against the state-of-the-art top-down saliency methods for target object localization and the dictionary update significantly improves the performance of our top-down saliency model . in addition , we demonstrate the merits of the proposed top-down saliency model by applying top-down saliency model to human fixation prediction ."
  },
  {
    "title": "Bounds for the mixing parameter within the CC-CMA algorithm applied in non ideal multiuser environments .",
    "entities": [
      "cross-correlation constant modulus algirithm",
      "error performance surface",
      "blind source separation",
      "non-ideal multiuser environments",
      "cc-cma algorithm",
      "channel undermodelling",
      "surface topography",
      "mixing parameter",
      "tighter bounds",
      "simulation studies",
      "equalization",
      "noise"
    ],
    "types": "<method> <otherscientificterm> <task> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm>",
    "relations": [
      "blind source separation -- CONJUNCTION -- non-ideal multiuser environments",
      "surface topography -- USED-FOR -- cc-cma algorithm",
      "cross-correlation constant modulus algirithm -- USED-FOR -- blind source separation",
      "channel undermodelling -- CONJUNCTION -- noise",
      "blind source separation -- CONJUNCTION -- equalization"
    ],
    "abstract": "we derive new bounds for the <otherscientificterm_7> , $ , within the <method_0> for <task_2> and <task_10> in <otherscientificterm_3> . <task_5> and <otherscientificterm_11> are considered when the complex sources are circularly symmetric . these <otherscientificterm_8> are obtained by <otherscientificterm_6> of the <otherscientificterm_1> of the <method_4> , and replace earlier work which suggested that \u00a5 \u00a7 \u00a6 \u00a9 \u00a8 the validity of the bounds is con rmed by <method_9> .",
    "abstract_og": "we derive new bounds for the mixing parameter , $ , within the cross-correlation constant modulus algirithm for blind source separation and equalization in non-ideal multiuser environments . channel undermodelling and noise are considered when the complex sources are circularly symmetric . these tighter bounds are obtained by surface topography of the error performance surface of the cc-cma algorithm , and replace earlier work which suggested that \u00a5 \u00a7 \u00a6 \u00a9 \u00a8 the validity of the bounds is con rmed by simulation studies ."
  },
  {
    "title": "Performance analysis of the GLRT-based array receivers for the detection of a known signal corrupted by noncircular interference .",
    "entities": [
      "likelihood ratio test",
      "false alarm -lrb- p fa -rrb-",
      "data length -rrb- closed-form expression",
      "known signal",
      "closed-form expression",
      "glrt-based receivers",
      "noncircular interference",
      "lrt-based receivers",
      "glrt",
      "detection"
    ],
    "types": "<metric> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric> <task>",
    "relations": [
      "likelihood ratio test -- CONJUNCTION -- glrt"
    ],
    "abstract": "this paper presents a performance analysis of <metric_0> - based and generalized likelihood ratio test -lrb- <metric_8> -rrb- - based array receivers for the <task_9> of a <otherscientificterm_3> corrupted by a potentially <otherscientificterm_6> . studying the distribution of the statistics associated with the <metric_0> and <metric_8> , expressions of the probability of <task_9> -lrb- p d -rrb- and <metric_1> are given . in particular , an exact <otherscientificterm_4> of p d and p fa are given for two <method_7> and asymptotic -lrb- with respect to the <otherscientificterm_2> are given for p d and p fa for four <otherscientificterm_5> . finally illustrative examples are presented in order to strengthen the obtained results .",
    "abstract_og": "this paper presents a performance analysis of likelihood ratio test - based and generalized likelihood ratio test -lrb- glrt -rrb- - based array receivers for the detection of a known signal corrupted by a potentially noncircular interference . studying the distribution of the statistics associated with the likelihood ratio test and glrt , expressions of the probability of detection -lrb- p d -rrb- and false alarm -lrb- p fa -rrb- are given . in particular , an exact closed-form expression of p d and p fa are given for two lrt-based receivers and asymptotic -lrb- with respect to the data length -rrb- closed-form expression are given for p d and p fa for four glrt-based receivers . finally illustrative examples are presented in order to strengthen the obtained results ."
  },
  {
    "title": "Optimal subset selection for adaptive signal representation .",
    "entities": [
      "signal representation",
      "over-complete dictionaries",
      "signal decomposition",
      "over-complete dictionary",
      "data vector",
      "cosine packets",
      "computational complexity",
      "wave packets",
      "sparsity",
      "wavelets"
    ],
    "types": "<method> <material> <task> <material> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "wave packets -- HYPONYM-OF -- over-complete dictionaries",
      "cosine packets -- HYPONYM-OF -- over-complete dictionaries",
      "computational complexity -- EVALUATE-FOR -- signal representation",
      "sparsity -- USED-FOR -- signal representation",
      "wavelets -- CONJUNCTION -- wave packets",
      "wave packets -- CONJUNCTION -- cosine packets",
      "wavelets -- HYPONYM-OF -- over-complete dictionaries"
    ],
    "abstract": "recently , a number of <material_1> such as <otherscientificterm_9> , <otherscientificterm_7> , <otherscientificterm_5> etc. have been proposed . <task_2> on such <material_1> is not unique . this non-uniqueness provides us with the opportunity to adapt the <method_0> to the signal . the adaptation is based on <otherscientificterm_8> , resolution and stability of the <method_0> . the <metric_6> of the <method_0> is of primary concern . we propose a new approach for identifying the sparsest representation of a given signal in terms of a given <material_3> . we assume that the <otherscientificterm_4> can be exactly represented in terms of a known number of vectors .",
    "abstract_og": "recently , a number of over-complete dictionaries such as wavelets , wave packets , cosine packets etc. have been proposed . signal decomposition on such over-complete dictionaries is not unique . this non-uniqueness provides us with the opportunity to adapt the signal representation to the signal . the adaptation is based on sparsity , resolution and stability of the signal representation . the computational complexity of the signal representation is of primary concern . we propose a new approach for identifying the sparsest representation of a given signal in terms of a given over-complete dictionary . we assume that the data vector can be exactly represented in terms of a known number of vectors ."
  },
  {
    "title": "Feature-Rich Two-Stage Logistic Regression for Monolingual Alignment .",
    "entities": [
      "two-stage logistic regression framework",
      "short text snippets",
      "f 1 scores",
      "supervised aligner",
      "monolingual alignment",
      "extrinsic evaluation",
      "error reductions",
      "semantic units",
      "features"
    ],
    "types": "<method> <material> <metric> <method> <task> <task> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "features -- USED-FOR -- monolingual alignment",
      "two-stage logistic regression framework -- USED-FOR -- monolingual alignment",
      "features -- USED-FOR -- two-stage logistic regression framework",
      "short text snippets -- USED-FOR -- supervised aligner",
      "error reductions -- EVALUATE-FOR -- supervised aligner"
    ],
    "abstract": "monolingual <task_4> is the task of pair-ing semantically similar units from two pieces of text . we report a top-performing <method_3> that operates on <material_1> . we employ a large feature set to -lrb- 1 -rrb- encode similarities among <otherscientificterm_7> -lrb- words and named entities -rrb- in context , and -lrb- 2 -rrb- address cooperation and competition for <task_4> among units in the same snippet . these <otherscientificterm_8> are deployed in a <method_0> for <task_4> . on two benchmark data sets , our <method_3> achieves <metric_2> of 92.1 % and 88.5 % , with statistically significant <metric_6> of 4.8 % and 7.3 % over the previous best <method_3> . <method_3> produces top results in <task_5> as well .",
    "abstract_og": "monolingual monolingual alignment is the task of pair-ing semantically similar units from two pieces of text . we report a top-performing supervised aligner that operates on short text snippets . we employ a large feature set to -lrb- 1 -rrb- encode similarities among semantic units -lrb- words and named entities -rrb- in context , and -lrb- 2 -rrb- address cooperation and competition for monolingual alignment among units in the same snippet . these features are deployed in a two-stage logistic regression framework for monolingual alignment . on two benchmark data sets , our supervised aligner achieves f 1 scores of 92.1 % and 88.5 % , with statistically significant error reductions of 4.8 % and 7.3 % over the previous best supervised aligner . supervised aligner produces top results in extrinsic evaluation as well ."
  },
  {
    "title": "Speaker adaptation based on confidence-weighted training .",
    "entities": [
      "weighted smap",
      "speaker adaptation algorithm",
      "native speaker models",
      "model adaptation methods",
      "nonnative speaker environment",
      "discriminative adaptation procedure",
      "confidence measure",
      "parameter tying",
      "adaptation data",
      "sigmoid weight-ing",
      "non-linear weighting",
      "outliers",
      "tidigit"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <method> <metric> <method> <material> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "confidence measure -- USED-FOR -- speaker adaptation algorithm",
      "non-linear weighting -- USED-FOR -- discriminative adaptation procedure",
      "discriminative adaptation procedure -- USED-FOR -- speaker adaptation algorithm",
      "confidence measure -- USED-FOR -- sigmoid weight-ing",
      "native speaker models -- USED-FOR -- nonnative speaker environment",
      "confidence measure -- USED-FOR -- discriminative adaptation procedure",
      "adaptation data -- USED-FOR -- model adaptation methods",
      "confidence measure -- CONJUNCTION -- non-linear weighting",
      "non-linear weighting -- USED-FOR -- speaker adaptation algorithm"
    ],
    "abstract": "this paper presents a novel method to enhance the performance of traditional <method_1> using <method_5> based on a novel <metric_6> and <method_10> . regardless of the distribution of the <material_8> , traditional <method_3> incorporate the <material_8> undiscriminatingly . when the data size is small and the <method_7> is extensive , adaptation based on <otherscientificterm_11> can be detrimental . a way to discriminate the contribution of each data in the adaptation is to incorporate a <metric_6> based on likelihood . we evaluate and compare the performances of the proposed <method_0> which controls the contribution of each data by <otherscientificterm_9> using a novel <metric_6> . the effectiveness of the proposed algorithm is experimentally verified by adapting <method_2> to <otherscientificterm_4> using <method_12> .",
    "abstract_og": "this paper presents a novel method to enhance the performance of traditional speaker adaptation algorithm using discriminative adaptation procedure based on a novel confidence measure and non-linear weighting . regardless of the distribution of the adaptation data , traditional model adaptation methods incorporate the adaptation data undiscriminatingly . when the data size is small and the parameter tying is extensive , adaptation based on outliers can be detrimental . a way to discriminate the contribution of each data in the adaptation is to incorporate a confidence measure based on likelihood . we evaluate and compare the performances of the proposed weighted smap which controls the contribution of each data by sigmoid weight-ing using a novel confidence measure . the effectiveness of the proposed algorithm is experimentally verified by adapting native speaker models to nonnative speaker environment using tidigit ."
  },
  {
    "title": "Relational Random Forests Based on Random Relational Rules .",
    "entities": [
      "random forests over relational data",
      "r 4 f",
      "propositional learning",
      "ensemble size",
      "tree initialization",
      "random forests",
      "computational complexity",
      "static proposition-alization",
      "dynamic propositionalization",
      "relational data",
      "forf",
      "node",
      "tree"
    ],
    "types": "<material> <method> <task> <otherscientificterm> <task> <material> <metric> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "random forests -- USED-FOR -- relational data",
      "forf -- COMPARE -- static proposition-alization",
      "relational data -- USED-FOR -- random forests",
      "random forests -- USED-FOR -- propositional learning",
      "forf -- HYPONYM-OF -- random forests",
      "random forests -- CONJUNCTION -- static proposition-alization"
    ],
    "abstract": "random forests have been shown to perform very well in <task_2> . <method_10> is an upgrade of <material_5> for <material_9> . in this paper we investigate shortcomings of <method_10> and propose an alternative algorithm , <method_1> , for generating <material_0> . <method_1> employs randomly generated relational rules as fully self-contained boolean tests inside each <otherscientificterm_11> in a <otherscientificterm_12> and thus can be viewed as an instance of <otherscientificterm_8> . the implementation of <method_1> allows for the simultaneous or parallel growth of all the branches of all the trees in the ensemble in an efficient shared , but still single-threaded way . experiments favorably compare <method_1> to both <method_10> and the combination of <otherscientificterm_7> together with standard <material_5> . various strategies for <task_4> and splitting of nodes , as well as resulting <otherscientificterm_3> , diversity , and <metric_6> of <method_1> are also investigated .",
    "abstract_og": "random forests have been shown to perform very well in propositional learning . forf is an upgrade of random forests for relational data . in this paper we investigate shortcomings of forf and propose an alternative algorithm , r 4 f , for generating random forests over relational data . r 4 f employs randomly generated relational rules as fully self-contained boolean tests inside each node in a tree and thus can be viewed as an instance of dynamic propositionalization . the implementation of r 4 f allows for the simultaneous or parallel growth of all the branches of all the trees in the ensemble in an efficient shared , but still single-threaded way . experiments favorably compare r 4 f to both forf and the combination of static proposition-alization together with standard random forests . various strategies for tree initialization and splitting of nodes , as well as resulting ensemble size , diversity , and computational complexity of r 4 f are also investigated ."
  },
  {
    "title": "Semi-continuous segmental probability model for speech signals .",
    "entities": [
      "continuous segmental probability model",
      "semi-continuous segmental probability model",
      "segmental probability modeling of speech signals",
      "continuous output probability density functions",
      "continuous mixture segmental probability model",
      "continuous segmental probability model",
      "semi-continuous segmental probability model",
      "semi-continuous hidden markov model",
      "mixture gaussian density codebook",
      "vector quantization codebook",
      "segmental probability model",
      "unified modeling approach",
      "vector quantization",
      "model/codebook combination",
      "computational complexity",
      "recognition accuracy"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <method> <method> <method> <method> <method> <method> <method> <method> <task> <method> <metric> <metric>",
    "relations": [
      "continuous output probability density functions -- CONJUNCTION -- mixture gaussian density codebook",
      "vector quantization codebook -- CONJUNCTION -- segmental probability model",
      "computational complexity -- EVALUATE-FOR -- semi-continuous segmental probability model",
      "continuous mixture segmental probability model -- USED-FOR -- semi-continuous segmental probability model",
      "unified modeling approach -- USED-FOR -- segmental probability modeling of speech signals",
      "recognition accuracy -- EVALUATE-FOR -- semi-continuous segmental probability model",
      "recognition accuracy -- EVALUATE-FOR -- semi-continuous hidden markov model",
      "semi-continuous segmental probability model -- COMPARE -- semi-continuous hidden markov model",
      "vector quantization -- CONJUNCTION -- segmental probability modeling of speech signals",
      "semi-continuous segmental probability model -- COMPARE -- continuous segmental probability model",
      "unified modeling approach -- USED-FOR -- vector quantization",
      "semi-continuous hidden markov model -- CONJUNCTION -- continuous segmental probability model",
      "vector quantization codebook -- USED-FOR -- model/codebook combination",
      "continuous output probability density functions -- CONJUNCTION -- continuous mixture segmental probability model"
    ],
    "abstract": "a <method_6> , which can be considered as a special form of <method_4> with <otherscientificterm_3> sharing in a <method_8> , is proposed in this paper . the amount of training data required , as well as the <metric_14> of the <method_1> -lsb- 2 -rsb- , can be significantly reduced in comparison with the <method_0> . parameters of the <method_9> and <method_10> can be mutually optimized to achieve an optimal <method_13> , which leads to a <method_11> to <task_12> and <task_2> . the experimental results show that the <metric_15> of the <method_6> is higher than the <method_7> and <method_5> .",
    "abstract_og": "a semi-continuous segmental probability model , which can be considered as a special form of continuous mixture segmental probability model with continuous output probability density functions sharing in a mixture gaussian density codebook , is proposed in this paper . the amount of training data required , as well as the computational complexity of the semi-continuous segmental probability model -lsb- 2 -rsb- , can be significantly reduced in comparison with the continuous segmental probability model . parameters of the vector quantization codebook and segmental probability model can be mutually optimized to achieve an optimal model/codebook combination , which leads to a unified modeling approach to vector quantization and segmental probability modeling of speech signals . the experimental results show that the recognition accuracy of the semi-continuous segmental probability model is higher than the semi-continuous hidden markov model and continuous segmental probability model ."
  },
  {
    "title": "Tractable Reasoning with Incomplete First-Order Knowledge in Dynamic Systems with Context-Dependent Actions .",
    "entities": [
      "knowledge base",
      "incomplete first-order knowledge",
      "reasoning problem",
      "projection problem",
      "action sequence",
      "dynamic systems",
      "context-dependent actions",
      "progression"
    ],
    "types": "<material> <otherscientificterm> <task> <task> <material> <method> <otherscientificterm> <method>",
    "relations": [
      "incomplete first-order knowledge -- CONJUNCTION -- context-dependent actions"
    ],
    "abstract": "a basic <task_2> in <method_5> is the <task_3> : determine if a formula holds after a sequence of actions has been performed . in this paper , we propose a tractable 1 solution to the <task_3> in the presence of <otherscientificterm_1> and <otherscientificterm_6> . our solution is based on a type of <method_7> , that is , we progress the initial <material_0> wrt the <material_4> and answer the query against the resulting <material_0> . the form of reasoning we propose is always logically sound and is also logically complete when the query is in a certain normal form and the agent has complete knowledge about the context of any <otherscientificterm_6> .",
    "abstract_og": "a basic reasoning problem in dynamic systems is the projection problem : determine if a formula holds after a sequence of actions has been performed . in this paper , we propose a tractable 1 solution to the projection problem in the presence of incomplete first-order knowledge and context-dependent actions . our solution is based on a type of progression , that is , we progress the initial knowledge base wrt the action sequence and answer the query against the resulting knowledge base . the form of reasoning we propose is always logically sound and is also logically complete when the query is in a certain normal form and the agent has complete knowledge about the context of any context-dependent actions ."
  },
  {
    "title": "Minimum trajectory error training for deep neural networks , combined with stacked bottleneck features .",
    "entities": [
      "dynamic and continuous nature of speech parameters",
      "deep neural networks",
      "dnn parameter estimation methods",
      "speech parameter trajectory errors",
      "statistical parametric speech synthesis",
      "mean squared error",
      "stacked bottleneck features",
      "wide linguistic context",
      "wide acoustic context",
      "synthesised speech",
      "dynamic constraints",
      "model accuracy",
      "training data",
      "training criterion",
      "linguistic features",
      "acoustic model"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <metric> <material> <method> <otherscientificterm> <method>",
    "relations": [
      "linguistic features -- USED-FOR -- acoustic model",
      "training criterion -- USED-FOR -- speech parameter trajectory errors",
      "deep neural networks -- USED-FOR -- acoustic model",
      "training criterion -- COMPARE -- stacked bottleneck features",
      "deep neural networks -- USED-FOR -- statistical parametric speech synthesis",
      "acoustic model -- USED-FOR -- statistical parametric speech synthesis"
    ],
    "abstract": "recently , <method_1> have shown promise as an <method_15> for <task_4> . their ability to learn complex mappings from <otherscientificterm_14> to <method_15> has advanced the naturalness of synthesis speech significantly . however , because <method_2> typically attempt to minimise the <otherscientificterm_5> of each individual frame in the <material_12> , the <otherscientificterm_0> is neglected . in this paper , we propose a <method_13> that minimises <otherscientificterm_3> , and so takes <otherscientificterm_10> from a <otherscientificterm_8> into account during training . we combine this novel <method_13> with our previously proposed <otherscientificterm_6> , which provide <otherscientificterm_7> . both objective and subjective evaluation results confirm the effectiveness of the proposed <method_13> for improving <metric_11> and naturalness of <material_9> .",
    "abstract_og": "recently , deep neural networks have shown promise as an acoustic model for statistical parametric speech synthesis . their ability to learn complex mappings from linguistic features to acoustic model has advanced the naturalness of synthesis speech significantly . however , because dnn parameter estimation methods typically attempt to minimise the mean squared error of each individual frame in the training data , the dynamic and continuous nature of speech parameters is neglected . in this paper , we propose a training criterion that minimises speech parameter trajectory errors , and so takes dynamic constraints from a wide acoustic context into account during training . we combine this novel training criterion with our previously proposed stacked bottleneck features , which provide wide linguistic context . both objective and subjective evaluation results confirm the effectiveness of the proposed training criterion for improving model accuracy and naturalness of synthesised speech ."
  },
  {
    "title": "An integrated background model for video surveillance based on primal sketch and 3D scene geometry .",
    "entities": [
      "reflections of moving objects",
      "integrated video surveillance system",
      "locations of foreground blobs",
      "object and tra-jectory level",
      "background modeling methods",
      "primal sketch representation",
      "integrated background model",
      "camera exposure adjusting",
      "3d scene geometry",
      "3d scene",
      "background image",
      "pixel level",
      "geometric information",
      "false alarms",
      "video surveillance",
      "image primitives",
      "lbp histograms",
      "ground plane",
      "horizontal surfaces",
      "ground",
      "walls",
      "stairs",
      "shadows",
      "regions",
      "gaussians",
      "highlights"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "ground -- CONJUNCTION -- walls",
      "integrated background model -- USED-FOR -- video surveillance",
      "ground -- CONJUNCTION -- horizontal surfaces",
      "horizontal surfaces -- CONJUNCTION -- stairs",
      "walls -- CONJUNCTION -- stairs",
      "integrated background model -- COMPARE -- background modeling methods",
      "shadows -- CONJUNCTION -- highlights",
      "integrated background model -- COMPARE -- integrated video surveillance system",
      "horizontal surfaces -- CONJUNCTION -- walls",
      "object and tra-jectory level -- FEATURE-OF -- integrated video surveillance system",
      "image primitives -- CONJUNCTION -- lbp histograms",
      "gaussians -- CONJUNCTION -- image primitives",
      "integrated background model -- USED-FOR -- background image",
      "highlights -- CONJUNCTION -- reflections of moving objects",
      "primal sketch representation -- USED-FOR -- integrated background model"
    ],
    "abstract": "this paper presents a novel <method_6> for <task_14> . our <method_6> uses a <method_5> for image appearance and <task_8> to capture the <otherscientificterm_17> and major surfaces in the scene . the <method_6> divides the <otherscientificterm_10> into three types of <otherscientificterm_23> -- flat , sketchable and textured . the three types of <otherscientificterm_23> are modeled respectively by mixture of <method_24> , <otherscientificterm_15> and <method_16> . we calibrate the camera and recover important planes such as <otherscientificterm_19> , <otherscientificterm_18> , <otherscientificterm_20> , <otherscientificterm_21> in the <otherscientificterm_9> , and use <otherscientificterm_12> to predict the sizes and <otherscientificterm_2> to further reduce <otherscientificterm_13> . compared with the state-of-the-art <method_4> , our <method_6> is more effective , especially for indoor scenes where <otherscientificterm_22> , <otherscientificterm_25> and <otherscientificterm_0> and <otherscientificterm_7> usually cause problems . experiment results demonstrate that our <method_6> improves the performance of background/foreground separation at <otherscientificterm_11> , and the <method_1> at the <otherscientificterm_3> .",
    "abstract_og": "this paper presents a novel integrated background model for video surveillance . our integrated background model uses a primal sketch representation for image appearance and 3d scene geometry to capture the ground plane and major surfaces in the scene . the integrated background model divides the background image into three types of regions -- flat , sketchable and textured . the three types of regions are modeled respectively by mixture of gaussians , image primitives and lbp histograms . we calibrate the camera and recover important planes such as ground , horizontal surfaces , walls , stairs in the 3d scene , and use geometric information to predict the sizes and locations of foreground blobs to further reduce false alarms . compared with the state-of-the-art background modeling methods , our integrated background model is more effective , especially for indoor scenes where shadows , highlights and reflections of moving objects and camera exposure adjusting usually cause problems . experiment results demonstrate that our integrated background model improves the performance of background/foreground separation at pixel level , and the integrated video surveillance system at the object and tra-jectory level ."
  },
  {
    "title": "Predictive state vector encoding for decentralized field estimation in sensor networks .",
    "entities": [
      "differential encoding of state vectors",
      "decentralized physics-based field estimation",
      "kalman prediction step",
      "spatio-temporal field dependencies",
      "clustered sensor networks",
      "acoustic field",
      "state-space equations",
      "communication overhead",
      "computational complexity",
      "encoding"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "differential encoding of state vectors -- USED-FOR -- communication overhead",
      "state-space equations -- USED-FOR -- kalman prediction step"
    ],
    "abstract": "decentralized physics-based field estimation in <method_4> requires the exchange of state vectors between neighboring clusters . we reduce the <otherscientificterm_7> between clusters by using a <method_0> that exploits the <otherscientificterm_3> . this <otherscientificterm_9> involves a <method_2> that builds on the <method_6> governing the field 's spatio-temporal evolution . the <method_2> keeps the <metric_8> low . simulation results for an <otherscientificterm_5> demonstrate the approach .",
    "abstract_og": "decentralized physics-based field estimation in clustered sensor networks requires the exchange of state vectors between neighboring clusters . we reduce the communication overhead between clusters by using a differential encoding of state vectors that exploits the spatio-temporal field dependencies . this encoding involves a kalman prediction step that builds on the state-space equations governing the field 's spatio-temporal evolution . the kalman prediction step keeps the computational complexity low . simulation results for an acoustic field demonstrate the approach ."
  },
  {
    "title": "A novel quantization watermarking scheme by modulating the normalized correlation .",
    "entities": [
      "quantization based watermarking scheme",
      "modulated normalized correlation",
      "stronger noise",
      "watermarked signal",
      "normalized correlation",
      "embedding distortion",
      "well-known spread",
      "watermark embedding",
      "real images",
      "dither modulation",
      "numerical simulations",
      "host vector",
      "random vector",
      "valumet-ric scaling",
      "robustness"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <metric>",
    "relations": [
      "robustness -- EVALUATE-FOR -- quantization based watermarking scheme",
      "host vector -- CONJUNCTION -- random vector",
      "watermarked signal -- USED-FOR -- modulated normalized correlation",
      "quantization based watermarking scheme -- USED-FOR -- stronger noise",
      "dither modulation -- FEATURE-OF -- random vector"
    ],
    "abstract": "this paper presents a novel <method_0> . <method_7> is performed through modulating the <otherscientificterm_4> between the <otherscientificterm_11> and a <otherscientificterm_12> with <otherscientificterm_9> . the <otherscientificterm_3> is derived to provide the <otherscientificterm_1> in the sense of minimizing the <otherscientificterm_5> . the proposed <method_0> is theoretically invariant to <task_13> and can resist <otherscientificterm_2> than the <otherscientificterm_6> transform <otherscientificterm_9> . <method_10> on <material_8> show that <method_0> achieves the good imperceptibility and strong <metric_14> against a wide range of attacks .",
    "abstract_og": "this paper presents a novel quantization based watermarking scheme . watermark embedding is performed through modulating the normalized correlation between the host vector and a random vector with dither modulation . the watermarked signal is derived to provide the modulated normalized correlation in the sense of minimizing the embedding distortion . the proposed quantization based watermarking scheme is theoretically invariant to valumet-ric scaling and can resist stronger noise than the well-known spread transform dither modulation . numerical simulations on real images show that quantization based watermarking scheme achieves the good imperceptibility and strong robustness against a wide range of attacks ."
  },
  {
    "title": "Expected Error Analysis for Model Selection .",
    "entities": [
      "eecient model selection algorithm",
      "large-scale text categorization problem",
      "10-fold cross validation",
      "boolean decision trees",
      "model selection problems",
      "error rates",
      "generalization performance",
      "precision"
    ],
    "types": "<method> <task> <method> <material> <task> <metric> <metric> <metric>",
    "relations": [
      "error rates -- EVALUATE-FOR -- eecient model selection algorithm",
      "boolean decision trees -- CONJUNCTION -- large-scale text categorization problem"
    ],
    "abstract": "in order to select a good hypothesis language -lrb- or model -rrb- from a collection of possible models , one has to assess the <metric_6> of the hypothesis which is returned by a learner that is bound to use some particular model . this paper deals with a new and very eecient way of assessing this <metric_6> . we present a new analysis which characterizes the expected generalization error of the hypothesis with least training error in terms of the distribution of <metric_5> of the hypotheses in the model . this distribution can be estimated very eeciently from the data which immediately leads to an <method_0> . the analysis predicts learning curves with a very high <metric_7> and thus contributes to a better understanding of why and when over-tting occurs . we present empirical studies -lrb- controlled experiments on <material_3> and a <task_1> -rrb- which show that the <method_0> leads to <metric_5> which are often as low as those obtained by <method_2> -lrb- sometimes even superior -rrb- . however , the <method_0> is much more eecient -lrb- because the learner does not have to be invoked at all -rrb- and thus solves <task_4> with as many as thousand relevant attributes and 12,000 examples .",
    "abstract_og": "in order to select a good hypothesis language -lrb- or model -rrb- from a collection of possible models , one has to assess the generalization performance of the hypothesis which is returned by a learner that is bound to use some particular model . this paper deals with a new and very eecient way of assessing this generalization performance . we present a new analysis which characterizes the expected generalization error of the hypothesis with least training error in terms of the distribution of error rates of the hypotheses in the model . this distribution can be estimated very eeciently from the data which immediately leads to an eecient model selection algorithm . the analysis predicts learning curves with a very high precision and thus contributes to a better understanding of why and when over-tting occurs . we present empirical studies -lrb- controlled experiments on boolean decision trees and a large-scale text categorization problem -rrb- which show that the eecient model selection algorithm leads to error rates which are often as low as those obtained by 10-fold cross validation -lrb- sometimes even superior -rrb- . however , the eecient model selection algorithm is much more eecient -lrb- because the learner does not have to be invoked at all -rrb- and thus solves model selection problems with as many as thousand relevant attributes and 12,000 examples ."
  },
  {
    "title": "An unmixing-based method for the analysis of thermal hyperspectral images .",
    "entities": [
      "estimation of surface emissivity",
      "high spectral resolution sensor",
      "hyperspectral sensor configurations",
      "thermal hyperspectral data",
      "black body law",
      "mixed pixel",
      "noise conditions",
      "simulated data",
      "temperature",
      "pixel",
      "accuracy"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "temperature -- FEATURE-OF -- pixel"
    ],
    "abstract": "the <task_0> and <otherscientificterm_8> from <material_3> is a challenge . methods that estimate the <otherscientificterm_8> and emissivity on a <otherscientificterm_9> composed by one single material exist . however , the estimation of the <otherscientificterm_8> on a <otherscientificterm_5> , i.e. a <otherscientificterm_9> composed by more than one material , is more complex and has scarcely been investigated in the literature . this paper addresses this issue by proposing an estimator which linearizes the <otherscientificterm_4> around the mean <otherscientificterm_8> of each material . the performance of this estimator is studied using <material_7> with different <otherscientificterm_2> and under various <otherscientificterm_6> . the obtained results are encouraging and show an <metric_10> on the estimated <otherscientificterm_8> of 0.5 k while using <otherscientificterm_1> .",
    "abstract_og": "the estimation of surface emissivity and temperature from thermal hyperspectral data is a challenge . methods that estimate the temperature and emissivity on a pixel composed by one single material exist . however , the estimation of the temperature on a mixed pixel , i.e. a pixel composed by more than one material , is more complex and has scarcely been investigated in the literature . this paper addresses this issue by proposing an estimator which linearizes the black body law around the mean temperature of each material . the performance of this estimator is studied using simulated data with different hyperspectral sensor configurations and under various noise conditions . the obtained results are encouraging and show an accuracy on the estimated temperature of 0.5 k while using high spectral resolution sensor ."
  },
  {
    "title": "Reducing the footprint of the IBM trainable speech synthesis system .",
    "entities": [
      "spectral acoustic feature based speech representation",
      "small footprint formant based synthesizers",
      "ibm trainable speech synthesis system",
      "concatenative speech synthesis",
      "concatenative text-to-speech system",
      "cost function",
      "dataset size",
      "speech generation",
      "segment selection"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "spectral acoustic feature based speech representation -- USED-FOR -- cost function",
      "ibm trainable speech synthesis system -- HYPONYM-OF -- concatenative text-to-speech system",
      "dataset size -- FEATURE-OF -- concatenative text-to-speech system",
      "spectral acoustic feature based speech representation -- USED-FOR -- segment selection"
    ],
    "abstract": "this paper presents a novel approach for <task_3> . this approach enables reduction of the <otherscientificterm_6> of a <method_4> , namely the <method_2> , by more than an order of magnitude . a <method_0> is used for computing a <otherscientificterm_5> during <task_8> as well as for <task_7> . initial results indicate that even with a <otherscientificterm_6> of a few megabytes it is possible to achieve quality which is significantly higher than existing <otherscientificterm_1> .",
    "abstract_og": "this paper presents a novel approach for concatenative speech synthesis . this approach enables reduction of the dataset size of a concatenative text-to-speech system , namely the ibm trainable speech synthesis system , by more than an order of magnitude . a spectral acoustic feature based speech representation is used for computing a cost function during segment selection as well as for speech generation . initial results indicate that even with a dataset size of a few megabytes it is possible to achieve quality which is significantly higher than existing small footprint formant based synthesizers ."
  },
  {
    "title": "Query Weighting for Ranking Model Adaptation .",
    "entities": [
      "letor3 .0 data set",
      "document instance weighting methods",
      "ranking model adaptation",
      "query weighting schemes",
      "fine-grained similarity values",
      "compressing query data",
      "query feature vector",
      "query weighting",
      "document level",
      "ranking algorithms",
      "query weighting",
      "query importance",
      "information loss"
    ],
    "types": "<material> <method> <task> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <metric> <otherscientificterm>",
    "relations": [
      "query weighting -- COMPARE -- document instance weighting methods",
      "compressing query data -- USED-FOR -- query importance",
      "query feature vector -- USED-FOR -- query weighting",
      "query weighting -- USED-FOR -- ranking model adaptation",
      "query weighting schemes -- USED-FOR -- query importance",
      "letor3 .0 data set -- EVALUATE-FOR -- query weighting",
      "letor3 .0 data set -- EVALUATE-FOR -- document instance weighting methods"
    ],
    "abstract": "we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available , which is referred to as <method_7> . <task_10> is a key step in <task_2> . as the learning object of <method_9> is divided by query instances , we argue that it 's more reasonable to conduct importance weighting at query level than <otherscientificterm_8> . we present two <method_3> . the first compresses the query into a <otherscientificterm_6> , which aggregates all document instances in the same query , and then conducts <method_7> based on the <otherscientificterm_6> . this <method_3> can efficiently estimate <metric_11> by <task_5> , but the potential risk is <otherscientificterm_12> resulted from the compression . the second measures the similarity between the source query and each target query , and then combines these <otherscientificterm_4> for its importance estimation . adaptation experiments on <material_0> demonstrate that <method_7> significantly outperforms <method_1> .",
    "abstract_og": "we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available , which is referred to as query weighting . query weighting is a key step in ranking model adaptation . as the learning object of ranking algorithms is divided by query instances , we argue that it 's more reasonable to conduct importance weighting at query level than document level . we present two query weighting schemes . the first compresses the query into a query feature vector , which aggregates all document instances in the same query , and then conducts query weighting based on the query feature vector . this query weighting schemes can efficiently estimate query importance by compressing query data , but the potential risk is information loss resulted from the compression . the second measures the similarity between the source query and each target query , and then combines these fine-grained similarity values for its importance estimation . adaptation experiments on letor3 .0 data set demonstrate that query weighting significantly outperforms document instance weighting methods ."
  },
  {
    "title": "A comparison of human and automatic musical genre classification .",
    "entities": [
      "mel-frequency cepstral coefficients",
      "subjective perception of genre definitions",
      "automatic musical genre classification",
      "genre classification systems",
      "human genre classification",
      "auditory model",
      "genre classification",
      "human annotation",
      "audio format",
      "automatic algorithms",
      "musical content",
      "features"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <task> <method> <task> <otherscientificterm> <material> <method> <material> <otherscientificterm>",
    "relations": [
      "auditory model -- USED-FOR -- features",
      "mel-frequency cepstral coefficients -- USED-FOR -- features",
      "automatic algorithms -- CONJUNCTION -- human annotation",
      "mel-frequency cepstral coefficients -- USED-FOR -- auditory model"
    ],
    "abstract": "recently there has been an increasing amount of work in the area of automatic <task_6> of music in <material_8> . such systems can be used as a way to evaluate <otherscientificterm_11> describing <material_10> as well as a way to structure large collections of music . however the evaluation and comparison of <method_3> is hindered by the <otherscientificterm_1> by users . in this work we describe a set of experiments in <task_2> . an important contribution of this work is the comparison of the automatic results with <task_4> on the same dataset . the results show that , although there is significant room for improvement , <task_6> is inherently subjective and therefore perfect results can not be expected from either <method_9> or <otherscientificterm_7> . the experiments also show that the use of <otherscientificterm_11> derived from an <method_5> have similar performance with <otherscientificterm_11> based on <method_0> .",
    "abstract_og": "recently there has been an increasing amount of work in the area of automatic genre classification of music in audio format . such systems can be used as a way to evaluate features describing musical content as well as a way to structure large collections of music . however the evaluation and comparison of genre classification systems is hindered by the subjective perception of genre definitions by users . in this work we describe a set of experiments in automatic musical genre classification . an important contribution of this work is the comparison of the automatic results with human genre classification on the same dataset . the results show that , although there is significant room for improvement , genre classification is inherently subjective and therefore perfect results can not be expected from either automatic algorithms or human annotation . the experiments also show that the use of features derived from an auditory model have similar performance with features based on mel-frequency cepstral coefficients ."
  },
  {
    "title": "Speaking rate compensation based on likelihood criterion in acoustic model training and decoding .",
    "entities": [
      "spontaneous lecture speech recognition task",
      "acoustic model creation process",
      "speaking rate compensation method",
      "speaking rate compensation",
      "frame length adaptation",
      "frame period",
      "decoding process",
      "insertion penalty",
      "acoustic likelihood",
      "viterbi alignment",
      "speech analysis",
      "acoustic model",
      "input utterance",
      "language likelihood",
      "accuracy"
    ],
    "types": "<task> <method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <task> <method> <material> <otherscientificterm> <metric>",
    "relations": [
      "viterbi alignment -- USED-FOR -- acoustic likelihood",
      "insertion penalty -- EVALUATE-FOR -- speaking rate compensation method",
      "frame period -- CONJUNCTION -- language likelihood",
      "speaking rate compensation -- USED-FOR -- acoustic model creation process",
      "speaking rate compensation method -- USED-FOR -- acoustic model",
      "language likelihood -- CONJUNCTION -- insertion penalty",
      "frame period -- USED-FOR -- acoustic likelihood",
      "frame period -- USED-FOR -- speaking rate compensation method",
      "frame length adaptation -- USED-FOR -- speaking rate compensation method",
      "frame period -- CONJUNCTION -- frame length adaptation",
      "speaking rate compensation -- USED-FOR -- spontaneous lecture speech recognition task",
      "acoustic model creation process -- CONJUNCTION -- decoding process"
    ],
    "abstract": "in this paper , we propose a <method_2> using <otherscientificterm_5> and <method_4> . our <method_2> decodes an <material_12> using several sets of <otherscientificterm_5> and frame length parameters for <task_10> . then , this <method_2> selects the best set with the highest score which consists of the <otherscientificterm_8> normalized by <otherscientificterm_5> , <otherscientificterm_13> and <otherscientificterm_7> . furthermore , we apply this <method_2> to the training of the <method_11> . we calculate the <otherscientificterm_8> for each <otherscientificterm_5> and frame length using <method_9> and select the best one for each training utterance . the proposed <method_3> applied to both the <method_1> and <method_6> resulted in <metric_14> improvement of 2.9 % -lrb- absolute -rrb- for <task_0> .",
    "abstract_og": "in this paper , we propose a speaking rate compensation method using frame period and frame length adaptation . our speaking rate compensation method decodes an input utterance using several sets of frame period and frame length parameters for speech analysis . then , this speaking rate compensation method selects the best set with the highest score which consists of the acoustic likelihood normalized by frame period , language likelihood and insertion penalty . furthermore , we apply this speaking rate compensation method to the training of the acoustic model . we calculate the acoustic likelihood for each frame period and frame length using viterbi alignment and select the best one for each training utterance . the proposed speaking rate compensation applied to both the acoustic model creation process and decoding process resulted in accuracy improvement of 2.9 % -lrb- absolute -rrb- for spontaneous lecture speech recognition task ."
  },
  {
    "title": "Keystroke Patterns as Prosody in Digital Writings : A Case Study with Deceptive Reviews and Essays .",
    "entities": [
      "duration of pauses",
      "keystroke patterns",
      "deception detection",
      "speech analysis",
      "keystroke-based features",
      "keyboard strokes",
      "online authors",
      "editing maneuvers",
      "online reviews"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <method> <material> <otherscientificterm> <material>",
    "relations": [
      "editing maneuvers -- HYPONYM-OF -- keystroke patterns",
      "duration of pauses -- HYPONYM-OF -- keystroke patterns",
      "keystroke-based features -- USED-FOR -- deception detection",
      "editing maneuvers -- CONJUNCTION -- duration of pauses"
    ],
    "abstract": "in this paper , we explore the use of <method_5> as a means to access the real-time writing process of <material_6> , analogously to prosody in <task_3> , in the context of <task_2> . we show that differences in <otherscientificterm_1> like <otherscientificterm_7> and <otherscientificterm_0> can help distinguish between truthful and deceptive writing . empirical results show that incorporating <otherscientificterm_4> lead to improved performance in <task_2> in two different domains : <material_8> and essays .",
    "abstract_og": "in this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection . we show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing . empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays ."
  },
  {
    "title": "TDOA estimation for cyclostationary sources : New correlations-based bounds and estimators .",
    "entities": [
      "time difference of arrival estimation",
      "asymptotic cram\u00e9r-rao bounds",
      "additive white gaussian noise",
      "approximate maximum likelihood estimator",
      "-lrb- unbiased -rrb- estimator",
      "asymptotically gaussian distribution",
      "classical approaches",
      "cyclostationary signals",
      "cyclic cross-correlations",
      "mul-ticycle approach",
      "cyclostationarity"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "time difference of arrival estimation -- USED-FOR -- cyclostationary signals",
      "cyclic cross-correlations -- USED-FOR -- cyclostationarity"
    ],
    "abstract": "we consider the problem of <task_0> for <otherscientificterm_7> in <otherscientificterm_2> . <method_6> to the problem either ignore the <method_10> and use ordinary cross-correlations , or exploit the <method_10> by using <otherscientificterm_8> , or combine these approaches into a <method_9> . despite contradicting claims in the literature regarding the performance-ranking of these approaches , there has been almost no analytical comparative performance study . we propose to regard the estimated -lrb- ordinary or cyclic -rrb- correlations as the '' front-end '' data , and based on their <otherscientificterm_5> , to compute the <method_1> for the various combinations -lrb- ordinary/single-cycle/multi-cycle -rrb- . using our cyclic-correlations-based crb -lrb- termed '' crbcrb '' -rrb- , we can bound the performance of any <method_4> which exploits a given set of correlations . moreover , we propose an <method_3> -lrb- with respect to the correlations -rrb- , and show that <method_3> attains our crbcrb asymptotically in simulations , outperforming the competitors .",
    "abstract_og": "we consider the problem of time difference of arrival estimation for cyclostationary signals in additive white gaussian noise . classical approaches to the problem either ignore the cyclostationarity and use ordinary cross-correlations , or exploit the cyclostationarity by using cyclic cross-correlations , or combine these approaches into a mul-ticycle approach . despite contradicting claims in the literature regarding the performance-ranking of these approaches , there has been almost no analytical comparative performance study . we propose to regard the estimated -lrb- ordinary or cyclic -rrb- correlations as the '' front-end '' data , and based on their asymptotically gaussian distribution , to compute the asymptotic cram\u00e9r-rao bounds for the various combinations -lrb- ordinary/single-cycle/multi-cycle -rrb- . using our cyclic-correlations-based crb -lrb- termed '' crbcrb '' -rrb- , we can bound the performance of any -lrb- unbiased -rrb- estimator which exploits a given set of correlations . moreover , we propose an approximate maximum likelihood estimator -lrb- with respect to the correlations -rrb- , and show that approximate maximum likelihood estimator attains our crbcrb asymptotically in simulations , outperforming the competitors ."
  },
  {
    "title": "Multi-label learning with incomplete class assignments .",
    "entities": [
      "ranking based multi-label learning framework",
      "incompletely labeled data",
      "incomplete class assignment",
      "mir flickr dataset",
      "incompletely labeled data",
      "group lasso technique",
      "class assignment",
      "optimization problem",
      "multi-label learning",
      "learning algorithm",
      "ranking errors"
    ],
    "types": "<method> <material> <otherscientificterm> <material> <material> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm>",
    "relations": [
      "group lasso technique -- USED-FOR -- incompletely labeled data",
      "incomplete class assignment -- FEATURE-OF -- multi-label learning",
      "learning algorithm -- USED-FOR -- optimization problem"
    ],
    "abstract": "we consider a special type of <method_8> where class assignments of training examples are incomplete . as an example , an instance whose true <otherscientificterm_6> is -lrb- c 1 , c 2 , c 3 -rrb- is only assigned to class c 1 when it is used as a training sample . we refer to this problem as <method_8> with <otherscientificterm_2> . <material_4> is frequently encountered when the number of classes is very large -lrb- hundreds as in <material_3> -rrb- or when there is a large ambiguity between classes -lrb- e.g. , jet vs plane -rrb- . in both cases , it is difficult for users to provide complete class assignments for objects . we propose a <method_0> that explicitly addresses the challenge of learning from <material_1> by exploiting the <method_5> to combine the <otherscientificterm_10> . we present a <method_9> that is empirically shown to be efficient for solving the related <task_7> . our empirical study shows that the proposed <method_0> is more effective than the state-of-the-art algorithms for <method_8> in dealing with <material_1> .",
    "abstract_og": "we consider a special type of multi-label learning where class assignments of training examples are incomplete . as an example , an instance whose true class assignment is -lrb- c 1 , c 2 , c 3 -rrb- is only assigned to class c 1 when it is used as a training sample . we refer to this problem as multi-label learning with incomplete class assignment . incompletely labeled data is frequently encountered when the number of classes is very large -lrb- hundreds as in mir flickr dataset -rrb- or when there is a large ambiguity between classes -lrb- e.g. , jet vs plane -rrb- . in both cases , it is difficult for users to provide complete class assignments for objects . we propose a ranking based multi-label learning framework that explicitly addresses the challenge of learning from incompletely labeled data by exploiting the group lasso technique to combine the ranking errors . we present a learning algorithm that is empirically shown to be efficient for solving the related optimization problem . our empirical study shows that the proposed ranking based multi-label learning framework is more effective than the state-of-the-art algorithms for multi-label learning in dealing with incompletely labeled data ."
  },
  {
    "title": "Rate-Invariant Analysis of Trajectories on Riemannian Manifolds with Application in Visual Speech Recognition .",
    "entities": [
      "transported square-root vector field",
      "statistical summarization & modeling of trajectories",
      "statistical analysis of video sequences",
      "temporal evolutions of features",
      "visual and audio components",
      "audio and visual components",
      "mathematical representation of trajectories",
      "speaker-dependent classification rate",
      "l 2 norm",
      "statistics literature",
      "temporal registration",
      "nuisance variability",
      "parametrization-invariant distances",
      "evolution patterns",
      "riemannian manifolds",
      "speech recognition",
      "speech classification",
      "activity recognition",
      "cost function/distance",
      "features",
      "classification"
    ],
    "types": "<method> <task> <task> <otherscientificterm> <method> <method> <method> <metric> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <task> <method> <otherscientificterm> <task>",
    "relations": [
      "transported square-root vector field -- USED-FOR -- speech classification",
      "cost function/distance -- USED-FOR -- temporal registration",
      "speaker-dependent classification rate -- USED-FOR -- visual and audio components",
      "parametrization-invariant distances -- USED-FOR -- speech classification",
      "transported square-root vector field -- USED-FOR -- parametrization-invariant distances",
      "audio and visual components -- USED-FOR -- speech recognition",
      "statistical analysis of video sequences -- USED-FOR -- speech recognition"
    ],
    "abstract": "in <task_2> for <task_15> , and more generally <task_17> , it is natural to treat <otherscientificterm_3> as trajec-tories on <otherscientificterm_14> . however , different <otherscientificterm_13> result in arbitrary parameterizations of these trajectories . we investigate a recent framework from <material_9> -lsb- 15 -rsb- that handles this <otherscientificterm_11> using a <method_18> for <task_10> and <task_1> . it is based on a <method_6> , termed <method_0> , and the <otherscientificterm_8> on the space of <method_0> . we apply this framework to the problem of <task_15> using both <method_5> . in each case , we extract <otherscientificterm_19> , form trajectories on corresponding manifolds , and compute <otherscientificterm_12> using <method_0> for <task_16> . on the <method_0> the <task_20> performance under metric increases significantly , by nearly 100 % under both modalities and for all choices of <otherscientificterm_19> . we obtained <metric_7> of 70 % and 96 % for <method_4> , respectively .",
    "abstract_og": "in statistical analysis of video sequences for speech recognition , and more generally activity recognition , it is natural to treat temporal evolutions of features as trajec-tories on riemannian manifolds . however , different evolution patterns result in arbitrary parameterizations of these trajectories . we investigate a recent framework from statistics literature -lsb- 15 -rsb- that handles this nuisance variability using a cost function/distance for temporal registration and statistical summarization & modeling of trajectories . it is based on a mathematical representation of trajectories , termed transported square-root vector field , and the l 2 norm on the space of transported square-root vector field . we apply this framework to the problem of speech recognition using both audio and visual components . in each case , we extract features , form trajectories on corresponding manifolds , and compute parametrization-invariant distances using transported square-root vector field for speech classification . on the transported square-root vector field the classification performance under metric increases significantly , by nearly 100 % under both modalities and for all choices of features . we obtained speaker-dependent classification rate of 70 % and 96 % for visual and audio components , respectively ."
  },
  {
    "title": "Cyclostationary joint phase and timing estimation for staggered modulations .",
    "entities": [
      "low-snr unconditional maximum likelihood framework",
      "offset quadrature amplitude modulation",
      "minimum shift keying",
      "non-data-aided phase",
      "asymptotic uml cost function",
      "spectral line generation",
      "timing estimation problem",
      "cyclostationary approach",
      "second-order non-linearity",
      "staggered modulations",
      "timing parameter",
      "modulations"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "minimum shift keying -- HYPONYM-OF -- modulations",
      "spectral line generation -- USED-FOR -- asymptotic uml cost function",
      "cyclostationary approach -- USED-FOR -- timing estimation problem",
      "offset quadrature amplitude modulation -- HYPONYM-OF -- modulations",
      "timing estimation problem -- USED-FOR -- staggered modulations",
      "cyclostationary approach -- USED-FOR -- non-data-aided phase",
      "non-data-aided phase -- CONJUNCTION -- timing estimation problem",
      "offset quadrature amplitude modulation -- CONJUNCTION -- minimum shift keying"
    ],
    "abstract": "this paper presents a <method_7> to the <task_3> and <task_6> for <otherscientificterm_9> . the problem is addressed under the <method_0> , and <otherscientificterm_11> such as <method_1> and <method_2> are considered . in this sense , it is found that not only the <otherscientificterm_10> but also the phase , can be jointly obtained from the <otherscientificterm_4> based on the <task_5> after a <otherscientificterm_8> .",
    "abstract_og": "this paper presents a cyclostationary approach to the non-data-aided phase and timing estimation problem for staggered modulations . the problem is addressed under the low-snr unconditional maximum likelihood framework , and modulations such as offset quadrature amplitude modulation and minimum shift keying are considered . in this sense , it is found that not only the timing parameter but also the phase , can be jointly obtained from the asymptotic uml cost function based on the spectral line generation after a second-order non-linearity ."
  },
  {
    "title": "A Pitch Tracking Corpus with Evaluation on Multipitch Tracking Scenario .",
    "entities": [
      "pitch tracking database",
      "ground truth signals",
      "multipitch tracking systems",
      "timit corpus",
      "laryn-gograph"
    ],
    "types": "<method> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "laryn-gograph -- USED-FOR -- ground truth signals",
      "ground truth signals -- USED-FOR -- pitch tracking database"
    ],
    "abstract": "in this paper , we introduce a novel <method_0> including <otherscientificterm_1> obtained from a <otherscientificterm_4> . the <method_0> , referenced as <method_0> , consists of 2342 phonetically rich sentences taken from the <material_3> . each sentence was at least recorded once by a male and a female native speaker . in total , the <method_0> contains 4720 recordings from 10 male and 10 female speakers . furthermore , we evaluated two <method_2> on a subset of speakers to provide a benchmark for further research activities . the <method_0> can be downloaded at",
    "abstract_og": "in this paper , we introduce a novel pitch tracking database including ground truth signals obtained from a laryn-gograph . the pitch tracking database , referenced as pitch tracking database , consists of 2342 phonetically rich sentences taken from the timit corpus . each sentence was at least recorded once by a male and a female native speaker . in total , the pitch tracking database contains 4720 recordings from 10 male and 10 female speakers . furthermore , we evaluated two multipitch tracking systems on a subset of speakers to provide a benchmark for further research activities . the pitch tracking database can be downloaded at"
  },
  {
    "title": "Tri-Training for Authorship Attribution with Limited Training Data .",
    "entities": [
      "authorship attribution",
      "online review domain",
      "three-view tri-training method",
      "semi-supervised method cng+svm",
      "labeled data",
      "un-labeled documents",
      "labeled documents",
      "unlabeled data",
      "tri-training"
    ],
    "types": "<task> <material> <method> <method> <material> <material> <material> <material> <method>",
    "relations": [
      "three-view tri-training method -- COMPARE -- semi-supervised method cng+svm",
      "three-view tri-training method -- USED-FOR -- authorship attribution",
      "tri-training -- USED-FOR -- un-labeled documents"
    ],
    "abstract": "authorship attribution -lrb- <task_0> -rrb- aims to identify the authors of a set of documents . traditional studies in this area often assume that there are a large set of <material_6> available for training . however , in the real life , it is often difficult or expensive to collect a large set of <material_4> . for example , in the <material_1> , most reviewers -lrb- authors -rrb- only write a few reviews , which are not enough to serve as the training data for accurate classification . in this paper , we present a novel <method_2> to iteratively identify authors of <material_7> to augment the training set . the key idea is to first represent each document in three distinct views , and then perform <method_8> to exploit the large amount of <material_5> . starting from 10 training documents per author , we systematically evaluate the effectiveness of the proposed <method_2> for <task_0> . experimental results show that the proposed <method_2> outperforms the state-of-the-art <method_3> and other baselines .",
    "abstract_og": "authorship attribution -lrb- authorship attribution -rrb- aims to identify the authors of a set of documents . traditional studies in this area often assume that there are a large set of labeled documents available for training . however , in the real life , it is often difficult or expensive to collect a large set of labeled data . for example , in the online review domain , most reviewers -lrb- authors -rrb- only write a few reviews , which are not enough to serve as the training data for accurate classification . in this paper , we present a novel three-view tri-training method to iteratively identify authors of unlabeled data to augment the training set . the key idea is to first represent each document in three distinct views , and then perform tri-training to exploit the large amount of un-labeled documents . starting from 10 training documents per author , we systematically evaluate the effectiveness of the proposed three-view tri-training method for authorship attribution . experimental results show that the proposed three-view tri-training method outperforms the state-of-the-art semi-supervised method cng+svm and other baselines ."
  },
  {
    "title": "Annotation and detection of conflict escalation in Political debates .",
    "entities": [
      "annotation and detection of conflict escalation",
      "conversational and prosodic features",
      "machine-mediated conflict management system",
      "broadcast political debates",
      "level conflict escalation",
      "indirect inference method",
      "direct assessment method",
      "direct assessment",
      "indirect inference",
      "multi-party conversations",
      "crowd-sourced annotations",
      "annotation process",
      "ternary classes",
      "continuous values",
      "conflict escalation",
      "unweighted accuracy",
      "agreement",
      "escalation",
      "de-escalation"
    ],
    "types": "<task> <otherscientificterm> <method> <material> <otherscientificterm> <method> <method> <task> <method> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "crowd-sourced annotations -- USED-FOR -- conflict escalation",
      "escalation -- CONJUNCTION -- de-escalation",
      "indirect inference -- CONJUNCTION -- direct assessment",
      "escalation -- HYPONYM-OF -- conflict escalation",
      "de-escalation -- HYPONYM-OF -- conflict escalation"
    ],
    "abstract": "conflict <otherscientificterm_17> in <material_9> refers to an increase in the intensity of conflict during conversations . here we study <task_0> in <material_3> towards a <method_2> . in this regard , we label <otherscientificterm_14> using <otherscientificterm_10> and predict it with automatically extracted <otherscientificterm_1> . in particular , to annotate the <otherscientificterm_14> we deploy two different strategies , i.e. , <method_8> and <task_7> ; the <method_6> refers to a way that annotators watch and compare two consecutive clips during the <method_11> , while the <method_5> indicates that each clip is independently annotated with respect to the level of conflict then the <otherscientificterm_4> is inferred by comparing annotations of two consecutive clips . empirical results with 792 pairs of consecutive clips in classifying three types of <otherscientificterm_14> , i.e. , <otherscientificterm_17> , <otherscientificterm_18> , and constant , show that labels from <task_7> yield higher classification performance -lrb- 45.3 % <metric_15> -lrb- ua -rrb- -rrb- than the one from <method_8> -lrb- 39.7 % ua -rrb- , although the annotations from both methods are highly correlated -lrb- \u03c1 = 0.74 in <otherscientificterm_13> and 63 % <metric_16> in <otherscientificterm_12> -rrb- .",
    "abstract_og": "conflict escalation in multi-party conversations refers to an increase in the intensity of conflict during conversations . here we study annotation and detection of conflict escalation in broadcast political debates towards a machine-mediated conflict management system . in this regard , we label conflict escalation using crowd-sourced annotations and predict it with automatically extracted conversational and prosodic features . in particular , to annotate the conflict escalation we deploy two different strategies , i.e. , indirect inference and direct assessment ; the direct assessment method refers to a way that annotators watch and compare two consecutive clips during the annotation process , while the indirect inference method indicates that each clip is independently annotated with respect to the level of conflict then the level conflict escalation is inferred by comparing annotations of two consecutive clips . empirical results with 792 pairs of consecutive clips in classifying three types of conflict escalation , i.e. , escalation , de-escalation , and constant , show that labels from direct assessment yield higher classification performance -lrb- 45.3 % unweighted accuracy -lrb- ua -rrb- -rrb- than the one from indirect inference -lrb- 39.7 % ua -rrb- , although the annotations from both methods are highly correlated -lrb- \u03c1 = 0.74 in continuous values and 63 % agreement in ternary classes -rrb- ."
  },
  {
    "title": "Lifting Techniques for Sequential Decision Making and Probabilistic Inference -LRB- Extended Abstract -RRB- .",
    "entities": [
      "markov chain monte carlo methods",
      "monte carlo tree search framework",
      "state-action pair symmetries",
      "size of state space",
      "generic symmetry based framework",
      "markov decision processes",
      "sequential decision making",
      "ai algorithms",
      "unconditional symmetries",
      "probabilistic inference",
      "problem size",
      "symmetry exploitation",
      "symmetric states",
      "contextual symmetries",
      "features",
      "computation",
      "work-asap-uct"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method>",
    "relations": [
      "sequential decision making -- CONJUNCTION -- probabilistic inference",
      "state-action pair symmetries -- USED-FOR -- monte carlo tree search framework",
      "unconditional symmetries -- USED-FOR -- contextual symmetries",
      "unconditional symmetries -- PART-OF -- markov chain monte carlo methods"
    ],
    "abstract": "many traditional <method_7> fail to scale as the <otherscientificterm_3> increases exponentially with the number of <otherscientificterm_14> . one way to reduce <task_15> in such scenarios is to reduce the <otherscientificterm_10> by grouping <otherscientificterm_12> together and then running the algorithm on the reduced problem . the focus of this work is to exploit symmetry in problems of <task_6> and <task_9> . our recent <method_16> defines new <otherscientificterm_2> in <method_5> . we also apply these <otherscientificterm_2> in <method_1> . in <task_9> , we expand the notion of <otherscientificterm_8> to <otherscientificterm_13> and apply <otherscientificterm_8> in <method_0> . in future , we plan to explore interesting links in <task_11> in different problems and aim to develop a <method_4> .",
    "abstract_og": "many traditional ai algorithms fail to scale as the size of state space increases exponentially with the number of features . one way to reduce computation in such scenarios is to reduce the problem size by grouping symmetric states together and then running the algorithm on the reduced problem . the focus of this work is to exploit symmetry in problems of sequential decision making and probabilistic inference . our recent work-asap-uct defines new state-action pair symmetries in markov decision processes . we also apply these state-action pair symmetries in monte carlo tree search framework . in probabilistic inference , we expand the notion of unconditional symmetries to contextual symmetries and apply unconditional symmetries in markov chain monte carlo methods . in future , we plan to explore interesting links in symmetry exploitation in different problems and aim to develop a generic symmetry based framework ."
  },
  {
    "title": "Learning and adaptation of a tongue shape modelwith missing data .",
    "entities": [
      "missing data",
      "ultrasound imaging",
      "data-driven techniques",
      "ultrasound data",
      "submillimetric accuracy",
      "tongue contours",
      "noise",
      "shadowing",
      "learning"
    ],
    "types": "<material> <method> <method> <material> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "shadowing -- CONJUNCTION -- noise",
      "ultrasound imaging -- USED-FOR -- tongue contours"
    ],
    "abstract": "using <method_2> and <material_3> , it is possible to learn models that reconstruct the tongue shape of a speaker with <metric_4> given the location of 3 -- 4 fleshpoints , and to adapt these models to a new speaker for which little data is available . in practice , <otherscientificterm_5> extracted from <method_1> are often incomplete because of <otherscientificterm_7> , <otherscientificterm_6> and other factors . we extend these models to deal with <material_0> during <task_8> and adaptation , and show that <metric_4> can still be achieved even with relatively large amounts of <material_0> .",
    "abstract_og": "using data-driven techniques and ultrasound data , it is possible to learn models that reconstruct the tongue shape of a speaker with submillimetric accuracy given the location of 3 -- 4 fleshpoints , and to adapt these models to a new speaker for which little data is available . in practice , tongue contours extracted from ultrasound imaging are often incomplete because of shadowing , noise and other factors . we extend these models to deal with missing data during learning and adaptation , and show that submillimetric accuracy can still be achieved even with relatively large amounts of missing data ."
  },
  {
    "title": "A distributed wavelet compression algorithm for wireless multihop sensor networks using lifting .",
    "entities": [
      "multihop , distributed sensor networks",
      "distributed compression algorithm",
      "partial wavelet coefficients",
      "wireless sensor networks",
      "natural data flow",
      "lifting factorization",
      "partial computations",
      "wavelet transform",
      "central node",
      "compression"
    ],
    "types": "<task> <method> <otherscientificterm> <task> <material> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "distributed compression algorithm -- USED-FOR -- multihop , distributed sensor networks",
      "compression -- FEATURE-OF -- wireless sensor networks"
    ],
    "abstract": "we address the problem of <method_9> for <task_3> , where each of the sensors has limited power , and acquires data that should be sent to a <otherscientificterm_8> . the final goal is to have a reconstructed version of the sampled field at the <otherscientificterm_8> , with the sensors spending as little energy as possible . we propose a <method_1> for <task_0> based on the <method_5> of the <otherscientificterm_7> that exploits the <material_4> in the network to aggregate data by computing <otherscientificterm_2> that are refined as the data flows towards the <otherscientificterm_8> . a key result of our work is that by performing <method_6> we greatly reduce unnecessary transmission , significantly reducing the overall energy consumption .",
    "abstract_og": "we address the problem of compression for wireless sensor networks , where each of the sensors has limited power , and acquires data that should be sent to a central node . the final goal is to have a reconstructed version of the sampled field at the central node , with the sensors spending as little energy as possible . we propose a distributed compression algorithm for multihop , distributed sensor networks based on the lifting factorization of the wavelet transform that exploits the natural data flow in the network to aggregate data by computing partial wavelet coefficients that are refined as the data flows towards the central node . a key result of our work is that by performing partial computations we greatly reduce unnecessary transmission , significantly reducing the overall energy consumption ."
  },
  {
    "title": "Multiple description speech coding with diversities .",
    "entities": [
      "speech and audio coders",
      "fixed constraints",
      "mdc systems",
      "packet-lossy nature",
      "network communications",
      "algorithmic delay",
      "computational complexity",
      "speech"
    ],
    "types": "<material> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <material>",
    "relations": [
      "computational complexity -- CONJUNCTION -- algorithmic delay",
      "mdc systems -- USED-FOR -- speech"
    ],
    "abstract": "most existing <material_0> were developed to meet a single purpose of delivering the best quality possible under <otherscientificterm_1> in bit-rate , <metric_6> , and <otherscientificterm_5> . recent expansion in <method_4> demands the additional capability to cope with the <otherscientificterm_3> associated with these <method_4> . the problem lies within the research area of multiple description coding -lrb- mdc -rrb- . in this report we investigate its essence , state its inherent limitations and tradeoffs , and propose a novel method to design efficient <method_2> for <material_7> .",
    "abstract_og": "most existing speech and audio coders were developed to meet a single purpose of delivering the best quality possible under fixed constraints in bit-rate , computational complexity , and algorithmic delay . recent expansion in network communications demands the additional capability to cope with the packet-lossy nature associated with these network communications . the problem lies within the research area of multiple description coding -lrb- mdc -rrb- . in this report we investigate its essence , state its inherent limitations and tradeoffs , and propose a novel method to design efficient mdc systems for speech ."
  },
  {
    "title": "Line drawing interpretation in a multi-view context .",
    "entities": [
      "creation of new objects",
      "reconstruction of real-world scenes",
      "multi-view stereo algorithms",
      "urban planing",
      "computer vision",
      "labeling algorithm",
      "line drawings",
      "furniture design",
      "home remodeling",
      "cultural heritage",
      "design tasks",
      "hand algorithms",
      "line-drawing interpretation",
      "line drawing",
      "3d structure"
    ],
    "types": "<task> <task> <method> <task> <task> <method> <task> <task> <task> <material> <task> <method> <task> <task> <otherscientificterm>",
    "relations": [
      "urban planing -- CONJUNCTION -- home remodeling",
      "furniture design -- CONJUNCTION -- cultural heritage",
      "home remodeling -- CONJUNCTION -- cultural heritage",
      "line drawings -- USED-FOR -- urban planing",
      "home remodeling -- CONJUNCTION -- furniture design",
      "multi-view stereo algorithms -- USED-FOR -- reconstruction of real-world scenes",
      "hand algorithms -- USED-FOR -- line-drawing interpretation"
    ],
    "abstract": "many <task_10> involve the <task_0> in the context of an existing scene . existing work in <task_4> only provides partial support for such tasks . on the one hand , <method_2> allow the <task_1> , while on the other <method_11> for <task_12> do not take context into account . our work combines the strength of these two domains to interpret <task_6> of imaginary objects drawn over photographs of an existing scene . the main challenge we face is to identify the existing <otherscientificterm_14> that correlates with the <task_13> while also allowing the creation of new structure that is not present in the real world . we propose a <method_5> to tackle this problem , where some of the labels capture dominant orientations of the real scene while a free label allows the discovery of new orientations in the imaginary scene . we illustrate our <method_5> by interpreting <task_6> for <task_3> , <task_8> , <task_7> and <material_9> .",
    "abstract_og": "many design tasks involve the creation of new objects in the context of an existing scene . existing work in computer vision only provides partial support for such tasks . on the one hand , multi-view stereo algorithms allow the reconstruction of real-world scenes , while on the other hand algorithms for line-drawing interpretation do not take context into account . our work combines the strength of these two domains to interpret line drawings of imaginary objects drawn over photographs of an existing scene . the main challenge we face is to identify the existing 3d structure that correlates with the line drawing while also allowing the creation of new structure that is not present in the real world . we propose a labeling algorithm to tackle this problem , where some of the labels capture dominant orientations of the real scene while a free label allows the discovery of new orientations in the imaginary scene . we illustrate our labeling algorithm by interpreting line drawings for urban planing , home remodeling , furniture design and cultural heritage ."
  },
  {
    "title": "Intersession Compensation and Scoring Methods in the i-vectors Space for Speaker Recognition .",
    "entities": [
      "low dimensional total factor space",
      "factor analysis",
      "speaker verification system architecture",
      "total variability factor space",
      "carrying out channel compensation",
      "gmm supervector space",
      "total factor space",
      "session variability characteristics",
      "scoring methods",
      "speaker recognition"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "total variability factor space -- USED-FOR -- speaker verification system architecture",
      "factor analysis -- USED-FOR -- speaker recognition",
      "factor analysis -- USED-FOR -- speaker verification system architecture",
      "total factor space -- FEATURE-OF -- session variability characteristics"
    ],
    "abstract": "the <otherscientificterm_3> in <method_2> based on <method_1> has greatly improved <task_9> performances . <task_4> in a <otherscientificterm_0> , rather than in the <otherscientificterm_5> , allows for the application of new techniques . we propose here new intersession compensation and <method_8> . furthermore , this new approach contributes to a better understanding of the <otherscientificterm_7> in the <otherscientificterm_6> .",
    "abstract_og": "the total variability factor space in speaker verification system architecture based on factor analysis has greatly improved speaker recognition performances . carrying out channel compensation in a low dimensional total factor space , rather than in the gmm supervector space , allows for the application of new techniques . we propose here new intersession compensation and scoring methods . furthermore , this new approach contributes to a better understanding of the session variability characteristics in the total factor space ."
  },
  {
    "title": "A keyword-aware grammar framework for LVCSR-based spoken keyword search .",
    "entities": [
      "weight finite-state automata",
      "openkws14 tamil limited-language pack tasks",
      "compact and deterministic grammar wfsa",
      "n-gram lm based approximation approach",
      "lvcsr-based keyword search",
      "n-gram wfsa",
      "keyword-aware grammar",
      "evalpart1 data",
      "keyword paths",
      "grammar"
    ],
    "types": "<method> <material> <method> <method> <task> <method> <method> <material> <otherscientificterm> <method>",
    "relations": [
      "keyword-aware grammar -- CONJUNCTION -- n-gram lm based approximation approach",
      "n-gram lm based approximation approach -- USED-FOR -- grammar",
      "keyword paths -- USED-FOR -- n-gram wfsa",
      "keyword-aware grammar -- USED-FOR -- lvcsr-based keyword search"
    ],
    "abstract": "in this paper , we proposed a method to realize the recently developed <method_6> for <task_4> using <method_0> . the approach creates a <method_2> by inserting <otherscientificterm_8> to an existing <method_5> . tested on the <material_7> of the iarpa babel openkws13 vietnamese and <material_1> , the experimental results indicate the proposed keyword-aware framework achieves significant improvement , with about 50 % relative actual term weighted value -lrb- atwv -rrb- enhancement for both languages . comparisons between the <method_6> and our previously proposed <method_3> for the <method_9> also show that the kws performances of these two realizations are complementary .",
    "abstract_og": "in this paper , we proposed a method to realize the recently developed keyword-aware grammar for lvcsr-based keyword search using weight finite-state automata . the approach creates a compact and deterministic grammar wfsa by inserting keyword paths to an existing n-gram wfsa . tested on the evalpart1 data of the iarpa babel openkws13 vietnamese and openkws14 tamil limited-language pack tasks , the experimental results indicate the proposed keyword-aware framework achieves significant improvement , with about 50 % relative actual term weighted value -lrb- atwv -rrb- enhancement for both languages . comparisons between the keyword-aware grammar and our previously proposed n-gram lm based approximation approach for the grammar also show that the kws performances of these two realizations are complementary ."
  },
  {
    "title": "Closed-form supervised dimensionality reduction with generalized linear models .",
    "entities": [
      "supervised dimensionality reduction algorithms",
      "class-appropriate generalized linear models",
      "feature extraction -lrb- dimensionality reduction -rrb-",
      "closed-form update rules",
      "unified optimization framework",
      "predictive model",
      "high-dimensional datasets",
      "classification"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <method> <material> <task>",
    "relations": [
      "class-appropriate generalized linear models -- USED-FOR -- predictive model",
      "predictive model -- USED-FOR -- unified optimization framework",
      "feature extraction -lrb- dimensionality reduction -rrb- -- USED-FOR -- supervised dimensionality reduction algorithms",
      "supervised dimensionality reduction algorithms -- USED-FOR -- predictive model",
      "closed-form update rules -- USED-FOR -- supervised dimensionality reduction algorithms",
      "feature extraction -lrb- dimensionality reduction -rrb- -- USED-FOR -- predictive model"
    ],
    "abstract": "we propose a family of <method_0> that combine <method_2> with learning a <method_5> in a <method_4> , using data - and <method_1> , and handling both <task_7> and regression problems . our <method_0> uses simple <otherscientificterm_3> and is provably convergent . promising empirical results are demonstrated on a variety of <material_6> .",
    "abstract_og": "we propose a family of supervised dimensionality reduction algorithms that combine feature extraction -lrb- dimensionality reduction -rrb- with learning a predictive model in a unified optimization framework , using data - and class-appropriate generalized linear models , and handling both classification and regression problems . our supervised dimensionality reduction algorithms uses simple closed-form update rules and is provably convergent . promising empirical results are demonstrated on a variety of high-dimensional datasets ."
  },
  {
    "title": "Automatic estimation of language model parameters for unseen words using morpho-syntactic contextual information .",
    "entities": [
      "language model parameters",
      "european portuguese broadcast news transcription system",
      "word error rate",
      "part-of-speech word classes",
      "speech recognition system",
      "lm unigram distribution",
      "archived documents",
      "in-domain corpus",
      "information sources",
      "information dissemination",
      "lm retraining",
      "morpho-syntatic information",
      "closed-captioning"
    ],
    "types": "<otherscientificterm> <task> <metric> <otherscientificterm> <method> <otherscientificterm> <material> <material> <material> <task> <method> <otherscientificterm> <task>",
    "relations": [
      "closed-captioning -- CONJUNCTION -- information dissemination",
      "in-domain corpus -- CONJUNCTION -- part-of-speech word classes"
    ],
    "abstract": "various <material_8> naturally contains new words that appear in a daily basis and which are not present in the vocabulary of the <method_4> but are important for applications such as <task_12> or <task_9> . to be recognized , those words need to be included in the vocabulary and the <otherscientificterm_0> updated . in this context , we propose a new method that allows including new words in the vocabulary even if no well suited training data is available , as is the case of <material_6> , and without the need of <method_10> . it uses <otherscientificterm_11> about an <material_7> and <otherscientificterm_3> to define a new <otherscientificterm_5> associated to the updated vocabulary . experiments were carried out for a <task_1> . results showed a relative reduction of 4 % in <metric_2> , with 78 % of the occurrences of those newly included words being correctly recognized .",
    "abstract_og": "various information sources naturally contains new words that appear in a daily basis and which are not present in the vocabulary of the speech recognition system but are important for applications such as closed-captioning or information dissemination . to be recognized , those words need to be included in the vocabulary and the language model parameters updated . in this context , we propose a new method that allows including new words in the vocabulary even if no well suited training data is available , as is the case of archived documents , and without the need of lm retraining . it uses morpho-syntatic information about an in-domain corpus and part-of-speech word classes to define a new lm unigram distribution associated to the updated vocabulary . experiments were carried out for a european portuguese broadcast news transcription system . results showed a relative reduction of 4 % in word error rate , with 78 % of the occurrences of those newly included words being correctly recognized ."
  },
  {
    "title": "FAemb : A function approximation-based embedding method for image retrieval .",
    "entities": [
      "public image retrieval benchmarks",
      "function approximation process",
      "high dimensional space",
      "higher dimensional representation",
      "image retrieval framework",
      "image retrieval problem",
      "image retrieval",
      "nonlinear function",
      "local features",
      "embedded vectors",
      "feature representation",
      "embedding method",
      "vlad"
    ],
    "types": "<material> <method> <otherscientificterm> <method> <method> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "higher dimensional representation -- USED-FOR -- image retrieval problem",
      "high dimensional space -- FEATURE-OF -- image retrieval",
      "embedding method -- USED-FOR -- image retrieval",
      "high dimensional space -- FEATURE-OF -- feature representation",
      "high dimensional space -- FEATURE-OF -- nonlinear function",
      "nonlinear function -- USED-FOR -- image retrieval",
      "function approximation process -- USED-FOR -- embedded vectors",
      "feature representation -- USED-FOR -- image retrieval"
    ],
    "abstract": "the objective of this paper is to design an <method_11> mapping <otherscientificterm_8> describing image -lrb- e.g. sift -rrb- to a <method_3> used for <task_5> . by investigating the relationship between the linear approximation of a <otherscientificterm_7> in <otherscientificterm_2> and state-of-the-art <method_10> used in <task_6> , i.e. , <method_12> , we first introduce a new approach for the approximation . the <otherscientificterm_9> resulted by the <method_1> are then aggregated to form a single representation used in the <method_4> . the evaluation shows that our <method_11> gives a performance boost over the state of the art in <task_6> , as demonstrated by our experiments on the standard <material_0> .",
    "abstract_og": "the objective of this paper is to design an embedding method mapping local features describing image -lrb- e.g. sift -rrb- to a higher dimensional representation used for image retrieval problem . by investigating the relationship between the linear approximation of a nonlinear function in high dimensional space and state-of-the-art feature representation used in image retrieval , i.e. , vlad , we first introduce a new approach for the approximation . the embedded vectors resulted by the function approximation process are then aggregated to form a single representation used in the image retrieval framework . the evaluation shows that our embedding method gives a performance boost over the state of the art in image retrieval , as demonstrated by our experiments on the standard public image retrieval benchmarks ."
  },
  {
    "title": "On Real-Time Detecting Duplicate Web Videos .",
    "entities": [
      "detecting excessive content duplication",
      "intelligence propriety protection",
      "real-time detection method",
      "digital devices",
      "hash codes",
      "video search",
      "duplicate videos",
      "digital format",
      "real-time applications",
      "telecommunication techniques"
    ],
    "types": "<task> <task> <method> <method> <otherscientificterm> <task> <material> <material> <task> <method>",
    "relations": [
      "video search -- CONJUNCTION -- intelligence propriety protection",
      "real-time applications -- EVALUATE-FOR -- real-time detection method",
      "telecommunication techniques -- CONJUNCTION -- digital devices"
    ],
    "abstract": "with the rapid development of <method_9> and <method_3> , it is quite easy to copy , modify and republish videos in <material_7> , resulting in large volume of <material_6> on the web in recent years . in this paper we mainly investigate the problem of <task_0> , so as to facilitate <task_5> and <task_1> . a <method_2> is hence proposed , which first selects videos ' representative frames and then reduces each to a 64 bit hash code . then the similarity of any two videos can be estimated by the proportion of their similar <otherscientificterm_4> . the experiments demonstrate that our <method_2> is both efficient and effective in terms of <task_8> .",
    "abstract_og": "with the rapid development of telecommunication techniques and digital devices , it is quite easy to copy , modify and republish videos in digital format , resulting in large volume of duplicate videos on the web in recent years . in this paper we mainly investigate the problem of detecting excessive content duplication , so as to facilitate video search and intelligence propriety protection . a real-time detection method is hence proposed , which first selects videos ' representative frames and then reduces each to a 64 bit hash code . then the similarity of any two videos can be estimated by the proportion of their similar hash codes . the experiments demonstrate that our real-time detection method is both efficient and effective in terms of real-time applications ."
  },
  {
    "title": "Sparse representation classification with manifold constraints transfer .",
    "entities": [
      "sparse representation-based classification approaches",
      "multipliers",
      "learning and inference problems",
      "alternating direction method",
      "linear embedding method",
      "image data samples",
      "general recognition tasks",
      "manifold constraints",
      "optimized variables",
      "geodesic distance",
      "manifold priors",
      "recognition accuracies",
      "objects recognition",
      "optimization process",
      "mani-fold",
      "manifold",
      "projection"
    ],
    "types": "<method> <method> <task> <method> <method> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "manifold -- USED-FOR -- image data samples",
      "manifold priors -- PART-OF -- sparse representation-based classification approaches",
      "geodesic distance -- USED-FOR -- linear embedding method",
      "manifold constraints -- PART-OF -- optimization process"
    ],
    "abstract": "the fact that <material_5> lie on a <otherscientificterm_15> has been successfully exploited in many <task_2> . in this paper we leverage the specific structure of data in order to improve <task_11> in <task_6> . in particular we propose a novel framework that allows to embed <otherscientificterm_10> into <method_0> . we also show that <otherscientificterm_7> can be transferred from the data to the <otherscientificterm_8> if these are linearly correlated . using this new insight , we define an efficient <method_3> of <method_1> that can consistently integrate the <otherscientificterm_7> during the <method_13> . this is based on the property that we can recast the problem as the <otherscientificterm_16> over the <otherscientificterm_14> via a <method_4> based on the <otherscientificterm_9> . the proposed approach is successfully applied on face , digit , action and <task_12> showing a consistently increase on performance when compared to the state of the art .",
    "abstract_og": "the fact that image data samples lie on a manifold has been successfully exploited in many learning and inference problems . in this paper we leverage the specific structure of data in order to improve recognition accuracies in general recognition tasks . in particular we propose a novel framework that allows to embed manifold priors into sparse representation-based classification approaches . we also show that manifold constraints can be transferred from the data to the optimized variables if these are linearly correlated . using this new insight , we define an efficient alternating direction method of multipliers that can consistently integrate the manifold constraints during the optimization process . this is based on the property that we can recast the problem as the projection over the mani-fold via a linear embedding method based on the geodesic distance . the proposed approach is successfully applied on face , digit , action and objects recognition showing a consistently increase on performance when compared to the state of the art ."
  },
  {
    "title": "Single channel speech enhancement using MDL-based subspace approach in Bark domain .",
    "entities": [
      "minimum description length criterion",
      "white and colored noise",
      "overes-timation of the signal",
      "single channel speech enhancement",
      "optimal subspace selection",
      "human auditory system",
      "input signal-to-noise ratio",
      "maximum noise reduction",
      "mdl-subspace approach",
      "selection criteria",
      "noise subspace",
      "signal distortions",
      "empirical parameters",
      "subspace selection",
      "subspace approach",
      "mdl criterion",
      "bark domain"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <method> <method> <metric> <task> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <material>",
    "relations": [
      "mdl criterion -- USED-FOR -- subspace selection",
      "overes-timation of the signal -- HYPONYM-OF -- selection criteria",
      "subspace approach -- USED-FOR -- bark domain"
    ],
    "abstract": "we present in this paper a novel algorithm for <task_3> . it is based on a <method_14> in the <material_16> and an <method_4> by the <method_0> . the processing in the <material_16> allows us to take into account in an optimal manner the masking properties of the <method_5> . the <method_13> provided by the <otherscientificterm_15> overcomes the limitations encountered with other <metric_9> , like the <otherscientificterm_2> -- plus -- <otherscientificterm_10> or the need for <otherscientificterm_12> . together , the resulting <method_8> in the <material_16> provides <task_7> while minimizing <otherscientificterm_11> . the performance of our algorithm is assessed in <otherscientificterm_1> . it shows that our algorithm provides high performance for a large scale of <metric_6> .",
    "abstract_og": "we present in this paper a novel algorithm for single channel speech enhancement . it is based on a subspace approach in the bark domain and an optimal subspace selection by the minimum description length criterion . the processing in the bark domain allows us to take into account in an optimal manner the masking properties of the human auditory system . the subspace selection provided by the mdl criterion overcomes the limitations encountered with other selection criteria , like the overes-timation of the signal -- plus -- noise subspace or the need for empirical parameters . together , the resulting mdl-subspace approach in the bark domain provides maximum noise reduction while minimizing signal distortions . the performance of our algorithm is assessed in white and colored noise . it shows that our algorithm provides high performance for a large scale of input signal-to-noise ratio ."
  },
  {
    "title": "Towards a Knowledge Compilation Map for Heterogeneous Representation Languages .",
    "entities": [
      "relative adequacy of representation languages",
      "knowledge compilation map",
      "comparison of languages",
      "generalized setting",
      "ai problems",
      "generalized framework",
      "boolean functions",
      "formal basis",
      "add",
      "obdd",
      "expressiveness",
      "mdd",
      "transformations",
      "succinctness",
      "languages"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "mdd -- HYPONYM-OF -- languages",
      "transformations -- CONJUNCTION -- expressiveness",
      "mdd -- CONJUNCTION -- add",
      "expressiveness -- CONJUNCTION -- succinctness",
      "obdd -- HYPONYM-OF -- languages",
      "obdd -- CONJUNCTION -- mdd"
    ],
    "abstract": "the <method_1> introduced by darwiche and marquis takes advantage of a number of concepts -lrb- mainly queries , <otherscientificterm_12> , <otherscientificterm_10> , and <otherscientificterm_13> -rrb- to compare the <otherscientificterm_0> to some <task_4> . however , the framework is limited to the <otherscientificterm_2> that are interpreted in a homogeneous way -lrb- formulae are interpreted as <otherscientificterm_6> -rrb- . this prevents one from comparing , on a <otherscientificterm_7> , <material_14> that are close in essence , such as <otherscientificterm_9> , <otherscientificterm_11> , and <otherscientificterm_8> . to fill the gap , we present a <method_5> into which comparing formally heterogeneous representation <material_14> becomes feasible . in particular , we explain how the key notions of queries and <otherscientificterm_12> , <otherscientificterm_10> , and <otherscientificterm_13> can be lifted to the <otherscientificterm_3> .",
    "abstract_og": "the knowledge compilation map introduced by darwiche and marquis takes advantage of a number of concepts -lrb- mainly queries , transformations , expressiveness , and succinctness -rrb- to compare the relative adequacy of representation languages to some ai problems . however , the framework is limited to the comparison of languages that are interpreted in a homogeneous way -lrb- formulae are interpreted as boolean functions -rrb- . this prevents one from comparing , on a formal basis , languages that are close in essence , such as obdd , mdd , and add . to fill the gap , we present a generalized framework into which comparing formally heterogeneous representation languages becomes feasible . in particular , we explain how the key notions of queries and transformations , expressiveness , and succinctness can be lifted to the generalized setting ."
  },
  {
    "title": "Neural networks versus codebooks in an application for bandwidth extension of speech signals .",
    "entities": [
      "bandwidth extension of speech signals",
      "objective and subjective distortion measures",
      "spectral envelope",
      "neural networks",
      "codebooks"
    ],
    "types": "<task> <metric> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "neural networks -- CONJUNCTION -- codebooks"
    ],
    "abstract": "this paper presents two versions of an algorithm for <task_0> . we focus on the generation of the <otherscientificterm_2> and compare the performance of two different approaches -- <method_3> versus <otherscientificterm_4> -- in terms of <metric_1> .",
    "abstract_og": "this paper presents two versions of an algorithm for bandwidth extension of speech signals . we focus on the generation of the spectral envelope and compare the performance of two different approaches -- neural networks versus codebooks -- in terms of objective and subjective distortion measures ."
  },
  {
    "title": "A novel approach for power control in a multi-user data transmission system .",
    "entities": [
      "multiuser data transmission system",
      "ten-line vdsl-2 system",
      "bit rate distributions",
      "maximum transmission power",
      "overall transmission power",
      "transmission power",
      "iterative optimization",
      "power control"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task>",
    "relations": [
      "transmission power -- USED-FOR -- ten-line vdsl-2 system",
      "bit rate distributions -- FEATURE-OF -- ten-line vdsl-2 system"
    ],
    "abstract": "in this paper , we present a novel approach for <task_7> in a <method_0> . the main objective of our approach is to distribute the <otherscientificterm_4> among the lines in a way , such that a certain ratio between the bit rate of the lines is achieved . given the <otherscientificterm_3> per line and the ratio of the bit rates both the <otherscientificterm_5> and the achievable bit rate for each line are found by an <method_6> . we show the impact of our novel <task_7> in an experimental section , where the approach is used for finding the <otherscientificterm_5> for a <method_1> with various <otherscientificterm_2> .",
    "abstract_og": "in this paper , we present a novel approach for power control in a multiuser data transmission system . the main objective of our approach is to distribute the overall transmission power among the lines in a way , such that a certain ratio between the bit rate of the lines is achieved . given the maximum transmission power per line and the ratio of the bit rates both the transmission power and the achievable bit rate for each line are found by an iterative optimization . we show the impact of our novel power control in an experimental section , where the approach is used for finding the transmission power for a ten-line vdsl-2 system with various bit rate distributions ."
  },
  {
    "title": "A Variational Approach to Learning Curves .",
    "entities": [
      "gaussian process regression",
      "empirical error measures",
      "varia-tional approach",
      "posterior variance",
      "ap-proximative relations",
      "replica approach",
      "statistical physics",
      "generalization error",
      "learning curves"
    ],
    "types": "<task> <metric> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <task>",
    "relations": [
      "generalization error -- CONJUNCTION -- posterior variance",
      "statistical physics -- CONJUNCTION -- varia-tional approach",
      "replica approach -- USED-FOR -- gaussian process regression",
      "statistical physics -- USED-FOR -- replica approach",
      "replica approach -- USED-FOR -- learning curves",
      "empirical error measures -- CONJUNCTION -- generalization error",
      "varia-tional approach -- USED-FOR -- learning curves"
    ],
    "abstract": "we combine the <method_5> from <method_6> with a <method_2> to analyze <task_8> analytically . we apply the <method_5> to <task_0> . as a main result we derive <otherscientificterm_4> between <metric_1> , the <otherscientificterm_7> and the <otherscientificterm_3> .",
    "abstract_og": "we combine the replica approach from statistical physics with a varia-tional approach to analyze learning curves analytically . we apply the replica approach to gaussian process regression . as a main result we derive ap-proximative relations between empirical error measures , the generalization error and the posterior variance ."
  },
  {
    "title": "A two-layer lexical tree based beam search in continuous Chinese speech recognition .",
    "entities": [
      "crossword context-dependent triphone models",
      "dynamic programming",
      "crossword context-dependent triphone models",
      "chinese fuzzy syllable mapping",
      "continuous speech recognition",
      "two-layer lexical tree",
      "memory cost",
      "pronunciation modeling",
      "acoustic information",
      "search algorithm",
      "token-passing algorithm",
      "decoding"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <task>",
    "relations": [
      "two-layer lexical tree -- USED-FOR -- continuous speech recognition",
      "chinese fuzzy syllable mapping -- CONJUNCTION -- pronunciation modeling",
      "two-layer lexical tree -- USED-FOR -- search algorithm",
      "crossword context-dependent triphone models -- CONJUNCTION -- chinese fuzzy syllable mapping",
      "token-passing algorithm -- USED-FOR -- search algorithm",
      "crossword context-dependent triphone models -- USED-FOR -- two-layer lexical tree"
    ],
    "abstract": "in this paper , an approach to <task_4> based on a <method_5> is proposed . the search network is maintained by the <method_5> , in which the first layer reflects the word net and the phone net while the second layer the <method_1> . because the <otherscientificterm_8> is tied in the second layer , the <otherscientificterm_6> is so small that it has the ability to process some complicated applications , such as the use of <method_0> , the <method_3> and the <method_7> . the <method_9> based on the <method_5> is also proposed , which is derived from the <method_10> . finally , an implementation of the <method_5> using the <method_2> is presented , and the experimental results show that the highly efficient <task_11> can be achieved without too much <otherscientificterm_6> .",
    "abstract_og": "in this paper , an approach to continuous speech recognition based on a two-layer lexical tree is proposed . the search network is maintained by the two-layer lexical tree , in which the first layer reflects the word net and the phone net while the second layer the dynamic programming . because the acoustic information is tied in the second layer , the memory cost is so small that it has the ability to process some complicated applications , such as the use of crossword context-dependent triphone models , the chinese fuzzy syllable mapping and the pronunciation modeling . the search algorithm based on the two-layer lexical tree is also proposed , which is derived from the token-passing algorithm . finally , an implementation of the two-layer lexical tree using the crossword context-dependent triphone models is presented , and the experimental results show that the highly efficient decoding can be achieved without too much memory cost ."
  },
  {
    "title": "Robust adaptive beamforming using sequential quadratic programming .",
    "entities": [
      "over/under estimation of the upper bound",
      "quadratic convex optimization problem",
      "erroneous presumed steering vector",
      "robust beam-forming techniques",
      "mismatch vector",
      "estimation process"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "quadratic convex optimization problem -- USED-FOR -- estimation process"
    ],
    "abstract": "in this paper , a new algorithm for robust adaptive beamform-ing is developed . the basic idea of the proposed algorithm is to estimate the difference between the actual and presumed steering vectors and to use this difference to correct the <otherscientificterm_2> . the <method_5> is performed iteratively where a <task_1> is solved at each iteration . unlike other <method_3> , our algorithm does not assume that the norm of the <otherscientificterm_4> is upper bounded , and hence it does not suffer from the negative effects of <otherscientificterm_0> . simulation results show the effectiveness of the proposed algorithm .",
    "abstract_og": "in this paper , a new algorithm for robust adaptive beamform-ing is developed . the basic idea of the proposed algorithm is to estimate the difference between the actual and presumed steering vectors and to use this difference to correct the erroneous presumed steering vector . the estimation process is performed iteratively where a quadratic convex optimization problem is solved at each iteration . unlike other robust beam-forming techniques , our algorithm does not assume that the norm of the mismatch vector is upper bounded , and hence it does not suffer from the negative effects of over/under estimation of the upper bound . simulation results show the effectiveness of the proposed algorithm ."
  },
  {
    "title": "Distributed correlated Q-learning for dynamic transmission control of sensor networks .",
    "entities": [
      "markovian dynamical game theoretic setting",
      "wireless sensor network",
      "correlated equilibrium policies",
      "correlated q-learning algorithm",
      "distributed transmission control",
      "spectrum bandwidth",
      "distributed algorithm",
      "markov chain",
      "decentralized feature"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "correlated q-learning algorithm -- USED-FOR -- correlated equilibrium policies",
      "markovian dynamical game theoretic setting -- USED-FOR -- distributed transmission control",
      "markov chain -- USED-FOR -- spectrum bandwidth",
      "distributed algorithm -- USED-FOR -- correlated equilibrium policies"
    ],
    "abstract": "this paper considers a <method_0> for <task_4> in a <method_1> . the available <otherscientificterm_5> is modeled as a <otherscientificterm_7> . a <method_6> named <method_3> is proposed to obtain the <otherscientificterm_2> of the <method_1> . this <method_3> has the <otherscientificterm_8> and is easily implementable in a real <method_1> . numerical example is also provided to verify the performances of the proposed <method_3> .",
    "abstract_og": "this paper considers a markovian dynamical game theoretic setting for distributed transmission control in a wireless sensor network . the available spectrum bandwidth is modeled as a markov chain . a distributed algorithm named correlated q-learning algorithm is proposed to obtain the correlated equilibrium policies of the wireless sensor network . this correlated q-learning algorithm has the decentralized feature and is easily implementable in a real wireless sensor network . numerical example is also provided to verify the performances of the proposed correlated q-learning algorithm ."
  },
  {
    "title": "Introduction of speech log-spectral priors into dereverberation based on Itakura-Saito distance minimization .",
    "entities": [
      "maximum a posteriori estimation",
      "itakura-saito distance minimization",
      "gmm of speech mel-frequency cepstral coefficients",
      "maximum-likelihood estimation",
      "is distance minimization",
      "blind speech dereverberation",
      "multi-channel linear prediction",
      "unknown reverberation processes",
      "speech log-spectral priors",
      "speech distortion",
      "priors",
      "dereverberation",
      "reverberation"
    ],
    "types": "<method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "multi-channel linear prediction -- USED-FOR -- blind speech dereverberation",
      "maximum-likelihood estimation -- USED-FOR -- multi-channel linear prediction"
    ],
    "abstract": "it has recently been shown that a <method_6> can effectively achieve <otherscientificterm_5> based on <task_3> . this approach can estimate and cancel <otherscientificterm_7> from only a few seconds of observation . however , one problem with this approach is that <task_9> may increase if we iterate the <otherscientificterm_11> more than once based on <method_1> to further reduce the <otherscientificterm_12> . to overcome this problem , we introduce <otherscientificterm_8> into this approach , and refor-mulate it based on <method_0> . two types of <otherscientificterm_10> are introduced , a gaussian mixture model -lrb- gmm -rrb- of speech log spectra , and a <otherscientificterm_2> . in the formulation , we also propose a new versatile technique to integrate such <otherscientificterm_8> with the <method_4> in a computationally efficient manner . preliminary experiments show the effectiveness of the proposed approach .",
    "abstract_og": "it has recently been shown that a multi-channel linear prediction can effectively achieve blind speech dereverberation based on maximum-likelihood estimation . this approach can estimate and cancel unknown reverberation processes from only a few seconds of observation . however , one problem with this approach is that speech distortion may increase if we iterate the dereverberation more than once based on itakura-saito distance minimization to further reduce the reverberation . to overcome this problem , we introduce speech log-spectral priors into this approach , and refor-mulate it based on maximum a posteriori estimation . two types of priors are introduced , a gaussian mixture model -lrb- gmm -rrb- of speech log spectra , and a gmm of speech mel-frequency cepstral coefficients . in the formulation , we also propose a new versatile technique to integrate such speech log-spectral priors with the is distance minimization in a computationally efficient manner . preliminary experiments show the effectiveness of the proposed approach ."
  },
  {
    "title": "PP-Attachment Disambiguation Boosted by a Gigantic Volume of Unambiguous Examples .",
    "entities": [
      "pp-attachment disambiguation method",
      "machine learning method",
      "raw corpus",
      "unambiguous examples",
      "pp-attachment disambiguation",
      "lexical preferences",
      "attachment decisions"
    ],
    "types": "<method> <method> <material> <material> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "machine learning method -- USED-FOR -- attachment decisions",
      "raw corpus -- USED-FOR -- pp-attachment disambiguation method",
      "unambiguous examples -- USED-FOR -- pp-attachment disambiguation method",
      "raw corpus -- USED-FOR -- unambiguous examples"
    ],
    "abstract": "we present a <method_0> based on a gigantic volume of <material_3> extracted from <material_2> . the <material_3> are utilized to acquire precise <otherscientificterm_5> for <task_4> . <otherscientificterm_6> are made by a <method_1> that optimizes the use of the <otherscientificterm_5> . our experiments indicate that the precise <otherscientificterm_5> work effectively .",
    "abstract_og": "we present a pp-attachment disambiguation method based on a gigantic volume of unambiguous examples extracted from raw corpus . the unambiguous examples are utilized to acquire precise lexical preferences for pp-attachment disambiguation . attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences . our experiments indicate that the precise lexical preferences work effectively ."
  },
  {
    "title": "Low cost duration modelling for noise robust speech recognition .",
    "entities": [
      "implicit geometric state duration distributions",
      "state transition matrices",
      "clean speech recognition",
      "connected digit recognition",
      "state transition probabilities",
      "explicit duration model",
      "explicit duration models",
      "noisy speech recognition",
      "true duration distributions",
      "acoustic probabilities",
      "hmm decoders",
      "duration models",
      "maximum duration",
      "complexity"
    ],
    "types": "<otherscientificterm> <method> <task> <task> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <metric>",
    "relations": [
      "duration models -- USED-FOR -- noisy speech recognition",
      "state transition matrices -- USED-FOR -- hmm decoders"
    ],
    "abstract": "state transition matrices as used in standard <method_10> have two widely perceived limitations . one is that the <otherscientificterm_0> which they model do not accurately reflect <otherscientificterm_8> . the other is that they impose no hard limit on <otherscientificterm_12> with the result that <otherscientificterm_4> often have little influence when combined with <otherscientificterm_9> , which are of a different order of magnitude . <method_6> were developed in the past to address the first problem . these were not widely taken up because their performance advantage in <task_2> was often not sufficiently great to offset the extra <metric_13> which they introduced . however , <method_11> have much greater potential when applied to <task_7> . in <method_5> paper we present a simple and generic form of <method_5> and show that <method_5> leads to strong performance improvements when applied to <task_3> in noise .",
    "abstract_og": "state transition matrices as used in standard hmm decoders have two widely perceived limitations . one is that the implicit geometric state duration distributions which they model do not accurately reflect true duration distributions . the other is that they impose no hard limit on maximum duration with the result that state transition probabilities often have little influence when combined with acoustic probabilities , which are of a different order of magnitude . explicit duration models were developed in the past to address the first problem . these were not widely taken up because their performance advantage in clean speech recognition was often not sufficiently great to offset the extra complexity which they introduced . however , duration models have much greater potential when applied to noisy speech recognition . in explicit duration model paper we present a simple and generic form of explicit duration model and show that explicit duration model leads to strong performance improvements when applied to connected digit recognition in noise ."
  },
  {
    "title": "Exact Recovery of Hard Thresholding Pursuit .",
    "entities": [
      "hard thresholding pursuit",
      "restricted strong condition number bounding conditions",
      "bounded restricted strong condition number",
      "truncated gradient descent methods",
      "parameter estimation consistency",
      "global sparse minimizer",
      "simulated data",
      "np-hard problem",
      "global minimizer",
      "approximation guarantee",
      "htp-style methods"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "global sparse minimizer -- USED-FOR -- htp-style methods",
      "restricted strong condition number bounding conditions -- FEATURE-OF -- htp-style methods",
      "hard thresholding pursuit -- HYPONYM-OF -- truncated gradient descent methods"
    ],
    "abstract": "the <method_0> is a class of <method_3> for finding sparse solutions of \u2113 0-constrained loss minimization problems . the <method_10> have been shown to have strong <otherscientificterm_9> and impressive numerical performance in high dimensional statistical learning applications . however , the current theoretical treatment of these <method_10> has traditionally been restricted to the analysis of <metric_4> . it remains an open problem to analyze the support recovery performance -lrb- a.k.a. , sparsistency -rrb- of this type of <method_10> for recovering the <otherscientificterm_8> of the original <task_7> . in this paper , we bridge this gap by showing , for the first time , that exact recovery of the <otherscientificterm_5> is possible for <method_10> under <otherscientificterm_1> . we further show that <method_10> are able to recover the support of certain relaxed sparse solutions without assuming <otherscientificterm_2> . numerical results on <material_6> confirms our theoretical predictions .",
    "abstract_og": "the hard thresholding pursuit is a class of truncated gradient descent methods for finding sparse solutions of \u2113 0-constrained loss minimization problems . the htp-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications . however , the current theoretical treatment of these htp-style methods has traditionally been restricted to the analysis of parameter estimation consistency . it remains an open problem to analyze the support recovery performance -lrb- a.k.a. , sparsistency -rrb- of this type of htp-style methods for recovering the global minimizer of the original np-hard problem . in this paper , we bridge this gap by showing , for the first time , that exact recovery of the global sparse minimizer is possible for htp-style methods under restricted strong condition number bounding conditions . we further show that htp-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number . numerical results on simulated data confirms our theoretical predictions ."
  },
  {
    "title": "Breaking Symmetries in Graph Representation .",
    "entities": [
      "extremal graph theory",
      "undirected graph search",
      "symmetry breaking constraints",
      "isomorphic representations",
      "symmetry breaking",
      "graph representation",
      "combinatorial problems",
      "undirected graph"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "symmetry breaking -- USED-FOR -- graph representation",
      "symmetry breaking constraints -- USED-FOR -- undirected graph search"
    ],
    "abstract": "there are many complex <task_6> which involve searching for an <otherscientificterm_7> satisfying a certain property . these <task_6> are often highly challenging because of the large number of <method_3> of a possible solution . in this paper we introduce novel , effective and compact , <otherscientificterm_2> for <task_1> . while incomplete , <method_3> prove highly beneficial in pruning the search for a <otherscientificterm_5> . we illustrate the application of <method_4> in <otherscientificterm_5> to resolve several open instances in <method_0> .",
    "abstract_og": "there are many complex combinatorial problems which involve searching for an undirected graph satisfying a certain property . these combinatorial problems are often highly challenging because of the large number of isomorphic representations of a possible solution . in this paper we introduce novel , effective and compact , symmetry breaking constraints for undirected graph search . while incomplete , isomorphic representations prove highly beneficial in pruning the search for a graph representation . we illustrate the application of symmetry breaking in graph representation to resolve several open instances in extremal graph theory ."
  },
  {
    "title": "Building a Scientific Concept Hierarchy Database -LRB- SCHBase -RRB- .",
    "entities": [
      "semantically-meaningful '' phylogenetic '' structure",
      "evolution of scientific discourse",
      "hierarchical database of keyphrases",
      "extracted keyphrases",
      "scientific literature",
      "keyphrases/concepts",
      "search",
      "schbase",
      "keyphrases"
    ],
    "types": "<otherscientificterm> <task> <material> <method> <material> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "schbase -- HYPONYM-OF -- hierarchical database of keyphrases",
      "scientific literature -- USED-FOR -- hierarchical database of keyphrases"
    ],
    "abstract": "extracted <otherscientificterm_8> can enhance numerous applications ranging from <task_6> to tracking the <task_1> . we present <method_7> , a <material_2> extracted from large collections of <material_4> . <method_7> relies on a tendency of scientists to generate new abbreviations that '' extend '' existing forms as a form of signaling novelty . we demonstrate how these <otherscientificterm_5> can be extracted , and their viability as a database in relation to existing collections . we further show how <otherscientificterm_8> can be placed into a <otherscientificterm_0> and describe key features of this structure .",
    "abstract_og": "extracted keyphrases can enhance numerous applications ranging from search to tracking the evolution of scientific discourse . we present schbase , a hierarchical database of keyphrases extracted from large collections of scientific literature . schbase relies on a tendency of scientists to generate new abbreviations that '' extend '' existing forms as a form of signaling novelty . we demonstrate how these keyphrases/concepts can be extracted , and their viability as a database in relation to existing collections . we further show how keyphrases can be placed into a semantically-meaningful '' phylogenetic '' structure and describe key features of this structure ."
  },
  {
    "title": "College Towns , Vacation Spots , and Tech Hubs : Using Geo-Social Media to Model and Compare Locations .",
    "entities": [
      "mul-tivariate multiple linear regression",
      "neural network models",
      "location 's interest profile",
      "location-based interest profiles",
      "per-city interest model",
      "city-based interest models",
      "interest models",
      "demo-graphics features",
      "geo-social media",
      "vacation spots",
      "geo-tagged tweets",
      "categorical perspectives",
      "art",
      "sports"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <material> <material>",
    "relations": [
      "mul-tivariate multiple linear regression -- CONJUNCTION -- neural network models",
      "mul-tivariate multiple linear regression -- USED-FOR -- location 's interest profile",
      "art -- CONJUNCTION -- sports",
      "geo-social media -- USED-FOR -- location-based interest profiles",
      "demo-graphics features -- USED-FOR -- location 's interest profile"
    ],
    "abstract": "in this paper , we explore the potential of <method_8> to construct <material_3> to uncover the hidden relationships among disparate locations . through an investigation of millions of <material_10> , we construct a <method_4> based on fourteen high-level categories -lrb- e.g. , technology , <material_12> , <material_13> -rrb- . these <method_6> support the discovery of related locations that are connected based on these <otherscientificterm_11> -lrb- e.g. , college towns or <otherscientificterm_9> -rrb- but perhaps not on the individual tweet level . we then connect these <method_5> to underlying demographic data . by building <method_0> and <method_1> we show how a <otherscientificterm_2> may be estimated based purely on its <otherscientificterm_7> .",
    "abstract_og": "in this paper , we explore the potential of geo-social media to construct location-based interest profiles to uncover the hidden relationships among disparate locations . through an investigation of millions of geo-tagged tweets , we construct a per-city interest model based on fourteen high-level categories -lrb- e.g. , technology , art , sports -rrb- . these interest models support the discovery of related locations that are connected based on these categorical perspectives -lrb- e.g. , college towns or vacation spots -rrb- but perhaps not on the individual tweet level . we then connect these city-based interest models to underlying demographic data . by building mul-tivariate multiple linear regression and neural network models we show how a location 's interest profile may be estimated based purely on its demo-graphics features ."
  },
  {
    "title": "Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning .",
    "entities": [
      "label propagation based semi-supervised learning algorithm",
      "supervised word sense dis-ambiguation methods",
      "labeled and unlabeled data",
      "global consistency assumption",
      "manually sense-tagged data",
      "learning process",
      "benchmark corpora",
      "bilingual bootstrapping",
      "monolingual bootstrapping",
      "wsd"
    ],
    "types": "<method> <method> <material> <otherscientificterm> <material> <task> <material> <method> <method> <method>",
    "relations": [
      "label propagation based semi-supervised learning algorithm -- USED-FOR -- wsd",
      "manually sense-tagged data -- USED-FOR -- supervised word sense dis-ambiguation methods",
      "label propagation based semi-supervised learning algorithm -- COMPARE -- wsd",
      "monolingual bootstrapping -- COMPARE -- bilingual bootstrapping",
      "label propagation based semi-supervised learning algorithm -- COMPARE -- bilingual bootstrapping",
      "label propagation based semi-supervised learning algorithm -- COMPARE -- monolingual bootstrapping"
    ],
    "abstract": "shortage of <material_4> is an obstacle to <method_1> . in this paper we investigate a <method_0> for <method_9> , which combines <material_2> in <task_5> to fully realize a <otherscientificterm_3> : similar examples should have similar labels . our experimental results on <material_6> indicate that <method_0> consistently out-performs <method_9> when only very few labeled examples are available , and its performance is also better than <method_8> , and comparable to <method_7> .",
    "abstract_og": "shortage of manually sense-tagged data is an obstacle to supervised word sense dis-ambiguation methods . in this paper we investigate a label propagation based semi-supervised learning algorithm for wsd , which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption : similar examples should have similar labels . our experimental results on benchmark corpora indicate that label propagation based semi-supervised learning algorithm consistently out-performs wsd when only very few labeled examples are available , and its performance is also better than monolingual bootstrapping , and comparable to bilingual bootstrapping ."
  },
  {
    "title": "Subspace matching : Unique solution to point matching with geometric constraints .",
    "entities": [
      "structure-from-motion and object recognition",
      "computation-ally hard integer problem",
      "linear programming-based algorithm",
      "vector base",
      "visual tasks",
      "feature vector",
      "unknown permutation",
      "convex set",
      "subspace",
      "noise"
    ],
    "types": "<task> <task> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "convex set -- USED-FOR -- computation-ally hard integer problem"
    ],
    "abstract": "finding correspondences between feature points is one of the most relevant problems in the whole set of <task_4> . in this paper we address the problem of matching a <otherscientificterm_5> -lrb- or a matrix -rrb- to a given <otherscientificterm_8> . given any <otherscientificterm_3> of such a <otherscientificterm_8> , we observe a linear combination of its elements with all entries swapped by an <otherscientificterm_6> . we prove that such a <task_1> is uniquely solved in a <otherscientificterm_7> resulting from relaxing the original problem . also , if <otherscientificterm_9> is present , based on this result , we provide a robust estimate recurring to a <method_2> . we use <task_0> as motivating examples .",
    "abstract_og": "finding correspondences between feature points is one of the most relevant problems in the whole set of visual tasks . in this paper we address the problem of matching a feature vector -lrb- or a matrix -rrb- to a given subspace . given any vector base of such a subspace , we observe a linear combination of its elements with all entries swapped by an unknown permutation . we prove that such a computation-ally hard integer problem is uniquely solved in a convex set resulting from relaxing the original problem . also , if noise is present , based on this result , we provide a robust estimate recurring to a linear programming-based algorithm . we use structure-from-motion and object recognition as motivating examples ."
  },
  {
    "title": "Motion Context : A New Representation for Human Action Recognition .",
    "entities": [
      "motion context",
      "human action video datasets",
      "rich 3d mc descriptor",
      "image representation techniques",
      "human action recognition",
      "direct graphical model",
      "local motion information",
      "reference point",
      "video sequences",
      "mc descriptors",
      "action recognition",
      "relative locations",
      "recognition configurations",
      "motion-based representation",
      "plsa -rrb-",
      "3d descriptor",
      "human action",
      "3-plsa",
      "mc+svm"
    ],
    "types": "<method> <material> <otherscientificterm> <method> <task> <method> <otherscientificterm> <otherscientificterm> <material> <method> <task> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "mc+svm -- USED-FOR -- direct graphical model",
      "3d descriptor -- USED-FOR -- human action",
      "rich 3d mc descriptor -- FEATURE-OF -- local motion information",
      "video sequences -- USED-FOR -- human action recognition",
      "mc+svm -- HYPONYM-OF -- recognition configurations"
    ],
    "abstract": "one of the key challenges in <task_4> from <material_8> is how to model an <otherscientificterm_16> sufficiently . therefore , in this paper we propose a novel <method_13> called <method_0> , which is insensitive to the scale and direction of an <otherscientificterm_16> , by employing <method_3> . a <method_0> captures the distribution of the motion words -lrb- mws -rrb- over <otherscientificterm_11> in a local region of the motion image -lrb- mi -rrb- around a <otherscientificterm_7> and thus summarizes the <otherscientificterm_6> in a <otherscientificterm_2> . in this way , any <otherscientificterm_16> can be represented as a <method_15> by summing up all the <method_9> of this <otherscientificterm_16> . for <task_10> , we propose 4 different <method_12> : mw + plsa , mw+svm , <method_0> + w <method_17> -lrb- a new <method_5> by extending <method_14> , and <method_18> . we test our approach on two <material_1> from kth and weizmann institute of science -lrb- wis -rrb- and our performances are quite promising . for the <method_0> , the proposed <method_13> achieves the highest performance using the proposed w <method_17> . for the <method_0> , the best performance of the proposed <method_0> is comparable to the state of the art .",
    "abstract_og": "one of the key challenges in human action recognition from video sequences is how to model an human action sufficiently . therefore , in this paper we propose a novel motion-based representation called motion context , which is insensitive to the scale and direction of an human action , by employing image representation techniques . a motion context captures the distribution of the motion words -lrb- mws -rrb- over relative locations in a local region of the motion image -lrb- mi -rrb- around a reference point and thus summarizes the local motion information in a rich 3d mc descriptor . in this way , any human action can be represented as a 3d descriptor by summing up all the mc descriptors of this human action . for action recognition , we propose 4 different recognition configurations : mw + plsa , mw+svm , motion context + w 3-plsa -lrb- a new direct graphical model by extending plsa -rrb- , and mc+svm . we test our approach on two human action video datasets from kth and weizmann institute of science -lrb- wis -rrb- and our performances are quite promising . for the motion context , the proposed motion-based representation achieves the highest performance using the proposed w 3-plsa . for the motion context , the best performance of the proposed motion context is comparable to the state of the art ."
  },
  {
    "title": "An effective and efficient utterance verification technology using word n-gram filler models .",
    "entities": [
      "interactive voice response systems",
      "utterance verification technology",
      "equal error rate",
      "word n-gram based filler model",
      "posterior probability based confidence measurement",
      "false accept rate",
      "false reject rate",
      "access control",
      "context-free grammar",
      "carrier words",
      "secret answer",
      "acoustic fillers",
      "robustness",
      "predictor",
      "accuracy"
    ],
    "types": "<method> <method> <metric> <method> <method> <metric> <metric> <task> <method> <otherscientificterm> <otherscientificterm> <method> <metric> <method> <metric>",
    "relations": [
      "utterance verification technology -- USED-FOR -- access control",
      "accuracy -- EVALUATE-FOR -- interactive voice response systems",
      "equal error rate -- EVALUATE-FOR -- utterance verification technology",
      "access control -- USED-FOR -- interactive voice response systems",
      "utterance verification technology -- USED-FOR -- context-free grammar",
      "utterance verification technology -- USED-FOR -- interactive voice response systems",
      "utterance verification technology -- USED-FOR -- carrier words"
    ],
    "abstract": "in this paper we propose a novel , effective , and efficient <method_1> for <task_7> in the <method_0> . the key of our <method_1> is to construct a <method_8> by using the <otherscientificterm_10> to a question and a <method_3> . the <method_1> provides rich alternatives to the <otherscientificterm_10> and can potentially improve the <metric_14> of the <method_0> . <method_1> can also absorb <otherscientificterm_9> used by callers and thus can improve the <metric_12> . we also propose using a <method_13> based on the best alternative to calculate the confidence . we show detailed experimental results on a tough uv test set that contains 930 positive and 930 negative cases and discuss types of questions that are suitable for the <method_0> . we demonstrate that our <method_1> can achieve a 2.14 % <metric_2> on average and 0.8 % <metric_5> if the <metric_6> is 2.6 % and above . this is a 49 % <metric_2> compared with the approaches using <method_11> , and a 72 % <metric_2> compared with the <method_4> .",
    "abstract_og": "in this paper we propose a novel , effective , and efficient utterance verification technology for access control in the interactive voice response systems . the key of our utterance verification technology is to construct a context-free grammar by using the secret answer to a question and a word n-gram based filler model . the utterance verification technology provides rich alternatives to the secret answer and can potentially improve the accuracy of the interactive voice response systems . utterance verification technology can also absorb carrier words used by callers and thus can improve the robustness . we also propose using a predictor based on the best alternative to calculate the confidence . we show detailed experimental results on a tough uv test set that contains 930 positive and 930 negative cases and discuss types of questions that are suitable for the interactive voice response systems . we demonstrate that our utterance verification technology can achieve a 2.14 % equal error rate on average and 0.8 % false accept rate if the false reject rate is 2.6 % and above . this is a 49 % equal error rate compared with the approaches using acoustic fillers , and a 72 % equal error rate compared with the posterior probability based confidence measurement ."
  },
  {
    "title": "3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points .",
    "entities": [
      "latent regression forest-based hand pose estimation framework",
      "real-time 3d hand pose estimation algorithm",
      "segmentation index points",
      "randomized decision forest framework",
      "public benchmark datasets",
      "decision forest-based methods",
      "forest growing strategy",
      "randomized feature",
      "non-leaf nodes",
      "depth image",
      "skeletal joints",
      "point cloud",
      "leaf node",
      "cpu",
      "cen-troid",
      "parallelism"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "depth image -- USED-FOR -- real-time 3d hand pose estimation algorithm",
      "segmentation index points -- USED-FOR -- forest growing strategy",
      "randomized decision forest framework -- USED-FOR -- real-time 3d hand pose estimation algorithm",
      "randomized feature -- USED-FOR -- forest growing strategy",
      "segmentation index points -- USED-FOR -- randomized feature",
      "public benchmark datasets -- EVALUATE-FOR -- real-time 3d hand pose estimation algorithm",
      "latent regression forest-based hand pose estimation framework -- USED-FOR -- forest growing strategy"
    ],
    "abstract": "in this paper , we propose a <method_1> using the <method_3> . our <method_1> takes a <otherscientificterm_9> as input and generates a set of <otherscientificterm_10> as output . previous <method_5> often give labels to all points in a <otherscientificterm_11> at a very early stage and vote for the joint locations . by contrast , our <method_1> only tracks a set of more flexible virtual landmark points , named <otherscientificterm_2> , before reaching the final decision at a <otherscientificterm_12> . roughly speaking , an <otherscientificterm_2> represents the <otherscientificterm_14> of a subset of <otherscientificterm_10> , which are to be located at the leaves of the branch expanded from the <otherscientificterm_2> . inspired by recent <method_0> -lrb- tang et al. 2014 -rrb- , we integrate <otherscientificterm_2> into the framework with several important improvements : first , we devise a new <method_6> , whose decision is made using a <otherscientificterm_7> guided by <otherscientificterm_2> . second , we speed-up the <method_6> since only <otherscientificterm_2> , not the <otherscientificterm_10> , are estimated at <otherscientificterm_8> . third , the experimental results on <material_4> show clearly the advantage of the proposed <method_1> over previous state-of-the-art methods , and our <method_1> runs at 55.5 fps on a normal <otherscientificterm_13> without <otherscientificterm_15> .",
    "abstract_og": "in this paper , we propose a real-time 3d hand pose estimation algorithm using the randomized decision forest framework . our real-time 3d hand pose estimation algorithm takes a depth image as input and generates a set of skeletal joints as output . previous decision forest-based methods often give labels to all points in a point cloud at a very early stage and vote for the joint locations . by contrast , our real-time 3d hand pose estimation algorithm only tracks a set of more flexible virtual landmark points , named segmentation index points , before reaching the final decision at a leaf node . roughly speaking , an segmentation index points represents the cen-troid of a subset of skeletal joints , which are to be located at the leaves of the branch expanded from the segmentation index points . inspired by recent latent regression forest-based hand pose estimation framework -lrb- tang et al. 2014 -rrb- , we integrate segmentation index points into the framework with several important improvements : first , we devise a new forest growing strategy , whose decision is made using a randomized feature guided by segmentation index points . second , we speed-up the forest growing strategy since only segmentation index points , not the skeletal joints , are estimated at non-leaf nodes . third , the experimental results on public benchmark datasets show clearly the advantage of the proposed real-time 3d hand pose estimation algorithm over previous state-of-the-art methods , and our real-time 3d hand pose estimation algorithm runs at 55.5 fps on a normal cpu without parallelism ."
  },
  {
    "title": "Dimensionality reduction and principal surfaces via Kernel Map Manifolds .",
    "entities": [
      "synthetic and real data sets",
      "manifold learning approach",
      "explicit projection operator",
      "principal surfaces",
      "projection operator",
      "kernel regression",
      "dimension-ality reduction",
      "objective function",
      "projection distance",
      "extremal points",
      "principal surfaces",
      "parametrized surface",
      "natural mapping",
      "distribution",
      "mapping"
    ],
    "types": "<material> <method> <method> <otherscientificterm> <method> <task> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task>",
    "relations": [
      "parametrized surface -- USED-FOR -- manifold learning approach",
      "manifold learning approach -- USED-FOR -- dimension-ality reduction",
      "kernel regression -- FEATURE-OF -- natural mapping",
      "natural mapping -- USED-FOR -- projection operator",
      "manifold learning approach -- USED-FOR -- manifold learning approach",
      "synthetic and real data sets -- EVALUATE-FOR -- manifold learning approach"
    ],
    "abstract": "we present a <method_1> to <task_6> that explicitly models the <method_1> as a <task_14> from low to high dimensional space . the <method_1> is represented as a <otherscientificterm_11> represented by a set of parameters that are defined on the input samples . the <method_1> also provides a <method_12> from high to low dimensional space , and a concatenation of these two <method_12> induces a <method_4> onto the <method_1> . the <method_2> allows for a clearly defined <otherscientificterm_7> in terms of <task_8> and reconstruction error . a formulation of the <method_12> in terms of <task_5> permits a direct optimization of the <otherscientificterm_7> and the <otherscientificterm_9> converge to <otherscientificterm_10> as the number of data to learn from increases . <otherscientificterm_3> have the desirable property that they , informally speaking , pass through the middle of a <otherscientificterm_13> . we provide a proof on the convergence to <otherscientificterm_10> and illustrate the effectiveness of the proposed <method_1> on <material_0> .",
    "abstract_og": "we present a manifold learning approach to dimension-ality reduction that explicitly models the manifold learning approach as a mapping from low to high dimensional space . the manifold learning approach is represented as a parametrized surface represented by a set of parameters that are defined on the input samples . the manifold learning approach also provides a natural mapping from high to low dimensional space , and a concatenation of these two natural mapping induces a projection operator onto the manifold learning approach . the explicit projection operator allows for a clearly defined objective function in terms of projection distance and reconstruction error . a formulation of the natural mapping in terms of kernel regression permits a direct optimization of the objective function and the extremal points converge to principal surfaces as the number of data to learn from increases . principal surfaces have the desirable property that they , informally speaking , pass through the middle of a distribution . we provide a proof on the convergence to principal surfaces and illustrate the effectiveness of the proposed manifold learning approach on synthetic and real data sets ."
  },
  {
    "title": "Multidimensional independent component analysis .",
    "entities": [
      "multidimen-sional independent component analysis",
      "independent component analysis",
      "mica decomposition of ecg signals",
      "multidimensional components",
      "bss/ica model",
      "one-dimensional subspaces"
    ],
    "types": "<task> <method> <task> <method> <method> <otherscientificterm>",
    "relations": [
      "one-dimensional subspaces -- USED-FOR -- independent component analysis",
      "independent component analysis -- USED-FOR -- multidimen-sional independent component analysis",
      "bss/ica model -- USED-FOR -- multidimensional components"
    ],
    "abstract": "this discussion paper proposes to generalize the notion of <method_1> to the notion of <task_0> . we start from the ica or blind source separation -lrb- bss -rrb- model and show that <method_1> can be uniquely identified provided <method_1> is properly parameterized in terms of <otherscientificterm_5> . from this standpoint , the <method_4> is generalized to <method_3> . we discuss how <method_1> can be adapted to <task_0> . the relevance of these ideas is illustrated by a <task_2> .",
    "abstract_og": "this discussion paper proposes to generalize the notion of independent component analysis to the notion of multidimen-sional independent component analysis . we start from the ica or blind source separation -lrb- bss -rrb- model and show that independent component analysis can be uniquely identified provided independent component analysis is properly parameterized in terms of one-dimensional subspaces . from this standpoint , the bss/ica model is generalized to multidimensional components . we discuss how independent component analysis can be adapted to multidimen-sional independent component analysis . the relevance of these ideas is illustrated by a mica decomposition of ecg signals ."
  },
  {
    "title": "Quadtree structured restoration algorithms for piecewise polynomial images .",
    "entities": [
      "image restoration algorithms",
      "piecewise polynomial images",
      "iterative shrinkage ideas",
      "nonlinear quadtree decomposition",
      "piecewise polynomial signals",
      "nonlinear manifold",
      "linear space",
      "nonlinear transformations"
    ],
    "types": "<method> <material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "iterative shrinkage ideas -- CONJUNCTION -- nonlinear quadtree decomposition",
      "nonlinear manifold -- USED-FOR -- piecewise polynomial signals",
      "nonlinear transformations -- USED-FOR -- image restoration algorithms",
      "nonlinear quadtree decomposition -- USED-FOR -- image restoration algorithms",
      "image restoration algorithms -- USED-FOR -- piecewise polynomial images"
    ],
    "abstract": "iterative shrinkage of sparse and redundant representations are at the heart of many state of the art denoising and decon-volution algorithms . they assume the signal is well approximated by a few elements from an overcomplete basis of a <otherscientificterm_6> . if one instead selects the elements from a <otherscientificterm_5> it is possible to more efficiently represent <otherscientificterm_4> . this suggests that <method_0> based around <otherscientificterm_7> could provide better results for this class of signals . this paper uses <method_2> and a <method_3> to develop <method_0> suitable for <material_1> .",
    "abstract_og": "iterative shrinkage of sparse and redundant representations are at the heart of many state of the art denoising and decon-volution algorithms . they assume the signal is well approximated by a few elements from an overcomplete basis of a linear space . if one instead selects the elements from a nonlinear manifold it is possible to more efficiently represent piecewise polynomial signals . this suggests that image restoration algorithms based around nonlinear transformations could provide better results for this class of signals . this paper uses iterative shrinkage ideas and a nonlinear quadtree decomposition to develop image restoration algorithms suitable for piecewise polynomial images ."
  },
  {
    "title": "Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model .",
    "entities": [
      "spectral graph partitioning methods",
      "random m-uniform hypergraphs",
      "planted partition model",
      "multi-way affinity relations",
      "higher order tensors",
      "rate of convergence",
      "computer science",
      "spectral algorithm",
      "m-uniform hypergraphs",
      "m-way affinities",
      "uniform hyper-graphs",
      "stochastic blockmodel",
      "spectral technique"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "planted partition model -- USED-FOR -- m-uniform hypergraphs"
    ],
    "abstract": "spectral graph partitioning <method_0> have received significant attention from both practitioners and theorists in <task_6> . some notable studies have been carried out regarding the behavior of these <method_0> for infinitely large sample size -lrb- von luxburg et al. , 2008 ; rohe et al. , 2011 -rrb- , which provide sufficient confidence to practitioners about the effectiveness of these <method_0> . on the other hand , recent developments in <task_6> have led to a plethora of applications , where the model deals with <otherscientificterm_3> and can be posed as <otherscientificterm_10> . in this paper , we view these models as <otherscientificterm_1> and establish the consistency of <method_7> in this general setting . we develop a <method_2> or <method_11> for such problems using <otherscientificterm_4> , present a <method_12> suited for the purpose and study its large sample behavior . the analysis reveals that the <method_2> is consistent for <otherscientificterm_8> for larger values of m , and also the <metric_5> improves for increasing m . our result provides the first theoretical evidence that establishes the importance of <otherscientificterm_9> .",
    "abstract_og": "spectral graph partitioning spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science . some notable studies have been carried out regarding the behavior of these spectral graph partitioning methods for infinitely large sample size -lrb- von luxburg et al. , 2008 ; rohe et al. , 2011 -rrb- , which provide sufficient confidence to practitioners about the effectiveness of these spectral graph partitioning methods . on the other hand , recent developments in computer science have led to a plethora of applications , where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs . in this paper , we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting . we develop a planted partition model or stochastic blockmodel for such problems using higher order tensors , present a spectral technique suited for the purpose and study its large sample behavior . the analysis reveals that the planted partition model is consistent for m-uniform hypergraphs for larger values of m , and also the rate of convergence improves for increasing m . our result provides the first theoretical evidence that establishes the importance of m-way affinities ."
  },
  {
    "title": "Letizia : An Agent That Assists Web Browsing .",
    "entities": [
      "world wide web",
      "user interface agent",
      "browsing behavior",
      "browsing strategy",
      "user behavior",
      "best-first search",
      "web browser",
      "netscape",
      "heuristics",
      "letizia"
    ],
    "types": "<material> <method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <method> <method>",
    "relations": [
      "best-first search -- PART-OF -- browsing strategy",
      "netscape -- HYPONYM-OF -- web browser",
      "browsing behavior -- USED-FOR -- best-first search",
      "letizia -- HYPONYM-OF -- user interface agent",
      "heuristics -- USED-FOR -- best-first search"
    ],
    "abstract": "letizia is a <method_1> that assists a user browsing the <material_0> . as the user operates a conventional <method_6> such as <method_7> , the agent tracks <otherscientificterm_4> and attempts to anticipate items of interest by doing concurrent , autonomous exploration of links from the user 's current position . the agent automates a <method_3> consisting of a <method_5> augmented by <method_8> inferring user interest from <otherscientificterm_2> .",
    "abstract_og": "letizia is a user interface agent that assists a user browsing the world wide web . as the user operates a conventional web browser such as netscape , the agent tracks user behavior and attempts to anticipate items of interest by doing concurrent , autonomous exploration of links from the user 's current position . the agent automates a browsing strategy consisting of a best-first search augmented by heuristics inferring user interest from browsing behavior ."
  },
  {
    "title": "Implementation of interconnective systems .",
    "entities": [
      "mixed constraint-and input-output-based representations of signal processing systems",
      "sensitivity analysis of systems",
      "automated system inversion",
      "signal-flow graphs",
      "systematic strategy",
      "delay-free loops",
      "edx course"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "systematic strategy -- USED-FOR -- mixed constraint-and input-output-based representations of signal processing systems"
    ],
    "abstract": "this paper proposes a <method_4> for the automated implementation of <method_0> . examples of the <method_4> are provided in synthesizing algorithms derived from <otherscientificterm_3> having <otherscientificterm_5> , as well as in performing <task_2> . an algorithm that follows the <method_4> , and which has been deployed online as part of an <otherscientificterm_6> , is discussed in greater focus . <method_1> designed using the algorithm is provided .",
    "abstract_og": "this paper proposes a systematic strategy for the automated implementation of mixed constraint-and input-output-based representations of signal processing systems . examples of the systematic strategy are provided in synthesizing algorithms derived from signal-flow graphs having delay-free loops , as well as in performing automated system inversion . an algorithm that follows the systematic strategy , and which has been deployed online as part of an edx course , is discussed in greater focus . sensitivity analysis of systems designed using the algorithm is provided ."
  },
  {
    "title": "Query Translation Disambiguation as Graph Partitioning .",
    "entities": [
      "spectral query translation model",
      "word co-occurrence statistics",
      "query translation dis-ambiguation",
      "query translation disam-biguation",
      "query translation disambiguation",
      "graph partitioning problem",
      "cross-language information retrieval",
      "translation probabilities",
      "query words",
      "query translation",
      "trec datasets",
      "bilingual dictionary",
      "weighted graph",
      "translation words"
    ],
    "types": "<method> <metric> <task> <task> <task> <task> <task> <otherscientificterm> <otherscientificterm> <task> <material> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "word co-occurrence statistics -- USED-FOR -- query translation dis-ambiguation",
      "graph partitioning problem -- USED-FOR -- query translation disambiguation",
      "query translation -- USED-FOR -- cross-language information retrieval",
      "cross-language information retrieval -- EVALUATE-FOR -- spectral query translation model",
      "trec datasets -- EVALUATE-FOR -- spectral query translation model"
    ],
    "abstract": "resolving ambiguity in the process of <task_9> is crucial to <task_6> when only a <material_11> is available . in this paper we propose a novel approach for <task_3> , named '' <method_0> '' . the proposed approach views the problem of <task_4> as a <task_5> . for a given query , a <otherscientificterm_12> is first created for all possible translations of <otherscientificterm_8> based on the co-occurrence statistics of the <otherscientificterm_13> . the best translation of the query is then determined by the most strongly connected component within the <otherscientificterm_12> . the proposed approach distinguishes from previous approaches in that the translations of all <otherscientificterm_8> are estimated simultaneously . furthermore , <otherscientificterm_7> are introduced in the proposed approach to capture the uncertainty in translating queries . empirical studies with <material_10> have shown that the <method_0> achieves a relative 20 % -50 % improvement in <task_6> , compared to other approaches that also exploit <metric_1> for <task_2> .",
    "abstract_og": "resolving ambiguity in the process of query translation is crucial to cross-language information retrieval when only a bilingual dictionary is available . in this paper we propose a novel approach for query translation disam-biguation , named '' spectral query translation model '' . the proposed approach views the problem of query translation disambiguation as a graph partitioning problem . for a given query , a weighted graph is first created for all possible translations of query words based on the co-occurrence statistics of the translation words . the best translation of the query is then determined by the most strongly connected component within the weighted graph . the proposed approach distinguishes from previous approaches in that the translations of all query words are estimated simultaneously . furthermore , translation probabilities are introduced in the proposed approach to capture the uncertainty in translating queries . empirical studies with trec datasets have shown that the spectral query translation model achieves a relative 20 % -50 % improvement in cross-language information retrieval , compared to other approaches that also exploit word co-occurrence statistics for query translation dis-ambiguation ."
  },
  {
    "title": "Similarity Metric for Curved Shapes in Euclidean Space .",
    "entities": [
      "square root velocity mapping",
      "deformation space modulo shape orientation",
      "open and closed curves",
      "direct product lie groups",
      "orientation preserving diffeomorphisms",
      "shape matching approaches",
      "elastic shape metric",
      "rigid transformation matrices",
      "full curve",
      "curved shapes",
      "local perturbations",
      "geodesic curves",
      "deformation space",
      "similarity metric",
      "lie algebra",
      "computing geodesics",
      "pre-smoothing",
      "matrices",
      "invariance"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "curved shapes -- USED-FOR -- geodesic curves",
      "direct product lie groups -- USED-FOR -- curved shapes",
      "similarity metric -- USED-FOR -- curved shapes",
      "deformation space modulo shape orientation -- CONJUNCTION -- orientation preserving diffeomorphisms",
      "square root velocity mapping -- USED-FOR -- elastic shape metric"
    ],
    "abstract": "in this paper , we introduce a <method_13> for <otherscientificterm_9> that can be described , distinctively , by ordered points . the proposed <method_13> represents a given curve as a point in the <otherscientificterm_12> , the direct product of <otherscientificterm_7> , such that the successive action of the <otherscientificterm_17> on a fixed starting point reconstructs the <otherscientificterm_8> . in general , both <otherscientificterm_2> are represented in the <otherscientificterm_1> and <otherscientificterm_4> . the use of <method_3> to represent <otherscientificterm_9> led to an explicit formula for <otherscientificterm_11> and the formulation of a <method_13> between shapes by the l 2-norm on the <otherscientificterm_14> . additionally , <otherscientificterm_18> to reparametrization or estimation of point correspondence between shapes is performed as an intermediate step for <task_15> . furthermore , since there is no computation of differential quantities on the curves , our <method_3> is more robust to <otherscientificterm_10> and needs no <method_16> . we compare our <method_13> with the <method_6> defined through the <method_0> , and other <method_5> .",
    "abstract_og": "in this paper , we introduce a similarity metric for curved shapes that can be described , distinctively , by ordered points . the proposed similarity metric represents a given curve as a point in the deformation space , the direct product of rigid transformation matrices , such that the successive action of the matrices on a fixed starting point reconstructs the full curve . in general , both open and closed curves are represented in the deformation space modulo shape orientation and orientation preserving diffeomorphisms . the use of direct product lie groups to represent curved shapes led to an explicit formula for geodesic curves and the formulation of a similarity metric between shapes by the l 2-norm on the lie algebra . additionally , invariance to reparametrization or estimation of point correspondence between shapes is performed as an intermediate step for computing geodesics . furthermore , since there is no computation of differential quantities on the curves , our direct product lie groups is more robust to local perturbations and needs no pre-smoothing . we compare our similarity metric with the elastic shape metric defined through the square root velocity mapping , and other shape matching approaches ."
  },
  {
    "title": "Speaker normalization in the MFCC domain .",
    "entities": [
      "maximum likelihood linear regression",
      "vocal tract normalization",
      "phoneme recognition task",
      "speaker independent recognisers",
      "parameter estimates",
      "mfcc domain",
      "filterbank domain",
      "recognition",
      "accuracy"
    ],
    "types": "<method> <method> <task> <method> <method> <material> <material> <task> <metric>",
    "relations": [
      "vocal tract normalization -- USED-FOR -- vocal tract normalization",
      "phoneme recognition task -- EVALUATE-FOR -- recognition"
    ],
    "abstract": "it has been shown in several recent publications that application of <method_1> is a successful method for improving the <metric_8> of <method_3> . we argue that <method_1> can be implemented in the <material_6> and propose a model to achieve this . we show how the model can be implemented directly in the <material_5> , where <method_1> may be viewed as a constrained version of <method_0> . the <method_4> produced by the model are in accord with our ideas about how <method_1> should operate to perform <method_1> . <task_7> results on a <task_2> are presented which show a small improvement in <metric_8> .",
    "abstract_og": "it has been shown in several recent publications that application of vocal tract normalization is a successful method for improving the accuracy of speaker independent recognisers . we argue that vocal tract normalization can be implemented in the filterbank domain and propose a model to achieve this . we show how the model can be implemented directly in the mfcc domain , where vocal tract normalization may be viewed as a constrained version of maximum likelihood linear regression . the parameter estimates produced by the model are in accord with our ideas about how vocal tract normalization should operate to perform vocal tract normalization . recognition results on a phoneme recognition task are presented which show a small improvement in accuracy ."
  },
  {
    "title": "Matching of Double-Sided Document Images to Remove Interference .",
    "entities": [
      "canny edge detection",
      "historical handwritten documents",
      "clear textual images",
      "remaining background interference",
      "interfering strokes",
      "cancellation effect",
      "reverse side",
      "front side",
      "images"
    ],
    "types": "<method> <material> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "canny edge detection -- USED-FOR -- remaining background interference"
    ],
    "abstract": "the national archives of singapore keeps a large volume of <material_1> . one common problem with the archives is that over the years , ink sipped through the pages of these documents such that characters on the <otherscientificterm_6> become visible and interfere with the characters on the <otherscientificterm_7> . this paper addresses this problem and develops a novel algorithm to extract <material_2> from the interference . we achieve this by mapping <material_8> from both sides of a page such that <otherscientificterm_4> seen on the <otherscientificterm_7> are matched with the strokes originating from the <otherscientificterm_6> so as to achieve a <otherscientificterm_5> . the resultant image is further subjected to an improved <method_0> to eliminate <otherscientificterm_3> . experimental results have confirmed the validity of our proposed method .",
    "abstract_og": "the national archives of singapore keeps a large volume of historical handwritten documents . one common problem with the archives is that over the years , ink sipped through the pages of these documents such that characters on the reverse side become visible and interfere with the characters on the front side . this paper addresses this problem and develops a novel algorithm to extract clear textual images from the interference . we achieve this by mapping images from both sides of a page such that interfering strokes seen on the front side are matched with the strokes originating from the reverse side so as to achieve a cancellation effect . the resultant image is further subjected to an improved canny edge detection to eliminate remaining background interference . experimental results have confirmed the validity of our proposed method ."
  },
  {
    "title": "Divergent stereo for robot navigation : learning from bees .",
    "entities": [
      "computation of optical ow eld",
      "reeex-type control of motion",
      "robot 's motion",
      "metric depth estimation",
      "range computation",
      "funneled corridor",
      "visual elds",
      "mobile robot",
      "diicult task",
      "qualitative approach",
      "diierential estimation",
      "optical axis",
      "visually-guided navigation",
      "images"
    ],
    "types": "<task> <task> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <material> <task> <method> <method> <otherscientificterm> <task> <material>",
    "relations": [
      "computation of optical ow eld -- USED-FOR -- qualitative approach",
      "images -- USED-FOR -- range computation",
      "qualitative approach -- USED-FOR -- visually-guided navigation",
      "funneled corridor -- USED-FOR -- diicult task",
      "optical axis -- FEATURE-OF -- mobile robot"
    ],
    "abstract": "a <method_9> to <task_12> based on the <task_0> is presented . the <method_9> is based on the use of two cameras mounted on a <material_7> and with the <otherscientificterm_11> directed in opposite directions such that the two <otherscientificterm_6> do not overlap -lrb- divergent stereo -rrb- ; <method_4> is based on the computation of the apparent image speed on <material_13> acquired during <otherscientificterm_2> . an example of <task_1> , driven by <method_10> of the ow eld measured by the two eyes , is presented . in particular it is shown how a <task_8> like navigating through a <otherscientificterm_5> with obstacles , is possible without the need for <task_3> .",
    "abstract_og": "a qualitative approach to visually-guided navigation based on the computation of optical ow eld is presented . the qualitative approach is based on the use of two cameras mounted on a mobile robot and with the optical axis directed in opposite directions such that the two visual elds do not overlap -lrb- divergent stereo -rrb- ; range computation is based on the computation of the apparent image speed on images acquired during robot 's motion . an example of reeex-type control of motion , driven by diierential estimation of the ow eld measured by the two eyes , is presented . in particular it is shown how a diicult task like navigating through a funneled corridor with obstacles , is possible without the need for metric depth estimation ."
  },
  {
    "title": "Collaborative Users ' Brand Preference Mining across Multiple Domains from Implicit Feedbacks .",
    "entities": [
      "bayesian personalized ranking optimization criterion",
      "user web browsing log data",
      "synthetic and real world datasets",
      "user brand preference learning problem",
      "searching and browsing behaviors",
      "latent factor model",
      "implicit feedbacks",
      "positive feedbacks",
      "collaborative filtering",
      "brand preferences",
      "learning processes",
      "data scarcity",
      "collective learning"
    ],
    "types": "<method> <material> <material> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <task> <method>",
    "relations": [
      "searching and browsing behaviors -- HYPONYM-OF -- implicit feedbacks",
      "bayesian personalized ranking optimization criterion -- USED-FOR -- latent factor model",
      "synthetic and real world datasets -- EVALUATE-FOR -- latent factor model",
      "bayesian personalized ranking optimization criterion -- USED-FOR -- collaborative filtering"
    ],
    "abstract": "advanced e-applications require comprehensive knowledge about their users ' preferences in order to provide accurate personalized services . in this paper , we propose to learn users ' preferences to product brands from their <otherscientificterm_6> such as their <otherscientificterm_4> in <material_1> . the <task_3> is challenge since -lrb- 1 -rrb- the users ' <otherscientificterm_6> are extremely sparse in various product domains ; and -lrb- 2 -rrb- we can only observe <otherscientificterm_7> from users ' behaviors . in this paper , we propose a <method_5> to collaboratively mine users ' <otherscientificterm_9> across multiple domains simultaneously . by <method_12> , the <method_10> in all the domains are mutually enhanced and hence the problem of <task_11> in each single domain can be effectively addressed . on the other hand , we learn our <method_5> with an adaption of the <method_0> which is a general learning framework for <task_8> from <otherscientificterm_6> . experiments with both <material_2> show that our proposed <method_5> significantly outperforms the baselines .",
    "abstract_og": "advanced e-applications require comprehensive knowledge about their users ' preferences in order to provide accurate personalized services . in this paper , we propose to learn users ' preferences to product brands from their implicit feedbacks such as their searching and browsing behaviors in user web browsing log data . the user brand preference learning problem is challenge since -lrb- 1 -rrb- the users ' implicit feedbacks are extremely sparse in various product domains ; and -lrb- 2 -rrb- we can only observe positive feedbacks from users ' behaviors . in this paper , we propose a latent factor model to collaboratively mine users ' brand preferences across multiple domains simultaneously . by collective learning , the learning processes in all the domains are mutually enhanced and hence the problem of data scarcity in each single domain can be effectively addressed . on the other hand , we learn our latent factor model with an adaption of the bayesian personalized ranking optimization criterion which is a general learning framework for collaborative filtering from implicit feedbacks . experiments with both synthetic and real world datasets show that our proposed latent factor model significantly outperforms the baselines ."
  },
  {
    "title": "The Role of Macros in Tractable Planning over Causal Graphs .",
    "entities": [
      "exponential length plans",
      "planning algorithm",
      "causal graph",
      "polynomial time",
      "planning domains",
      "complexity",
      "macros"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <metric> <otherscientificterm>",
    "relations": [
      "causal graph -- USED-FOR -- planning algorithm",
      "polynomial time -- FEATURE-OF -- exponential length plans",
      "macros -- USED-FOR -- planning domains"
    ],
    "abstract": "the <metric_5> of existing planners is bounded by the length of the resulting plan , a fact that limits planning to domains with relatively short solutions . we present a novel <method_1> that uses the <otherscientificterm_2> of a domain to decompose it into sub-problems and stores subproblem plans in memory as <otherscientificterm_6> . in many domains , the resulting plan can be expressed using relatively few <otherscientificterm_6> , making it possible to generate <otherscientificterm_0> in <otherscientificterm_3> . we show that our <method_1> is complete , and that there exist special cases for which it is optimal and polynomial . experimental results demonstrate the potential of using <otherscientificterm_6> to solve <task_4> with long solution plans .",
    "abstract_og": "the complexity of existing planners is bounded by the length of the resulting plan , a fact that limits planning to domains with relatively short solutions . we present a novel planning algorithm that uses the causal graph of a domain to decompose it into sub-problems and stores subproblem plans in memory as macros . in many domains , the resulting plan can be expressed using relatively few macros , making it possible to generate exponential length plans in polynomial time . we show that our planning algorithm is complete , and that there exist special cases for which it is optimal and polynomial . experimental results demonstrate the potential of using macros to solve planning domains with long solution plans ."
  },
  {
    "title": "Two-stream emotion recognition for call center monitoring .",
    "entities": [
      "two-stream processing of speech signals",
      "semantics of the conversation",
      "real-word call-center data",
      "emotion detection",
      "ldc corpus",
      "two-stream process",
      "extracting emotion",
      "acoustic features",
      "emotion category",
      "call-center data",
      "confidence level",
      "probabilistic measure",
      "semantics"
    ],
    "types": "<task> <otherscientificterm> <material> <task> <material> <method> <task> <otherscientificterm> <otherscientificterm> <material> <metric> <metric> <otherscientificterm>",
    "relations": [
      "two-stream processing of speech signals -- USED-FOR -- emotion detection",
      "two-stream process -- USED-FOR -- extracting emotion"
    ],
    "abstract": "we present a technique for <task_0> for <task_3> . the first stream recognises emotion from <otherscientificterm_7> while the second stream recognises emotion from the <otherscientificterm_1> . a <metric_11> is derived for each of the individual streams and the <otherscientificterm_8> from the two streams is recognised . the output of the two streams is combined to generate a score for a particular <otherscientificterm_8> . the <metric_10> of each stream is used to weigh the scores from the two streams while generating the final score . this technique is extremely significant for <material_9> that have some <otherscientificterm_12> associated with the speech . the proposed technique is evaluated on the <material_4> and on the <material_2> . experiments suggest that use of a <method_5> provides better results than the existing techniques of <task_6> only from <otherscientificterm_7> .",
    "abstract_og": "we present a technique for two-stream processing of speech signals for emotion detection . the first stream recognises emotion from acoustic features while the second stream recognises emotion from the semantics of the conversation . a probabilistic measure is derived for each of the individual streams and the emotion category from the two streams is recognised . the output of the two streams is combined to generate a score for a particular emotion category . the confidence level of each stream is used to weigh the scores from the two streams while generating the final score . this technique is extremely significant for call-center data that have some semantics associated with the speech . the proposed technique is evaluated on the ldc corpus and on the real-word call-center data . experiments suggest that use of a two-stream process provides better results than the existing techniques of extracting emotion only from acoustic features ."
  },
  {
    "title": "Target Tracking with Online Feature Selection in FLIR Imagery .",
    "entities": [
      "particle filter-based target tracking algorithm",
      "dual foreground and background model",
      "online feature selection technique",
      "unified bayesian estimation framework",
      "particle filtering approach",
      "foreground-based target model",
      "online feature selection",
      "joint target tracking",
      "tracking confidence",
      "target tracking",
      "feature selection",
      "flir imagery",
      "target representation",
      "feature",
      "tracker"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <task> <metric> <task> <task> <task> <method> <otherscientificterm> <method>",
    "relations": [
      "dual foreground and background model -- USED-FOR -- target representation",
      "particle filter-based target tracking algorithm -- USED-FOR -- flir imagery",
      "foreground-based target model -- CONJUNCTION -- tracker",
      "online feature selection technique -- USED-FOR -- tracking confidence",
      "particle filtering approach -- USED-FOR -- feature selection"
    ],
    "abstract": "we present a <method_0> for <task_11> . a <method_1> is proposed for <method_12> which supports robust and accurate <task_9> and size estimation . a novel <method_2> is introduced that is able to adaptively select the optimal <otherscientificterm_13> to maximize the <metric_8> . moreover , a coupled <method_4> is developed for <task_7> and <task_10> in an <method_3> . the experimental results show that the proposed <method_0> can accurately track poorly-visible targets in <task_11> even with strong ego-motion . the <metric_8> performance is improved when compared to the <method_14> with a <method_5> and without <method_6> .",
    "abstract_og": "we present a particle filter-based target tracking algorithm for flir imagery . a dual foreground and background model is proposed for target representation which supports robust and accurate target tracking and size estimation . a novel online feature selection technique is introduced that is able to adaptively select the optimal feature to maximize the tracking confidence . moreover , a coupled particle filtering approach is developed for joint target tracking and feature selection in an unified bayesian estimation framework . the experimental results show that the proposed particle filter-based target tracking algorithm can accurately track poorly-visible targets in flir imagery even with strong ego-motion . the tracking confidence performance is improved when compared to the tracker with a foreground-based target model and without online feature selection ."
  },
  {
    "title": "Signal processing challenges for radio astronomical arrays .",
    "entities": [
      "square kilometre array",
      "data processing systems",
      "imaging limitations",
      "image reconstruction",
      "computational requirements",
      "radio telescopes"
    ],
    "types": "<method> <task> <otherscientificterm> <task> <otherscientificterm> <material>",
    "relations": [
      "square kilometre array -- HYPONYM-OF -- radio telescopes"
    ],
    "abstract": "current and future <material_5> , in particular the <method_0> , are envisaged to produce large images -lrb- > 10 8 pixels -rrb- with over 60 db dynamic range . this poses a number of <task_3> and technological challenges , which will require novel approaches to <task_3> and design of <task_1> . in this paper , we sketch the limitations of current algorithms by extrapolating their <otherscientificterm_4> to future <material_5> as well as by discussing their <otherscientificterm_2> . we discuss a number of potential research directions to cope with these challenges .",
    "abstract_og": "current and future radio telescopes , in particular the square kilometre array , are envisaged to produce large images -lrb- > 10 8 pixels -rrb- with over 60 db dynamic range . this poses a number of image reconstruction and technological challenges , which will require novel approaches to image reconstruction and design of data processing systems . in this paper , we sketch the limitations of current algorithms by extrapolating their computational requirements to future radio telescopes as well as by discussing their imaging limitations . we discuss a number of potential research directions to cope with these challenges ."
  },
  {
    "title": "Semantic Co-segmentation in Videos .",
    "entities": [
      "video object segmentation datasets",
      "temporally connected segments",
      "semantic objects",
      "semantic co-segmentation",
      "object-like tracklets",
      "visual semantics",
      "co-segmentation process",
      "tracking-based approach",
      "object properties",
      "submodular function",
      "cluttered backgrounds",
      "deformed shapes",
      "appearances",
      "shapes"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "tracking-based approach -- USED-FOR -- semantic objects",
      "appearances -- HYPONYM-OF -- object properties",
      "appearances -- CONJUNCTION -- shapes",
      "shapes -- HYPONYM-OF -- object properties"
    ],
    "abstract": "discovering and segmenting objects in videos is a challenging task due to large variations of objects in <otherscientificterm_12> , <otherscientificterm_11> and <otherscientificterm_10> . in this paper , we propose to segment objects and understand their <otherscientificterm_5> from a collection of videos that link to each other , which we refer to as <method_3> . without any prior knowledge on videos , we first extract <otherscientificterm_2> and utilize a <method_7> to generate multiple <otherscientificterm_4> across the video . each tracklet maintains <otherscientificterm_1> and is associated with a predicted category . to exploit rich information from other videos , we collect tracklets that are assigned to the same category from all videos , and co-select tracklets that belong to true objects by solving a <otherscientificterm_9> . this function accounts for <otherscientificterm_8> such as <otherscientificterm_12> , <otherscientificterm_13> and motions , and hence facilitates the <task_6> . experiments on three <material_0> show that the proposed algorithm performs favorably against the other state-of-the-art methods .",
    "abstract_og": "discovering and segmenting objects in videos is a challenging task due to large variations of objects in appearances , deformed shapes and cluttered backgrounds . in this paper , we propose to segment objects and understand their visual semantics from a collection of videos that link to each other , which we refer to as semantic co-segmentation . without any prior knowledge on videos , we first extract semantic objects and utilize a tracking-based approach to generate multiple object-like tracklets across the video . each tracklet maintains temporally connected segments and is associated with a predicted category . to exploit rich information from other videos , we collect tracklets that are assigned to the same category from all videos , and co-select tracklets that belong to true objects by solving a submodular function . this function accounts for object properties such as appearances , shapes and motions , and hence facilitates the co-segmentation process . experiments on three video object segmentation datasets show that the proposed algorithm performs favorably against the other state-of-the-art methods ."
  },
  {
    "title": "Magnetic beamforming for wireless power transfer .",
    "entities": [
      "near-field wireless power transfer",
      "semidefinite programming problem",
      "magnetic resonant coupling",
      "receiver coil",
      "semidefinite relaxation",
      "identical tx resistances",
      "uncoordinated benchmark scheme",
      "equal current allocation",
      "mrc-wpt system",
      "rx load",
      "tx coils",
      "minimum power",
      "magnetic beamforming",
      "tx coil",
      "voltage sources",
      "optimization problem",
      "peak voltage",
      "rx coil",
      "wpt efficiency",
      "magnetic fields",
      "deliverable power",
      "beam",
      "txs",
      "rx"
    ],
    "types": "<task> <task> <task> <otherscientificterm> <method> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "magnetic resonant coupling -- USED-FOR -- near-field wireless power transfer",
      "txs -- CONJUNCTION -- rx",
      "tx coil -- CONJUNCTION -- rx coil"
    ],
    "abstract": "magnetic resonant coupling -lrb- mrc -rrb- is an efficient method for realizing the <task_0> . the use of multiple transmitters -lrb- <otherscientificterm_22> -rrb- each with one coil can be applied to enhance the wpt performance by focusing the <otherscientificterm_19> from all <otherscientificterm_10> in a <otherscientificterm_21> toward the <otherscientificterm_3> , termed as '' <method_12> '' . in this paper , we study the optimal <method_12> for an <method_8> with multiple <otherscientificterm_22> and a single <otherscientificterm_23> . we formulate an <task_15> to jointly design the currents flowing through different <otherscientificterm_22> so as to minimize the total power drawn from their <otherscientificterm_14> , subject to the <otherscientificterm_11> required by the <otherscientificterm_9> as well as the <otherscientificterm_22> ' constraints on the <otherscientificterm_16> and current . for the special case of <otherscientificterm_5> and neglecting all <otherscientificterm_22> ' constraints on the <otherscientificterm_16> and current , we show that the optimal current magnitude of each <task_2> is proportional to the mutual inductance between its <otherscientificterm_13> and the <otherscientificterm_17> . in general , the problem is a non-convex quadratically constrained quadratic programming -lrb- qcqp -rrb- problem , which is reformulated as a <task_1> . we show that its <method_4> is tight . numerical results show that <method_12> significantly enhances the <otherscientificterm_20> as well as the <metric_18> over the <method_6> of <metric_7> .",
    "abstract_og": "magnetic resonant coupling -lrb- mrc -rrb- is an efficient method for realizing the near-field wireless power transfer . the use of multiple transmitters -lrb- txs -rrb- each with one coil can be applied to enhance the wpt performance by focusing the magnetic fields from all tx coils in a beam toward the receiver coil , termed as '' magnetic beamforming '' . in this paper , we study the optimal magnetic beamforming for an mrc-wpt system with multiple txs and a single rx . we formulate an optimization problem to jointly design the currents flowing through different txs so as to minimize the total power drawn from their voltage sources , subject to the minimum power required by the rx load as well as the txs ' constraints on the peak voltage and current . for the special case of identical tx resistances and neglecting all txs ' constraints on the peak voltage and current , we show that the optimal current magnitude of each magnetic resonant coupling is proportional to the mutual inductance between its tx coil and the rx coil . in general , the problem is a non-convex quadratically constrained quadratic programming -lrb- qcqp -rrb- problem , which is reformulated as a semidefinite programming problem . we show that its semidefinite relaxation is tight . numerical results show that magnetic beamforming significantly enhances the deliverable power as well as the wpt efficiency over the uncoordinated benchmark scheme of equal current allocation ."
  },
  {
    "title": "Variable-length versus fixed-length coding : On tradeoffs for soft-decision decoding .",
    "entities": [
      "variable-length codes",
      "fixed-length codes",
      "bit-wise channel reliability information",
      "variable-length soft-decision decoder",
      "quantization bit rate",
      "variable block lengths",
      "media transmission",
      "awgn channels",
      "transmission errors",
      "bcjr algorithm",
      "error robustness",
      "trellis representation",
      "source correlation",
      "audio coding",
      "hard-decision decoding",
      "soft-decision decoding"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <task> <method> <otherscientificterm> <method> <metric> <method> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "trellis representation -- USED-FOR -- bcjr algorithm",
      "trellis representation -- USED-FOR -- variable-length codes",
      "error robustness -- EVALUATE-FOR -- hard-decision decoding",
      "source correlation -- CONJUNCTION -- variable-length codes",
      "quantization bit rate -- CONJUNCTION -- variable-length codes",
      "quantization bit rate -- CONJUNCTION -- source correlation",
      "variable-length codes -- COMPARE -- awgn channels",
      "source correlation -- CONJUNCTION -- variable block lengths",
      "variable block lengths -- FEATURE-OF -- variable-length codes",
      "bit-wise channel reliability information -- USED-FOR -- variable-length soft-decision decoder",
      "variable-length codes -- USED-FOR -- transmission errors",
      "fixed-length codes -- COMPARE -- variable-length codes",
      "variable-length codes -- USED-FOR -- audio coding",
      "variable-length codes -- USED-FOR -- media transmission"
    ],
    "abstract": "variable-length codes -lrb- <method_0> -rrb- are widely used in <task_6> . compared to <method_1> , <method_0> can represent the same message with a lower bit rate , thus having a better compression performance . but inevitably , <method_0> are very sensitive to <otherscientificterm_8> . in this work , based on the <method_11> for <method_0> and the <method_9> , we present a <method_3> utilizing <otherscientificterm_2> and achieving a better <metric_10> in contrast to <method_14> . given the application of <method_0> in <task_13> showing both <otherscientificterm_12> and <otherscientificterm_5> , a strong dependency of performance is observed for both . therefore , we point out tradeoffs of -lrb- soft-decision -rrb- decoded flcs and <method_0> depending on <metric_4> , <otherscientificterm_12> , and block length . we find that <method_0> over <method_7> are only recommended for very low <otherscientificterm_12> in combination with very short block lengths and <method_15> .",
    "abstract_og": "variable-length codes -lrb- variable-length codes -rrb- are widely used in media transmission . compared to fixed-length codes , variable-length codes can represent the same message with a lower bit rate , thus having a better compression performance . but inevitably , variable-length codes are very sensitive to transmission errors . in this work , based on the trellis representation for variable-length codes and the bcjr algorithm , we present a variable-length soft-decision decoder utilizing bit-wise channel reliability information and achieving a better error robustness in contrast to hard-decision decoding . given the application of variable-length codes in audio coding showing both source correlation and variable block lengths , a strong dependency of performance is observed for both . therefore , we point out tradeoffs of -lrb- soft-decision -rrb- decoded flcs and variable-length codes depending on quantization bit rate , source correlation , and block length . we find that variable-length codes over awgn channels are only recommended for very low source correlation in combination with very short block lengths and soft-decision decoding ."
  },
  {
    "title": "Predictive Projections .",
    "entities": [
      "nearest neighbor style learning",
      "linear dimensionality reduction algorithm",
      "high dimensional state spaces",
      "synthetic pendulum balancing domain",
      "reinforcement learning algorithms",
      "learning control policies",
      "visually guided control",
      "robot domain",
      "features"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "visually guided control -- USED-FOR -- robot domain",
      "synthetic pendulum balancing domain -- USED-FOR -- linear dimensionality reduction algorithm"
    ],
    "abstract": "this paper addresses the problem of <task_5> in very <otherscientificterm_2> . we propose a <method_1> that discovers predictive projections : projections in which accurate predictions of future states can be made using simple <method_0> . the goal of this work is to extend the reach of existing <method_4> to domains where they would otherwise be inap-plicable without extensive engineering of <otherscientificterm_8> . the <method_1> is demonstrated on a <material_3> , as well as on a <otherscientificterm_7> requiring <otherscientificterm_6> .",
    "abstract_og": "this paper addresses the problem of learning control policies in very high dimensional state spaces . we propose a linear dimensionality reduction algorithm that discovers predictive projections : projections in which accurate predictions of future states can be made using simple nearest neighbor style learning . the goal of this work is to extend the reach of existing reinforcement learning algorithms to domains where they would otherwise be inap-plicable without extensive engineering of features . the linear dimensionality reduction algorithm is demonstrated on a synthetic pendulum balancing domain , as well as on a robot domain requiring visually guided control ."
  },
  {
    "title": "Optimizing components for handheld two-way speech translation for an English-iraqi Arabic system .",
    "entities": [
      "handheld two-way speech translation system",
      "field usable handheld device",
      "translation quality feedback",
      "speech-to-speech translation",
      "training time",
      "back translations",
      "handheld device",
      "fieldable systems",
      "tts components",
      "handheld",
      "asr",
      "iraqi",
      "english"
    ],
    "types": "<method> <method> <otherscientificterm> <task> <metric> <otherscientificterm> <task> <method> <method> <material> <task> <material> <material>",
    "relations": [
      "field usable handheld device -- USED-FOR -- speech-to-speech translation",
      "handheld device -- USED-FOR -- speech-to-speech translation",
      "english -- CONJUNCTION -- iraqi"
    ],
    "abstract": "this paper described our <method_0> for <material_12> and <material_11> . the focus is on developing a <method_1> for <task_3> . the computation and memory limitations on the <material_9> impose critical constraints on the <task_10> , smt , and <method_8> . in this paper we discuss our approaches to optimize these components for the <task_6> and present performance numbers from the evaluations that were an integral part of the project . since one major aspect of the transtac program is to build <method_7> , we spent significant effort on developing an intuitive interface that minimizes the <metric_4> for users but also provides useful information such as <otherscientificterm_5> for <otherscientificterm_2> .",
    "abstract_og": "this paper described our handheld two-way speech translation system for english and iraqi . the focus is on developing a field usable handheld device for speech-to-speech translation . the computation and memory limitations on the handheld impose critical constraints on the asr , smt , and tts components . in this paper we discuss our approaches to optimize these components for the handheld device and present performance numbers from the evaluations that were an integral part of the project . since one major aspect of the transtac program is to build fieldable systems , we spent significant effort on developing an intuitive interface that minimizes the training time for users but also provides useful information such as back translations for translation quality feedback ."
  },
  {
    "title": "Corroborating Text Evaluation Results with Heterogeneous Measures .",
    "entities": [
      "text evaluation measure",
      "n-gram based measures",
      "human-produced references",
      "text evaluation",
      "heterogeneous measures",
      "rouge",
      "bleu"
    ],
    "types": "<metric> <method> <otherscientificterm> <task> <method> <metric> <metric>",
    "relations": [
      "bleu -- HYPONYM-OF -- n-gram based measures",
      "bleu -- CONJUNCTION -- rouge",
      "rouge -- HYPONYM-OF -- n-gram based measures"
    ],
    "abstract": "automatically produced texts -lrb- e.g. translations or summaries -rrb- are usually evaluated with <method_1> such as <metric_6> or <metric_5> , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes . in this paper we first present an in-depth analysis of the state of the art in order to clarify this issue . after this , we formalize and verify empirically a set of properties that every <metric_0> based on similarity to <otherscientificterm_2> satisfies . these properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process . in addition , the greater the heterogeneity of the measures -lrb- which is measurable -rrb- the higher their combined reliability . these results support the use of <method_4> in order to consolidate <task_3> results .",
    "abstract_og": "automatically produced texts -lrb- e.g. translations or summaries -rrb- are usually evaluated with n-gram based measures such as bleu or rouge , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes . in this paper we first present an in-depth analysis of the state of the art in order to clarify this issue . after this , we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies . these properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process . in addition , the greater the heterogeneity of the measures -lrb- which is measurable -rrb- the higher their combined reliability . these results support the use of heterogeneous measures in order to consolidate text evaluation results ."
  },
  {
    "title": "Directions in Hybrid Intelligence : Complementing AI Systems with Human Intelligence .",
    "entities": [
      "machine and human intelligence",
      "hybrid intelligence systems",
      "reasoning methods",
      "human intelligence",
      "ai systems"
    ],
    "types": "<otherscientificterm> <method> <method> <material> <method>",
    "relations": [
      "hybrid intelligence systems -- USED-FOR -- ai systems",
      "reasoning methods -- USED-FOR -- human intelligence",
      "reasoning methods -- USED-FOR -- hybrid intelligence systems",
      "hybrid intelligence systems -- USED-FOR -- human intelligence",
      "machine and human intelligence -- USED-FOR -- hybrid intelligence systems"
    ],
    "abstract": "hybrid intelligence systems combine <otherscientificterm_0> to overcome the shortcomings of existing <method_4> . this paper reviews recent research efforts towards developing <method_1> focusing on <method_2> for optimizing access to <material_3> and on gaining comprehensive understanding of humans as helpers of <method_4> . it concludes by discussing short and long term research directions .",
    "abstract_og": "hybrid intelligence systems combine machine and human intelligence to overcome the shortcomings of existing ai systems . this paper reviews recent research efforts towards developing hybrid intelligence systems focusing on reasoning methods for optimizing access to human intelligence and on gaining comprehensive understanding of humans as helpers of ai systems . it concludes by discussing short and long term research directions ."
  },
  {
    "title": "Greedy Layer-Wise Training of Deep Networks .",
    "entities": [
      "deep belief networks",
      "greedy layer-wise unsupervised learning algorithm",
      "greedy layer-wise unsu-pervised training strategy",
      "complexity theory of circuits",
      "non-linear and highly-varying functions",
      "deep multi-layer neural networks",
      "internal distributed representations",
      "hidden causal variables",
      "input distribution",
      "shallow architectures",
      "computational elements",
      "generative model",
      "gradient-based optimization",
      "optimization problem",
      "supervised task",
      "random initialization",
      "deep architectures",
      "non-linearities",
      "optimization",
      "generalization"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <task> <otherscientificterm> <method> <otherscientificterm> <task> <method>",
    "relations": [
      "deep architectures -- COMPARE -- shallow architectures",
      "random initialization -- USED-FOR -- gradient-based optimization"
    ],
    "abstract": "complexity theory of circuits strongly suggests that <method_16> can be much more efficient -lrb- sometimes exponentially -rrb- than <otherscientificterm_9> , in terms of <otherscientificterm_10> required to represent some functions . <method_5> have many levels of <otherscientificterm_17> allowing them to compactly represent highly <otherscientificterm_4> . however , until recently it was not clear how to train such <method_16> , since <method_12> starting from <otherscientificterm_15> appears to often get stuck in poor solutions . hin-ton et al. recently introduced a <method_1> for <method_0> , a <method_11> with many layers of <otherscientificterm_7> . in the context of the above <task_13> , we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the <otherscientificterm_8> is not revealing enough about the variable to be predicted in a <task_14> . our experiments also confirm the hypothesis that the <method_2> mostly helps the <task_18> , by initializing weights in a region near a good local minimum , giving rise to <method_6> that are high-level abstractions of the input , bringing better <method_19> .",
    "abstract_og": "complexity theory of circuits strongly suggests that deep architectures can be much more efficient -lrb- sometimes exponentially -rrb- than shallow architectures , in terms of computational elements required to represent some functions . deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions . however , until recently it was not clear how to train such deep architectures , since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions . hin-ton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for deep belief networks , a generative model with many layers of hidden causal variables . in the context of the above optimization problem , we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task . our experiments also confirm the hypothesis that the greedy layer-wise unsu-pervised training strategy mostly helps the optimization , by initializing weights in a region near a good local minimum , giving rise to internal distributed representations that are high-level abstractions of the input , bringing better generalization ."
  },
  {
    "title": "A Model for Aggregating Contributions of Synergistic Crowdsourcing Workflows .",
    "entities": [
      "quality control methods",
      "iterative improvement workflow",
      "assembly model",
      "task processing",
      "crowdsourcing tasks",
      "iterative method",
      "crowdsourcing topics",
      "pomdp"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <material> <method>",
    "relations": [
      "pomdp -- USED-FOR -- iterative method"
    ],
    "abstract": "one of the most important <material_6> is to study the effective <method_0> so as to reduce the cost and to guarantee the quality of <method_3> . as an effective approach , <method_1> is known to choose the best result from multiple workflows . however , for complex <task_4> that consists of a certain number of subtasks under some specific constraints , but can not be split into subtasks to be crowdsourced , the approach merely considers the best workflow without integrating the contributions of all workflows , which potentially results in extra costs for more iterations . in this paper , we propose an <method_2> to integrate the best output of subtasks from different workflows . moreover , we devise an efficient <method_5> based on <method_7> to improve the quality of assembled output . empirical studies confirms the superiority of our proposed <method_2> .",
    "abstract_og": "one of the most important crowdsourcing topics is to study the effective quality control methods so as to reduce the cost and to guarantee the quality of task processing . as an effective approach , iterative improvement workflow is known to choose the best result from multiple workflows . however , for complex crowdsourcing tasks that consists of a certain number of subtasks under some specific constraints , but can not be split into subtasks to be crowdsourced , the approach merely considers the best workflow without integrating the contributions of all workflows , which potentially results in extra costs for more iterations . in this paper , we propose an assembly model to integrate the best output of subtasks from different workflows . moreover , we devise an efficient iterative method based on pomdp to improve the quality of assembled output . empirical studies confirms the superiority of our proposed assembly model ."
  },
  {
    "title": "Random Multiresolution Representations for Arbitrary Sensor Network Graphs .",
    "entities": [
      "distributed multiresolution representation of sensor network data",
      "local neighborhoods of the communication graph",
      "long distance coordination",
      "distributed encoding algorithm",
      "large-scale summaries",
      "large-scale trends",
      "mobile collector",
      "randomized encoding",
      "cluster heads",
      "small-scale details",
      "local details",
      "lossless representation",
      "de-terministic hierarchies",
      "sensor node",
      "sensor nodes",
      "local querier",
      "global querier",
      "distributed computation",
      "lossy",
      "encoding",
      "robustness"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <metric>",
    "relations": [
      "cluster heads -- CONJUNCTION -- de-terministic hierarchies",
      "distributed multiresolution representation of sensor network data -- USED-FOR -- large-scale summaries",
      "mobile collector -- HYPONYM-OF -- global querier",
      "large-scale trends -- CONJUNCTION -- local details"
    ],
    "abstract": "we propose a <method_0> so that <material_4> are readily available by querying a small fraction of <otherscientificterm_14> , anywhere in the network , and <otherscientificterm_9> are available by querying a larger number of sensors , locally in the region of interest . a <otherscientificterm_16> -lrb- such as a <method_6> or unmanned aerial vehicle -rrb- can obtain a <method_18> to <method_11> of the network data , according to the desired resolution . a <method_15> -lrb- such as a <otherscientificterm_13> -rrb- can also obtain either <otherscientificterm_5> or <otherscientificterm_10> , by querying its immediate neighborhood . we want the <otherscientificterm_19> to be robust to arbitrary , even time-varying , wireless communication con-nectivity graphs . thus we want to avoid <otherscientificterm_8> or <otherscientificterm_12> that are not robust to single points of failure . we propose a <method_7> which enables both <metric_20> , and <method_17> that does not require <otherscientificterm_2> or awareness of network connectivity at individual sensors . our <method_3> operates on <otherscientificterm_1> .",
    "abstract_og": "we propose a distributed multiresolution representation of sensor network data so that large-scale summaries are readily available by querying a small fraction of sensor nodes , anywhere in the network , and small-scale details are available by querying a larger number of sensors , locally in the region of interest . a global querier -lrb- such as a mobile collector or unmanned aerial vehicle -rrb- can obtain a lossy to lossless representation of the network data , according to the desired resolution . a local querier -lrb- such as a sensor node -rrb- can also obtain either large-scale trends or local details , by querying its immediate neighborhood . we want the encoding to be robust to arbitrary , even time-varying , wireless communication con-nectivity graphs . thus we want to avoid cluster heads or de-terministic hierarchies that are not robust to single points of failure . we propose a randomized encoding which enables both robustness , and distributed computation that does not require long distance coordination or awareness of network connectivity at individual sensors . our distributed encoding algorithm operates on local neighborhoods of the communication graph ."
  },
  {
    "title": "Maximum Variance Correction with Application to A * Search .",
    "entities": [
      "maximum variance correction",
      "maximum variance unfolding",
      "speed-up a * search",
      "man-ifold learning literature",
      "manifold learning algorithm",
      "search time",
      "manifold learning",
      "post-processing embed-dings",
      "mvu embeddings",
      "heuristics"
    ],
    "types": "<method> <task> <task> <material> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "maximum variance correction -- USED-FOR -- mvu embeddings",
      "mvu embeddings -- USED-FOR -- heuristics",
      "maximum variance correction -- USED-FOR -- maximum variance unfolding",
      "manifold learning algorithm -- USED-FOR -- post-processing embed-dings"
    ],
    "abstract": "in this paper we introduce <method_0> , which finds large-scale feasible solutions to <task_1> by <otherscientificterm_7> from any <method_4> . <method_0> increases the scale of <otherscientificterm_8> by several orders of magnitude and is naturally parallel . this unprecedented scala-bility opens up new avenues of applications for <task_6> , in particular the use of <otherscientificterm_8> as effective <method_9> to <task_2> . we demonstrate unmatched reductions in <otherscientificterm_5> across several non-trivial a * benchmark search problems and bridge the gap between the <material_3> and one of its most promising high impact applications .",
    "abstract_og": "in this paper we introduce maximum variance correction , which finds large-scale feasible solutions to maximum variance unfolding by post-processing embed-dings from any manifold learning algorithm . maximum variance correction increases the scale of mvu embeddings by several orders of magnitude and is naturally parallel . this unprecedented scala-bility opens up new avenues of applications for manifold learning , in particular the use of mvu embeddings as effective heuristics to speed-up a * search . we demonstrate unmatched reductions in search time across several non-trivial a * benchmark search problems and bridge the gap between the man-ifold learning literature and one of its most promising high impact applications ."
  },
  {
    "title": "Sub-state tying in tied mixture hidden Markov models .",
    "entities": [
      "complexity-accuracy tradeoff solution",
      "classification error rate",
      "gaussian density sharing",
      "state emission probabilities",
      "sub-state level",
      "gaussian sharing",
      "reduction technique",
      "optimization technique",
      "whole-state tying",
      "sub-state tying",
      "error rate",
      "gaussians"
    ],
    "types": "<method> <metric> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <task> <task> <metric> <method>",
    "relations": [
      "classification error rate -- COMPARE -- gaussian sharing",
      "optimization technique -- USED-FOR -- complexity-accuracy tradeoff solution",
      "gaussian density sharing -- CONJUNCTION -- sub-state tying",
      "gaussian sharing -- CONJUNCTION -- whole-state tying"
    ],
    "abstract": "an approach is proposed for partial tying of states of tied-mixture hidden markov models . to facilitate tying at the <otherscientificterm_4> , the <otherscientificterm_3> are constructed in two stages , or equivalently , are viewed as a '' mixture of mixtures of <method_11> . '' this paradigm allows , and is complemented with , an <method_7> to seek the best <method_0> , which jointly exploits <method_2> and <task_9> . experimental results on the e-set show that the <metric_1> is reduced by over 20 % compared to standard <method_5> and <task_8> . the approach is then embedded within the recently developed procedure of combined parameter training and <method_6> . experiments with the overall technique show that the <metric_10> is further reduced by 8 % .",
    "abstract_og": "an approach is proposed for partial tying of states of tied-mixture hidden markov models . to facilitate tying at the sub-state level , the state emission probabilities are constructed in two stages , or equivalently , are viewed as a '' mixture of mixtures of gaussians . '' this paradigm allows , and is complemented with , an optimization technique to seek the best complexity-accuracy tradeoff solution , which jointly exploits gaussian density sharing and sub-state tying . experimental results on the e-set show that the classification error rate is reduced by over 20 % compared to standard gaussian sharing and whole-state tying . the approach is then embedded within the recently developed procedure of combined parameter training and reduction technique . experiments with the overall technique show that the error rate is further reduced by 8 % ."
  },
  {
    "title": "Signal Subspace Speech Enhancement for Audible Noise Reduction .",
    "entities": [
      "perceptual evaluation of speech quality",
      "segmental signal-to-noise ratio",
      "assumption of white noise",
      "subspace-based speech enhancement scheme",
      "estimated speech autocorrelation matrix",
      "audible noise reduction scheme",
      "human auditory system",
      "audible noise quantity",
      "audible noise reduction",
      "signal subspace technique",
      "colored noise case",
      "informal listening tests",
      "sub-space methods",
      "masking properties"
    ],
    "types": "<task> <metric> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <task> <method> <task> <otherscientificterm> <method> <material>",
    "relations": [
      "audible noise reduction scheme -- COMPARE -- sub-space methods",
      "perceptual evaluation of speech quality -- EVALUATE-FOR -- sub-space methods",
      "assumption of white noise -- FEATURE-OF -- estimated speech autocorrelation matrix",
      "perceptual evaluation of speech quality -- CONJUNCTION -- informal listening tests",
      "signal subspace technique -- USED-FOR -- audible noise reduction scheme",
      "segmental signal-to-noise ratio -- EVALUATE-FOR -- sub-space methods",
      "segmental signal-to-noise ratio -- CONJUNCTION -- perceptual evaluation of speech quality",
      "audible noise reduction scheme -- USED-FOR -- colored noise case",
      "masking properties -- USED-FOR -- audible noise quantity"
    ],
    "abstract": "a novel <method_3> based on a criterion of <task_8> is considered . <material_13> of the <method_6> is used to define the <otherscientificterm_7> in the eigen-domain . subsequently , an <method_5> is developed based on a <method_9> . we derive the eigen-decomposition of the <otherscientificterm_4> with the <otherscientificterm_2> and outline the implementation of our proposed <method_5> . we further extend the <method_5> to the <task_10> . simulation results show that our proposed <method_5> outperforms many existing <method_12> in terms of <metric_1> , <task_0> and <otherscientificterm_11> .",
    "abstract_og": "a novel subspace-based speech enhancement scheme based on a criterion of audible noise reduction is considered . masking properties of the human auditory system is used to define the audible noise quantity in the eigen-domain . subsequently , an audible noise reduction scheme is developed based on a signal subspace technique . we derive the eigen-decomposition of the estimated speech autocorrelation matrix with the assumption of white noise and outline the implementation of our proposed audible noise reduction scheme . we further extend the audible noise reduction scheme to the colored noise case . simulation results show that our proposed audible noise reduction scheme outperforms many existing sub-space methods in terms of segmental signal-to-noise ratio , perceptual evaluation of speech quality and informal listening tests ."
  },
  {
    "title": "Defining constraints for multilinear speech processing .",
    "entities": [
      "phonotactic description of the language",
      "internal structure of phonologial features",
      "phonological and the phonetic domains",
      "multilinear representations of speech utterances",
      "phonetic and phonological information",
      "explicit structural constraints",
      "well-formed syllable structures",
      "speech recognition applications",
      "constraint relaxation procedure",
      "fine-grained information",
      "constraint model",
      "precedence relations",
      "underspecified input",
      "phonological domain",
      "phonetic realisations",
      "speech recognition",
      "phonetic domain",
      "rankings",
      "features",
      "robustness"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "robustness -- FEATURE-OF -- speech recognition",
      "fine-grained information -- USED-FOR -- speech recognition applications",
      "constraint relaxation procedure -- USED-FOR -- underspecified input",
      "robustness -- EVALUATE-FOR -- constraint relaxation procedure",
      "phonological domain -- FEATURE-OF -- phonotactic description of the language"
    ],
    "abstract": "this paper presents a <method_10> for the interpretation of <task_3> which can provide important <otherscientificterm_9> for <task_7> . the <method_10> uses <otherscientificterm_5> specifying overlap and <otherscientificterm_11> between <otherscientificterm_18> in both the <otherscientificterm_2> in order to recognise <otherscientificterm_6> . in the <material_13> , these constraints together form a complete <task_0> , while in the <otherscientificterm_16> , the constraints define the <otherscientificterm_1> based on <otherscientificterm_14> . the constraints are enhanced by a <method_8> to cater for <otherscientificterm_12> and allows output representations to be extrapolated based on the <otherscientificterm_4> contained in the constraints and the <otherscientificterm_17> which have been assigned to them . this <method_8> thus addresses issues of <metric_19> in <task_15> .",
    "abstract_og": "this paper presents a constraint model for the interpretation of multilinear representations of speech utterances which can provide important fine-grained information for speech recognition applications . the constraint model uses explicit structural constraints specifying overlap and precedence relations between features in both the phonological and the phonetic domains in order to recognise well-formed syllable structures . in the phonological domain , these constraints together form a complete phonotactic description of the language , while in the phonetic domain , the constraints define the internal structure of phonologial features based on phonetic realisations . the constraints are enhanced by a constraint relaxation procedure to cater for underspecified input and allows output representations to be extrapolated based on the phonetic and phonological information contained in the constraints and the rankings which have been assigned to them . this constraint relaxation procedure thus addresses issues of robustness in speech recognition ."
  },
  {
    "title": "Parallel GPU implementation of null space based alternating optimization algorithm for large-scale matrix rank minimization .",
    "entities": [
      "large-scale matrix rank minimization problems",
      "null space based algorithm",
      "matrix rank minimization problem",
      "computing inverse matrix",
      "parallel gpu computing",
      "singular value decomposition",
      "alternating optimization algorithm",
      "parallel implementation",
      "low-rank solution",
      "computational cost",
      "signal processing",
      "large-scale problem",
      "gpu"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <metric> <task> <task> <material>",
    "relations": [
      "computing inverse matrix -- CONJUNCTION -- singular value decomposition",
      "matrix rank minimization problem -- USED-FOR -- signal processing",
      "null space based algorithm -- USED-FOR -- low-rank solution",
      "alternating optimization algorithm -- USED-FOR -- large-scale matrix rank minimization problems"
    ],
    "abstract": "this paper provides an <method_6> for <task_0> and its <task_7> on <material_12> . the <task_2> has a lot of important applications in <task_10> , and several useful algorithms have been proposed . however most algorithms can not be applied to a <task_11> because of high <metric_9> . this paper proposes a <method_1> , which provides a <method_8> without <otherscientificterm_3> nor <otherscientificterm_5> . the <method_1> can be parallelized easily without any approximation and can be applied to a <task_11> . numerical examples show that the <method_1> provides a <method_8> efficiently and can be speed up by <method_4> .",
    "abstract_og": "this paper provides an alternating optimization algorithm for large-scale matrix rank minimization problems and its parallel implementation on gpu . the matrix rank minimization problem has a lot of important applications in signal processing , and several useful algorithms have been proposed . however most algorithms can not be applied to a large-scale problem because of high computational cost . this paper proposes a null space based algorithm , which provides a low-rank solution without computing inverse matrix nor singular value decomposition . the null space based algorithm can be parallelized easily without any approximation and can be applied to a large-scale problem . numerical examples show that the null space based algorithm provides a low-rank solution efficiently and can be speed up by parallel gpu computing ."
  },
  {
    "title": "Learning and matching multiscale template descriptors for real-time detection , localization and tracking .",
    "entities": [
      "training video sequence",
      "salient locations",
      "partial occlusion",
      "local descriptors",
      "object template",
      "normalized intensity",
      "object-specific variability",
      "video stream",
      "low-level tracking",
      "live video",
      "shape",
      "detection"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <task> <material> <otherscientificterm> <task>",
    "relations": [
      "detection -- CONJUNCTION -- low-level tracking",
      "video stream -- USED-FOR -- object template",
      "salient locations -- USED-FOR -- low-level tracking",
      "low-level tracking -- USED-FOR -- object-specific variability"
    ],
    "abstract": "we describe a system to learn an <otherscientificterm_4> from a <material_7> , and localize and track the corresponding object in <material_9> . the template is decomposed into a number of <otherscientificterm_3> , thus enabling <task_11> and <task_8> in spite of <otherscientificterm_2> . each local descriptor aggregates contrast invariant statistics -lrb- <otherscientificterm_5> and gradient orientation -rrb- across scales , in a way that enables matching under significant scale variations . <task_8> during the <material_0> enables capturing <otherscientificterm_6> due to the <otherscientificterm_10> of the object , which is encapsulated in the descriptor . <otherscientificterm_1> on both the template and the target image are used as hypotheses to expedite matching .",
    "abstract_og": "we describe a system to learn an object template from a video stream , and localize and track the corresponding object in live video . the template is decomposed into a number of local descriptors , thus enabling detection and low-level tracking in spite of partial occlusion . each local descriptor aggregates contrast invariant statistics -lrb- normalized intensity and gradient orientation -rrb- across scales , in a way that enables matching under significant scale variations . low-level tracking during the training video sequence enables capturing object-specific variability due to the shape of the object , which is encapsulated in the descriptor . salient locations on both the template and the target image are used as hypotheses to expedite matching ."
  },
  {
    "title": "Demodulation for wireless ATM network using modified SOM network .",
    "entities": [
      "data link control protocols",
      "medium access control",
      "self-organizing-map demodulator",
      "rician flat fading channels",
      "rician fading channels",
      "semi-blind adaptive demodulator",
      "training sequence",
      "linear interpolation",
      "decision feedback",
      "demodulation problem",
      "lidf-som"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <material> <method> <otherscientificterm> <task> <method>",
    "relations": [
      "lidf-som -- COMPARE -- rician fading channels",
      "medium access control -- USED-FOR -- training sequence",
      "self-organizing-map demodulator -- USED-FOR -- linear interpolation",
      "medium access control -- CONJUNCTION -- data link control protocols",
      "decision feedback -- USED-FOR -- linear interpolation",
      "data link control protocols -- USED-FOR -- semi-blind adaptive demodulator",
      "data link control protocols -- USED-FOR -- training sequence"
    ],
    "abstract": "we study the <task_9> in time division multiple access -lrb- tdma -rrb- wireless asynchronous transfer mode -lrb- atm -rrb- networks , where <otherscientificterm_3> are considered . a <method_7> with <otherscientificterm_8> combined with a modified version of the <method_2> is proposed for such a system . we obtain the <material_6> by exploiting <method_1> and <method_0> such that a <method_5> is implemented . simulation results show that <method_10> obtains 0.4 \u2212 1.0 db gain over <otherscientificterm_4> as compared to lidf alone .",
    "abstract_og": "we study the demodulation problem in time division multiple access -lrb- tdma -rrb- wireless asynchronous transfer mode -lrb- atm -rrb- networks , where rician flat fading channels are considered . a linear interpolation with decision feedback combined with a modified version of the self-organizing-map demodulator is proposed for such a system . we obtain the training sequence by exploiting medium access control and data link control protocols such that a semi-blind adaptive demodulator is implemented . simulation results show that lidf-som obtains 0.4 \u2212 1.0 db gain over rician fading channels as compared to lidf alone ."
  },
  {
    "title": "Maximum Likelihood Based Temporal Frame Selection .",
    "entities": [
      "fixed frame rate",
      "phoneme recognition task",
      "frame selection approach",
      "pitch asynchronous representation",
      "speech recognition systems",
      "time axis",
      "noisy frames"
    ],
    "types": "<otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "fixed frame rate -- PART-OF -- speech recognition systems"
    ],
    "abstract": "in this paper , we propose a maximum likelihood -lrb- ml -rrb- based <method_2> . a <otherscientificterm_0> adopted in most state-of-the-art <method_4> can face some problems , such as accidentally meeting <otherscientificterm_6> , assigning the same importance to each frame , and <method_3> . as an attempt to avoid those problems , our approach selects reliable frames from a fine resolution along the <otherscientificterm_5> . in a <task_1> , we show that significant improvements are achieved with the <method_2> comparing to a system with a <otherscientificterm_0> .",
    "abstract_og": "in this paper , we propose a maximum likelihood -lrb- ml -rrb- based frame selection approach . a fixed frame rate adopted in most state-of-the-art speech recognition systems can face some problems , such as accidentally meeting noisy frames , assigning the same importance to each frame , and pitch asynchronous representation . as an attempt to avoid those problems , our approach selects reliable frames from a fine resolution along the time axis . in a phoneme recognition task , we show that significant improvements are achieved with the frame selection approach comparing to a system with a fixed frame rate ."
  },
  {
    "title": "Topic-specific parser design in an air travel natural language understanding application .",
    "entities": [
      "supervised and unsupervised subject selection modes",
      "natural language understanding applications",
      "darpa communicator task",
      "semantic parsing",
      "smoothing mechanism",
      "data sparseness",
      "training corpus",
      "subject-specific parsers",
      "application domain",
      "parser",
      "accuracy"
    ],
    "types": "<method> <task> <task> <task> <method> <otherscientificterm> <material> <method> <material> <method> <metric>",
    "relations": [
      "parser -- USED-FOR -- application domain",
      "smoothing mechanism -- USED-FOR -- data sparseness",
      "semantic parsing -- USED-FOR -- natural language understanding applications",
      "smoothing mechanism -- USED-FOR -- subject-specific parsers",
      "subject-specific parsers -- COMPARE -- parser"
    ],
    "abstract": "in this paper we contrast a traditional approach to <task_3> for <task_1> in which a single <method_9> captures a whole <material_8> , with an alternative approach consisting of a collection of smaller parsers , each able to handle only a portion of the domain . we implement this topic-specific parsing strategy by fragmenting the <material_6> into subject specific subsets and developing from each subset a corresponding subject <method_9> . we demonstrate this procedure on the <task_2> , and we observe that given an appropriate <method_4> to overcome <otherscientificterm_5> , the set of <method_7> performs as effectively -lrb- in <metric_10> terms -rrb- as the original <method_9> . we present experiments both under <method_0> .",
    "abstract_og": "in this paper we contrast a traditional approach to semantic parsing for natural language understanding applications in which a single parser captures a whole application domain , with an alternative approach consisting of a collection of smaller parsers , each able to handle only a portion of the domain . we implement this topic-specific parsing strategy by fragmenting the training corpus into subject specific subsets and developing from each subset a corresponding subject parser . we demonstrate this procedure on the darpa communicator task , and we observe that given an appropriate smoothing mechanism to overcome data sparseness , the set of subject-specific parsers performs as effectively -lrb- in accuracy terms -rrb- as the original parser . we present experiments both under supervised and unsupervised subject selection modes ."
  },
  {
    "title": "Speaker clustering using vector quantization and spectral clustering .",
    "entities": [
      "vector of vq code frequencies",
      "speaker diarization error rate",
      "bic stopping criterion",
      "cluster number estimation",
      "utterance length distributions",
      "speaker clustering method",
      "spectral clustering algorithm",
      "conversational speech recordings",
      "hierarchical agglomerative clustering",
      "short utterances",
      "eigen structure",
      "similarity matrix",
      "speech segment",
      "similarity measure",
      "purity metrics",
      "cosine",
      "clustering"
    ],
    "types": "<otherscientificterm> <metric> <otherscientificterm> <method> <otherscientificterm> <method> <method> <material> <method> <material> <otherscientificterm> <otherscientificterm> <method> <metric> <metric> <otherscientificterm> <method>",
    "relations": [
      "bic stopping criterion -- USED-FOR -- hierarchical agglomerative clustering",
      "similarity matrix -- USED-FOR -- spectral clustering algorithm",
      "spectral clustering algorithm -- USED-FOR -- clustering",
      "cluster number estimation -- USED-FOR -- clustering",
      "hierarchical agglomerative clustering -- USED-FOR -- speaker clustering method",
      "speaker clustering method -- USED-FOR -- conversational speech recordings",
      "purity metrics -- EVALUATE-FOR -- speaker clustering method",
      "eigen structure -- USED-FOR -- spectral clustering algorithm",
      "cluster number estimation -- USED-FOR -- spectral clustering algorithm"
    ],
    "abstract": "we present a <method_5> for <material_7> that contain <material_9> from multiple speakers . the proposed <method_5> represents a <method_12> with a <otherscientificterm_0> and uses a <otherscientificterm_15> between two vectors as their <metric_13> . the <method_16> is performed by a <method_6> with <method_3> based on an <otherscientificterm_10> of the <otherscientificterm_11> . we conducted experiments on five test sets with different <otherscientificterm_4> to compare the proposed <method_5> with the conventional approach based on a <method_8> using <otherscientificterm_2> . the results show that the proposed <method_5> significantly outperforms the conventional one in <metric_1> and <metric_14> .",
    "abstract_og": "we present a speaker clustering method for conversational speech recordings that contain short utterances from multiple speakers . the proposed speaker clustering method represents a speech segment with a vector of vq code frequencies and uses a cosine between two vectors as their similarity measure . the clustering is performed by a spectral clustering algorithm with cluster number estimation based on an eigen structure of the similarity matrix . we conducted experiments on five test sets with different utterance length distributions to compare the proposed speaker clustering method with the conventional approach based on a hierarchical agglomerative clustering using bic stopping criterion . the results show that the proposed speaker clustering method significantly outperforms the conventional one in speaker diarization error rate and purity metrics ."
  },
  {
    "title": "Reconstruction of 3-D Figure Motion from 2-D Correspondences .",
    "entities": [
      "3d motion of articulated models",
      "partial or missing data",
      "human motion sequences",
      "iterative batch algorithm",
      "3d kinematic model",
      "maximum aposteriori trajectory",
      "2d correspondences",
      "dynamic smoothing",
      "2d measurements",
      "kinematic constraints"
    ],
    "types": "<task> <material> <material> <method> <method> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "iterative batch algorithm -- USED-FOR -- maximum aposteriori trajectory",
      "3d kinematic model -- USED-FOR -- kinematic constraints"
    ],
    "abstract": "we present a method for computing the <task_0> from <material_6> . an <method_3> is proposed which estimates the <otherscientificterm_5> based on the <otherscientificterm_8> subject to a number of constraints . these include -lrb- i -rrb- <otherscientificterm_9> based on a <method_4> , -lrb- ii -rrb- joint angle limits , -lrb- iii -rrb- <method_7> and -lrb- iv -rrb- 3d key frames which can be specified the user . the framework handles any variation in the number of constraints as well as <material_1> . this method is shown to obtain favorable reconstruction results on a number of complex <material_2> .",
    "abstract_og": "we present a method for computing the 3d motion of articulated models from 2d correspondences . an iterative batch algorithm is proposed which estimates the maximum aposteriori trajectory based on the 2d measurements subject to a number of constraints . these include -lrb- i -rrb- kinematic constraints based on a 3d kinematic model , -lrb- ii -rrb- joint angle limits , -lrb- iii -rrb- dynamic smoothing and -lrb- iv -rrb- 3d key frames which can be specified the user . the framework handles any variation in the number of constraints as well as partial or missing data . this method is shown to obtain favorable reconstruction results on a number of complex human motion sequences ."
  },
  {
    "title": "MLLR transforms as features in speaker recognition .",
    "entities": [
      "maximum likelihood linear regression",
      "support vector machines",
      "frame-based cepstral speaker recognition models",
      "baseline and mllr-based systems",
      "cepstral gaussian mixture",
      "speech recognition systems",
      "text-independent speaker verification",
      "speaker features",
      "affine transforms",
      "spoken words",
      "adaptation transforms",
      "high-dimensional vectors",
      "speaker recognition",
      "svm systems",
      "acoustic models",
      "features",
      "recognizer"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "maximum likelihood linear regression -- USED-FOR -- recognizer",
      "support vector machines -- USED-FOR -- speaker features",
      "cepstral gaussian mixture -- CONJUNCTION -- svm systems",
      "speaker features -- USED-FOR -- high-dimensional vectors",
      "speech recognition systems -- USED-FOR -- speaker recognition",
      "speech recognition systems -- COMPARE -- svm systems",
      "adaptation transforms -- USED-FOR -- speech recognition systems",
      "support vector machines -- USED-FOR -- high-dimensional vectors",
      "speech recognition systems -- COMPARE -- cepstral gaussian mixture",
      "features -- USED-FOR -- speaker recognition"
    ],
    "abstract": "we explore the use of <method_10> employed in <method_5> as <otherscientificterm_15> for <task_12> . this approach is attractive because , unlike standard <method_2> , it normalizes for the choice of <otherscientificterm_9> in <task_6> . <otherscientificterm_8> are computed for the gaussian means of the <method_14> used in a <method_16> , using <method_0> . the <otherscientificterm_11> formed by the transform coefficients are then modeled as <otherscientificterm_7> using <method_1> . the resulting <method_5> is competitive , and in some cases significantly more accurate , than state-of-the-art <method_4> and <method_13> . further improvements are obtained by combining <method_3> .",
    "abstract_og": "we explore the use of adaptation transforms employed in speech recognition systems as features for speaker recognition . this approach is attractive because , unlike standard frame-based cepstral speaker recognition models , it normalizes for the choice of spoken words in text-independent speaker verification . affine transforms are computed for the gaussian means of the acoustic models used in a recognizer , using maximum likelihood linear regression . the high-dimensional vectors formed by the transform coefficients are then modeled as speaker features using support vector machines . the resulting speech recognition systems is competitive , and in some cases significantly more accurate , than state-of-the-art cepstral gaussian mixture and svm systems . further improvements are obtained by combining baseline and mllr-based systems ."
  },
  {
    "title": "Video Event Understanding Using Natural Language Descriptions .",
    "entities": [
      "natural language descriptions of the training videos",
      "spa-tio temporal annotations of actions and roles",
      "topic-based semantic re-latedness measure",
      "spatio temporal annotations",
      "complex event understanding",
      "posterior regularization objective",
      "trecvid-med11 event kit",
      "role recognition",
      "video description",
      "weak supervision",
      "high-level summary"
    ],
    "types": "<material> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <material> <task> <material> <otherscientificterm> <material>",
    "relations": [
      "topic-based semantic re-latedness measure -- PART-OF -- posterior regularization objective"
    ],
    "abstract": "human action and <task_7> play an important part in <task_4> . state-of-the-art methods learn action and role models from detailed <otherscientificterm_3> , which requires extensive human effort . in this work , we propose a method to learn such models based on <material_0> , which are easier to collect and scale with the number of actions and roles . there are two challenges with using this form of <otherscientificterm_9> : first , these descriptions only provide a <material_10> and often do not directly mention the actions and roles occurring in a video . second , natural language descriptions do not provide <otherscientificterm_1> . to tackle these challenges , we introduce a <method_2> between a <material_8> and an action and role label , and incorporate <method_2> into a <otherscientificterm_5> . our event recognition system based on these action and role models matches the state-of-the-art method on the <material_6> , despite weaker supervision .",
    "abstract_og": "human action and role recognition play an important part in complex event understanding . state-of-the-art methods learn action and role models from detailed spatio temporal annotations , which requires extensive human effort . in this work , we propose a method to learn such models based on natural language descriptions of the training videos , which are easier to collect and scale with the number of actions and roles . there are two challenges with using this form of weak supervision : first , these descriptions only provide a high-level summary and often do not directly mention the actions and roles occurring in a video . second , natural language descriptions do not provide spa-tio temporal annotations of actions and roles . to tackle these challenges , we introduce a topic-based semantic re-latedness measure between a video description and an action and role label , and incorporate topic-based semantic re-latedness measure into a posterior regularization objective . our event recognition system based on these action and role models matches the state-of-the-art method on the trecvid-med11 event kit , despite weaker supervision ."
  },
  {
    "title": "Processing Complex Sentences in the Centering Framework .",
    "entities": [
      "resolution of intia-sentential anaphora",
      "functional information structure",
      "centering model"
    ],
    "types": "<task> <otherscientificterm> <method>",
    "relations": [
      "centering model -- USED-FOR -- resolution of intia-sentential anaphora"
    ],
    "abstract": "we extend the <method_2> for the <task_0> and specify how to handle complex sentences . an empirical evaluation indicates that the <otherscientificterm_1> guides the search for an antecedent within the sentence .",
    "abstract_og": "we extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences . an empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence ."
  },
  {
    "title": "Trading Computation for Communication : Distributed Stochastic Dual Coordinate Ascent .",
    "entities": [
      "distributed stochastic dual coordinate ascent algorithm",
      "stochastic dual coordinate ascent methods",
      "stochas-tic dual coordinate ascent method",
      "distributed stochastic gradient descent methods",
      "regularized loss minimization problems",
      "stochas-tic gradient descent methods",
      "real data sets",
      "distributed optimization algorithm",
      "theoretical guarantees",
      "star network",
      "distributed framework",
      "multipliers",
      "computation",
      "svms"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <material> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "stochastic dual coordinate ascent methods -- COMPARE -- stochas-tic gradient descent methods",
      "stochas-tic gradient descent methods -- USED-FOR -- regularized loss minimization problems",
      "distributed framework -- USED-FOR -- svms",
      "stochas-tic dual coordinate ascent method -- USED-FOR -- distributed optimization algorithm",
      "distributed optimization algorithm -- COMPARE -- distributed stochastic gradient descent methods"
    ],
    "abstract": "we present and study a <method_7> by employing a <method_2> . <method_1> enjoy strong <otherscientificterm_8> and often have better performances than <method_5> in optimizing <task_4> . it still lacks of efforts in studying <method_1> in a <method_10> . we make a progress along the line by presenting a <method_0> in a <method_9> , with an analysis of the tradeoff between <otherscientificterm_12> and communication . we verify our analysis by experiments on <material_6> . moreover , we compare the proposed <method_7> with <method_3> and distributed alternating direction methods of <otherscientificterm_11> for optimizing <method_13> in the same <method_10> , and observe competitive performances .",
    "abstract_og": "we present and study a distributed optimization algorithm by employing a stochas-tic dual coordinate ascent method . stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochas-tic gradient descent methods in optimizing regularized loss minimization problems . it still lacks of efforts in studying stochastic dual coordinate ascent methods in a distributed framework . we make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network , with an analysis of the tradeoff between computation and communication . we verify our analysis by experiments on real data sets . moreover , we compare the proposed distributed optimization algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing svms in the same distributed framework , and observe competitive performances ."
  },
  {
    "title": "Tracking Forecast Memories in stochastic decoders .",
    "entities": [
      "tracking forecast memories",
      "stochastic channel de-coders",
      "edge memories",
      "stochastic de-coders",
      "hardware complexity",
      "asic implementations",
      "decoding"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <metric> <method> <task>",
    "relations": [
      "tracking forecast memories -- USED-FOR -- decoding"
    ],
    "abstract": "this paper proposes <task_0> as a novel method for implementing re-randomization and de-correlation of stochastic bit streams in <method_1> . we show that <task_0> are able to achieve <task_6> performance similar to that of the previous methods in the literature -lrb- i.e. , <otherscientificterm_2> or ems -rrb- , but they exhibit much lower <metric_4> . <task_0> significantly reduce the area requirements of <method_5> of <method_3> .",
    "abstract_og": "this paper proposes tracking forecast memories as a novel method for implementing re-randomization and de-correlation of stochastic bit streams in stochastic channel de-coders . we show that tracking forecast memories are able to achieve decoding performance similar to that of the previous methods in the literature -lrb- i.e. , edge memories or ems -rrb- , but they exhibit much lower hardware complexity . tracking forecast memories significantly reduce the area requirements of asic implementations of stochastic de-coders ."
  },
  {
    "title": "Kernel information embeddings .",
    "entities": [
      "nonparametric estimates of mutual information",
      "` conditional dimensionality reduction",
      "joint -lrb- input",
      "latent data representatives",
      "parzen window estimates",
      "mi-based objective function",
      "dimensionality reduction",
      "plain mi",
      "manifold alignment",
      "supervision signal",
      "conditional mi",
      "embedding algorithms",
      "embedding method"
    ],
    "types": "<method> <task> <otherscientificterm> <material> <method> <otherscientificterm> <task> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "nonparametric estimates of mutual information -- USED-FOR -- embedding algorithms",
      "mi-based objective function -- USED-FOR -- dimensionality reduction",
      "parzen window estimates -- USED-FOR -- joint -lrb- input"
    ],
    "abstract": "we describe a family of <method_11> that are based on <method_0> . using <method_4> of the distribution in the <otherscientificterm_2> , embedding -rrb- - space , we derive a <otherscientificterm_5> for <task_6> that can be optimized directly with respect to a set of <material_3> . various types of <otherscientificterm_9> can be introduced within the framework by replacing <method_7> with several forms of <otherscientificterm_10> . examples of the semi - -lrb- un -rrb- supervised algorithms that we obtain this way are a new model for <task_8> , and a new type of <method_12> that performs <task_1> ' .",
    "abstract_og": "we describe a family of embedding algorithms that are based on nonparametric estimates of mutual information . using parzen window estimates of the distribution in the joint -lrb- input , embedding -rrb- - space , we derive a mi-based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representatives . various types of supervision signal can be introduced within the framework by replacing plain mi with several forms of conditional mi . examples of the semi - -lrb- un -rrb- supervised algorithms that we obtain this way are a new model for manifold alignment , and a new type of embedding method that performs ` conditional dimensionality reduction ' ."
  },
  {
    "title": "The Distinctiveness , Detectability , and Robustness of Local Image Features .",
    "entities": [
      "database of model features",
      "classifying local image features",
      "local image features",
      "computation time",
      "quantitative models",
      "image data",
      "image deformations",
      "discriminant classifier",
      "regression network",
      "recognition time",
      "local features",
      "phase feature",
      "detection distributions",
      "recognition task",
      "sift",
      "scalability",
      "features",
      "distinctiveness",
      "detectability",
      "classifier",
      "robustness"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <metric> <method> <material> <otherscientificterm> <method> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <metric> <otherscientificterm> <metric> <metric> <method> <metric>",
    "relations": [
      "local features -- USED-FOR -- classifying local image features",
      "regression network -- COMPARE -- recognition task",
      "computation time -- EVALUATE-FOR -- regression network",
      "distinctiveness -- CONJUNCTION -- robustness",
      "regression network -- USED-FOR -- detection distributions",
      "computation time -- EVALUATE-FOR -- recognition task",
      "classifying local image features -- USED-FOR -- recognition task",
      "local features -- USED-FOR -- recognition task",
      "classifier -- CONJUNCTION -- regression network",
      "distinctiveness -- CONJUNCTION -- detectability",
      "detectability -- CONJUNCTION -- robustness",
      "image data -- USED-FOR -- regression network",
      "recognition time -- EVALUATE-FOR -- recognition task",
      "regression network -- USED-FOR -- recognition task",
      "sift -- HYPONYM-OF -- local image features"
    ],
    "abstract": "we introduce a new method that characterizes typical <otherscientificterm_2> -lrb- e.g. , <method_14> -lsb- 9 -rsb- , <otherscientificterm_11> -lsb- 3 -rsb- -rrb- in terms of their <metric_17> , <metric_18> , and <metric_20> to <otherscientificterm_6> . this is useful for the task of <task_1> in terms of those three properties . the importance of this <task_1> for a <method_13> using <otherscientificterm_10> is as follows : a -rrb- reduce the <metric_9> due to a smaller number of <otherscientificterm_16> present in the test image and in the <otherscientificterm_0> ; b -rrb- improve the <metric_9> since only the most useful <otherscientificterm_16> for the <method_13> are kept in the model database ; and c -rrb- increase the <metric_15> of the <method_13> given the smaller number of <otherscientificterm_16> per model . a <method_7> is trained to select well behaved feature points . a <method_8> is then trained to provide <method_4> of the <otherscientificterm_12> for each selected feature point . it is important to note that both the <method_19> and the <method_8> use <material_5> alone as their input . experimental results show that the use of these trained <method_8> not only improves the performance of our <method_13> , but <method_8> also significantly reduces the <metric_3> for the <method_13> .",
    "abstract_og": "we introduce a new method that characterizes typical local image features -lrb- e.g. , sift -lsb- 9 -rsb- , phase feature -lsb- 3 -rsb- -rrb- in terms of their distinctiveness , detectability , and robustness to image deformations . this is useful for the task of classifying local image features in terms of those three properties . the importance of this classifying local image features for a recognition task using local features is as follows : a -rrb- reduce the recognition time due to a smaller number of features present in the test image and in the database of model features ; b -rrb- improve the recognition time since only the most useful features for the recognition task are kept in the model database ; and c -rrb- increase the scalability of the recognition task given the smaller number of features per model . a discriminant classifier is trained to select well behaved feature points . a regression network is then trained to provide quantitative models of the detection distributions for each selected feature point . it is important to note that both the classifier and the regression network use image data alone as their input . experimental results show that the use of these trained regression network not only improves the performance of our recognition task , but regression network also significantly reduces the computation time for the recognition task ."
  },
  {
    "title": "Automatic detection of vocal fold paralysis and edema .",
    "entities": [
      "database of disordered speech",
      "known pattern recognition methods",
      "nearest mean classifier",
      "fisher linear discriminant",
      "vocal fold pathology",
      "\u00e3-nearest neighbor classifier",
      "vocal fold edema",
      "vocal fold paralysis",
      "receiver operating characteristic",
      "linear prediction analysis",
      "linear projection methods",
      "false alarm",
      "feature reduction",
      "feature extraction",
      "edema",
      "classifier",
      "detection",
      "classifiers"
    ],
    "types": "<material> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <task> <task> <task> <otherscientificterm> <method> <task> <method>",
    "relations": [
      "fisher linear discriminant -- CONJUNCTION -- \u00e3-nearest neighbor classifier",
      "\u00e3-nearest neighbor classifier -- CONJUNCTION -- nearest mean classifier",
      "detection -- CONJUNCTION -- receiver operating characteristic",
      "linear projection methods -- USED-FOR -- feature reduction",
      "fisher linear discriminant -- HYPONYM-OF -- classifiers",
      "linear prediction analysis -- USED-FOR -- feature reduction",
      "vocal fold paralysis -- CONJUNCTION -- vocal fold edema",
      "vocal fold edema -- HYPONYM-OF -- vocal fold pathology",
      "linear prediction analysis -- USED-FOR -- feature extraction",
      "feature extraction -- USED-FOR -- feature reduction",
      "vocal fold paralysis -- CONJUNCTION -- edema"
    ],
    "abstract": "in this paper we propose a combined scheme of <method_9> for <task_13> along with <method_10> for <task_12> followed by <method_1> on the purpose of discriminating between normal and pathological voice samples . two different cases of speech under <otherscientificterm_4> are examined : <otherscientificterm_7> and <otherscientificterm_6> . three known <method_17> are tested and compared in both cases , namely the <method_3> , the <method_5> , and the <method_2> . the performance of each <method_15> is evaluated in terms of the probabilities of <task_11> and <task_16> or the <otherscientificterm_8> . the datasets used are part of a <material_0> developed by massachusetts eye and ear infirmary . the experimental results indicate that <otherscientificterm_7> and <otherscientificterm_14> can easily be detected by any of the aforementioned <method_17> .",
    "abstract_og": "in this paper we propose a combined scheme of linear prediction analysis for feature extraction along with linear projection methods for feature reduction followed by known pattern recognition methods on the purpose of discriminating between normal and pathological voice samples . two different cases of speech under vocal fold pathology are examined : vocal fold paralysis and vocal fold edema . three known classifiers are tested and compared in both cases , namely the fisher linear discriminant , the \u00e3-nearest neighbor classifier , and the nearest mean classifier . the performance of each classifier is evaluated in terms of the probabilities of false alarm and detection or the receiver operating characteristic . the datasets used are part of a database of disordered speech developed by massachusetts eye and ear infirmary . the experimental results indicate that vocal fold paralysis and edema can easily be detected by any of the aforementioned classifiers ."
  },
  {
    "title": "Image processing techniques for blind TV ghost cancellation .",
    "entities": [
      "ghost cancellation reference signal",
      "tv synchronisation signal",
      "equalizer structure",
      "ghosts cancellation",
      "channel characteristics",
      "tv image",
      "vertical edges",
      "convergence"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "vertical edges -- USED-FOR -- channel characteristics",
      "vertical edges -- PART-OF -- tv image"
    ],
    "abstract": "this paper describes a technique to <task_3> without a <otherscientificterm_0> . the existing <otherscientificterm_6> in <material_5> are used to estimate the <otherscientificterm_4> . with the estimating results , the coefficients of the equalizer are updated . in this paper , a method to speed up the <otherscientificterm_7> is given . the <otherscientificterm_2> for cancelling all kinds of ghosts is discussed too . a new improved solution is achieved by interpreting the <otherscientificterm_1> as a genuine edge .",
    "abstract_og": "this paper describes a technique to ghosts cancellation without a ghost cancellation reference signal . the existing vertical edges in tv image are used to estimate the channel characteristics . with the estimating results , the coefficients of the equalizer are updated . in this paper , a method to speed up the convergence is given . the equalizer structure for cancelling all kinds of ghosts is discussed too . a new improved solution is achieved by interpreting the tv synchronisation signal as a genuine edge ."
  },
  {
    "title": "Approximate Dimension Equalization in Vector-based Information Retrieval .",
    "entities": [
      "generalized vector space model",
      "vector space model",
      "latent semantic indexing",
      "training corpus of text",
      "vector-based information retrieval methods",
      "large and small collections",
      "mono-lingual and bilingual text",
      "term -- term correlations",
      "translingual retrieval",
      "high-dimensional vectors",
      "bilingual collections",
      "lsi"
    ],
    "types": "<method> <method> <method> <material> <method> <material> <material> <otherscientificterm> <task> <otherscientificterm> <material> <method>",
    "relations": [
      "generalized vector space model -- HYPONYM-OF -- vector-based information retrieval methods",
      "latent semantic indexing -- CONJUNCTION -- generalized vector space model",
      "latent semantic indexing -- HYPONYM-OF -- vector-based information retrieval methods",
      "training corpus of text -- USED-FOR -- high-dimensional vectors",
      "vector space model -- USED-FOR -- translingual retrieval",
      "lsi -- CONJUNCTION -- vector space model",
      "high-dimensional vectors -- USED-FOR -- vector-based information retrieval methods",
      "high-dimensional vectors -- USED-FOR -- generalized vector space model",
      "vector space model -- CONJUNCTION -- latent semantic indexing",
      "vector space model -- HYPONYM-OF -- vector-based information retrieval methods",
      "vector space model -- CONJUNCTION -- lsi"
    ],
    "abstract": "vector-based information retrieval methods such as the <method_1> , <method_2> , and the <method_0> represent both queries and documents by <otherscientificterm_9> learned from analyzing a <material_3> . <method_1> scales well to large collections , but can not represent <otherscientificterm_7> , which prevents <method_1> from being used in <task_8> . <method_1> and <method_11> can represent <otherscientificterm_7> , but do not scale well to very large retrieval collections . we present a novel method we call approximate dimension equalization -lrb- ade -rrb- that combines ideas from <method_1> , <method_11> , and <method_1> to produce a method that performs well on large collections , scales well computationally , and can represent <otherscientificterm_7> . we compare the performance of ade to the other methods on both <material_5> of both <material_6> . ade outperforms all other methods on large <material_10> , and performs close to the best in all other cases .",
    "abstract_og": "vector-based information retrieval methods such as the vector space model , latent semantic indexing , and the generalized vector space model represent both queries and documents by high-dimensional vectors learned from analyzing a training corpus of text . vector space model scales well to large collections , but can not represent term -- term correlations , which prevents vector space model from being used in translingual retrieval . vector space model and lsi can represent term -- term correlations , but do not scale well to very large retrieval collections . we present a novel method we call approximate dimension equalization -lrb- ade -rrb- that combines ideas from vector space model , lsi , and vector space model to produce a method that performs well on large collections , scales well computationally , and can represent term -- term correlations . we compare the performance of ade to the other methods on both large and small collections of both mono-lingual and bilingual text . ade outperforms all other methods on large bilingual collections , and performs close to the best in all other cases ."
  },
  {
    "title": "Smart Vision Chip Fabricated Using Three Dimensional Integration Technology .",
    "entities": [
      "smart vision chip",
      "photo detector compactly",
      "dimensional lsi technology",
      "smart vision chips",
      "biological structure",
      "biological function",
      "neuromorphic systems"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "smart vision chips -- CONJUNCTION -- smart vision chips",
      "biological function -- CONJUNCTION -- biological structure",
      "dimensional lsi technology -- USED-FOR -- neuromorphic systems",
      "dimensional lsi technology -- USED-FOR -- smart vision chips"
    ],
    "abstract": "the <method_0> has a large potential for application in general purpose high speed image processing systems . in order to fabricate <method_3> including <otherscientificterm_1> , we have proposed the application of three <method_2> for <method_3> . three <method_2> has great potential to realize new <method_6> inspired by not only the <otherscientificterm_5> but also the <otherscientificterm_4> . in this paper , we describe our three <method_2> for <method_3> and the design of <method_3> .",
    "abstract_og": "the smart vision chip has a large potential for application in general purpose high speed image processing systems . in order to fabricate smart vision chips including photo detector compactly , we have proposed the application of three dimensional lsi technology for smart vision chips . three dimensional lsi technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure . in this paper , we describe our three dimensional lsi technology for smart vision chips and the design of smart vision chips ."
  },
  {
    "title": "Robust and Discriminative Self-Taught Learning .",
    "entities": [
      "robust and discriminative self-taught learning approach",
      "structured sparse regularization",
      "transfer learning methods",
      "semi-supervised learning methods",
      "machine learning problems",
      "dictionary basis vectors",
      "unlabeled data",
      "labeled data",
      "unlabeled images",
      "labeled images",
      "iterative algorithm",
      "optimization problem",
      "supervision information"
    ],
    "types": "<method> <method> <method> <method> <task> <otherscientificterm> <material> <material> <material> <material> <method> <task> <otherscientificterm>",
    "relations": [
      "semi-supervised learning methods -- CONJUNCTION -- transfer learning methods",
      "iterative algorithm -- USED-FOR -- optimization problem",
      "structured sparse regularization -- USED-FOR -- dictionary basis vectors",
      "semi-supervised learning methods -- USED-FOR -- machine learning problems",
      "transfer learning methods -- USED-FOR -- machine learning problems",
      "robust and discriminative self-taught learning approach -- USED-FOR -- dictionary basis vectors"
    ],
    "abstract": "the lack of training data is a common challenge in many <task_4> , which is often tackled by <method_3> or <method_2> . the former requires <material_8> from the same distribution as the labeled ones and the latter leverages <material_9> from related homogenous tasks . however , these restrictions often can not be satisfied . to address this , we propose a novel <method_0> to utilize any <material_6> without the above restrictions . our new <method_0> employs a robust loss function to learn the dictionary , and enforces the <method_1> to automatically select the optimal <otherscientificterm_5> and incorporate the <otherscientificterm_12> contained in the <material_7> . we derive an efficient <method_10> to solve the <task_11> and rigorously prove its convergence . promising results in extensive experiments have validated the proposed <method_0> .",
    "abstract_og": "the lack of training data is a common challenge in many machine learning problems , which is often tackled by semi-supervised learning methods or transfer learning methods . the former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks . however , these restrictions often can not be satisfied . to address this , we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions . our new robust and discriminative self-taught learning approach employs a robust loss function to learn the dictionary , and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data . we derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence . promising results in extensive experiments have validated the proposed robust and discriminative self-taught learning approach ."
  },
  {
    "title": "Sequential Neural Models with Stochastic Layers .",
    "entities": [
      "blizzard and timit speech modeling data sets",
      "stochastic and sequential neural generative model",
      "stochastic recurrent neural networks",
      "deterministic recurrent neural network",
      "structured variational inference network",
      "deterministic and stochastic layers",
      "state space model",
      "recurrent neural networks",
      "latent state representation",
      "nonlinear recursive structure",
      "polyphonic music modeling",
      "recurrent neural network",
      "latent path"
    ],
    "types": "<material> <method> <method> <method> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm>",
    "relations": [
      "stochastic recurrent neural networks -- USED-FOR -- stochastic and sequential neural generative model",
      "recurrent neural networks -- FEATURE-OF -- latent state representation",
      "nonlinear recursive structure -- USED-FOR -- recurrent neural network",
      "stochastic recurrent neural networks -- USED-FOR -- deterministic recurrent neural network"
    ],
    "abstract": "how can we efficiently propagate uncertainty in a <method_8> with <method_7> ? this paper introduces <method_2> which glue a <method_3> and a <method_6> together to form a <method_1> . the clear separation of <otherscientificterm_5> allows a <method_4> to track the factorization of the <method_2> 's posterior distribution . by retaining both the <otherscientificterm_9> of a <method_11> and averaging over the uncertainty in a <otherscientificterm_12> , like a <method_6> , we improve the state of the art results on the <material_0> by a large margin , while achieving comparable performances to competing methods on <task_10> .",
    "abstract_og": "how can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks ? this paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model . the clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the stochastic recurrent neural networks 's posterior distribution . by retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path , like a state space model , we improve the state of the art results on the blizzard and timit speech modeling data sets by a large margin , while achieving comparable performances to competing methods on polyphonic music modeling ."
  },
  {
    "title": "Combining time - and frequency-domain convolution in convolutional neural network-based phone recognition .",
    "entities": [
      "speaker and speaking style variations",
      "timit phone recognition task",
      "convolutional neural networks",
      "image recognition",
      "background motivations",
      "small translations",
      "hierarchical manner",
      "frequency axis",
      "spectral representation",
      "time-domain convolution",
      "speech recognition",
      "network architecture",
      "error rate",
      "convolution"
    ],
    "types": "<otherscientificterm> <task> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <metric> <otherscientificterm>",
    "relations": [
      "time-domain convolution -- USED-FOR -- network architecture",
      "spectral representation -- USED-FOR -- convolutional neural networks",
      "convolutional neural networks -- USED-FOR -- image recognition",
      "convolutional neural networks -- USED-FOR -- speech recognition",
      "error rate -- EVALUATE-FOR -- timit phone recognition task"
    ],
    "abstract": "convolutional neural networks have proved very successful in <task_3> , thanks to their tolerance to <otherscientificterm_5> . <method_2> have recently been applied to <task_10> as well , using a <method_8> as input . however , in this case the translations along the two axes -- time and frequency -- should be handled quite differently . so far , most authors have focused on <otherscientificterm_13> along the <otherscientificterm_7> , which offers invariance to <otherscientificterm_0> . other researchers have developed a different <method_11> that applies <otherscientificterm_9> in order to process a longer time-span of input in a <otherscientificterm_6> . these two <method_2> have different <otherscientificterm_4> , and both offer significant gains over a standard fully connected network . here we show that the two <method_11> can be readily combined , like their advantages . with the combined model we report an <metric_12> of 16.7 % on the <task_1> , a new record on this dataset .",
    "abstract_og": "convolutional neural networks have proved very successful in image recognition , thanks to their tolerance to small translations . convolutional neural networks have recently been applied to speech recognition as well , using a spectral representation as input . however , in this case the translations along the two axes -- time and frequency -- should be handled quite differently . so far , most authors have focused on convolution along the frequency axis , which offers invariance to speaker and speaking style variations . other researchers have developed a different network architecture that applies time-domain convolution in order to process a longer time-span of input in a hierarchical manner . these two convolutional neural networks have different background motivations , and both offer significant gains over a standard fully connected network . here we show that the two network architecture can be readily combined , like their advantages . with the combined model we report an error rate of 16.7 % on the timit phone recognition task , a new record on this dataset ."
  },
  {
    "title": "An Extensive Empirical Study of Collocation Extraction Methods .",
    "entities": [
      "automatic collocation extraction methods",
      "natural language processing",
      "precision-recall measures",
      "statistical classification"
    ],
    "types": "<method> <task> <method> <method>",
    "relations": [
      "precision-recall measures -- USED-FOR -- automatic collocation extraction methods"
    ],
    "abstract": "this paper presents a status quo of an ongoing research study of collocations -- an essential linguistic phenomenon having a wide spectrum of applications in the field of <task_1> . the core of the work is an empirical evaluation of a comprehensive list of <method_0> using <method_2> and a proposal of a new approach integrating multiple basic methods and <method_3> . we demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparison with individual basic methods .",
    "abstract_og": "this paper presents a status quo of an ongoing research study of collocations -- an essential linguistic phenomenon having a wide spectrum of applications in the field of natural language processing . the core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classification . we demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparison with individual basic methods ."
  },
  {
    "title": "Structured Parameter Elicitation .",
    "entities": [
      "partially observable markov decision processes",
      "problem parameter elicitation",
      "factored belief representation",
      "special structural properties",
      "uncertainty planning tasks",
      "autonomous agent",
      "parameter elicitation",
      "parameter values",
      "computational complexity",
      "factorization",
      "momdp",
      "symmetry",
      "sarsop"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <method> <task> <otherscientificterm> <metric> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "sarsop -- CONJUNCTION -- momdp",
      "partially observable markov decision processes -- USED-FOR -- uncertainty planning tasks",
      "partially observable markov decision processes -- USED-FOR -- parameter elicitation",
      "factored belief representation -- USED-FOR -- partially observable markov decision processes",
      "partially observable markov decision processes -- COMPARE -- momdp",
      "factorization -- CONJUNCTION -- symmetry"
    ],
    "abstract": "the behavior of a complex system often depends on parameters whose values are unknown in advance . to operate effectively , an <method_5> must actively gather information on the <otherscientificterm_7> while progressing towards its goal . we call this <method_1> . <method_0> provide a principled framework for such <task_4> , but they suffer from high <metric_8> . however , <method_0> for <task_6> often possess <otherscientificterm_3> , specifically , <otherscientificterm_9> and <otherscientificterm_11> . this work identifies these properties and exploits <method_0> for efficient solution through a <method_2> . the experimental results show that our new <method_0> outperform <method_12> and <method_10> , two of the fastest general-purpose <method_0> available , and can handle significantly larger problems .",
    "abstract_og": "the behavior of a complex system often depends on parameters whose values are unknown in advance . to operate effectively , an autonomous agent must actively gather information on the parameter values while progressing towards its goal . we call this problem parameter elicitation . partially observable markov decision processes provide a principled framework for such uncertainty planning tasks , but they suffer from high computational complexity . however , partially observable markov decision processes for parameter elicitation often possess special structural properties , specifically , factorization and symmetry . this work identifies these properties and exploits partially observable markov decision processes for efficient solution through a factored belief representation . the experimental results show that our new partially observable markov decision processes outperform sarsop and momdp , two of the fastest general-purpose partially observable markov decision processes available , and can handle significantly larger problems ."
  },
  {
    "title": "Robust Low Rank Kernel Embeddings of Multivariate Distributions .",
    "entities": [
      "hierarchical low rank decomposition of kernels embeddings",
      "latent and low rank information",
      "latent and low rank structures",
      "kernel embedding of distributions",
      "low rank structures",
      "kernel embedding literature",
      "real world distributions",
      "machine learning",
      "robust embedding",
      "model misspeci-fication",
      "rank embeddings",
      "density estimation"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <task> <task> <task> <otherscientificterm> <task>",
    "relations": [
      "kernel embedding of distributions -- USED-FOR -- machine learning"
    ],
    "abstract": "kernel embedding of distributions has led to many recent advances in <task_7> . however , <otherscientificterm_2> prevalent in <otherscientificterm_6> have rarely been taken into account in this setting . furthermore , no prior work in <task_5> has addressed the issue of <task_8> when the <otherscientificterm_1> are misspecified . in this paper , we propose a <method_0> which can exploit such <otherscientificterm_4> in data while being robust to <task_9> . we also illustrate with empirical evidence that the estimated low <otherscientificterm_10> lead to improved performance in <task_11> .",
    "abstract_og": "kernel embedding of distributions has led to many recent advances in machine learning . however , latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting . furthermore , no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified . in this paper , we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspeci-fication . we also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation ."
  },
  {
    "title": "Deep hashing for compact binary codes learning .",
    "entities": [
      "deep hashing approach",
      "supervised dh",
      "large scale visual search",
      "nonlinear relationship of samples",
      "binary codes learning methods",
      "real-valued feature descrip-tor",
      "hierarchical non-linear transformations",
      "compact binary codes",
      "deep neural network",
      "dis-criminative power",
      "discrimi-native term",
      "intra-class variations",
      "inter-class variations",
      "binary vector",
      "linear projection",
      "binary codes",
      "dh"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "deep hashing approach -- USED-FOR -- compact binary codes",
      "deep neural network -- USED-FOR -- binary codes",
      "real-valued feature descrip-tor -- CONJUNCTION -- binary vector",
      "binary codes learning methods -- USED-FOR -- linear projection",
      "dh -- USED-FOR -- supervised dh",
      "compact binary codes -- USED-FOR -- large scale visual search",
      "linear projection -- USED-FOR -- binary vector"
    ],
    "abstract": "in this paper , we propose a new <method_0> to learn <otherscientificterm_7> for <task_2> . unlike most existing <method_4> which seek a single <method_14> to map each sample into a <otherscientificterm_13> , we develop a <method_8> to seek multiple <otherscientificterm_6> to learn these <otherscientificterm_15> , so that the <otherscientificterm_3> can be well exploited . our <method_0> is learned under three constraints at the top layer of the <method_8> : 1 -rrb- the loss between the original <otherscientificterm_5> and the learned <otherscientificterm_13> is minimized , 2 -rrb- the <otherscientificterm_15> distribute evenly on each bit , and 3 -rrb- different bits are as independent as possible . to further improve the <otherscientificterm_9> of the learned <otherscientificterm_15> , we extend <method_16> into <method_1> by including one <otherscientificterm_10> into the objective function of <method_16> which simultaneously maximizes the <otherscientificterm_12> and minimizes the <otherscientificterm_11> of the learned <otherscientificterm_15> . experimental results show the superiority of the proposed <method_0> over the state-of-the-arts .",
    "abstract_og": "in this paper , we propose a new deep hashing approach to learn compact binary codes for large scale visual search . unlike most existing binary codes learning methods which seek a single linear projection to map each sample into a binary vector , we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes , so that the nonlinear relationship of samples can be well exploited . our deep hashing approach is learned under three constraints at the top layer of the deep neural network : 1 -rrb- the loss between the original real-valued feature descrip-tor and the learned binary vector is minimized , 2 -rrb- the binary codes distribute evenly on each bit , and 3 -rrb- different bits are as independent as possible . to further improve the dis-criminative power of the learned binary codes , we extend dh into supervised dh by including one discrimi-native term into the objective function of dh which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes . experimental results show the superiority of the proposed deep hashing approach over the state-of-the-arts ."
  },
  {
    "title": "Scene understanding by statistical modeling of motion patterns .",
    "entities": [
      "discovery and statistical representation of motion patterns",
      "low level cues of noisy optical flow",
      "pixel level representation of motion patterns",
      "learning of patterns of activity",
      "conditional expectation of optical flow",
      "temporally segmented clips of video",
      "hierarchical , unsupervised fashion",
      "pedestrian and vehicular traffic",
      "gaussian mixture model",
      "dense optical flow",
      "motion patterns",
      "surveillance sequences",
      "motion model",
      "static camera",
      "related methods",
      "motion patterns",
      "kl divergence",
      "object detection",
      "k-means"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <otherscientificterm> <material> <method> <material> <otherscientificterm> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method>",
    "relations": [
      "k-means -- USED-FOR -- gaussian mixture model",
      "conditional expectation of optical flow -- USED-FOR -- pixel level representation of motion patterns",
      "motion model -- USED-FOR -- motion patterns",
      "kl divergence -- USED-FOR -- motion patterns",
      "gaussian mixture model -- USED-FOR -- temporally segmented clips of video"
    ],
    "abstract": "we present a novel method for the <task_0> in a scene observed by a <otherscientificterm_13> . <method_14> involving <task_3> rely on trajectories obtained from <task_17> and tracking systems , which are unreliable in complex scenes of crowded motion . we propose a <method_8> model representation of salient patterns of optical flow , and present an algorithm for learning these patterns from <material_9> in a <otherscientificterm_6> . using <otherscientificterm_1> , <method_18> is employed to initialize a <method_8> for <material_5> . the components of this <method_8> are then filtered and instances of <otherscientificterm_15> are computed using a simple <method_12> , by linking components across space and time . <otherscientificterm_10> are then initialized and membership of instances in different <otherscientificterm_15> is established by using <otherscientificterm_16> between <method_8> distributions of pattern instances . finally , a <otherscientificterm_2> is proposed by deriving <otherscientificterm_4> . results of extensive experiments are presented for multiple <material_11> containing numerous patterns involving both <material_7> .",
    "abstract_og": "we present a novel method for the discovery and statistical representation of motion patterns in a scene observed by a static camera . related methods involving learning of patterns of activity rely on trajectories obtained from object detection and tracking systems , which are unreliable in complex scenes of crowded motion . we propose a gaussian mixture model model representation of salient patterns of optical flow , and present an algorithm for learning these patterns from dense optical flow in a hierarchical , unsupervised fashion . using low level cues of noisy optical flow , k-means is employed to initialize a gaussian mixture model for temporally segmented clips of video . the components of this gaussian mixture model are then filtered and instances of motion patterns are computed using a simple motion model , by linking components across space and time . motion patterns are then initialized and membership of instances in different motion patterns is established by using kl divergence between gaussian mixture model distributions of pattern instances . finally , a pixel level representation of motion patterns is proposed by deriving conditional expectation of optical flow . results of extensive experiments are presented for multiple surveillance sequences containing numerous patterns involving both pedestrian and vehicular traffic ."
  },
  {
    "title": "Modeling Dynamic Multi-Topic Discussions in Online Forums .",
    "entities": [
      "evolution of topic discussion",
      "evolution of topic discussions",
      "dynamic topic discussion models",
      "time lapse factor",
      "sociological phenomena",
      "online forums",
      "social network",
      "information flow",
      "topic discussions",
      "recommendation systems",
      "internet"
    ],
    "types": "<task> <task> <method> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <method> <material>",
    "relations": [
      "evolution of topic discussion -- USED-FOR -- sociological phenomena",
      "time lapse factor -- PART-OF -- dynamic topic discussion models"
    ],
    "abstract": "in the form of <otherscientificterm_8> , users interact with each other to share knowledge and exchange information in <material_5> . modeling the <task_0> reveals how information propagates on <material_10> and can thus help understand <otherscientificterm_4> and improve the performance of applications such as <method_9> . in this paper , we argue that a user 's participation in <otherscientificterm_8> is motivated by either her friends or her own preferences . inspired by the theory of <otherscientificterm_7> , we propose <method_2> by mining influential relationships between users and individual preferences . reply relations of users are exploited to construct the fundamental influential <method_6> . the property of discussed topics and <otherscientificterm_3> are also considered in our <method_2> . furthermore , we propose a novel measure called participationrank to rank users according to how important they are in the <method_6> and to what extent they prefer to participate in the discussion of a certain topic . the experiments show our model can simulate the <task_1> well and predict the tendency of user 's participation accurately .",
    "abstract_og": "in the form of topic discussions , users interact with each other to share knowledge and exchange information in online forums . modeling the evolution of topic discussion reveals how information propagates on internet and can thus help understand sociological phenomena and improve the performance of applications such as recommendation systems . in this paper , we argue that a user 's participation in topic discussions is motivated by either her friends or her own preferences . inspired by the theory of information flow , we propose dynamic topic discussion models by mining influential relationships between users and individual preferences . reply relations of users are exploited to construct the fundamental influential social network . the property of discussed topics and time lapse factor are also considered in our dynamic topic discussion models . furthermore , we propose a novel measure called participationrank to rank users according to how important they are in the social network and to what extent they prefer to participate in the discussion of a certain topic . the experiments show our model can simulate the evolution of topic discussions well and predict the tendency of user 's participation accurately ."
  },
  {
    "title": "Learning Label Trees for Probabilistic Modelling of Implicit Feedback .",
    "entities": [
      "user 's item selection process",
      "implicit feedback models",
      "probabilistic approach",
      "collaborative filtering",
      "tree-structured distributions",
      "evaluation protocol",
      "implicit feedback"
    ],
    "types": "<otherscientificterm> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "evaluation protocol -- USED-FOR -- implicit feedback models",
      "implicit feedback -- USED-FOR -- probabilistic approach",
      "probabilistic approach -- USED-FOR -- collaborative filtering"
    ],
    "abstract": "an efficient <method_2> to <task_3> with <otherscientificterm_6> , based on modelling the <otherscientificterm_0> . \u2022 <otherscientificterm_4> over items for scalability . \u2022 a principled and efficient algorithm for learning effective item trees from data . \u2022 a fix for the standard <method_5> for <method_1> , addressing its unrealistic assumptions .",
    "abstract_og": "an efficient probabilistic approach to collaborative filtering with implicit feedback , based on modelling the user 's item selection process . \u2022 tree-structured distributions over items for scalability . \u2022 a principled and efficient algorithm for learning effective item trees from data . \u2022 a fix for the standard evaluation protocol for implicit feedback models , addressing its unrealistic assumptions ."
  },
  {
    "title": "Nested Monte-Carlo Search .",
    "entities": [
      "nested levels of random games",
      "nested monte-carlo search",
      "random games",
      "morpion solitaire",
      "abstract problems",
      "state space",
      "samegame"
    ],
    "types": "<otherscientificterm> <method> <material> <material> <task> <otherscientificterm> <material>",
    "relations": [
      "morpion solitaire -- CONJUNCTION -- samegame"
    ],
    "abstract": "many problems have a huge <otherscientificterm_5> and no good heuristic to order moves so as to guide the search toward the best positions . <material_2> can be used to score positions and evaluate their interest . <material_2> can also be improved using random games to choose a move to try at each step of a game . <method_1> addresses the problem of guiding the search toward better states when there is no available heuristic . <method_1> uses <otherscientificterm_0> in order to guide the search . the <method_1> is studied theoretically on simple <task_4> and applied successfully to three different games : <material_3> , <material_6> and 16x16 sudoku .",
    "abstract_og": "many problems have a huge state space and no good heuristic to order moves so as to guide the search toward the best positions . random games can be used to score positions and evaluate their interest . random games can also be improved using random games to choose a move to try at each step of a game . nested monte-carlo search addresses the problem of guiding the search toward better states when there is no available heuristic . nested monte-carlo search uses nested levels of random games in order to guide the search . the nested monte-carlo search is studied theoretically on simple abstract problems and applied successfully to three different games : morpion solitaire , samegame and 16x16 sudoku ."
  },
  {
    "title": "Understanding Probabilistic Sparse Gaussian Process Approximations .",
    "entities": [
      "fully independent training conditional",
      "variational free energy approximations",
      "gaussian processes",
      "computational cost",
      "sparse approximations",
      "theoretical properties",
      "superficial similarities",
      "inference"
    ],
    "types": "<method> <method> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "fully independent training conditional -- CONJUNCTION -- variational free energy approximations"
    ],
    "abstract": "good <otherscientificterm_4> are essential for practical <task_7> in <method_2> as the <metric_3> of exact methods is prohibitive for large datasets . the <method_0> and the <method_1> are two recent popular methods . despite <otherscientificterm_6> , these approximations have surprisingly different <otherscientificterm_5> and behave differently in practice . we thoroughly investigate the two methods for regression both analytically and through illustrative examples , and draw conclusions to guide practical application .",
    "abstract_og": "good sparse approximations are essential for practical inference in gaussian processes as the computational cost of exact methods is prohibitive for large datasets . the fully independent training conditional and the variational free energy approximations are two recent popular methods . despite superficial similarities , these approximations have surprisingly different theoretical properties and behave differently in practice . we thoroughly investigate the two methods for regression both analytically and through illustrative examples , and draw conclusions to guide practical application ."
  },
  {
    "title": "Re-thinking non-rigid structure from motion .",
    "entities": [
      "object shape space",
      "man-ifold of dimensionality",
      "linear basis method",
      "orthographic video sequence",
      "initialization algorithm",
      "nrsfm problem",
      "linear subspace",
      "non-rigid structure"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <material> <method> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "linear subspace -- USED-FOR -- object shape space"
    ],
    "abstract": "we present a novel approach to <otherscientificterm_7> from motion -lrb- nrsfm -rrb- from an <material_3> , based on a new interpretation of the problem . existing approaches assume the <otherscientificterm_0> is well-modeled by a <otherscientificterm_6> . our approach only assumes that small neighborhoods of shapes are well-modeled with a <otherscientificterm_6> . this constrains the shapes to belong to a <otherscientificterm_1> equal to the number of degrees of freedom of the object . after showing that the problem is still overconstrained , we present a solution composed of a novel <method_4> , followed by a robust extension of the locally smooth manifold learning algorithm tailored to the <task_5> . we finally present some test cases where the <method_2> fails -lrb- and is actually not meant to work -rrb- while the proposed approach is successful .",
    "abstract_og": "we present a novel approach to non-rigid structure from motion -lrb- nrsfm -rrb- from an orthographic video sequence , based on a new interpretation of the problem . existing approaches assume the object shape space is well-modeled by a linear subspace . our approach only assumes that small neighborhoods of shapes are well-modeled with a linear subspace . this constrains the shapes to belong to a man-ifold of dimensionality equal to the number of degrees of freedom of the object . after showing that the problem is still overconstrained , we present a solution composed of a novel initialization algorithm , followed by a robust extension of the locally smooth manifold learning algorithm tailored to the nrsfm problem . we finally present some test cases where the linear basis method fails -lrb- and is actually not meant to work -rrb- while the proposed approach is successful ."
  },
  {
    "title": "Learning Sparse Gaussian Graphical Models with Overlapping Blocks .",
    "entities": [
      "data matrix of p variables",
      "block coordinate descent method",
      "cancer gene expression data",
      "densely connected components",
      "joint optimization problem",
      "cancer driver genes",
      "network estimation methods",
      "overlapping blocks",
      "network estimate",
      "synthetic data",
      "network structure",
      "priori",
      "convex"
    ],
    "types": "<otherscientificterm> <method> <material> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "densely connected components -- PART-OF -- network estimate"
    ],
    "abstract": "we present a novel framework , called grab -lrb- graphical models with overlapping blocks -rrb- , to capture <otherscientificterm_3> in a <method_8> . grab takes as input a <otherscientificterm_0> and n samples and jointly learns both a network of the p variables and densely connected groups of variables -lrb- called ` blocks ' -rrb- . grab has four major novelties as compared to existing <method_6> : 1 -rrb- it does not require blocks to be given a <otherscientificterm_11> . 2 -rrb- blocks can overlap . 3 -rrb- it can jointly learn a <otherscientificterm_10> and <otherscientificterm_7> . 4 -rrb- it solves a <task_4> with the <method_1> that is <otherscientificterm_12> in each step . we show that grab reveals the underlying <otherscientificterm_10> substantially better than four state-of-the-art competitors on <material_9> . when applied to <material_2> , grab outperforms its competitors in revealing known functional gene sets and potentially novel <otherscientificterm_5> .",
    "abstract_og": "we present a novel framework , called grab -lrb- graphical models with overlapping blocks -rrb- , to capture densely connected components in a network estimate . grab takes as input a data matrix of p variables and n samples and jointly learns both a network of the p variables and densely connected groups of variables -lrb- called ` blocks ' -rrb- . grab has four major novelties as compared to existing network estimation methods : 1 -rrb- it does not require blocks to be given a priori . 2 -rrb- blocks can overlap . 3 -rrb- it can jointly learn a network structure and overlapping blocks . 4 -rrb- it solves a joint optimization problem with the block coordinate descent method that is convex in each step . we show that grab reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data . when applied to cancer gene expression data , grab outperforms its competitors in revealing known functional gene sets and potentially novel cancer driver genes ."
  },
  {
    "title": "Unimodal Bandits : Regret Lower Bounds and Optimal Algorithms .",
    "entities": [
      "continuous sets of arms",
      "asymptotic lower bounds",
      "discrete unimodal bandits",
      "stochastic multi-armed bandits",
      "unimodal structure",
      "asymptotic regret",
      "order-optimal regret",
      "unimodal function",
      "ucb algorithm",
      "non-stationary environments",
      "bounded interval",
      "finite graph",
      "osub"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "non-stationary environments -- FEATURE-OF -- osub",
      "asymptotic lower bounds -- USED-FOR -- discrete unimodal bandits"
    ],
    "abstract": "we consider <method_3> where the expected reward is a <otherscientificterm_7> over partially ordered arms . this important class of problems has been recently inves-the set of arms is either discrete , in which case arms correspond to the vertices of a <otherscientificterm_11> whose structure represents similarity in rewards , or continuous , in which case arms belong to a <otherscientificterm_10> . for <otherscientificterm_2> , we derive <otherscientificterm_1> for the regret achieved under any algorithm , and propose <method_12> , an algorithm whose regret matches this lower bound . our algorithm optimally exploits the <otherscientificterm_4> of the problem , and surprisingly , its <otherscientificterm_5> does not depend on the number of arms . we also provide a regret upper bound for <method_12> in <otherscientificterm_9> where the expected rewards smoothly evolve over time . the analytical results are supported by numerical experiments showing that <method_12> performs significantly better than the state-of-the-art algorithms . for <otherscientificterm_0> , we provide a brief discussion . we show that combining an appropriate discretization of the set of arms with the <method_8> yields an <otherscientificterm_6> , and in practice , outperforms recently proposed algorithms designed to exploit the <otherscientificterm_4> .",
    "abstract_og": "we consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms . this important class of problems has been recently inves-the set of arms is either discrete , in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards , or continuous , in which case arms belong to a bounded interval . for discrete unimodal bandits , we derive asymptotic lower bounds for the regret achieved under any algorithm , and propose osub , an algorithm whose regret matches this lower bound . our algorithm optimally exploits the unimodal structure of the problem , and surprisingly , its asymptotic regret does not depend on the number of arms . we also provide a regret upper bound for osub in non-stationary environments where the expected rewards smoothly evolve over time . the analytical results are supported by numerical experiments showing that osub performs significantly better than the state-of-the-art algorithms . for continuous sets of arms , we provide a brief discussion . we show that combining an appropriate discretization of the set of arms with the ucb algorithm yields an order-optimal regret , and in practice , outperforms recently proposed algorithms designed to exploit the unimodal structure ."
  },
  {
    "title": "Generalized Gaussian process models .",
    "entities": [
      "generalized gaussian process model",
      "gaussian process models",
      "closed-form efficient taylor approximation",
      "exponential family distribution",
      "task-specific output domains",
      "generalized gp model",
      "model-specific closed-form approximations",
      "approximate inference algorithms",
      "task-specific gp models",
      "unifying framework",
      "observation likelihood",
      "computer vision",
      "gp regression",
      "likelihood function",
      "counting",
      "inference",
      "classification"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <material> <method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "task-specific gp models -- USED-FOR -- computer vision",
      "approximate inference algorithms -- USED-FOR -- generalized gp model",
      "gp regression -- HYPONYM-OF -- gaussian process models",
      "exponential family distribution -- USED-FOR -- gaussian process models",
      "classification -- CONJUNCTION -- counting",
      "closed-form efficient taylor approximation -- USED-FOR -- generalized gaussian process model",
      "gp regression -- CONJUNCTION -- classification",
      "closed-form efficient taylor approximation -- USED-FOR -- inference",
      "generalized gaussian process model -- USED-FOR -- gaussian process models",
      "classification -- HYPONYM-OF -- gaussian process models"
    ],
    "abstract": "we propose a <method_0> , which is a <method_9> that encompasses many existing <method_1> , such as <method_12> , <task_16> , and <otherscientificterm_14> . in the <method_0> , the <otherscientificterm_10> of the <method_1> is itself parameterized using the <otherscientificterm_3> . by deriving <method_7> for the <method_5> , we are able to easily apply the same <method_0> to all other <method_1> . novel <method_1> are created by changing the parameterization of the <otherscientificterm_13> , which greatly simplifies their creation for <material_4> . we also derive a <method_2> for <task_15> on the <method_0> , and draw interesting connections with other <otherscientificterm_6> . finally , using the <method_0> , we create several new <method_1> and show their efficacy in building <method_8> for <task_11> .",
    "abstract_og": "we propose a generalized gaussian process model , which is a unifying framework that encompasses many existing gaussian process models , such as gp regression , classification , and counting . in the generalized gaussian process model , the observation likelihood of the gaussian process models is itself parameterized using the exponential family distribution . by deriving approximate inference algorithms for the generalized gp model , we are able to easily apply the same generalized gaussian process model to all other gaussian process models . novel gaussian process models are created by changing the parameterization of the likelihood function , which greatly simplifies their creation for task-specific output domains . we also derive a closed-form efficient taylor approximation for inference on the generalized gaussian process model , and draw interesting connections with other model-specific closed-form approximations . finally , using the generalized gaussian process model , we create several new gaussian process models and show their efficacy in building task-specific gp models for computer vision ."
  },
  {
    "title": "Multiple Non-Redundant Spectral Clustering Views .",
    "entities": [
      "spectral clustering objective function",
      "dimensionality reduction",
      "clustering solution",
      "high-dimensional setting",
      "clustering algorithms",
      "subspaces"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "clustering algorithms -- USED-FOR -- clustering solution"
    ],
    "abstract": "many <method_4> only find one <method_2> . however , data can often be grouped and interpreted in many different ways . this is particularly true in the <otherscientificterm_3> where different <otherscientificterm_5> reveal different possible groupings of the data . instead of committing to one <method_2> , here we introduce a novel method that can provide several non-redundant clustering solutions to the user . our approach simultaneously learns non-redundant <otherscientificterm_5> that provide multiple views and finds a <method_2> in each view . we achieve this by augmenting a <method_0> to incorporate <method_1> and multiple views and to penalize for redundancy between the views .",
    "abstract_og": "many clustering algorithms only find one clustering solution . however , data can often be grouped and interpreted in many different ways . this is particularly true in the high-dimensional setting where different subspaces reveal different possible groupings of the data . instead of committing to one clustering solution , here we introduce a novel method that can provide several non-redundant clustering solutions to the user . our approach simultaneously learns non-redundant subspaces that provide multiple views and finds a clustering solution in each view . we achieve this by augmenting a spectral clustering objective function to incorporate dimensionality reduction and multiple views and to penalize for redundancy between the views ."
  },
  {
    "title": "From contours to 3D object detection and pose estimation .",
    "entities": [
      "bag of boundaries",
      "object-centered representations of point-based object features",
      "non-rigid , locally affine shape deformations",
      "detecting object occurrences",
      "view-dependent shape templates",
      "convex optimization problem",
      "view-invariant object detection",
      "sparse object model",
      "pose estimation",
      "viewer-centered framework",
      "object boundaries",
      "background clutter",
      "image contours",
      "deformable grids",
      "mid-level feature",
      "benchmark datasets",
      "inference",
      "bobs"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <task> <method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <task> <otherscientificterm>",
    "relations": [
      "bag of boundaries -- HYPONYM-OF -- mid-level feature",
      "sparse object model -- USED-FOR -- detecting object occurrences",
      "view-invariant object detection -- CONJUNCTION -- pose estimation",
      "view-dependent shape templates -- USED-FOR -- sparse object model"
    ],
    "abstract": "this paper addresses <task_6> and <task_8> from a single image . while recent work fo-cuses on <otherscientificterm_1> , we revisit the <method_9> , and use <otherscientificterm_12> as basic features . given training examples of arbitrary views of an object , we learn a <method_7> in terms of a few <otherscientificterm_4> . the <method_7> are jointly used for <task_3> and estimating their 3d poses in a new image . instrumental to this is our new <otherscientificterm_14> , called <method_0> , aimed at lifting from individual edges toward their more informative summaries for identifying <otherscientificterm_10> amidst the <otherscientificterm_11> . in <task_16> , <otherscientificterm_17> are placed on <otherscientificterm_13> both in the image and the <method_7> , and then matched . this is formulated as a <task_5> that accommodates invariance to <otherscientificterm_2> . evaluation on <material_15> demonstrates our competitive results relative to the state of the art .",
    "abstract_og": "this paper addresses view-invariant object detection and pose estimation from a single image . while recent work fo-cuses on object-centered representations of point-based object features , we revisit the viewer-centered framework , and use image contours as basic features . given training examples of arbitrary views of an object , we learn a sparse object model in terms of a few view-dependent shape templates . the sparse object model are jointly used for detecting object occurrences and estimating their 3d poses in a new image . instrumental to this is our new mid-level feature , called bag of boundaries , aimed at lifting from individual edges toward their more informative summaries for identifying object boundaries amidst the background clutter . in inference , bobs are placed on deformable grids both in the image and the sparse object model , and then matched . this is formulated as a convex optimization problem that accommodates invariance to non-rigid , locally affine shape deformations . evaluation on benchmark datasets demonstrates our competitive results relative to the state of the art ."
  },
  {
    "title": "An Association Network for Computing Semantic Relatedness .",
    "entities": [
      "cognitive human perceptions of relatedness",
      "rich structures of wikipedia",
      "computing semantic relatedness",
      "limited lexical coverage",
      "free association norms",
      "cognitive process",
      "free association",
      "perceptional gap",
      "human perceptions",
      "textual windows",
      "lexical coverage",
      "co-occurrences",
      "sparseness"
    ],
    "types": "<otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "free association -- USED-FOR -- human perceptions"
    ],
    "abstract": "to judge how much a pair of words -lrb- or texts -rrb- are semantically related is a <method_5> . however , previous algorithms for <task_2> are largely based on <otherscientificterm_11> within <otherscientificterm_9> , and do not actively leverage <otherscientificterm_0> . to bridge this <otherscientificterm_7> , we propose to utilize <otherscientificterm_6> as signals to capture such <otherscientificterm_8> . however , <otherscientificterm_6> , being manually evaluated , has <otherscientificterm_3> and is inherently sparse . we propose to expand <otherscientificterm_10> and overcome <otherscientificterm_12> by constructing an association network of terms and concepts that combines signals from <otherscientificterm_4> and five types of <otherscientificterm_11> extracted from the <material_1> . our evaluation results validate that simple algorithms on this network give competitive results in computing semantic re-latedness between words and between short texts .",
    "abstract_og": "to judge how much a pair of words -lrb- or texts -rrb- are semantically related is a cognitive process . however , previous algorithms for computing semantic relatedness are largely based on co-occurrences within textual windows , and do not actively leverage cognitive human perceptions of relatedness . to bridge this perceptional gap , we propose to utilize free association as signals to capture such human perceptions . however , free association , being manually evaluated , has limited lexical coverage and is inherently sparse . we propose to expand lexical coverage and overcome sparseness by constructing an association network of terms and concepts that combines signals from free association norms and five types of co-occurrences extracted from the rich structures of wikipedia . our evaluation results validate that simple algorithms on this network give competitive results in computing semantic re-latedness between words and between short texts ."
  },
  {
    "title": "Cumulative Attribute Space for Age and Crowd Density Estimation .",
    "entities": [
      "body/face pose -lrb- view angle -rrb- estimation",
      "sparse and imbalanced training data",
      "high dimensional vector-formed feature input",
      "sparse and imbalanced image samples",
      "sparse and imbalanced data",
      "low-level visual features",
      "human age estimation",
      "uncertain viewing conditions",
      "observable visual features",
      "crowd density estimation",
      "labelled training data",
      "computer vision problems",
      "cumulative attribute space",
      "sparse training data",
      "large feature variations",
      "cumulative attribute concept",
      "crowd counting",
      "regression models",
      "mapping function",
      "intrinsic ambiguities",
      "regression model",
      "scalar-valued output",
      "regression problem",
      "imbalanced sampling",
      "age estimation",
      "scalar values",
      "accuracy"
    ],
    "types": "<task> <material> <otherscientificterm> <material> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <material> <task> <otherscientificterm> <material> <otherscientificterm> <method> <task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <task> <otherscientificterm> <metric>",
    "relations": [
      "scalar-valued output -- USED-FOR -- mapping function",
      "sparse and imbalanced image samples -- USED-FOR -- cumulative attribute space",
      "cumulative attribute space -- USED-FOR -- low-level visual features",
      "sparse and imbalanced image samples -- USED-FOR -- low-level visual features",
      "mapping function -- USED-FOR -- regression problem",
      "sparse and imbalanced training data -- USED-FOR -- regression problem",
      "age estimation -- CONJUNCTION -- crowd counting",
      "cumulative attribute concept -- COMPARE -- regression models",
      "human age estimation -- HYPONYM-OF -- computer vision problems",
      "crowd density estimation -- CONJUNCTION -- body/face pose -lrb- view angle -rrb- estimation",
      "human age estimation -- CONJUNCTION -- crowd density estimation",
      "cumulative attribute concept -- USED-FOR -- regression model",
      "accuracy -- EVALUATE-FOR -- cumulative attribute concept",
      "mapping function -- USED-FOR -- computer vision problems",
      "sparse and imbalanced data -- USED-FOR -- regression model",
      "body/face pose -lrb- view angle -rrb- estimation -- HYPONYM-OF -- computer vision problems",
      "crowd density estimation -- HYPONYM-OF -- computer vision problems"
    ],
    "abstract": "a number of <task_11> such as <task_6> , <task_9> and <task_0> can be formulated as a <task_22> by learning a <otherscientificterm_18> between a <otherscientificterm_2> and a <otherscientificterm_21> . such a <task_22> is made difficult due to <material_1> and <otherscientificterm_14> caused by both <otherscientificterm_7> and <otherscientificterm_19> between <otherscientificterm_8> and the <otherscientificterm_25> to be estimated . encouraged by the recent success in using attributes for solving <task_11> with <material_13> , this paper introduces a novel <method_15> for learning a <method_20> when only <material_4> are available . more precisely , <otherscientificterm_5> extracted from <material_3> are mapped onto a <otherscientificterm_12> where each dimension has clearly defined semantic interpretation -lrb- a label -rrb- that captures how the scalar output value -lrb- e.g. age , people count -rrb- changes continuously and cumulatively . extensive experiments show that our <method_15> gains notable advantage on <metric_26> for both <task_24> and <task_16> when compared against conventional <method_17> , especially when the <material_10> is sparse with <method_23> .",
    "abstract_og": "a number of computer vision problems such as human age estimation , crowd density estimation and body/face pose -lrb- view angle -rrb- estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalar-valued output . such a regression problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated . encouraged by the recent success in using attributes for solving computer vision problems with sparse training data , this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available . more precisely , low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation -lrb- a label -rrb- that captures how the scalar output value -lrb- e.g. age , people count -rrb- changes continuously and cumulatively . extensive experiments show that our cumulative attribute concept gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models , especially when the labelled training data is sparse with imbalanced sampling ."
  },
  {
    "title": "Hierarchical Recognition of Human Activities Interacting with Objects .",
    "entities": [
      "semantic description of occurring events",
      "recognition of hierarchical human-object interactions",
      "human-object interactions",
      "motion estimation",
      "recognition decisions",
      "semantic layer",
      "object recognition",
      "airport-like environment",
      "activity recognition",
      "semantic-level recognition",
      "recognition",
      "accuracy",
      "feedback"
    ],
    "types": "<task> <task> <otherscientificterm> <task> <task> <otherscientificterm> <task> <otherscientificterm> <task> <task> <task> <metric> <otherscientificterm>",
    "relations": [
      "motion estimation -- CONJUNCTION -- semantic-level recognition",
      "object recognition -- CONJUNCTION -- motion estimation",
      "recognition -- CONJUNCTION -- semantic description of occurring events",
      "semantic-level recognition -- USED-FOR -- recognition of hierarchical human-object interactions"
    ],
    "abstract": "the paper presents a system that recognizes humans interacting with objects . we delineate a new framework that integrates <task_6> , <task_3> , and <task_9> for the reliable <task_1> . the framework is designed to integrate <task_4> made by each component , and to probabilistically compensate for the failure of the components with the use of the decisions made by the other components . as a result , <otherscientificterm_2> in an <otherscientificterm_7> , such as ' a person carrying a baggage ' , ' a person leaving his/her baggage ' , or ' a person snatching another 's baggage ' , are recognized . the experimental results show that not only the performance of the final <task_8> is superior to that of previous approaches , but also the <metric_11> of the <task_6> and the <task_3> increases using <otherscientificterm_12> from the <otherscientificterm_5> . several real examples illustrate the superior performance in <task_10> and <task_0> .",
    "abstract_og": "the paper presents a system that recognizes humans interacting with objects . we delineate a new framework that integrates object recognition , motion estimation , and semantic-level recognition for the reliable recognition of hierarchical human-object interactions . the framework is designed to integrate recognition decisions made by each component , and to probabilistically compensate for the failure of the components with the use of the decisions made by the other components . as a result , human-object interactions in an airport-like environment , such as ' a person carrying a baggage ' , ' a person leaving his/her baggage ' , or ' a person snatching another 's baggage ' , are recognized . the experimental results show that not only the performance of the final activity recognition is superior to that of previous approaches , but also the accuracy of the object recognition and the motion estimation increases using feedback from the semantic layer . several real examples illustrate the superior performance in recognition and semantic description of occurring events ."
  },
  {
    "title": "Image Feature Learning for Cold Start Problem in Display Advertising .",
    "entities": [
      "discrimina-tive and meaningful features",
      "handcrafted image features",
      "feature learning architecture",
      "image display ads",
      "dis-criminative image features",
      "online display advertising",
      "real world dataset",
      "cold start problem",
      "human heuristic",
      "sift features",
      "handcrafted features",
      "raw pixels",
      "multimedia features",
      "historical information",
      "user feedback",
      "image ads"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <task> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "feature learning architecture -- COMPARE -- handcrafted features",
      "multimedia features -- HYPONYM-OF -- handcrafted image features",
      "real world dataset -- EVALUATE-FOR -- handcrafted features",
      "feature learning architecture -- USED-FOR -- dis-criminative image features",
      "raw pixels -- USED-FOR -- dis-criminative image features",
      "raw pixels -- CONJUNCTION -- user feedback",
      "real world dataset -- EVALUATE-FOR -- feature learning architecture",
      "feature learning architecture -- USED-FOR -- discrimina-tive and meaningful features",
      "multimedia features -- CONJUNCTION -- sift features",
      "sift features -- HYPONYM-OF -- handcrafted image features"
    ],
    "abstract": "in <task_5> , state-of-the-art click through rate -lrb- ctr -rrb- prediction algorithms rely heavily on <otherscientificterm_13> , and they work poorly on growing number of new ads without any <otherscientificterm_13> . this is known as the the <task_7> . for <material_15> , current state-of-the-art systems use <otherscientificterm_1> such as <otherscientificterm_12> and <otherscientificterm_9> to capture the attractiveness of ads . however , these <otherscientificterm_10> are task dependent , inflexible and heuristic . in order to tackle the <task_7> in <material_3> , we propose a new <method_2> to learn the most <otherscientificterm_4> directly from <material_11> and <otherscientificterm_14> in the target task . the proposed <method_2> is flexible and does not depend on <otherscientificterm_8> . extensive experiments on a <material_6> with 47 billion records show that our <method_2> outperforms existing <otherscientificterm_10> significantly , and <method_2> can extract <otherscientificterm_0> .",
    "abstract_og": "in online display advertising , state-of-the-art click through rate -lrb- ctr -rrb- prediction algorithms rely heavily on historical information , and they work poorly on growing number of new ads without any historical information . this is known as the the cold start problem . for image ads , current state-of-the-art systems use handcrafted image features such as multimedia features and sift features to capture the attractiveness of ads . however , these handcrafted features are task dependent , inflexible and heuristic . in order to tackle the cold start problem in image display ads , we propose a new feature learning architecture to learn the most dis-criminative image features directly from raw pixels and user feedback in the target task . the proposed feature learning architecture is flexible and does not depend on human heuristic . extensive experiments on a real world dataset with 47 billion records show that our feature learning architecture outperforms existing handcrafted features significantly , and feature learning architecture can extract discrimina-tive and meaningful features ."
  },
  {
    "title": "Different Structures for Evaluating Answers to Complex Questions : Pyramids Wo n't Topple , and Neither Will Human Assessors .",
    "entities": [
      "trec qa tracks",
      "nugget pyramids scheme",
      "dis-criminative power",
      "nugget-based methodology",
      "assessor opinions",
      "classification tasks",
      "micro-and macro-averaging",
      "manual assessment",
      "nugget importance"
    ],
    "types": "<material> <method> <metric> <method> <method> <task> <task> <task> <otherscientificterm>",
    "relations": [
      "micro-and macro-averaging -- PART-OF -- classification tasks"
    ],
    "abstract": "the idea of '' nugget pyramids '' has recently been introduced as a refinement to the <method_3> used to evaluate answers to complex questions in the <material_0> . this paper examines data from the 2006 evaluation , the first large-scale deployment of the <method_1> . we show that this method of combining judgments of <otherscientificterm_8> from multiple assessors increases the stability and <metric_2> of the evaluation while introducing only a small additional burden in terms of <task_7> . we also consider an alternative method for combining <method_4> , which yields a distinction similar to <task_6> in the context of <task_5> . while the two approaches differ in terms of underlying assumptions , their results are nevertheless highly correlated .",
    "abstract_og": "the idea of '' nugget pyramids '' has recently been introduced as a refinement to the nugget-based methodology used to evaluate answers to complex questions in the trec qa tracks . this paper examines data from the 2006 evaluation , the first large-scale deployment of the nugget pyramids scheme . we show that this method of combining judgments of nugget importance from multiple assessors increases the stability and dis-criminative power of the evaluation while introducing only a small additional burden in terms of manual assessment . we also consider an alternative method for combining assessor opinions , which yields a distinction similar to micro-and macro-averaging in the context of classification tasks . while the two approaches differ in terms of underlying assumptions , their results are nevertheless highly correlated ."
  },
  {
    "title": "A probabilistic approach to simultaneous extraction of beats and downbeats .",
    "entities": [
      "hidden markov models",
      "automatic extraction of beat structure",
      "impulsive and harmonic components",
      "imposed deterministic rules",
      "modeling beat sequences",
      "viterbi decoder",
      "observation vectors",
      "reassigned spectrogram",
      "beat labels",
      "musical piece",
      "lattice rescoring",
      "statistical approach"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <method>",
    "relations": [
      "hidden markov models -- USED-FOR -- statistical approach",
      "statistical approach -- USED-FOR -- modeling beat sequences",
      "musical piece -- USED-FOR -- automatic extraction of beat structure",
      "viterbi decoder -- CONJUNCTION -- lattice rescoring"
    ],
    "abstract": "this paper focuses on the <task_1> from a <material_9> . a novel <method_11> to <task_4> based on the application of <method_0> is introduced . the resulting <otherscientificterm_8> are obtained by running the <method_5> and subsequent <method_10> . for the <otherscientificterm_6> we propose a new feature set that is based on the <method_2> of the <otherscientificterm_7> . different components of <otherscientificterm_6> have been investigated for their efficiency . the main advantage of the proposed approach is the absence of <otherscientificterm_3> . all the parameters are learned from the training data , and the experimental results show the efficiency of the proposed schema .",
    "abstract_og": "this paper focuses on the automatic extraction of beat structure from a musical piece . a novel statistical approach to modeling beat sequences based on the application of hidden markov models is introduced . the resulting beat labels are obtained by running the viterbi decoder and subsequent lattice rescoring . for the observation vectors we propose a new feature set that is based on the impulsive and harmonic components of the reassigned spectrogram . different components of observation vectors have been investigated for their efficiency . the main advantage of the proposed approach is the absence of imposed deterministic rules . all the parameters are learned from the training data , and the experimental results show the efficiency of the proposed schema ."
  },
  {
    "title": "Feature extraction in Through-the-Wall radar imaging .",
    "entities": [
      "synthetic aperture through-the-wall radar imaging experiments",
      "classi \u00bf cation methods",
      "classi \u00bf cation",
      "automatic target classi",
      "through-the-wall radar imaging",
      "support vector machines",
      "recursive splitting tree",
      "stationary objects",
      "real data",
      "enclosed structures",
      "optimum parameters",
      "feature extraction",
      "indoor targets",
      "raw data",
      "sar image",
      "su-perquadrics",
      "segmentation"
    ],
    "types": "<material> <method> <otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "optimum parameters -- USED-FOR -- feature extraction",
      "raw data -- USED-FOR -- sar image",
      "recursive splitting tree -- USED-FOR -- feature extraction",
      "su-perquadrics -- USED-FOR -- feature extraction",
      "synthetic aperture through-the-wall radar imaging experiments -- USED-FOR -- real data",
      "recursive splitting tree -- USED-FOR -- optimum parameters",
      "real data -- USED-FOR -- classi \u00bf cation methods",
      "su-perquadrics -- CONJUNCTION -- classi \u00bf cation",
      "segmentation -- CONJUNCTION -- feature extraction"
    ],
    "abstract": "this paper deals with the problem of <task_3> - \u00bf cation or <method_4> . the proposed scheme considers <otherscientificterm_7> in <otherscientificterm_9> and works on the <material_14> rather than the <material_13> . it comprises <otherscientificterm_16> , <method_11> based on <otherscientificterm_15> , and <otherscientificterm_2> . we present a <method_6> to obtain <otherscientificterm_10> for <method_11> . <method_5> and nearest neighbor classi \u00bf ers are then applied to successfully classify among different <otherscientificterm_12> . the <method_1> are tested and evaluated using <material_8> generated from <material_0> .",
    "abstract_og": "this paper deals with the problem of automatic target classi - \u00bf cation or through-the-wall radar imaging . the proposed scheme considers stationary objects in enclosed structures and works on the sar image rather than the raw data . it comprises segmentation , feature extraction based on su-perquadrics , and classi \u00bf cation . we present a recursive splitting tree to obtain optimum parameters for feature extraction . support vector machines and nearest neighbor classi \u00bf ers are then applied to successfully classify among different indoor targets . the classi \u00bf cation methods are tested and evaluated using real data generated from synthetic aperture through-the-wall radar imaging experiments ."
  },
  {
    "title": "Summarizing Spoken and Written Conversations .",
    "entities": [
      "meeting and email data",
      "meetings and emails domains",
      "conversation summarization system",
      "general conversational features",
      "domain-dependent systems",
      "conversational features",
      "domain-specific features",
      "emails"
    ],
    "types": "<material> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "general conversational features -- USED-FOR -- conversation summarization system",
      "conversation summarization system -- COMPARE -- domain-dependent systems",
      "domain-dependent systems -- USED-FOR -- meeting and email data"
    ],
    "abstract": "in this paper we describe research on summarizing conversations in the <material_1> . we introduce a <method_2> that works in multiple domains utilizing <otherscientificterm_3> , and compare our results with <method_4> for <material_0> . we find that by treating meetings and <material_7> as conversations with <otherscientificterm_3> in common , we can achieve competitive results with state-of-the-art systems that rely on more <otherscientificterm_6> .",
    "abstract_og": "in this paper we describe research on summarizing conversations in the meetings and emails domains . we introduce a conversation summarization system that works in multiple domains utilizing general conversational features , and compare our results with domain-dependent systems for meeting and email data . we find that by treating meetings and emails as conversations with general conversational features in common , we can achieve competitive results with state-of-the-art systems that rely on more domain-specific features ."
  },
  {
    "title": "Blind beamformer for constant modulus signals based on relevance vector machine .",
    "entities": [
      "constant modulus signals",
      "constant modulus algorithm",
      "relevance vector machine",
      "probabilistic bayesian learning procedure",
      "assumption of gaussian prior",
      "blind beamforming method",
      "crowded interference signals",
      "error function",
      "beamfomer"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "probabilistic bayesian learning procedure -- USED-FOR -- blind beamforming method",
      "assumption of gaussian prior -- USED-FOR -- probabilistic bayesian learning procedure",
      "constant modulus algorithm -- USED-FOR -- blind beamforming method",
      "blind beamforming method -- USED-FOR -- constant modulus signals",
      "crowded interference signals -- USED-FOR -- blind beamforming method",
      "blind beamforming method -- USED-FOR -- beamfomer",
      "error function -- PART-OF -- blind beamforming method",
      "probabilistic bayesian learning procedure -- USED-FOR -- beamfomer",
      "relevance vector machine -- USED-FOR -- blind beamforming method"
    ],
    "abstract": "the <method_5> for <otherscientificterm_0> based on <method_2> is proposed . the proposed <method_5> is obtained by incorporating the <method_1> - like <otherscientificterm_7> into the conventional <method_5> . the <method_5> formulates the parameters of <method_8> by exploiting a <method_3> with <otherscientificterm_4> for parameters . the simulation results show that the proposed <method_5> can restore the desired signals with <otherscientificterm_6> .",
    "abstract_og": "the blind beamforming method for constant modulus signals based on relevance vector machine is proposed . the proposed blind beamforming method is obtained by incorporating the constant modulus algorithm - like error function into the conventional blind beamforming method . the blind beamforming method formulates the parameters of beamfomer by exploiting a probabilistic bayesian learning procedure with assumption of gaussian prior for parameters . the simulation results show that the proposed blind beamforming method can restore the desired signals with crowded interference signals ."
  },
  {
    "title": "Efficient Conversion of X.Y Surround Sound Content to Binaural Head-Tracked Form for HRTF-Enabled Playback .",
    "entities": [
      "binaural presentation of x.y sound",
      "\u017fxed playback cost",
      "virtual audio principles",
      "reference room con\u017fg-uration",
      "spatio-temporal representation",
      "x.y setup",
      "multipole expansion",
      "listening area",
      "binaural signal",
      "user pose",
      "computational cost",
      "head-tracked playback",
      "ear position"
    ],
    "types": "<task> <metric> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <metric> <material> <otherscientificterm>",
    "relations": [
      "computational cost -- EVALUATE-FOR -- binaural presentation of x.y sound",
      "virtual audio principles -- USED-FOR -- binaural presentation of x.y sound",
      "multipole expansion -- USED-FOR -- binaural signal",
      "ear position -- FEATURE-OF -- multipole expansion"
    ],
    "abstract": "binaural presentation of x.y sound is usually performed using <otherscientificterm_2> -- that is , by attempting to virtually reproduce the setup of the x+y loudspeakers in the <otherscientificterm_3> . the <metric_10> of such <task_0> is linear in the number of channels in the <otherscientificterm_5> . we present a novel scheme that computes , offline , a <method_4> of the sound \u017feld in the <otherscientificterm_7> and store it as a <otherscientificterm_6> . during <material_11> , the <material_8> is obtained by evaluating the <otherscientificterm_6> at the <otherscientificterm_12> corresponding to the current <otherscientificterm_9> , resulting in a <metric_1> . the representation is further extended to incorporate individualized hrtfs at no additional cost . simulation results are presented .",
    "abstract_og": "binaural presentation of x.y sound is usually performed using virtual audio principles -- that is , by attempting to virtually reproduce the setup of the x+y loudspeakers in the reference room con\u017fg-uration . the computational cost of such binaural presentation of x.y sound is linear in the number of channels in the x.y setup . we present a novel scheme that computes , offline , a spatio-temporal representation of the sound \u017feld in the listening area and store it as a multipole expansion . during head-tracked playback , the binaural signal is obtained by evaluating the multipole expansion at the ear position corresponding to the current user pose , resulting in a \u017fxed playback cost . the representation is further extended to incorporate individualized hrtfs at no additional cost . simulation results are presented ."
  },
  {
    "title": "Tandem acoustic modeling in large-vocabulary recognition .",
    "entities": [
      "etsi aurora noisy digits",
      "small-vocabulary , high-noise task",
      "tandem approach",
      "neural-net preprocessor",
      "feature inputs",
      "hmm baseline",
      "spontaneous speech",
      "posterior probabilities",
      "context-independent models",
      "acoustic signal",
      "word-error rates",
      "sub-word units",
      "error-rate reductions",
      "context-dependent models",
      "spine1"
    ],
    "types": "<material> <material> <method> <method> <otherscientificterm> <method> <material> <otherscientificterm> <method> <otherscientificterm> <metric> <otherscientificterm> <metric> <method> <method>",
    "relations": [
      "neural-net preprocessor -- USED-FOR -- posterior probabilities",
      "tandem approach -- USED-FOR -- acoustic signal",
      "error-rate reductions -- EVALUATE-FOR -- tandem approach",
      "small-vocabulary , high-noise task -- EVALUATE-FOR -- tandem approach"
    ],
    "abstract": "in the <method_2> to modeling the <otherscientificterm_9> , a <method_3> is first discriminatively trained to estimate <otherscientificterm_7> across a phone set . these are then used as <otherscientificterm_4> for a conventional hidden markov model -lrb- hmm -rrb- based speech recognizer , which relearns the associations to <otherscientificterm_11> . in this paper , we apply the <method_2> to the data provided for the first speech in noisy environments -lrb- <method_14> -rrb- evaluation conducted by the naval research laboratory -lrb- nrl -rrb- in august 2000 . in our previous experience with the <material_0> -lrb- a <material_1> -rrb- the <method_2> achieved <metric_12> of over 50 % relative to the <method_5> . for <method_14> , a larger task involving more <material_6> , we find that , when <method_8> are used , the <method_2> continue to result in large reductions in <metric_10> relative to those achieved by systems using standard mfc or plp features . however , these improvements do not carry over to <method_13> . this may be attributable to several factors which are discussed in the paper .",
    "abstract_og": "in the tandem approach to modeling the acoustic signal , a neural-net preprocessor is first discriminatively trained to estimate posterior probabilities across a phone set . these are then used as feature inputs for a conventional hidden markov model -lrb- hmm -rrb- based speech recognizer , which relearns the associations to sub-word units . in this paper , we apply the tandem approach to the data provided for the first speech in noisy environments -lrb- spine1 -rrb- evaluation conducted by the naval research laboratory -lrb- nrl -rrb- in august 2000 . in our previous experience with the etsi aurora noisy digits -lrb- a small-vocabulary , high-noise task -rrb- the tandem approach achieved error-rate reductions of over 50 % relative to the hmm baseline . for spine1 , a larger task involving more spontaneous speech , we find that , when context-independent models are used , the tandem approach continue to result in large reductions in word-error rates relative to those achieved by systems using standard mfc or plp features . however , these improvements do not carry over to context-dependent models . this may be attributable to several factors which are discussed in the paper ."
  },
  {
    "title": "Variable Bandwidth Image Denoising Using Image-based Noise Models .",
    "entities": [
      "image intensity dependent noise variance",
      "spatial and photometric similarities",
      "variable bandwidth approach",
      "image formation process",
      "image denoising",
      "quadratic function",
      "variable bandwidth",
      "variational formulation",
      "restoration quality",
      "rgb one",
      "raw space",
      "noise model",
      "reconstruction process",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <method> <method> <metric>",
    "relations": [
      "variational formulation -- USED-FOR -- image denoising",
      "accuracy -- EVALUATE-FOR -- reconstruction process",
      "variational formulation -- USED-FOR -- spatial and photometric similarities",
      "variable bandwidth approach -- CONJUNCTION -- image intensity dependent noise variance",
      "raw space -- FEATURE-OF -- noise model",
      "noise model -- COMPARE -- image formation process",
      "quadratic function -- USED-FOR -- variational formulation"
    ],
    "abstract": "this paper introduces a <method_7> for <task_4> based on a <otherscientificterm_5> over <method_7> of <otherscientificterm_6> . these <method_7> are scale adaptive and reflect <otherscientificterm_1> between pixels . the bandwidth of the <method_7> is observation-dependent towards improving the <metric_13> of the <method_12> and is constrained to be locally smooth . we analyze the evolution of the <method_11> form the <otherscientificterm_10> to the <otherscientificterm_9> , by propagating <method_11> over the <method_3> . the experimental results demonstrate that the use of a <method_2> and an <otherscientificterm_0> ensures better <metric_8> .",
    "abstract_og": "this paper introduces a variational formulation for image denoising based on a quadratic function over variational formulation of variable bandwidth . these variational formulation are scale adaptive and reflect spatial and photometric similarities between pixels . the bandwidth of the variational formulation is observation-dependent towards improving the accuracy of the reconstruction process and is constrained to be locally smooth . we analyze the evolution of the noise model form the raw space to the rgb one , by propagating noise model over the image formation process . the experimental results demonstrate that the use of a variable bandwidth approach and an image intensity dependent noise variance ensures better restoration quality ."
  },
  {
    "title": "An Evolutionary Algorithm Extended by Ecological Analogy and its Application to the Game of Go .",
    "entities": [
      "infrequently used knowledge",
      "evolutionary algorithms",
      "ecological systems",
      "activation value",
      "variation-making operator",
      "go knowledge",
      "go maxims",
      "features",
      "patterns"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "activation value -- USED-FOR -- variation-making operator",
      "go maxims -- HYPONYM-OF -- go knowledge",
      "patterns -- HYPONYM-OF -- go knowledge"
    ],
    "abstract": "the following two important <otherscientificterm_7> of human experts ' knowledge are not realized by most <method_1> : one is that it is various and the other is that the amount of knowledge , including <otherscientificterm_0> , is large . to imitate these <otherscientificterm_7> , we introduce an <otherscientificterm_3> for individuals and a new <method_4> , splitting , both of which are inspired by <method_2> . this algorithm is applied to the game of go and a large amount of knowledge evaluated as appropriate by a human expert is acquired . various kinds of <otherscientificterm_5> may be acquired such as <otherscientificterm_8> , sequences of moves , and <otherscientificterm_6> , part of which has already been realized .",
    "abstract_og": "the following two important features of human experts ' knowledge are not realized by most evolutionary algorithms : one is that it is various and the other is that the amount of knowledge , including infrequently used knowledge , is large . to imitate these features , we introduce an activation value for individuals and a new variation-making operator , splitting , both of which are inspired by ecological systems . this algorithm is applied to the game of go and a large amount of knowledge evaluated as appropriate by a human expert is acquired . various kinds of go knowledge may be acquired such as patterns , sequences of moves , and go maxims , part of which has already been realized ."
  },
  {
    "title": "Disentangling Factors of Variation for Facial Expression Recognition .",
    "entities": [
      "contrac-tive discriminative analysis feature extractor",
      "multi-scale contractive convolutional network",
      "contractive auto-encoder",
      "facial expression recognition",
      "toronto face database",
      "emotion classification algorithm",
      "2d face images",
      "hierarchy of features",
      "semi-supervised approach",
      "feature representation",
      "deep learning",
      "state-of-art accuracy",
      "facial traits",
      "emotion-related factors",
      "accuracy"
    ],
    "types": "<method> <method> <method> <task> <material> <method> <task> <otherscientificterm> <method> <method> <task> <metric> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "accuracy -- EVALUATE-FOR -- contractive auto-encoder",
      "2d face images -- USED-FOR -- facial expression recognition",
      "semi-supervised approach -- USED-FOR -- 2d face images",
      "contrac-tive discriminative analysis feature extractor -- USED-FOR -- contractive auto-encoder",
      "semi-supervised approach -- USED-FOR -- facial expression recognition",
      "feature representation -- USED-FOR -- contrac-tive discriminative analysis feature extractor",
      "multi-scale contractive convolutional network -- CONJUNCTION -- contractive auto-encoder"
    ],
    "abstract": "we propose a <method_8> to solve the task of <task_3> in <task_6> using recent ideas in <task_10> for handling the factors of variation present in data . an <method_5> should be both robust to -lrb- 1 -rrb- remaining variations due to the pose of the face in the image after centering and alignment , -lrb- 2 -rrb- the identity or morphology of the face . in order to achieve this invariance , we propose to learn a <otherscientificterm_7> in which we gradually filter the factors of variation arising from both -lrb- 1 -rrb- and -lrb- 2 -rrb- . we address -lrb- 1 -rrb- by using a <method_1> in order to obtain invariance to translations of the <otherscientificterm_12> in the image . using the <method_9> produced by the <method_1> , we train a <method_0> , a novel variant of the <method_2> , designed to learn a representation separating out the <otherscientificterm_13> from the others -lrb- which mostly capture the subject identity , and what is left of pose after the <method_1> -rrb- . this <method_8> beats the state-of-the-art on a recently proposed dataset for <task_3> , the <material_4> , moving the <metric_11> from 82.4 % to 85.0 % , while the <method_1> and cda improve <metric_14> of a standard <method_2> by 8 % .",
    "abstract_og": "we propose a semi-supervised approach to solve the task of facial expression recognition in 2d face images using recent ideas in deep learning for handling the factors of variation present in data . an emotion classification algorithm should be both robust to -lrb- 1 -rrb- remaining variations due to the pose of the face in the image after centering and alignment , -lrb- 2 -rrb- the identity or morphology of the face . in order to achieve this invariance , we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both -lrb- 1 -rrb- and -lrb- 2 -rrb- . we address -lrb- 1 -rrb- by using a multi-scale contractive convolutional network in order to obtain invariance to translations of the facial traits in the image . using the feature representation produced by the multi-scale contractive convolutional network , we train a contrac-tive discriminative analysis feature extractor , a novel variant of the contractive auto-encoder , designed to learn a representation separating out the emotion-related factors from the others -lrb- which mostly capture the subject identity , and what is left of pose after the multi-scale contractive convolutional network -rrb- . this semi-supervised approach beats the state-of-the-art on a recently proposed dataset for facial expression recognition , the toronto face database , moving the state-of-art accuracy from 82.4 % to 85.0 % , while the multi-scale contractive convolutional network and cda improve accuracy of a standard contractive auto-encoder by 8 % ."
  },
  {
    "title": "Kernel-based invariant subspace method for hyperspectral target detection .",
    "entities": [
      "kernel principal component analysis",
      "linear mixture model",
      "small target detection of hyperspectral images",
      "subspaces of target and background",
      "kernel-based invariant subspace detection method",
      "generalized likelihood ratio test",
      "aviris hyperspectral data",
      "hyperspectral target detection",
      "hyperspectral image",
      "spectral variability",
      "hyperspectral images",
      "pixel",
      "noise",
      "target"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <method> <metric> <material> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "kernel principal component analysis -- CONJUNCTION -- linear mixture model",
      "linear mixture model -- PART-OF -- kernel-based invariant subspace detection method",
      "kernel principal component analysis -- PART-OF -- kernel-based invariant subspace detection method",
      "kernel-based invariant subspace detection method -- USED-FOR -- hyperspectral target detection",
      "kernel principal component analysis -- USED-FOR -- pixel",
      "kernel-based invariant subspace detection method -- USED-FOR -- small target detection of hyperspectral images",
      "kernel principal component analysis -- USED-FOR -- subspaces of target and background"
    ],
    "abstract": "in this paper , a <method_4> is proposed for <task_2> . the <method_4> combines <method_0> and <method_1> . the <method_0> is used to describe each <otherscientificterm_11> in the <material_10> as mixture of <otherscientificterm_13> , background and <otherscientificterm_12> . the <method_0> is used to build <otherscientificterm_3> . a <metric_5> is used to detect whether each <otherscientificterm_11> in <otherscientificterm_8> includes <otherscientificterm_13> . the numerical experiments are performed on <material_6> with 126 bands . the experimental results show the effectiveness of the proposed <method_4> and prove that this <method_4> can commendably overcome <otherscientificterm_9> in the <task_7> , and <method_4> has good ability to separate <otherscientificterm_13> from background .",
    "abstract_og": "in this paper , a kernel-based invariant subspace detection method is proposed for small target detection of hyperspectral images . the kernel-based invariant subspace detection method combines kernel principal component analysis and linear mixture model . the kernel principal component analysis is used to describe each pixel in the hyperspectral images as mixture of target , background and noise . the kernel principal component analysis is used to build subspaces of target and background . a generalized likelihood ratio test is used to detect whether each pixel in hyperspectral image includes target . the numerical experiments are performed on aviris hyperspectral data with 126 bands . the experimental results show the effectiveness of the proposed kernel-based invariant subspace detection method and prove that this kernel-based invariant subspace detection method can commendably overcome spectral variability in the hyperspectral target detection , and kernel-based invariant subspace detection method has good ability to separate target from background ."
  },
  {
    "title": "SIFT Flow : Dense Correspondence across Different Scenes .",
    "entities": [
      "transfer of moving objects",
      "temporally adjacent frame",
      "discontinuity-preserving spatial model",
      "matching of objects",
      "database of videos",
      "computer vision",
      "scene/object appearances",
      "spatial discontinu-ities",
      "histogram intersection",
      "optical flow",
      "static image",
      "query image",
      "sift flow",
      "bag-of-visual-words representation",
      "image registration",
      "image matching",
      "image",
      "recognition"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <task> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm> <method> <task> <task> <otherscientificterm> <task>",
    "relations": [
      "image registration -- PART-OF -- computer vision",
      "recognition -- COMPARE -- image matching",
      "bag-of-visual-words representation -- USED-FOR -- histogram intersection"
    ],
    "abstract": "while <task_14> has been studied in different areas of <task_5> , aligning images depicting different scenes remains a challenging problem , closer to <task_17> than to <task_15> . analogous to <otherscientificterm_9> , where an <otherscientificterm_16> is aligned to its <otherscientificterm_1> , we propose <otherscientificterm_12> , a method to align an <otherscientificterm_16> to its neighbors in a large <otherscientificterm_16> collection consisting of a variety of scenes . for a <material_11> , <otherscientificterm_8> on a <method_13> is used to find the set of nearest neighbors in the database . the <otherscientificterm_12> algorithm then consists of matching densely sampled <otherscientificterm_12> between the two images , while preserving <otherscientificterm_7> . the use of <otherscientificterm_12> allows robust matching across different <otherscientificterm_6> and the <method_2> allows <task_3> located at different parts of the scene . experiments show that the proposed approach is able to robustly align complicated scenes with large spatial distortions . we collect a large <material_4> and apply the <otherscientificterm_12> algorithm to two applications : -lrb- i -rrb- motion field prediction from a single <material_10> and -lrb- ii -rrb- motion synthesis via <otherscientificterm_0> .",
    "abstract_og": "while image registration has been studied in different areas of computer vision , aligning images depicting different scenes remains a challenging problem , closer to recognition than to image matching . analogous to optical flow , where an image is aligned to its temporally adjacent frame , we propose sift flow , a method to align an image to its neighbors in a large image collection consisting of a variety of scenes . for a query image , histogram intersection on a bag-of-visual-words representation is used to find the set of nearest neighbors in the database . the sift flow algorithm then consists of matching densely sampled sift flow between the two images , while preserving spatial discontinu-ities . the use of sift flow allows robust matching across different scene/object appearances and the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene . experiments show that the proposed approach is able to robustly align complicated scenes with large spatial distortions . we collect a large database of videos and apply the sift flow algorithm to two applications : -lrb- i -rrb- motion field prediction from a single static image and -lrb- ii -rrb- motion synthesis via transfer of moving objects ."
  },
  {
    "title": "Detecting Spammers in Community Question Answering .",
    "entities": [
      "community question answering",
      "graph regulariza-tion constraints",
      "spam-ming activities",
      "social information",
      "optimization problem",
      "text-based predic-tor",
      "cqa portal",
      "cqa sites",
      "spammers"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <material> <material> <material>",
    "relations": [
      "graph regulariza-tion constraints -- USED-FOR -- social information",
      "graph regulariza-tion constraints -- USED-FOR -- text-based predic-tor"
    ],
    "abstract": "as the popularity of <method_0> increases , <method_2> also picked up in numbers and variety . on <material_7> , <material_8> often pretend to ask questions , and select answers which were published by their partners or themselves as the best answers . these fake best answers can not be easily detected by neither existing methods nor common users . in this paper , we address the issue of detecting <material_8> on <material_7> . we formulate the task as an <task_4> . <otherscientificterm_3> is incorporated by adding <otherscientificterm_1> to the <otherscientificterm_5> . to evaluate the proposed approach , we crawled a data set from a <material_6> . experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods .",
    "abstract_og": "as the popularity of community question answering increases , spam-ming activities also picked up in numbers and variety . on cqa sites , spammers often pretend to ask questions , and select answers which were published by their partners or themselves as the best answers . these fake best answers can not be easily detected by neither existing methods nor common users . in this paper , we address the issue of detecting spammers on cqa sites . we formulate the task as an optimization problem . social information is incorporated by adding graph regulariza-tion constraints to the text-based predic-tor . to evaluate the proposed approach , we crawled a data set from a cqa portal . experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods ."
  },
  {
    "title": "Use of the pitch synchronous wavelet transform as a new decomposition method for WI .",
    "entities": [
      "pitch synchronous wavelet transform",
      "waveform interpolation paradigm",
      "causal , stable iir filters",
      "characteristic waveform decomposition method",
      "flexible quantisation of parameters",
      "evolutionary waveform domain",
      "wavelet filter bank",
      "waveform surfaces",
      "evolutionary surfaces",
      "characteristic surfaces",
      "iir filters",
      "decomposed surfaces",
      "evolution domain",
      "decomposition mechanism",
      "fir filters",
      "unvoiced speech",
      "pitch-cycle waveforms",
      "multi-scale characterisation",
      "wi decomposition",
      "fir counterparts",
      "voiced speech",
      "filtering"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <material> <otherscientificterm> <task> <method> <otherscientificterm> <material> <method>",
    "relations": [
      "fir filters -- USED-FOR -- wavelet filter bank",
      "iir filters -- USED-FOR -- evolutionary surfaces",
      "characteristic waveform decomposition method -- USED-FOR -- waveform interpolation paradigm"
    ],
    "abstract": "a new <method_3> based on wavelets is proposed for the <method_1> . in <method_1> , <otherscientificterm_16> are filtered in the <otherscientificterm_12> to decompose the signal into two <otherscientificterm_7> , one characterising <material_20> and a second representing <material_15> . the slow roll-off of <method_14> leads , however , to a significant interrelationship between the <otherscientificterm_11> . here we present the <method_0> as an alternative <method_13> . <method_21> is again performed in the <material_5> , producing <otherscientificterm_9> at several resolutions . this <task_17> leads to more <otherscientificterm_4> , especially at higher rates than <method_1> 's 2.4 kb/s . <method_14> are replaced in the <method_6> by <method_2> which achieve significant delay reductions over their <otherscientificterm_19> . furthermore , <method_10> track the dynamic aspects of the <otherscientificterm_8> faster , overcoming problems existing in the current <method_18> .",
    "abstract_og": "a new characteristic waveform decomposition method based on wavelets is proposed for the waveform interpolation paradigm . in waveform interpolation paradigm , pitch-cycle waveforms are filtered in the evolution domain to decompose the signal into two waveform surfaces , one characterising voiced speech and a second representing unvoiced speech . the slow roll-off of fir filters leads , however , to a significant interrelationship between the decomposed surfaces . here we present the pitch synchronous wavelet transform as an alternative decomposition mechanism . filtering is again performed in the evolutionary waveform domain , producing characteristic surfaces at several resolutions . this multi-scale characterisation leads to more flexible quantisation of parameters , especially at higher rates than waveform interpolation paradigm 's 2.4 kb/s . fir filters are replaced in the wavelet filter bank by causal , stable iir filters which achieve significant delay reductions over their fir counterparts . furthermore , iir filters track the dynamic aspects of the evolutionary surfaces faster , overcoming problems existing in the current wi decomposition ."
  },
  {
    "title": "Monte Carlo model-space noise adaptation for speech recognition .",
    "entities": [
      "gaussian mixture models",
      "spliced and projected mfcc features",
      "silence and speech frames",
      "gmm speech model",
      "single-gaussian noise model",
      "monte carlo method",
      "parallel model combination",
      "adaptation techniques",
      "model-space joint",
      "silence frames",
      "mfcc"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "single-gaussian noise model -- PART-OF -- monte carlo method",
      "monte carlo method -- HYPONYM-OF -- parallel model combination",
      "single-gaussian noise model -- CONJUNCTION -- gmm speech model",
      "monte carlo method -- USED-FOR -- spliced and projected mfcc features",
      "monte carlo method -- USED-FOR -- monte carlo method",
      "monte carlo method -- USED-FOR -- model-space joint",
      "monte carlo method -- COMPARE -- mfcc",
      "spliced and projected mfcc features -- COMPARE -- mfcc",
      "parallel model combination -- CONJUNCTION -- model-space joint",
      "gmm speech model -- PART-OF -- monte carlo method"
    ],
    "abstract": "we describe a <method_5> for model-space noise adaptation of <method_0> . this <method_5> combines a <method_4> with the <method_3> to produce an adapted <method_5> . <method_5> is similar to <method_6> or <method_8> , except that <method_5> applies to <otherscientificterm_1> rather than to <method_10> plus dynamic features . we demonstrate the necessity of re-estimating the noise using both the <otherscientificterm_2> rather than just estimating <method_5> from <otherscientificterm_9> , and obtain improvements on a matched test set without added noise using a system that includes all standard <method_7> .",
    "abstract_og": "we describe a monte carlo method for model-space noise adaptation of gaussian mixture models . this monte carlo method combines a single-gaussian noise model with the gmm speech model to produce an adapted monte carlo method . monte carlo method is similar to parallel model combination or model-space joint , except that monte carlo method applies to spliced and projected mfcc features rather than to mfcc plus dynamic features . we demonstrate the necessity of re-estimating the noise using both the silence and speech frames rather than just estimating monte carlo method from silence frames , and obtain improvements on a matched test set without added noise using a system that includes all standard adaptation techniques ."
  },
  {
    "title": "Real-time multiple sound source localization using a circular microphone array based on single-source confidence measures .",
    "entities": [
      "blind source separation methods",
      "weak sound source sparsity",
      "real-time adaptative localization approach",
      "lo-calization ambiguities",
      "circular array",
      "linear arrays"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "blind source separation methods -- USED-FOR -- weak sound source sparsity",
      "circular array -- USED-FOR -- real-time adaptative localization approach",
      "real-time adaptative localization approach -- USED-FOR -- lo-calization ambiguities"
    ],
    "abstract": "we propose a novel <method_2> for multiple sources using a <otherscientificterm_4> , in order to suppress the <otherscientificterm_3> faced with <otherscientificterm_5> , and assuming a <otherscientificterm_1> which is derived from <method_0> . our proposed <method_2> performs very well both in simulations and in real conditions at 50 % real-time .",
    "abstract_og": "we propose a novel real-time adaptative localization approach for multiple sources using a circular array , in order to suppress the lo-calization ambiguities faced with linear arrays , and assuming a weak sound source sparsity which is derived from blind source separation methods . our proposed real-time adaptative localization approach performs very well both in simulations and in real conditions at 50 % real-time ."
  },
  {
    "title": "An Unsupervised Method for Word Sense Tagging using Parallel Corpora .",
    "entities": [
      "word sense disambiguation",
      "diier-ing translator preferences",
      "in-uence of context",
      "machine translation systems",
      "word-level translation correspondences",
      "cross-language lexicalizations",
      "sense disam-biguation",
      "translation correspondences",
      "parallel corpora",
      "unsupervised method",
      "pseudo-translations",
      "dif-cult"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <task> <otherscientificterm> <material> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "word-level translation correspondences -- USED-FOR -- sense disam-biguation",
      "diier-ing translator preferences -- CONJUNCTION -- in-uence of context",
      "translation correspondences -- PART-OF -- parallel corpora",
      "unsupervised method -- USED-FOR -- word sense disambiguation"
    ],
    "abstract": "we present an <method_9> for <task_0> that exploits <otherscientificterm_7> in <material_8> . the <method_9> takes advantage of the fact that <otherscientificterm_5> of the same concept tend to be consistent , preserving some core element of its semantics , and yet also variable , reeecting <otherscientificterm_1> and the <otherscientificterm_2> . working with <material_8> introduces an extra complication for evaluation , since it is <otherscientificterm_11> to nd a corpus that is both sense tagged and parallel with another language ; therefore we use <otherscientificterm_10> , created by <method_3> , in order to make possible the evaluation of the <method_9> against a standard test set . the results demonstrate that <task_4> are a valuable source of information for <task_6> .",
    "abstract_og": "we present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora . the unsupervised method takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent , preserving some core element of its semantics , and yet also variable , reeecting diier-ing translator preferences and the in-uence of context . working with parallel corpora introduces an extra complication for evaluation , since it is dif-cult to nd a corpus that is both sense tagged and parallel with another language ; therefore we use pseudo-translations , created by machine translation systems , in order to make possible the evaluation of the unsupervised method against a standard test set . the results demonstrate that word-level translation correspondences are a valuable source of information for sense disam-biguation ."
  },
  {
    "title": "How Things are Intended to Work : Capturing Functional Knowledge in Device Design .",
    "entities": [
      "cfrl -lrb- causal functional representation language -rrb-",
      "engineer 's design rationale",
      "design support system",
      "causal mechanism",
      "functional specifications",
      "physical specification",
      "explicit representation",
      "physical structure",
      "design process",
      "manipulation"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "functional specifications -- USED-FOR -- engineer 's design rationale",
      "causal mechanism -- CONJUNCTION -- physical structure",
      "design support system -- USED-FOR -- functional specifications",
      "cfrl -lrb- causal functional representation language -rrb- -- USED-FOR -- physical specification",
      "design support system -- USED-FOR -- physical specification"
    ],
    "abstract": "when designing a device , the final product of the <method_8> is usually considered to be a <otherscientificterm_5> of a device . however , the design of the <otherscientificterm_3> underlying the <otherscientificterm_5> , i.e. how the device is intended to work to achieve its function , is a product just as important as the <otherscientificterm_5> , if not more . capturing this knowledge of <otherscientificterm_3> is necessary in order to understand the <otherscientificterm_5> of the device as well as to evaluate and refine the specifications during the <method_8> . despite the importance of such knowledge , existing <method_8> do not support its <method_6> or <otherscientificterm_9> . we describe a <method_2> under development in which knowledge of both the <otherscientificterm_3> and the <otherscientificterm_7> of a device being designed is explicitly represented and manipulated . the <method_2> allows the designer to provide <otherscientificterm_4> at various levels of abstraction in a language called <method_0> . the <method_0> acquired from the user enables the <method_2> to evaluate the <otherscientificterm_5> as <method_2> is being developed in order to provide useful feedback to the designer . furthermore , <otherscientificterm_4> provide an important basis for recording the <task_1> .",
    "abstract_og": "when designing a device , the final product of the design process is usually considered to be a physical specification of a device . however , the design of the causal mechanism underlying the physical specification , i.e. how the device is intended to work to achieve its function , is a product just as important as the physical specification , if not more . capturing this knowledge of causal mechanism is necessary in order to understand the physical specification of the device as well as to evaluate and refine the specifications during the design process . despite the importance of such knowledge , existing design process do not support its explicit representation or manipulation . we describe a design support system under development in which knowledge of both the causal mechanism and the physical structure of a device being designed is explicitly represented and manipulated . the design support system allows the designer to provide functional specifications at various levels of abstraction in a language called cfrl -lrb- causal functional representation language -rrb- . the cfrl -lrb- causal functional representation language -rrb- acquired from the user enables the design support system to evaluate the physical specification as design support system is being developed in order to provide useful feedback to the designer . furthermore , functional specifications provide an important basis for recording the engineer 's design rationale ."
  },
  {
    "title": "Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks .",
    "entities": [
      "large-scale visual recognition challenge",
      "convolutional neural networks",
      "pascal voc 2007 and 2012 datasets",
      "object and action classification",
      "rich mid-level image representations",
      "large-scale annotated datasets",
      "hand-designed low-level features",
      "image classification methods",
      "pascal voc dataset",
      "limited training data",
      "mid-level image representation",
      "visual recognition tasks",
      "annotated image samples",
      "image representations",
      "learning cnns",
      "reuse layers",
      "image statistics",
      "image classification",
      "images"
    ],
    "types": "<task> <method> <material> <task> <method> <material> <otherscientificterm> <method> <material> <material> <method> <task> <material> <method> <task> <otherscientificterm> <material> <task> <material>",
    "relations": [
      "image representations -- USED-FOR -- object and action classification",
      "images -- PART-OF -- pascal voc dataset",
      "convolutional neural networks -- USED-FOR -- image classification",
      "large-scale visual recognition challenge -- USED-FOR -- rich mid-level image representations",
      "hand-designed low-level features -- USED-FOR -- image classification methods",
      "large-scale annotated datasets -- USED-FOR -- image representations",
      "large-scale visual recognition challenge -- USED-FOR -- image representations"
    ],
    "abstract": "convolutional neural networks -lrb- cnn -rrb- have recently shown outstanding <task_17> performance in the <task_0> . the success of <task_0> is attributed to their ability to learn <method_4> as opposed to <otherscientificterm_6> used in other <method_7> . <task_14> , however , amounts to estimating millions of parameters and requires a very large number of <material_12> . this property currently prevents application of <task_0> to problems with <material_9> . in this work we show how <method_13> learned with <task_0> on <material_5> can be efficiently transferred to other <task_11> with limited amount of training data . we design a method to <otherscientificterm_15> trained on the <task_0> to compute <method_10> for <material_18> in the <material_8> . we show that despite differences in <material_16> and tasks in the two datasets , the <method_13> leads to significantly improved results for <task_3> , outperforming the current state of the art on <material_2> . we also show promising results for <task_3> .",
    "abstract_og": "convolutional neural networks -lrb- cnn -rrb- have recently shown outstanding image classification performance in the large-scale visual recognition challenge . the success of large-scale visual recognition challenge is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification methods . learning cnns , however , amounts to estimating millions of parameters and requires a very large number of annotated image samples . this property currently prevents application of large-scale visual recognition challenge to problems with limited training data . in this work we show how image representations learned with large-scale visual recognition challenge on large-scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data . we design a method to reuse layers trained on the large-scale visual recognition challenge to compute mid-level image representation for images in the pascal voc dataset . we show that despite differences in image statistics and tasks in the two datasets , the image representations leads to significantly improved results for object and action classification , outperforming the current state of the art on pascal voc 2007 and 2012 datasets . we also show promising results for object and action classification ."
  },
  {
    "title": "Recurrent Models of Visual Attention .",
    "entities": [
      "convolutional neural network baseline",
      "dynamic visual control problem",
      "recurrent neural network model",
      "translation invariance built-in",
      "convolutional neural networks",
      "reinforcement learning methods",
      "image or video",
      "image classification tasks",
      "task-specific policies",
      "cluttered images",
      "computation scales",
      "large images",
      "image pixels"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <method> <material> <task> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "image classification tasks -- EVALUATE-FOR -- recurrent neural network model",
      "recurrent neural network model -- COMPARE -- convolutional neural network baseline",
      "reinforcement learning methods -- USED-FOR -- task-specific policies",
      "reinforcement learning methods -- USED-FOR -- recurrent neural network model",
      "convolutional neural networks -- USED-FOR -- large images",
      "recurrent neural network model -- USED-FOR -- task-specific policies"
    ],
    "abstract": "applying <method_4> to <material_11> is computationally expensive because the amount of <otherscientificterm_10> linearly with the number of <otherscientificterm_12> . we present a novel <method_2> that is capable of extracting information from an <material_6> by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution . like <method_4> , the proposed <method_2> has a degree of <otherscientificterm_3> , but the amount of computation <method_2> performs can be controlled independently of the input image size . while the <method_2> is non-differentiable , <method_2> can be trained using <method_5> to learn <otherscientificterm_8> . we evaluate our <method_2> on several <task_7> , where <method_2> significantly outperforms a <method_0> on <material_9> , and on a <task_1> , where <method_2> learns to track a simple object without an explicit training signal for doing so .",
    "abstract_og": "applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels . we present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution . like convolutional neural networks , the proposed recurrent neural network model has a degree of translation invariance built-in , but the amount of computation recurrent neural network model performs can be controlled independently of the input image size . while the recurrent neural network model is non-differentiable , recurrent neural network model can be trained using reinforcement learning methods to learn task-specific policies . we evaluate our recurrent neural network model on several image classification tasks , where recurrent neural network model significantly outperforms a convolutional neural network baseline on cluttered images , and on a dynamic visual control problem , where recurrent neural network model learns to track a simple object without an explicit training signal for doing so ."
  },
  {
    "title": "Bilingual Sense Similarity for Statistical Machine Translation .",
    "entities": [
      "hierarchical phrase-based machine translation system",
      "sense similarity scores",
      "statistical machine translation",
      "vector space model",
      "parallel corpora",
      "translation model",
      "similarity scores",
      "sense similarity",
      "rules",
      "features",
      "phrases"
    ],
    "types": "<method> <metric> <task> <method> <material> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "features -- USED-FOR -- statistical machine translation",
      "similarity scores -- EVALUATE-FOR -- translation model",
      "vector space model -- USED-FOR -- sense similarity scores",
      "features -- USED-FOR -- translation model",
      "translation model -- USED-FOR -- statistical machine translation",
      "phrases -- CONJUNCTION -- rules"
    ],
    "abstract": "this paper proposes new algorithms to compute the <otherscientificterm_7> between two units -lrb- words , <otherscientificterm_10> , <otherscientificterm_8> , etc. -rrb- from <material_4> . the <metric_1> are computed by using the <method_3> . we then apply the algorithms to <task_2> by computing the <otherscientificterm_7> between the source and target side of <task_2> rule pairs . <metric_6> are used as additional <otherscientificterm_9> of the <method_5> to improve <task_2> performance . significant improvements are obtained over a state-of-the-art <method_0> .",
    "abstract_og": "this paper proposes new algorithms to compute the sense similarity between two units -lrb- words , phrases , rules , etc. -rrb- from parallel corpora . the sense similarity scores are computed by using the vector space model . we then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of statistical machine translation rule pairs . similarity scores are used as additional features of the translation model to improve statistical machine translation performance . significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system ."
  },
  {
    "title": "Corpus-based analysis of English spoken by Japanese students in view of the entire phonemic system of English .",
    "entities": [
      "hidden markov models",
      "japanese english",
      "phonemic system of english",
      "je database",
      "wsj database",
      "je pronunciation",
      "tree diagram",
      "english pronunciation",
      "speech recognition",
      "japanese students",
      "japanese",
      "english"
    ],
    "types": "<method> <task> <method> <material> <material> <task> <method> <material> <task> <material> <material> <material>",
    "relations": [
      "english -- CONJUNCTION -- japanese",
      "wsj database -- CONJUNCTION -- je database"
    ],
    "abstract": "english and <material_10> are quite different languages both phoneti-cally and linguistically and it is often very difficult for <material_9> to master <material_7> . to help students improve their pronunciation proficiency , a <material_10> national project of '' advanced utilization of multimedia for education '' has started in 2000 and under this project , a large database of <material_11> words and sentences read by 200 <material_9> was built mainly for call system development . this paper describes a corpus-based analysis and comparison of american <material_11> -lrb- ae -rrb- and <task_1> by using <material_4> and the new <material_3> . here , <method_0> , which are widely-used acoustic modeling techniques of <task_8> , were firstly made for individual phonemes in the two kinds of <material_11> , and then , a <method_6> was drawn for the entire phonemes of each hmm set . the analysis and comparison of the two trees showed many interesting characteristics of <task_1> , some of which are well-known habits observed in <task_5> . the authors consider that this study showed statistical differences between ae and <task_1> in view of the entire <method_2> for the first time .",
    "abstract_og": "english and japanese are quite different languages both phoneti-cally and linguistically and it is often very difficult for japanese students to master english pronunciation . to help students improve their pronunciation proficiency , a japanese national project of '' advanced utilization of multimedia for education '' has started in 2000 and under this project , a large database of english words and sentences read by 200 japanese students was built mainly for call system development . this paper describes a corpus-based analysis and comparison of american english -lrb- ae -rrb- and japanese english by using wsj database and the new je database . here , hidden markov models , which are widely-used acoustic modeling techniques of speech recognition , were firstly made for individual phonemes in the two kinds of english , and then , a tree diagram was drawn for the entire phonemes of each hmm set . the analysis and comparison of the two trees showed many interesting characteristics of japanese english , some of which are well-known habits observed in je pronunciation . the authors consider that this study showed statistical differences between ae and japanese english in view of the entire phonemic system of english for the first time ."
  },
  {
    "title": "Predicting Tongue Positions from Acoustics and Facial Features .",
    "entities": [
      "electromagnetic articulograph sensors",
      "jaw and lip information",
      "concatenating bimodal acoustic-visual units",
      "support vector regression",
      "acoustic-to-ema mapping system",
      "talking head",
      "tongue sensors",
      "tongue animation",
      "tongue information"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "support vector regression -- USED-FOR -- acoustic-to-ema mapping system",
      "acoustic-to-ema mapping system -- USED-FOR -- talking head",
      "electromagnetic articulograph sensors -- USED-FOR -- acoustic-to-ema mapping system",
      "tongue animation -- USED-FOR -- talking head",
      "acoustic-to-ema mapping system -- USED-FOR -- tongue animation",
      "jaw and lip information -- CONJUNCTION -- tongue information"
    ],
    "abstract": "we test the hypothesis that adding information regarding the positions of <otherscientificterm_0> on the lips and jaw can improve the results of a typical <method_4> , based on <method_3> , that targets the <otherscientificterm_6> . our initial motivation is to use such a <method_4> in the context of adding a <method_7> to a <otherscientificterm_5> built on the basis of <task_2> . for completeness , we also train a <method_4> that maps only <otherscientificterm_1> to <otherscientificterm_8> .",
    "abstract_og": "we test the hypothesis that adding information regarding the positions of electromagnetic articulograph sensors on the lips and jaw can improve the results of a typical acoustic-to-ema mapping system , based on support vector regression , that targets the tongue sensors . our initial motivation is to use such a acoustic-to-ema mapping system in the context of adding a tongue animation to a talking head built on the basis of concatenating bimodal acoustic-visual units . for completeness , we also train a acoustic-to-ema mapping system that maps only jaw and lip information to tongue information ."
  },
  {
    "title": "TRIC-track : Tracking by Regression with Incrementally Learned Cascades .",
    "entities": [
      "multiple temporal scale motion model",
      "shape and deformation parameters",
      "local image patches",
      "cvpr 2013 benchmark",
      "cascaded regression search",
      "part-based tracking",
      "generic objects",
      "appearance model",
      "part locations",
      "cascaded regression",
      "incremental learning",
      "online fashion",
      "spatial constraints",
      "occlusions",
      "appearance"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <material> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "multiple temporal scale motion model -- USED-FOR -- occlusions",
      "incremental learning -- USED-FOR -- cascaded regression",
      "cascaded regression -- USED-FOR -- generic objects",
      "incremental learning -- USED-FOR -- generic objects"
    ],
    "abstract": "this paper proposes a novel approach to <task_5> by replacing local matching of an <method_7> by direct prediction of the displacement between <otherscientificterm_2> and <otherscientificterm_8> . we propose to use <method_9> with <method_10> to track <otherscientificterm_6> without any prior knowledge of an object 's structure or <otherscientificterm_14> . we exploit the <otherscientificterm_12> between parts by implicitly learning the <otherscientificterm_1> of the object in an <otherscientificterm_11> . we integrate a <method_0> to initialise our <method_4> close to the target and to allow <method_0> to cope with <otherscientificterm_13> . experimental results show that our tracker ranks first on the <material_3> .",
    "abstract_og": "this paper proposes a novel approach to part-based tracking by replacing local matching of an appearance model by direct prediction of the displacement between local image patches and part locations . we propose to use cascaded regression with incremental learning to track generic objects without any prior knowledge of an object 's structure or appearance . we exploit the spatial constraints between parts by implicitly learning the shape and deformation parameters of the object in an online fashion . we integrate a multiple temporal scale motion model to initialise our cascaded regression search close to the target and to allow multiple temporal scale motion model to cope with occlusions . experimental results show that our tracker ranks first on the cvpr 2013 benchmark ."
  },
  {
    "title": "On the application of reverberation suppression to robust speech recognition .",
    "entities": [
      "single-channel reverberation suppression algorithm",
      "moderately reverber-ant data",
      "reverberation-robust speech recognition",
      "acoustic models",
      "inferred guidelines",
      "early reflections",
      "residual late-reverberation",
      "reverberation tail",
      "training data",
      "reverberation compensation",
      "reverberation conditions",
      "speech recognizer",
      "design parameters",
      "optimum configuration",
      "recognition"
    ],
    "types": "<method> <material> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "reverberation-robust speech recognition -- USED-FOR -- single-channel reverberation suppression algorithm",
      "reverberation compensation -- USED-FOR -- speech recognizer",
      "moderately reverber-ant data -- USED-FOR -- speech recognizer"
    ],
    "abstract": "in this paper , we study the effect of the <otherscientificterm_12> of a <method_0> on <task_2> . at the same time , <method_9> at the <method_11> is investigated . the analysis reveals that it is highly beneficial to attenuate only the <otherscientificterm_7> after approximately 50 ms while coping with the <otherscientificterm_5> and <otherscientificterm_6> by training the <method_11> on <material_1> . it will be shown that the overall system at its <otherscientificterm_13> yields a very promising <task_14> performance even in strongly reverberant environments . since the <method_0> is evidenced to significantly reduce the dependency on the <material_8> , it allows for a very efficient training of <method_3> that are suitable for a wide range of <otherscientificterm_10> . finally , experiments with an '' ideal '' <method_0> are carried out to cross-check the <otherscientificterm_4> .",
    "abstract_og": "in this paper , we study the effect of the design parameters of a single-channel reverberation suppression algorithm on reverberation-robust speech recognition . at the same time , reverberation compensation at the speech recognizer is investigated . the analysis reveals that it is highly beneficial to attenuate only the reverberation tail after approximately 50 ms while coping with the early reflections and residual late-reverberation by training the speech recognizer on moderately reverber-ant data . it will be shown that the overall system at its optimum configuration yields a very promising recognition performance even in strongly reverberant environments . since the single-channel reverberation suppression algorithm is evidenced to significantly reduce the dependency on the training data , it allows for a very efficient training of acoustic models that are suitable for a wide range of reverberation conditions . finally , experiments with an '' ideal '' single-channel reverberation suppression algorithm are carried out to cross-check the inferred guidelines ."
  },
  {
    "title": "A New Tractable Subclass of the Rectangle Algebra .",
    "entities": [
      "polynomial path-consistency algorithm",
      "2-dimensional euclidean space",
      "orthogonal basis",
      "decision method",
      "interval algebra",
      "propagation techniques",
      "rectangle algebra",
      "weak preconvexity",
      "interval networks",
      "inversion",
      "convexity",
      "intersection"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "propagation techniques -- USED-FOR -- interval networks",
      "intersection -- CONJUNCTION -- inversion"
    ],
    "abstract": "this paper presents the 169 permitted relations between two rectangles whose sides are parallel to the axes of some <otherscientificterm_2> in a <otherscientificterm_1> . elaborating <otherscientificterm_6> just like <method_4> , it deenes the concept of <otherscientificterm_10> as well as the ones of <otherscientificterm_7> and strong precon-vexity . it introduces afterwards the fundamental operations of <otherscientificterm_11> , composition and <otherscientificterm_9> and demonstrates that the concept of <otherscientificterm_7> is preserved by the operation of composition whereas the concept of strong preconvexity is preserved by the operation of <otherscientificterm_11> . finally , tting the <method_5> conceived to solve <method_8> , it shows that the <method_0> is a <method_3> for the problem of proving the consistency of strongly precon-vex rectangle networks .",
    "abstract_og": "this paper presents the 169 permitted relations between two rectangles whose sides are parallel to the axes of some orthogonal basis in a 2-dimensional euclidean space . elaborating rectangle algebra just like interval algebra , it deenes the concept of convexity as well as the ones of weak preconvexity and strong precon-vexity . it introduces afterwards the fundamental operations of intersection , composition and inversion and demonstrates that the concept of weak preconvexity is preserved by the operation of composition whereas the concept of strong preconvexity is preserved by the operation of intersection . finally , tting the propagation techniques conceived to solve interval networks , it shows that the polynomial path-consistency algorithm is a decision method for the problem of proving the consistency of strongly precon-vex rectangle networks ."
  },
  {
    "title": "Long-term Causal Effects via Behavioral Game Theory .",
    "entities": [
      "ad pricing policy",
      "latent space approach",
      "behavioral game theory",
      "behavioral model",
      "policy changes",
      "temporal model",
      "multiagent economies",
      "causal inference",
      "latent behaviors",
      "adaptation period",
      "baseline policy",
      "ignorability assumptions",
      "classical methodology"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "ignorability assumptions -- USED-FOR -- causal inference",
      "adaptation period -- FEATURE-OF -- ad pricing policy",
      "behavioral model -- USED-FOR -- latent behaviors"
    ],
    "abstract": "planned experiments are the gold standard in reliably comparing the causal effect of switching from a <otherscientificterm_10> to a new <otherscientificterm_0> . one critical shortcoming of classical experimental methods , however , is that they typically do not take into account the dynamic nature of response to <otherscientificterm_4> . for instance , in an experiment where we seek to understand the effects of a new <otherscientificterm_0> on auction revenue , agents may adapt their bidding in response to the experimental pricing changes . thus , causal effects of the new <otherscientificterm_0> after such <otherscientificterm_9> , the long-term causal effects , are not captured by the <method_12> even though they clearly are more indicative of the value of the new <otherscientificterm_0> . here , we formalize a framework to define and estimate long-term causal effects of <otherscientificterm_4> in <otherscientificterm_6> . central to our approach is <method_2> , which we leverage to formulate the <otherscientificterm_11> that are necessary for <task_7> . under such <otherscientificterm_11> we estimate long-term causal effects through a <method_1> , where a <method_3> of how agents act conditional on their <otherscientificterm_8> is combined with a <method_5> of how behaviors evolve over time .",
    "abstract_og": "planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new ad pricing policy . one critical shortcoming of classical experimental methods , however , is that they typically do not take into account the dynamic nature of response to policy changes . for instance , in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue , agents may adapt their bidding in response to the experimental pricing changes . thus , causal effects of the new ad pricing policy after such adaptation period , the long-term causal effects , are not captured by the classical methodology even though they clearly are more indicative of the value of the new ad pricing policy . here , we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies . central to our approach is behavioral game theory , which we leverage to formulate the ignorability assumptions that are necessary for causal inference . under such ignorability assumptions we estimate long-term causal effects through a latent space approach , where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time ."
  },
  {
    "title": "Online Graph Pruning for Pathfinding On Grid Maps .",
    "entities": [
      "hierarchical pathfinding algorithms",
      "uniform-cost grid environments",
      "small memory overheads",
      "memory overhead",
      "suboptimal paths",
      "macro operator",
      "intermediate nodes",
      "grid map",
      "search strategy",
      "video games",
      "nodes",
      "robotics",
      "grids"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "search strategy -- USED-FOR -- grids",
      "robotics -- CONJUNCTION -- video games",
      "nodes -- PART-OF -- grid map"
    ],
    "abstract": "pathfinding in <otherscientificterm_1> is a problem commonly found in application areas such as <material_11> and <material_9> . the state-of-the-art is dominated by <method_0> which are fast and have <otherscientificterm_2> but usually return <otherscientificterm_4> . in this paper we present a novel <method_8> , specific to <otherscientificterm_12> , which is fast , optimal and requires no <otherscientificterm_3> . our <method_8> can be described as a <method_5> which identifies and selectively expands only certain <otherscientificterm_10> in a <otherscientificterm_7> which we call jump points . <otherscientificterm_6> on a path connecting two jump points are never expanded . we prove that this <method_8> always computes optimal solutions and then undertake a thorough empirical analysis , comparing our <method_8> with related works from the literature . we find that searching with jump points can speed up a * by an order of magnitude and more and report significant improvement over the current state of the art .",
    "abstract_og": "pathfinding in uniform-cost grid environments is a problem commonly found in application areas such as robotics and video games . the state-of-the-art is dominated by hierarchical pathfinding algorithms which are fast and have small memory overheads but usually return suboptimal paths . in this paper we present a novel search strategy , specific to grids , which is fast , optimal and requires no memory overhead . our search strategy can be described as a macro operator which identifies and selectively expands only certain nodes in a grid map which we call jump points . intermediate nodes on a path connecting two jump points are never expanded . we prove that this search strategy always computes optimal solutions and then undertake a thorough empirical analysis , comparing our search strategy with related works from the literature . we find that searching with jump points can speed up a * by an order of magnitude and more and report significant improvement over the current state of the art ."
  },
  {
    "title": "Lower Bounds on Rate of Convergence of Cutting Plane Methods .",
    "entities": [
      "o -lrb- 1 / \u221a -rrb- iterations",
      "cutting plane method",
      "support vector methods",
      "multivari-ate performance score",
      "linear svms",
      "objective function",
      "hinge loss",
      "counter examples",
      "svm-perf"
    ],
    "types": "<otherscientificterm> <method> <method> <metric> <method> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "counter examples -- USED-FOR -- linear svms",
      "multivari-ate performance score -- EVALUATE-FOR -- support vector methods",
      "svm-perf -- HYPONYM-OF -- cutting plane method"
    ],
    "abstract": "in a recent paper joachims -lsb- 1 -rsb- presented <method_8> , a <method_1> for training linear support vector machines -lrb- svms -rrb- which converges to an accurate solution in o -lrb- 1 / / 2 -rrb- iterations . by tightening the analysis , teo et al. -lsb- 2 -rsb- showed that o -lrb- 1 / / -rrb- iterations suffice . given the impressive convergence speed of <method_1> on a number of practical problems , it was conjectured that these rates could be further improved . in this paper we disprove this conjecture . we present <material_7> which are not only applicable for training <method_4> with <otherscientificterm_6> , but also hold for <method_2> which optimize a <metric_3> . however , surprisingly , these problems are not inherently hard . by exploiting the structure of the <otherscientificterm_5> we can devise an algorithm that converges in <otherscientificterm_0> .",
    "abstract_og": "in a recent paper joachims -lsb- 1 -rsb- presented svm-perf , a cutting plane method for training linear support vector machines -lrb- svms -rrb- which converges to an accurate solution in o -lrb- 1 / / 2 -rrb- iterations . by tightening the analysis , teo et al. -lsb- 2 -rsb- showed that o -lrb- 1 / / -rrb- iterations suffice . given the impressive convergence speed of cutting plane method on a number of practical problems , it was conjectured that these rates could be further improved . in this paper we disprove this conjecture . we present counter examples which are not only applicable for training linear svms with hinge loss , but also hold for support vector methods which optimize a multivari-ate performance score . however , surprisingly , these problems are not inherently hard . by exploiting the structure of the objective function we can devise an algorithm that converges in o -lrb- 1 / \u221a -rrb- iterations ."
  },
  {
    "title": "Modeling human interaction in meetings .",
    "entities": [
      "recognition of group actions",
      "joint behaviour of participants",
      "audio and visual features",
      "corpus of meetings",
      "hmm-based approaches",
      "raw data",
      "global behaviour",
      "meeting actions",
      "discussions",
      "browsing"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <material> <method> <material> <otherscientificterm> <task> <otherscientificterm> <task>",
    "relations": [
      "discussions -- HYPONYM-OF -- meeting actions",
      "raw data -- USED-FOR -- audio and visual features"
    ],
    "abstract": "this paper investigates the <task_0> in meetings by modeling the <otherscientificterm_1> . many <task_7> , such as presentations , <otherscientificterm_8> and consensus , are characterised by similar or complementary behaviour across participants . recognising these meaningful actions is an important step towards the goal of providing effective <task_9> and sum-marisation of processed meetings . in this work , a <material_3> was collected in a room equipped with a number of microphones and cameras . the <material_3> was labeled in terms of a pre-defined set of <task_7> characterised by <otherscientificterm_6> . in experiments , <otherscientificterm_2> for each participant are extracted from the <material_5> and the interaction of participants is modeled using <method_4> . initial results on the <material_3> demonstrate the ability of the system to recognise the set of <task_7> .",
    "abstract_og": "this paper investigates the recognition of group actions in meetings by modeling the joint behaviour of participants . many meeting actions , such as presentations , discussions and consensus , are characterised by similar or complementary behaviour across participants . recognising these meaningful actions is an important step towards the goal of providing effective browsing and sum-marisation of processed meetings . in this work , a corpus of meetings was collected in a room equipped with a number of microphones and cameras . the corpus of meetings was labeled in terms of a pre-defined set of meeting actions characterised by global behaviour . in experiments , audio and visual features for each participant are extracted from the raw data and the interaction of participants is modeled using hmm-based approaches . initial results on the corpus of meetings demonstrate the ability of the system to recognise the set of meeting actions ."
  },
  {
    "title": "Real-Time Pose Estimation Piggybacked on Object Detection .",
    "entities": [
      "compact and simple model",
      "traffic surveillance dataset",
      "pose estimation",
      "vehicle detection",
      "object detector",
      "3d objects",
      "c++ code",
      "cod20k dataset",
      "image features",
      "pose estimator",
      "viewpoint/pose annotation",
      "object score",
      "classification",
      "kitti",
      "detection",
      "accuracy"
    ],
    "types": "<method> <material> <task> <task> <method> <material> <otherscientificterm> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <method> <task> <metric>",
    "relations": [
      "vehicle detection -- EVALUATE-FOR -- object detector",
      "pose estimation -- USED-FOR -- object detector",
      "accuracy -- EVALUATE-FOR -- object detector",
      "object detector -- USED-FOR -- vehicle detection"
    ],
    "abstract": "we present an <method_4> coupled with <task_2> directly in a single <method_0> , where the <method_4> shares extracted <otherscientificterm_8> with the <method_9> . the output of the <task_12> of each candidate window consists of both <otherscientificterm_11> and likelihood map of poses . this <method_4> introduces negligible overhead during <task_14> so that the <method_4> is still capable of real time operation . we evaluated the proposed <method_4> on the problem of <task_3> . we used existing datasets with <otherscientificterm_10> -lrb- wcvp , <material_5> , <method_13> -rrb- . besides that , we collected a new <material_1> cod20k which fills certain gaps of the existing datasets and we make <method_4> public . the experimental results show that the proposed <method_4> is comparable with state-of-the-art approaches in terms of <metric_15> , but <method_4> is considerably faster -- easily operating in real time -lrb- matlab with <otherscientificterm_6> -rrb- . the source codes and the collected <material_7> are made public along with the paper .",
    "abstract_og": "we present an object detector coupled with pose estimation directly in a single compact and simple model , where the object detector shares extracted image features with the pose estimator . the output of the classification of each candidate window consists of both object score and likelihood map of poses . this object detector introduces negligible overhead during detection so that the object detector is still capable of real time operation . we evaluated the proposed object detector on the problem of vehicle detection . we used existing datasets with viewpoint/pose annotation -lrb- wcvp , 3d objects , kitti -rrb- . besides that , we collected a new traffic surveillance dataset cod20k which fills certain gaps of the existing datasets and we make object detector public . the experimental results show that the proposed object detector is comparable with state-of-the-art approaches in terms of accuracy , but object detector is considerably faster -- easily operating in real time -lrb- matlab with c++ code -rrb- . the source codes and the collected cod20k dataset are made public along with the paper ."
  },
  {
    "title": "Using semantic class information for rapid development of language models within ASR dialogue systems .",
    "entities": [
      "atis and darpa communicator travel corpora",
      "language model training data",
      "probabilistic context free grammar",
      "natural language understanding module",
      "context free grammar",
      "dialogue system developers",
      "finite state parses",
      "semantic classes",
      "artificial data"
    ],
    "types": "<material> <material> <method> <method> <method> <method> <otherscientificterm> <method> <material>",
    "relations": [
      "semantic classes -- USED-FOR -- natural language understanding module",
      "finite state parses -- USED-FOR -- artificial data",
      "finite state parses -- USED-FOR -- probabilistic context free grammar"
    ],
    "abstract": "when <method_5> tackle a new domain , much effort is required ; the development of different parts of the system usually proceeds independently . yet it may be profitable to coordinate development efforts between different modules . here , we focus our efforts on extending small amounts of <material_1> by integrating <method_7> that were created for a <method_3> . by converting <otherscientificterm_6> of a training corpus into a <method_2> and subsequently generating <material_8> from the <method_4> , we can significantly reduce perplexity and asr word error for situations with little training data . experiments are presented using data from the <material_0> .",
    "abstract_og": "when dialogue system developers tackle a new domain , much effort is required ; the development of different parts of the system usually proceeds independently . yet it may be profitable to coordinate development efforts between different modules . here , we focus our efforts on extending small amounts of language model training data by integrating semantic classes that were created for a natural language understanding module . by converting finite state parses of a training corpus into a probabilistic context free grammar and subsequently generating artificial data from the context free grammar , we can significantly reduce perplexity and asr word error for situations with little training data . experiments are presented using data from the atis and darpa communicator travel corpora ."
  },
  {
    "title": "Density geodesics for similarity clustering .",
    "entities": [
      "riemannian geometry of curved surfaces",
      "algebraic context-independent sample-distance measures",
      "path-bottleneck or path-average distances",
      "principled context-adaptive similarity metric",
      "unit-hypercube uniform density",
      "density-geodesic distance measure",
      "context-dependent metric modifications",
      "similarity metric selection",
      "pairwise affinity clustering",
      "canonical metric",
      "euclidean distance",
      "bottleneck principle",
      "feature vectors",
      "geodesics",
      "robustness"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <task> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "bottleneck principle -- USED-FOR -- context-dependent metric modifications",
      "context-dependent metric modifications -- USED-FOR -- path-bottleneck or path-average distances",
      "similarity metric selection -- USED-FOR -- pairwise affinity clustering",
      "euclidean distance -- HYPONYM-OF -- algebraic context-independent sample-distance measures",
      "riemannian geometry of curved surfaces -- USED-FOR -- density-geodesic distance measure",
      "unit-hypercube uniform density -- USED-FOR -- canonical metric"
    ],
    "abstract": "we address the problem of <task_7> in <task_8> . traditional techniques employ standard <method_1> , such as the <otherscientificterm_10> . more recent <method_6> employ the <method_11> to develop <otherscientificterm_2> and define similarities based on <otherscientificterm_13> determined according to these metrics . this paper develops a <method_3> for pairs of <otherscientificterm_12> utilizing the probability density of all data . specifically , based on the postulate that <otherscientificterm_10> is the <method_9> for data drawn from a <otherscientificterm_4> , a <metric_5> stemming from <otherscientificterm_0> is derived . comparisons with alternative metrics demonstrate the superior properties such as <metric_14> .",
    "abstract_og": "we address the problem of similarity metric selection in pairwise affinity clustering . traditional techniques employ standard algebraic context-independent sample-distance measures , such as the euclidean distance . more recent context-dependent metric modifications employ the bottleneck principle to develop path-bottleneck or path-average distances and define similarities based on geodesics determined according to these metrics . this paper develops a principled context-adaptive similarity metric for pairs of feature vectors utilizing the probability density of all data . specifically , based on the postulate that euclidean distance is the canonical metric for data drawn from a unit-hypercube uniform density , a density-geodesic distance measure stemming from riemannian geometry of curved surfaces is derived . comparisons with alternative metrics demonstrate the superior properties such as robustness ."
  },
  {
    "title": "Efficient Searching Top-k Semantic Similar Words .",
    "entities": [
      "top-k semantic similar words",
      "semantic meaning between words",
      "word sense disambiguation",
      "document summarization",
      "efficient strategies",
      "precision",
      "recall"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <task> <method> <metric> <metric>",
    "relations": [
      "word sense disambiguation -- CONJUNCTION -- document summarization",
      "precision -- CONJUNCTION -- recall"
    ],
    "abstract": "measuring the <otherscientificterm_1> is an important issue because it is the basis for many applications , such as <task_2> , <task_3> , and so forth . although it has been explored for several decades , most of the studies focus on improving the effectiveness of the problem , i.e. , <metric_5> and <metric_6> . in this paper , we propose to address the efficiency issue , that given a collection of words , how to efficiently discover the top-k most semantic similar words to the query . this issue is very important for real applications yet the existing state-of-the-art strategies can not satisfy users with reasonable performance . <method_4> on searching <otherscientificterm_0> are proposed . we provide an extensive comparative experimental evaluation demonstrating the advantages of the introduced strategies over the state-of-the-art approaches .",
    "abstract_og": "measuring the semantic meaning between words is an important issue because it is the basis for many applications , such as word sense disambiguation , document summarization , and so forth . although it has been explored for several decades , most of the studies focus on improving the effectiveness of the problem , i.e. , precision and recall . in this paper , we propose to address the efficiency issue , that given a collection of words , how to efficiently discover the top-k most semantic similar words to the query . this issue is very important for real applications yet the existing state-of-the-art strategies can not satisfy users with reasonable performance . efficient strategies on searching top-k semantic similar words are proposed . we provide an extensive comparative experimental evaluation demonstrating the advantages of the introduced strategies over the state-of-the-art approaches ."
  },
  {
    "title": "Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation .",
    "entities": [
      "unlabeled target domain",
      "unsupervised domain adaptation",
      "labeled source domain",
      "information-theoretic metric",
      "gradient-based methods",
      "misclassification error",
      "sentiment analysis",
      "object recognition",
      "feature space",
      "classification accuracies",
      "domain-invariant features",
      "classifiers",
      "classi-fiers",
      "hyperparameters"
    ],
    "types": "<material> <task> <material> <otherscientificterm> <method> <otherscientificterm> <task> <task> <otherscientificterm> <task> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "labeled source domain -- USED-FOR -- unsupervised domain adaptation",
      "unsupervised domain adaptation -- USED-FOR -- classi-fiers",
      "labeled source domain -- USED-FOR -- unlabeled target domain",
      "labeled source domain -- USED-FOR -- classi-fiers"
    ],
    "abstract": "we study the problem of <task_1> , which aims to adapt <method_12> trained on a <material_2> to an <material_0> . many existing approaches first learn <otherscientificterm_10> and then construct <method_11> with them . we propose a novel approach that jointly learn the both . specifically , while the method identifies a <otherscientificterm_8> where data in the source and the target domains are similarly distributed , it also learns the <otherscientificterm_8> discriminatively , optimizing an <otherscientificterm_3> as an proxy to the expected <otherscientificterm_5> on the target domain . we show how this optimization can be effectively carried out with simple <method_4> and how <method_13> can be cross-validated without demanding any labeled data from the target domain . empirical studies on benchmark tasks of <task_7> and <task_6> validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in <task_9> .",
    "abstract_og": "we study the problem of unsupervised domain adaptation , which aims to adapt classi-fiers trained on a labeled source domain to an unlabeled target domain . many existing approaches first learn domain-invariant features and then construct classifiers with them . we propose a novel approach that jointly learn the both . specifically , while the method identifies a feature space where data in the source and the target domains are similarly distributed , it also learns the feature space discriminatively , optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain . we show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain . empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies ."
  },
  {
    "title": "Prior Knowledge in Support Vector Kernels .",
    "entities": [
      "support vector learning machines",
      "group transfonnations",
      "prior knowledge",
      "kernel functions",
      "locality",
      "images",
      "invari-ances"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "group transfonnations -- CONJUNCTION -- prior knowledge"
    ],
    "abstract": "we explore methods for incorporating <otherscientificterm_2> about a problem at hand in <task_0> . we show that both <otherscientificterm_6> under <otherscientificterm_1> and <otherscientificterm_2> about <otherscientificterm_4> in <material_5> can be incorporated by constructing appropriate <method_3> .",
    "abstract_og": "we explore methods for incorporating prior knowledge about a problem at hand in support vector learning machines . we show that both invari-ances under group transfonnations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions ."
  },
  {
    "title": "A Methodology for Evaluating Robustness of Face Recognition Algorithms with Respect to Variations in Pose Angle and Illumination Angle .",
    "entities": [
      "bayesian intra-personal classifier -rrb-",
      "linear discriminant analysis -rrb-",
      "principle component analysis",
      "subspace analysis methods",
      "face recognition algorithms",
      "probabilistic learning methods",
      "pose angle",
      "pose angles",
      "robustness measure",
      "illumination angle",
      "subspace methods",
      "recognition robustness",
      "illumination angles",
      "face images",
      "pose"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method> <metric> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "subspace analysis methods -- CONJUNCTION -- probabilistic learning methods",
      "linear discriminant analysis -rrb- -- HYPONYM-OF -- subspace analysis methods",
      "principle component analysis -- CONJUNCTION -- linear discriminant analysis -rrb-",
      "principle component analysis -- HYPONYM-OF -- subspace analysis methods",
      "linear discriminant analysis -rrb- -- CONJUNCTION -- probabilistic learning methods",
      "subspace methods -- COMPARE -- probabilistic learning methods",
      "principle component analysis -- CONJUNCTION -- probabilistic learning methods",
      "linear discriminant analysis -rrb- -- CONJUNCTION -- bayesian intra-personal classifier -rrb-"
    ],
    "abstract": "in this paper , we present a methodology for precisely comparing the robustness of <method_4> with respect to changes in <otherscientificterm_6> and <otherscientificterm_9> . for this study , we have chosen four widely-used algorithms : two <method_3> -lrb- <method_2> and <method_1> and two <method_5> -lrb- hidden markov models -lrb- hmm -rrb- and <method_0> . we compare the <metric_11> of these algorithms using a novel database -lrb- facepix -rrb- that captures <material_13> with a wide range of <otherscientificterm_7> and <otherscientificterm_12> . we propose a method for deriving a <metric_8> for each of these algorithms , with respect to <otherscientificterm_14> and <otherscientificterm_9> changes . the results of this comparison indicate that the <method_10> perform more robustly than the <method_5> in the presence of <otherscientificterm_14> and <otherscientificterm_9> changes .",
    "abstract_og": "in this paper , we present a methodology for precisely comparing the robustness of face recognition algorithms with respect to changes in pose angle and illumination angle . for this study , we have chosen four widely-used algorithms : two subspace analysis methods -lrb- principle component analysis and linear discriminant analysis -rrb- and two probabilistic learning methods -lrb- hidden markov models -lrb- hmm -rrb- and bayesian intra-personal classifier -rrb- . we compare the recognition robustness of these algorithms using a novel database -lrb- facepix -rrb- that captures face images with a wide range of pose angles and illumination angles . we propose a method for deriving a robustness measure for each of these algorithms , with respect to pose and illumination angle changes . the results of this comparison indicate that the subspace methods perform more robustly than the probabilistic learning methods in the presence of pose and illumination angle changes ."
  },
  {
    "title": "Localization using combinations of model views .",
    "entities": [
      "weak perspective projection",
      "weak perspective approximation",
      "iterative solution",
      "model views",
      "perspective distortions",
      "2d view",
      "localization",
      "localiza-tion"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method>",
    "relations": [
      "iterative solution -- USED-FOR -- perspective distortions"
    ],
    "abstract": "a method for <task_6> , the act of recognizing the environment , is presented . the method is based on representing the scene as a set of 2 0 views and predicting the appearances of novel views by linear combinations of the <otherscientificterm_3> . the method accurately approximates the appearance of scenes under <otherscientificterm_0> . analysis of this projection as well as experimental results demonstrate that in many cases this approximation is suficient t o accurately describe the scene . when <otherscientificterm_1> is invalid , either a larger number of models can be acquired or an <method_2> t o account for the <otherscientificterm_4> can be employed . the method has several advantages over other approaches . it uses relatively rich representations ; the representations are 2d rather than 9d ; and <method_7> can be done f r o m only a single <otherscientificterm_5> .",
    "abstract_og": "a method for localization , the act of recognizing the environment , is presented . the method is based on representing the scene as a set of 2 0 views and predicting the appearances of novel views by linear combinations of the model views . the method accurately approximates the appearance of scenes under weak perspective projection . analysis of this projection as well as experimental results demonstrate that in many cases this approximation is suficient t o accurately describe the scene . when weak perspective approximation is invalid , either a larger number of models can be acquired or an iterative solution t o account for the perspective distortions can be employed . the method has several advantages over other approaches . it uses relatively rich representations ; the representations are 2d rather than 9d ; and localiza-tion can be done f r o m only a single 2d view ."
  },
  {
    "title": "The Synthy Approach for End to End Web Services Composition : Planning with Decoupled Causal and Resource Reasoning .",
    "entities": [
      "functional and non-functional requirements",
      "synthy service composition system",
      "composing web services end",
      "staged composition approach",
      "web services composition",
      "semantic web services",
      "business web services",
      "viewing software components",
      "web services",
      "application integration",
      "web services",
      "goal-directed reasoning",
      "telecom domain",
      "ai planning",
      "contextual information",
      "composing services",
      "composition scenarios",
      "ontologies",
      "soap",
      "wsdl",
      "ai"
    ],
    "types": "<otherscientificterm> <method> <task> <method> <task> <method> <method> <method> <method> <task> <method> <method> <material> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task>",
    "relations": [
      "ontologies -- CONJUNCTION -- goal-directed reasoning",
      "viewing software components -- USED-FOR -- web services composition",
      "business web services -- USED-FOR -- web services composition",
      "telecom domain -- USED-FOR -- composition scenarios",
      "business web services -- USED-FOR -- viewing software components",
      "wsdl -- CONJUNCTION -- goal-directed reasoning",
      "staged composition approach -- USED-FOR -- contextual information",
      "web services -- USED-FOR -- application integration",
      "soap -- CONJUNCTION -- semantic web services"
    ],
    "abstract": "web services offer a unique opportunity to simplify <task_9> by defining common , web-based , platform-neutral , standards for publishing service descriptions to a registry , finding and invoking them -- not necessarily by the same parties . <method_7> as <method_8> , the current solutions to <task_4> based on <method_6> -lrb- using <method_19> , bpel , <method_18> etc. -rrb- or <method_5> -lrb- using <otherscientificterm_17> , <method_11> etc. -rrb- are both piecemeal and insufficient for building practical applications . inspired by the work in <task_13> on decoupling causal -lrb- planning -rrb- and resource reasoning -lrb- scheduling -rrb- , we introduced the first integrated work in <task_2> to end from specification to deployment by synergistically combining the strengths of the current approaches . the solution is based on a novel two -- <method_3> that addresses the information model-ing aspects of <method_8> , provides support for <otherscientificterm_14> while <task_15> , employs efficient de-coupling of <otherscientificterm_0> , and leads to improved scalability and failure handling . a prototype of the solution has been implemented in the <method_1> and applied to a number of <otherscientificterm_16> from the <material_12> . the application of planning to <method_8> has also brought new plan and planner usability-driven research issues to the fore for <task_20> .",
    "abstract_og": "web services offer a unique opportunity to simplify application integration by defining common , web-based , platform-neutral , standards for publishing service descriptions to a registry , finding and invoking them -- not necessarily by the same parties . viewing software components as web services , the current solutions to web services composition based on business web services -lrb- using wsdl , bpel , soap etc. -rrb- or semantic web services -lrb- using ontologies , goal-directed reasoning etc. -rrb- are both piecemeal and insufficient for building practical applications . inspired by the work in ai planning on decoupling causal -lrb- planning -rrb- and resource reasoning -lrb- scheduling -rrb- , we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches . the solution is based on a novel two -- staged composition approach that addresses the information model-ing aspects of web services , provides support for contextual information while composing services , employs efficient de-coupling of functional and non-functional requirements , and leads to improved scalability and failure handling . a prototype of the solution has been implemented in the synthy service composition system and applied to a number of composition scenarios from the telecom domain . the application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for ai ."
  },
  {
    "title": "The Local Projective Shape of Smooth Surfaces and their Outlines .",
    "entities": [
      "relative orientation of rim tangents",
      "oriented projective differential geometry",
      "inflections of apparent contours",
      "local shape of surfaces",
      "theoretical framework",
      "convexities",
      "concavities"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "concavities -- CONJUNCTION -- inflections of apparent contours"
    ],
    "abstract": "this paper examines projectively invariant local properties of smooth curves and surfaces . <method_1> is proposed as a <method_4> for establishing such invariants and describing the <otherscientificterm_3> and their outlines . this <method_4> is applied to two problems : a projective proof of koenderink 's famous characterization of <otherscientificterm_5> , <otherscientificterm_6> , and <otherscientificterm_2> ; and the determination of the <otherscientificterm_0> at frontier points .",
    "abstract_og": "this paper examines projectively invariant local properties of smooth curves and surfaces . oriented projective differential geometry is proposed as a theoretical framework for establishing such invariants and describing the local shape of surfaces and their outlines . this theoretical framework is applied to two problems : a projective proof of koenderink 's famous characterization of convexities , concavities , and inflections of apparent contours ; and the determination of the relative orientation of rim tangents at frontier points ."
  },
  {
    "title": "Fast and Accurate k-means For Large Datasets .",
    "entities": [
      "approximate nearest neighbor search",
      "memory requirements",
      "running time",
      "k-means problem",
      "approximation guarantee",
      "k-means",
      "algorithms-both",
      "clustering",
      "disk"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "memory requirements -- CONJUNCTION -- running time",
      "approximate nearest neighbor search -- USED-FOR -- k-means"
    ],
    "abstract": "clustering is a popular problem with many applications . we consider the <task_3> in the situation where the data is too large to be stored in main memory and must be accessed sequentially , such as from a <otherscientificterm_8> , and where we must use as little memory as possible . our algorithm is based on recent theoretical results , with significant improvements to make it practical . our approach greatly simplifies a recently developed algorithm , both in design and in analysis , and eliminates large constant factors in the <otherscientificterm_4> , the <otherscientificterm_1> , and the <otherscientificterm_2> . we then incorporate <method_0> to compute <method_5> in o -lrb- nk -rrb- -lrb- where n is the number of data points ; note that computing the cost , given a solution , takes \u03b8 -lrb- nk -rrb- time -rrb- . we show that our algorithm compares favorably to existing <method_6> theoretically and experimentally , thus providing state-of-the-art performance in both theory and practice .",
    "abstract_og": "clustering is a popular problem with many applications . we consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially , such as from a disk , and where we must use as little memory as possible . our algorithm is based on recent theoretical results , with significant improvements to make it practical . our approach greatly simplifies a recently developed algorithm , both in design and in analysis , and eliminates large constant factors in the approximation guarantee , the memory requirements , and the running time . we then incorporate approximate nearest neighbor search to compute k-means in o -lrb- nk -rrb- -lrb- where n is the number of data points ; note that computing the cost , given a solution , takes \u03b8 -lrb- nk -rrb- time -rrb- . we show that our algorithm compares favorably to existing algorithms-both theoretically and experimentally , thus providing state-of-the-art performance in both theory and practice ."
  },
  {
    "title": "Rapid Object Recognition from Discriminative Regions of Interest .",
    "entities": [
      "discriminative power of local appearance patterns",
      "object representation and recognition",
      "cognitive computer vision systems",
      "intelligent video surveillance systems",
      "gaus-sian image noise",
      "discriminative local patterns",
      "partial object occlusions",
      "posterior en-tropy measure",
      "local information content",
      "object recognition",
      "robot vision",
      "recognition system",
      "background clutter",
      "object models",
      "object identification",
      "detection tasks",
      "multi-modal interfaces",
      "object views",
      "discriminative regions",
      "robust recognition",
      "partial occlusion",
      "scale variation",
      "local information",
      "recognition",
      "noise"
    ],
    "types": "<otherscientificterm> <task> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <task> <task> <method> <otherscientificterm> <method> <task> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "intelligent video surveillance systems -- HYPONYM-OF -- cognitive computer vision systems",
      "partial object occlusions -- CONJUNCTION -- scale variation",
      "local information -- USED-FOR -- object identification",
      "partial occlusion -- EVALUATE-FOR -- recognition system",
      "partial object occlusions -- CONJUNCTION -- noise",
      "partial occlusion -- CONJUNCTION -- gaus-sian image noise",
      "robot vision -- CONJUNCTION -- intelligent video surveillance systems",
      "scale variation -- CONJUNCTION -- noise",
      "intelligent video surveillance systems -- CONJUNCTION -- multi-modal interfaces",
      "noise -- CONJUNCTION -- background clutter",
      "posterior en-tropy measure -- USED-FOR -- discriminative regions",
      "object recognition -- HYPONYM-OF -- cognitive computer vision systems",
      "discriminative local patterns -- USED-FOR -- object models",
      "background clutter -- CONJUNCTION -- detection tasks",
      "multi-modal interfaces -- HYPONYM-OF -- cognitive computer vision systems",
      "local information content -- USED-FOR -- object representation and recognition",
      "robot vision -- HYPONYM-OF -- cognitive computer vision systems",
      "object identification -- USED-FOR -- robust recognition"
    ],
    "abstract": "object <task_23> and detection represent a relevant component in <method_2> , such as in <task_10> , <task_3> , or <task_16> . <task_14> from <otherscientificterm_22> has recently been investigated with respect to its potential for <task_19> , e.g. , in case of <otherscientificterm_6> , <otherscientificterm_21> , <otherscientificterm_24> , and <otherscientificterm_12> in <task_15> . this work contributes to this research by a thorough analysis of the <otherscientificterm_0> and by proposing to exploit <otherscientificterm_8> to model <task_1> . we identify <otherscientificterm_18> in the <otherscientificterm_17> from a <metric_7> , and then derive <method_13> from selected <otherscientificterm_5> . for <task_23> , we determine rapid attentive search for locations of high information content from learned decision trees . the <method_11> is evaluated by various degrees of <otherscientificterm_20> and <otherscientificterm_4> , resulting in highly <task_19> even in the presence of severe occlusion effects .",
    "abstract_og": "object recognition and detection represent a relevant component in cognitive computer vision systems , such as in robot vision , intelligent video surveillance systems , or multi-modal interfaces . object identification from local information has recently been investigated with respect to its potential for robust recognition , e.g. , in case of partial object occlusions , scale variation , noise , and background clutter in detection tasks . this work contributes to this research by a thorough analysis of the discriminative power of local appearance patterns and by proposing to exploit local information content to model object representation and recognition . we identify discriminative regions in the object views from a posterior en-tropy measure , and then derive object models from selected discriminative local patterns . for recognition , we determine rapid attentive search for locations of high information content from learned decision trees . the recognition system is evaluated by various degrees of partial occlusion and gaus-sian image noise , resulting in highly robust recognition even in the presence of severe occlusion effects ."
  },
  {
    "title": "Distributed Gaussian particle filtering using likelihood consensus .",
    "entities": [
      "gaussian particle filter",
      "likelihood consensus scheme",
      "global state estimate",
      "wireless sensor network",
      "target tracking problem",
      "joint likelihood function",
      "local communications",
      "distributed implementation",
      "local gpf",
      "particle weights"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <task> <otherscientificterm> <method> <method> <method> <otherscientificterm>",
    "relations": [
      "gaussian particle filter -- USED-FOR -- wireless sensor network",
      "local communications -- USED-FOR -- joint likelihood function",
      "local gpf -- USED-FOR -- target tracking problem"
    ],
    "abstract": "we propose a <method_7> of the <method_0> for use in a <method_3> . each sensor runs a <method_8> that computes a <otherscientificterm_2> . the updating of the <otherscientificterm_9> at each sensor uses the <otherscientificterm_5> , which is calculated in a distributed way , using only <method_6> , via the recently proposed <method_1> . a significant reduction of the number of particles can be achieved by means of another <method_1> . the performance of the proposed <method_8> is demonstrated for a <task_4> .",
    "abstract_og": "we propose a distributed implementation of the gaussian particle filter for use in a wireless sensor network . each sensor runs a local gpf that computes a global state estimate . the updating of the particle weights at each sensor uses the joint likelihood function , which is calculated in a distributed way , using only local communications , via the recently proposed likelihood consensus scheme . a significant reduction of the number of particles can be achieved by means of another likelihood consensus scheme . the performance of the proposed local gpf is demonstrated for a target tracking problem ."
  },
  {
    "title": "A Filtering Approach to Underdetermined Blind Source Separation With Application to Temporomandibular Disorders .",
    "entities": [
      "diagnosis of temporomandibu-lar disorders",
      "temporomandibular joint sounds",
      "underdetermined blind source separation problem",
      "underdetermined blind source separation",
      "copyrighted component",
      "tem-poromandibular disorders",
      "fastica algorithm",
      "filtering approach",
      "mixing matrix",
      "temporomandibular disorders",
      "1-norm algorithm",
      "ltering approach",
      "ieee",
      "abstract"
    ],
    "types": "<task> <otherscientificterm> <task> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <material> <method>",
    "relations": [
      "ltering approach -- USED-FOR -- temporomandibular joint sounds",
      "fastica algorithm -- USED-FOR -- mixing matrix",
      "ltering approach -- USED-FOR -- underdetermined blind source separation"
    ],
    "abstract": "a <method_11> to <task_3> with application to <otherscientificterm_9> this item was submitted to loughborough university 's institutional repository by the/an author . <method_11> to <task_3> with application to <otherscientificterm_5> . however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any <method_4> of this work in other works must be obtained from the <material_12> . <method_13> this paper addresses the <task_2> , using a <method_7> . we have developed an extension of the <method_6> which exploits the disparity in the kurtoses of the underlying sources to estimate the <otherscientificterm_8> and thereafter the recovery of the sources is achieved by employing the <method_10> . also , we demonstrate how promising <method_6> can be to extract the sources , without utilizing the <method_10> . furthermore , we illustrate how this <method_11> is particularly suitable to the separation of the <otherscientificterm_1> , crucial in the <task_0> .",
    "abstract_og": "a ltering approach to underdetermined blind source separation with application to temporomandibular disorders this item was submitted to loughborough university 's institutional repository by the/an author . ltering approach to underdetermined blind source separation with application to tem-poromandibular disorders . however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any copyrighted component of this work in other works must be obtained from the ieee . abstract this paper addresses the underdetermined blind source separation problem , using a filtering approach . we have developed an extension of the fastica algorithm which exploits the disparity in the kurtoses of the underlying sources to estimate the mixing matrix and thereafter the recovery of the sources is achieved by employing the 1-norm algorithm . also , we demonstrate how promising fastica algorithm can be to extract the sources , without utilizing the 1-norm algorithm . furthermore , we illustrate how this ltering approach is particularly suitable to the separation of the temporomandibular joint sounds , crucial in the diagnosis of temporomandibu-lar disorders ."
  },
  {
    "title": "Flexibly Exploiting Prior Knowledge in Empirical Learning .",
    "entities": [
      "inductive learning of decision trees",
      "representa tiveness of data",
      "model-based or domain-theory knowledge",
      "knowledge acquisition strategies",
      "empirical learning-the rules",
      "strategy-a human analyst",
      "human expertise",
      "machine-induction system",
      "task automation",
      "printing error",
      "machine induction",
      "modfl-demed feature",
      "domain theory",
      "weak rules",
      "printing-plant humidity",
      "default bias",
      "domain-theory-denvcd features",
      "inductively-derived rules",
      "learning system",
      "decision trees",
      "analyst-specified model",
      "banding",
      "svslem",
      "veracitv"
    ],
    "types": "<task> <material> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "analyst-specified model -- USED-FOR -- inductive learning of decision trees",
      "analyst-specified model -- USED-FOR -- machine induction",
      "model-based or domain-theory knowledge -- CONJUNCTION -- human expertise",
      "domain-theory-denvcd features -- CONJUNCTION -- domain theory",
      "knowledge acquisition strategies -- USED-FOR -- task automation",
      "analyst-specified model -- USED-FOR -- domain theory",
      "knowledge acquisition strategies -- USED-FOR -- domain theory",
      "machine-induction system -- USED-FOR -- empirical learning-the rules",
      "machine-induction system -- USED-FOR -- decision trees",
      "analyst-specified model -- USED-FOR -- task automation",
      "knowledge acquisition strategies -- USED-FOR -- decision trees",
      "machine-induction system -- USED-FOR -- analyst-specified model",
      "knowledge acquisition strategies -- USED-FOR -- empirical learning-the rules",
      "analyst-specified model -- USED-FOR -- empirical learning-the rules",
      "domain-theory-denvcd features -- CONJUNCTION -- human expertise",
      "knowledge acquisition strategies -- USED-FOR -- analyst-specified model",
      "analyst-specified model -- USED-FOR -- decision trees",
      "machine induction -- CONJUNCTION -- human expertise",
      "machine-induction system -- USED-FOR -- human expertise",
      "machine induction -- USED-FOR -- empirical learning-the rules",
      "knowledge acquisition strategies -- USED-FOR -- machine induction"
    ],
    "abstract": "this paper presents a method to incorporate knowledge from possibly imperfect models and domain theories into <task_0> for classification the approach assumes that a model or <method_12> reflects useful prior knowledge of th < task thus the <otherscientificterm_15> should accept the model s predictions as accurate even in the face of somewhat contradictory data which may be unrepresen-lative or noisy however our approach allows the <method_22> to abandon the model or domain theorv , or portions thereof in the fact of suffi-cientlv contradictory data in particular we use c4 5 to induce <otherscientificterm_19> from data that ha \\ t heen augmented b \\ model or <otherscientificterm_16> ' we weakly bias the <method_22> to select model-derived features dur ing decision tree induction but this preference is not dogmatically applied our experiments vary imperfection in a model the <material_1> and the <method_23> with which <otherscientificterm_11> are preferred 1 introduction when <otherscientificterm_6> is nonexistent or very weak relative to a particular domain/task and when data is plentiful <task_10> from data mav be the only reasonable approach to <task_8> in contrast , when expertise is strong , then encoding the expert s model or <method_12> via traditional <method_3> ma > be the best approach in fact , this <otherscientificterm_6> may stem from induction over a much larger data sample than is available at the time <task_8> is undertaken in many cases , however , conditions are indeterminate as to whether sole reliance on <task_10> or <otherscientificterm_6> is most appropriate <otherscientificterm_6> may not be ` perfect and/or data may not be as plentiful as desired in cases where some data is available and <otherscientificterm_6> is less than perfect an advantageous strategy may be to exploit both in an appropriate way there is a growing body of work that combines <otherscientificterm_2> with empirical learning from data clark and matwin -lsb- 1993 -rsb- assume that an <method_20> mediates <otherscientificterm_4> derived from a <method_7> are ace3pted as long as they do not contradict the biases found in the model evans and fisher -lsb- 1994 -rsb- employ a similar <method_5> may specify <otherscientificterm_13> -lrb- e g when <otherscientificterm_14> is low , a certain kind of <otherscientificterm_9> known as <method_21> is more likely , to occur -rrb- if <otherscientificterm_17> indicate an opposite trend then the <method_18> s default strategy is to reject the ...",
    "abstract_og": "this paper presents a method to incorporate knowledge from possibly imperfect models and domain theories into inductive learning of decision trees for classification the approach assumes that a model or domain theory reflects useful prior knowledge of th < task thus the default bias should accept the model s predictions as accurate even in the face of somewhat contradictory data which may be unrepresen-lative or noisy however our approach allows the svslem to abandon the model or domain theorv , or portions thereof in the fact of suffi-cientlv contradictory data in particular we use c4 5 to induce decision trees from data that ha \\ t heen augmented b \\ model or domain-theory-denvcd features ' we weakly bias the svslem to select model-derived features dur ing decision tree induction but this preference is not dogmatically applied our experiments vary imperfection in a model the representa tiveness of data and the veracitv with which modfl-demed feature are preferred 1 introduction when human expertise is nonexistent or very weak relative to a particular domain/task and when data is plentiful machine induction from data mav be the only reasonable approach to task automation in contrast , when expertise is strong , then encoding the expert s model or domain theory via traditional knowledge acquisition strategies ma > be the best approach in fact , this human expertise may stem from induction over a much larger data sample than is available at the time task automation is undertaken in many cases , however , conditions are indeterminate as to whether sole reliance on machine induction or human expertise is most appropriate human expertise may not be ` perfect and/or data may not be as plentiful as desired in cases where some data is available and human expertise is less than perfect an advantageous strategy may be to exploit both in an appropriate way there is a growing body of work that combines model-based or domain-theory knowledge with empirical learning from data clark and matwin -lsb- 1993 -rsb- assume that an analyst-specified model mediates empirical learning-the rules derived from a machine-induction system are ace3pted as long as they do not contradict the biases found in the model evans and fisher -lsb- 1994 -rsb- employ a similar strategy-a human analyst may specify weak rules -lrb- e g when printing-plant humidity is low , a certain kind of printing error known as banding is more likely , to occur -rrb- if inductively-derived rules indicate an opposite trend then the learning system s default strategy is to reject the ..."
  },
  {
    "title": "Head-Driven Hierarchical Phrase-based Translation .",
    "entities": [
      "chi-ang 's hierarchical phrase-based model",
      "head-driven hpb",
      "nist mt test sets",
      "chiang 's model",
      "reordering search space",
      "syntax-driven information",
      "translation rules",
      "chinese-english translation",
      "head information",
      "reordering",
      "bleu"
    ],
    "types": "<method> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <metric>",
    "relations": [
      "nist mt test sets -- EVALUATE-FOR -- head-driven hpb",
      "head-driven hpb -- COMPARE -- chiang 's model",
      "chinese-english translation -- EVALUATE-FOR -- head-driven hpb",
      "nist mt test sets -- EVALUATE-FOR -- chinese-english translation",
      "bleu -- EVALUATE-FOR -- chiang 's model"
    ],
    "abstract": "this paper presents an extension of <method_0> , called <method_1> , which incorporates <otherscientificterm_8> in <otherscientificterm_6> to better capture <otherscientificterm_5> , as well as improved <task_9> between any two neighboring non-terminals at any stage of a derivation to explore a larger <otherscientificterm_4> . experiments on <task_7> on four <material_2> show that the <method_1> significantly outperforms <method_3> with average gains of 1.91 points absolute in <metric_10> .",
    "abstract_og": "this paper presents an extension of chi-ang 's hierarchical phrase-based model , called head-driven hpb , which incorporates head information in translation rules to better capture syntax-driven information , as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space . experiments on chinese-english translation on four nist mt test sets show that the head-driven hpb significantly outperforms chiang 's model with average gains of 1.91 points absolute in bleu ."
  },
  {
    "title": "Optimal Asset Allocation using Adaptive Dynamic Programming .",
    "entities": [
      "computerized asset allocation -lrb- portfolio management -rrb-",
      "neural network based reinforcement learning",
      "reinforcement learning",
      "high dimensional state space",
      "reinforcement learning based algorithms",
      "asset allocation strategy",
      "markovian decision problem",
      "artificial exchange rate",
      "heuristic benchmark policy",
      "value function approximators",
      "german stock market",
      "problem setting",
      "neural networks",
      "asset allocation",
      "liquid capital",
      "dynamic programming",
      "policy"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <method> <method> <task> <metric> <method> <method> <material> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "asset allocation strategy -- COMPARE -- heuristic benchmark policy",
      "dynamic programming -- USED-FOR -- policy",
      "liquid capital -- PART-OF -- german stock market",
      "artificial exchange rate -- USED-FOR -- asset allocation strategy",
      "markovian decision problem -- USED-FOR -- asset allocation",
      "neural network based reinforcement learning -- USED-FOR -- problem setting",
      "asset allocation strategy -- USED-FOR -- policy",
      "high dimensional state space -- FEATURE-OF -- problem setting",
      "dynamic programming -- CONJUNCTION -- reinforcement learning based algorithms",
      "neural networks -- USED-FOR -- value function approximators",
      "dynamic programming -- USED-FOR -- markovian decision problem"
    ],
    "abstract": "in recent years , the interest of investors has shifted to <task_0> to exploit the growing dynamics of the capital markets . in this paper , <method_13> is formalized as a <task_6> which can be optimized by applying <method_15> or <method_4> . using an <metric_7> , the <method_5> optimized with <method_2> is shown to be equivalent to a <otherscientificterm_16> computed by <method_15> . the <method_5> is then tested on the task to invest <otherscientificterm_14> in the <material_10> . here , <method_12> are used as <method_9> . the resulting <method_5> is superior to a <method_8> . this is a further example which demonstrates the applicability of <method_1> to a <task_11> with a <otherscientificterm_3> .",
    "abstract_og": "in recent years , the interest of investors has shifted to computerized asset allocation -lrb- portfolio management -rrb- to exploit the growing dynamics of the capital markets . in this paper , asset allocation is formalized as a markovian decision problem which can be optimized by applying dynamic programming or reinforcement learning based algorithms . using an artificial exchange rate , the asset allocation strategy optimized with reinforcement learning is shown to be equivalent to a policy computed by dynamic programming . the asset allocation strategy is then tested on the task to invest liquid capital in the german stock market . here , neural networks are used as value function approximators . the resulting asset allocation strategy is superior to a heuristic benchmark policy . this is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space ."
  },
  {
    "title": "Rao-Blackwellised particle filtering for blind system identification .",
    "entities": [
      "time-varying finite impulse response model",
      "sequential monte carlo methods",
      "time-varying autoregressive model",
      "nonlinear sequential state estimation problem",
      "convolution of the sources",
      "ar and fir coefficients",
      "rao-blackwellised particle filtering algorithm",
      "joint posterior distribution",
      "state space model",
      "blind system identification",
      "additive noise",
      "multi-sensor measurements",
      "bayesian solution",
      "numerical approximation",
      "particle filter"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "numerical approximation -- USED-FOR -- nonlinear sequential state estimation problem",
      "rao-blackwellised particle filtering algorithm -- USED-FOR -- blind system identification",
      "bayesian solution -- USED-FOR -- nonlinear sequential state estimation problem",
      "convolution of the sources -- USED-FOR -- multi-sensor measurements",
      "bayesian solution -- USED-FOR -- particle filter",
      "time-varying autoregressive model -- USED-FOR -- state space model",
      "numerical approximation -- USED-FOR -- bayesian solution"
    ],
    "abstract": "this paper develops a <method_6> for <task_9> . the <method_8> under consideration uses a <method_2> for the sources , and a <method_0> for the channel . the <otherscientificterm_11> result from the <otherscientificterm_4> with the channels in the presence of <otherscientificterm_10> . a <method_13> to the optimal <method_12> for the <task_3> is implemented using <method_1> . the <method_12> is applied to improve the efficiency of the <method_14> by marginalizing out the <otherscientificterm_5> from the <otherscientificterm_7> . simulation results are given to verify the performance of the proposed <method_6> .",
    "abstract_og": "this paper develops a rao-blackwellised particle filtering algorithm for blind system identification . the state space model under consideration uses a time-varying autoregressive model for the sources , and a time-varying finite impulse response model for the channel . the multi-sensor measurements result from the convolution of the sources with the channels in the presence of additive noise . a numerical approximation to the optimal bayesian solution for the nonlinear sequential state estimation problem is implemented using sequential monte carlo methods . the bayesian solution is applied to improve the efficiency of the particle filter by marginalizing out the ar and fir coefficients from the joint posterior distribution . simulation results are given to verify the performance of the proposed rao-blackwellised particle filtering algorithm ."
  },
  {
    "title": "Designing Spatially Coherent Minimizing Flows for Variational Problems Based on Active Contours .",
    "entities": [
      "application-specific spatially coherent minimizing flows",
      "canonical l 2 inner product",
      "classical gradient flows",
      "active contours method",
      "active contours literature",
      "irrelevant local minima",
      "inner products",
      "gradient descents",
      "variational problems",
      "gradient flows",
      "active contours",
      "spatial coherence",
      "admissible deformations",
      "gradient"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "active contours -- FEATURE-OF -- variational problems"
    ],
    "abstract": "this paper tackles an important aspect of the <task_8> involving <otherscientificterm_10> , which has been largely overlooked so far : the optimization by <otherscientificterm_9> . classically , the definition of a <otherscientificterm_13> depends directly on the choice of an inner product structure . this consideration is largely absent from the <otherscientificterm_4> . most authors , overtly or covertly , assume that the space of <otherscientificterm_12> is ruled by the <otherscientificterm_1> . the <otherscientificterm_2> reported in the literature are relative to this particular choice . in this paper , we investigate the relevance of using other <method_6> , yielding other <method_7> , and some other minimizing flows not deriving from any inner product . in particular , we show how to induce different degrees of <otherscientificterm_11> into the minimizing flow , in order to decrease the probability of getting trapped into <otherscientificterm_5> . we show with some numerical experiments that the sensitivity of the <method_3> to initial conditions , which seriously limits its applicability and its efficiency , is alleviated by our <otherscientificterm_0> .",
    "abstract_og": "this paper tackles an important aspect of the variational problems involving active contours , which has been largely overlooked so far : the optimization by gradient flows . classically , the definition of a gradient depends directly on the choice of an inner product structure . this consideration is largely absent from the active contours literature . most authors , overtly or covertly , assume that the space of admissible deformations is ruled by the canonical l 2 inner product . the classical gradient flows reported in the literature are relative to this particular choice . in this paper , we investigate the relevance of using other inner products , yielding other gradient descents , and some other minimizing flows not deriving from any inner product . in particular , we show how to induce different degrees of spatial coherence into the minimizing flow , in order to decrease the probability of getting trapped into irrelevant local minima . we show with some numerical experiments that the sensitivity of the active contours method to initial conditions , which seriously limits its applicability and its efficiency , is alleviated by our application-specific spatially coherent minimizing flows ."
  },
  {
    "title": "Multi-task Gaussian Process Learning of Robot Inverse Dynamics .",
    "entities": [
      "multi-task learning problem",
      "inverse dynamics problem",
      "multi-task gaussian process",
      "robotic manipulator",
      "latent functions",
      "adaptive control",
      "inter-task similarity",
      "inertial parameters",
      "end effector",
      "inverse dynamics"
    ],
    "types": "<task> <task> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "inverse dynamics problem -- USED-FOR -- robotic manipulator",
      "inertial parameters -- USED-FOR -- inter-task similarity"
    ],
    "abstract": "the <task_1> for a <method_3> is to compute the torques needed at the joints to drive it along a given trajectory ; it is beneficial to be able to learn this function for <task_5> . a <method_3> will often need to be controlled while holding different loads in its <otherscientificterm_8> , giving rise to a <task_0> . by placing independent gaussian process priors over the <otherscientificterm_4> of the <otherscientificterm_9> , we obtain a <method_2> prior for handling multiple loads , where the <otherscientificterm_6> depends on the underlying <otherscientificterm_7> . experiments demonstrate that this <method_3> is effective in sharing information among the various loads , and generally improves performance over either learning only on single tasks or pooling the data over all tasks .",
    "abstract_og": "the inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory ; it is beneficial to be able to learn this function for adaptive control . a robotic manipulator will often need to be controlled while holding different loads in its end effector , giving rise to a multi-task learning problem . by placing independent gaussian process priors over the latent functions of the inverse dynamics , we obtain a multi-task gaussian process prior for handling multiple loads , where the inter-task similarity depends on the underlying inertial parameters . experiments demonstrate that this robotic manipulator is effective in sharing information among the various loads , and generally improves performance over either learning only on single tasks or pooling the data over all tasks ."
  },
  {
    "title": "Strategy Representation and Reasoning for Incomplete Information Concurrent Games in the Situation Calculus .",
    "entities": [
      "strategy representation and reasoning",
      "multi-agent epistemic situation calculus",
      "incomplete information concurrent games",
      "strategy representation and reasoning",
      "strategy programming language",
      "concrete game models",
      "logical frameworks",
      "strategy programs",
      "multi-agent system",
      "golog"
    ],
    "types": "<method> <method> <otherscientificterm> <task> <method> <method> <method> <method> <task> <method>",
    "relations": [
      "strategy representation and reasoning -- USED-FOR -- incomplete information concurrent games",
      "concrete game models -- USED-FOR -- logical frameworks"
    ],
    "abstract": "strategy representation and reasoning for <otherscientificterm_2> has recently received much attention in <task_8> and ai communities . however , most of the <method_6> are based on <method_5> , lack the abilities to reason about strategies explicitly or specify strategies procedurally , and ignore the issue of coordination within a coalition . in this paper , by a simple extension of a variant of <method_1> with a strategy sort , we develop a general framework for <task_3> for <otherscientificterm_2> . based on <method_9> , we propose a <method_4> which can be conveniently used to specify collective strategies of coalitions at different granularities . we present a formalization of joint abilities of coalitions under commitments to <method_7> . different kinds of individual strategic abilities can be distinguished in our framework . both strategic abilities in atl and joint abilities of ghaderi et al. can be considered as joint abilities under special programs in our framework . we illustrate our work with a variant of levesque 's squirrels world .",
    "abstract_og": "strategy representation and reasoning for incomplete information concurrent games has recently received much attention in multi-agent system and ai communities . however , most of the logical frameworks are based on concrete game models , lack the abilities to reason about strategies explicitly or specify strategies procedurally , and ignore the issue of coordination within a coalition . in this paper , by a simple extension of a variant of multi-agent epistemic situation calculus with a strategy sort , we develop a general framework for strategy representation and reasoning for incomplete information concurrent games . based on golog , we propose a strategy programming language which can be conveniently used to specify collective strategies of coalitions at different granularities . we present a formalization of joint abilities of coalitions under commitments to strategy programs . different kinds of individual strategic abilities can be distinguished in our framework . both strategic abilities in atl and joint abilities of ghaderi et al. can be considered as joint abilities under special programs in our framework . we illustrate our work with a variant of levesque 's squirrels world ."
  },
  {
    "title": "Representing Sentence Structure in Hidden Markov Models for Information Extraction .",
    "entities": [
      "hidden markov models",
      "hmm training",
      "information extractors",
      "objective function",
      "biomedical domains",
      "free text",
      "grammatical structure",
      "binary relations",
      "extractors"
    ],
    "types": "<method> <method> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "biomedical domains -- FEATURE-OF -- binary relations",
      "objective function -- USED-FOR -- hmm training",
      "extractors -- USED-FOR -- binary relations"
    ],
    "abstract": "we study the application of <method_0> to learning <otherscientificterm_2> for $ - ary relations from <material_5> . we propose an approach to representing the <otherscientificterm_6> of sentences in the states of the model . we also investigate using an <otherscientificterm_3> during <method_1> which maximizes the ability of the learned models to identify the phrases of interest . we evaluate our methods by deriving <method_8> for two <otherscientificterm_7> in <material_4> . our experiments indicate that our approach learns more accurate models than several baseline approaches .",
    "abstract_og": "we study the application of hidden markov models to learning information extractors for $ - ary relations from free text . we propose an approach to representing the grammatical structure of sentences in the states of the model . we also investigate using an objective function during hmm training which maximizes the ability of the learned models to identify the phrases of interest . we evaluate our methods by deriving extractors for two binary relations in biomedical domains . our experiments indicate that our approach learns more accurate models than several baseline approaches ."
  },
  {
    "title": "A Generalized Optical Flow Constraint and its Physical Interpretation .",
    "entities": [
      "apparent motion of image irradiance patterns",
      "fluid mechanic mass conservation principle",
      "extended optical flow constraint",
      "extended optical flow constraints",
      "synthetic and meteorological images",
      "invariance brightness based hypothesis",
      "optical flow constraint",
      "motion estimation",
      "image sequences",
      "optical flow"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <task> <material> <otherscientificterm>",
    "relations": [
      "optical flow constraint -- HYPONYM-OF -- invariance brightness based hypothesis",
      "image sequences -- EVALUATE-FOR -- motion estimation",
      "fluid mechanic mass conservation principle -- USED-FOR -- optical flow constraint",
      "optical flow -- CONJUNCTION -- extended optical flow constraints"
    ],
    "abstract": "this paper addresses the issue of <task_7> on <material_8> . the standard <task_7> used to compute the <otherscientificterm_0> is an <method_5> called the <otherscientificterm_6> . other equations can be used , in particular the <otherscientificterm_2> , which is a variant of the <otherscientificterm_6> , inspired by the <method_1> . in this paper , we propose a physical interpretation of this <otherscientificterm_2> and a new model unifying the <otherscientificterm_9> and the <otherscientificterm_3> . we present results obtained for <material_4> .",
    "abstract_og": "this paper addresses the issue of motion estimation on image sequences . the standard motion estimation used to compute the apparent motion of image irradiance patterns is an invariance brightness based hypothesis called the optical flow constraint . other equations can be used , in particular the extended optical flow constraint , which is a variant of the optical flow constraint , inspired by the fluid mechanic mass conservation principle . in this paper , we propose a physical interpretation of this extended optical flow constraint and a new model unifying the optical flow and the extended optical flow constraints . we present results obtained for synthetic and meteorological images ."
  },
  {
    "title": "Classifying Facial Action .",
    "entities": [
      "detecting and classifying facial actions",
      "facial action coding system",
      "automated facial expression analysis",
      "local image features",
      "facial muscle contractions",
      "motion ow elds",
      "holistic spatial analysis",
      "template matching",
      "principal components",
      "action combinations",
      "facial actions",
      "image sequences",
      "facial expression",
      "wrinkles",
      "generalization"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <method> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "facial actions -- CONJUNCTION -- action combinations"
    ],
    "abstract": "the <method_1> , -lrb- facs -rrb- , devised by ekman and friesen -lrb- 1978 -rrb- , provides an objective means for measuring the <otherscientificterm_4> involved in a <otherscientificterm_12> . in this paper , we approach <task_2> by <task_0> . we generated a database of over 1100 <material_11> of 24 subjects performing over 150 distinct <material_10> or <otherscientificterm_9> . we compare three diierent approaches to classifying the <material_10> in these images : <method_6> based on <method_8> of graylevel images ; explicit measurement of <otherscientificterm_3> such as <otherscientificterm_13> ; and <task_7> with <otherscientificterm_5> . on a dataset containing six individual actions and 20 subjects , these methods had 89 % , 57 % , and 85 % performances respectively for <task_14> to novel subjects . when combined , performance improved to 92 % .",
    "abstract_og": "the facial action coding system , -lrb- facs -rrb- , devised by ekman and friesen -lrb- 1978 -rrb- , provides an objective means for measuring the facial muscle contractions involved in a facial expression . in this paper , we approach automated facial expression analysis by detecting and classifying facial actions . we generated a database of over 1100 image sequences of 24 subjects performing over 150 distinct facial actions or action combinations . we compare three diierent approaches to classifying the facial actions in these images : holistic spatial analysis based on principal components of graylevel images ; explicit measurement of local image features such as wrinkles ; and template matching with motion ow elds . on a dataset containing six individual actions and 20 subjects , these methods had 89 % , 57 % , and 85 % performances respectively for generalization to novel subjects . when combined , performance improved to 92 % ."
  },
  {
    "title": "Vector-sensor array processing for estimating angles and times of arrival of multipath communication signals .",
    "entities": [
      "vector-sensor array processing",
      "multipath channel estimation",
      "vector-sensor array",
      "mobile localization",
      "music-type algorithm",
      "angles-of-arrival aoas",
      "space-time-polarization parameterization",
      "resolution performance",
      "multi-path channels",
      "space-time-polarization domain",
      "accuracy"
    ],
    "types": "<method> <task> <otherscientificterm> <task> <method> <otherscientificterm> <method> <metric> <otherscientificterm> <material> <metric>",
    "relations": [
      "vector-sensor array processing -- USED-FOR -- angles-of-arrival aoas",
      "vector-sensor array -- USED-FOR -- music-type algorithm",
      "multipath channel estimation -- CONJUNCTION -- mobile localization",
      "space-time-polarization parameterization -- EVALUATE-FOR -- multi-path channels"
    ],
    "abstract": "we develop <method_0> to estimate the <otherscientificterm_5> and time delays of mul-tipath channels in the <material_9> . a <method_4> for joint angle and delay estimation with a <otherscientificterm_2> is derived . potential applications include <task_1> and <task_3> . simulation results show that the <method_6> of the <otherscientificterm_8> results in improved <metric_10> and <metric_7> .",
    "abstract_og": "we develop vector-sensor array processing to estimate the angles-of-arrival aoas and time delays of mul-tipath channels in the space-time-polarization domain . a music-type algorithm for joint angle and delay estimation with a vector-sensor array is derived . potential applications include multipath channel estimation and mobile localization . simulation results show that the space-time-polarization parameterization of the multi-path channels results in improved accuracy and resolution performance ."
  },
  {
    "title": "Voice source analysis using biomechanical modeling and glottal inverse filtering .",
    "entities": [
      "deterministic vocal fold model",
      "estimated glottal flow",
      "glottal inverse filtering",
      "glottal flow waveform",
      "glottal flow signal",
      "glottal flow",
      "natural speech",
      "biomechanical model",
      "deterministic components",
      "vocal folds",
      "inverse filtering",
      "optimization process",
      "parameters"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <method> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "inverse filtering -- USED-FOR -- glottal flow waveform",
      "biomechanical model -- USED-FOR -- glottal flow waveform",
      "deterministic components -- FEATURE-OF -- glottal flow signal",
      "glottal inverse filtering -- CONJUNCTION -- biomechanical model"
    ],
    "abstract": "this paper studies the use of <method_2> together with a <method_7> of the <otherscientificterm_9> to simulate the <otherscientificterm_3> . the <otherscientificterm_3> is first estimated by <method_10> the acoustic speech pressure signal of <material_6> . the <otherscientificterm_1> is used as a template in an <method_11> which searches for a set of <otherscientificterm_12> for a <method_0> such that the model output reproduces the <otherscientificterm_1> . the results indicate that the method can reproduce the main <method_8> of the <otherscientificterm_4> with good accuracy .",
    "abstract_og": "this paper studies the use of glottal inverse filtering together with a biomechanical model of the vocal folds to simulate the glottal flow waveform . the glottal flow waveform is first estimated by inverse filtering the acoustic speech pressure signal of natural speech . the estimated glottal flow is used as a template in an optimization process which searches for a set of parameters for a deterministic vocal fold model such that the model output reproduces the estimated glottal flow . the results indicate that the method can reproduce the main deterministic components of the glottal flow signal with good accuracy ."
  },
  {
    "title": "Scene cut detection from MPEG video stream coded without B pictures .",
    "entities": [
      "mpeg-1 video bit streams",
      "macroblock types",
      "gop structure",
      "gop boundaries",
      "scene cuts",
      "matching degree"
    ],
    "types": "<material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "gop structure -- FEATURE-OF -- mpeg-1 video bit streams"
    ],
    "abstract": "in this paper we propose an algorithm that automatically detects clear scene cut locations from an <material_0> coded with a <otherscientificterm_2> of m = 1 , without b pictures . the algorithm detects <otherscientificterm_4> at p t ype pictures by monitoring the percentage of intra-macroblocks per p picture . while <otherscientificterm_4> at i pictures are detected by matching the macroblocks type of the two p pictures at the <otherscientificterm_3> . a \\ type matching parameter '' -lrb- tmp -rrb- is developed to estimate the <otherscientificterm_5> between the <otherscientificterm_1> of two p pictures . it is shown that the method is able to identify the location of <otherscientificterm_4> in p and i pictures with a high success rate .",
    "abstract_og": "in this paper we propose an algorithm that automatically detects clear scene cut locations from an mpeg-1 video bit streams coded with a gop structure of m = 1 , without b pictures . the algorithm detects scene cuts at p t ype pictures by monitoring the percentage of intra-macroblocks per p picture . while scene cuts at i pictures are detected by matching the macroblocks type of the two p pictures at the gop boundaries . a \\ type matching parameter '' -lrb- tmp -rrb- is developed to estimate the matching degree between the macroblock types of two p pictures . it is shown that the method is able to identify the location of scene cuts in p and i pictures with a high success rate ."
  },
  {
    "title": "Speeded-up , relaxed spatial matching .",
    "entities": [
      "large scale image retrieval",
      "relaxed matching process",
      "spatial matching model",
      "single feature correspondences",
      "image retrieval",
      "rigidity constraints",
      "search engine",
      "geometric invariance",
      "computational complexity",
      "geometry re-ranking",
      "mapping constraints",
      "space requirements",
      "discrim-inative power",
      "hough voting",
      "transformation space",
      "non-rigid objects",
      "one-to-one mapping",
      "assumptions",
      "registration",
      "recognition",
      "descriptors",
      "features",
      "detection"
    ],
    "types": "<task> <method> <method> <material> <task> <otherscientificterm> <method> <otherscientificterm> <metric> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "features -- CONJUNCTION -- computational complexity",
      "features -- CONJUNCTION -- descriptors",
      "recognition -- CONJUNCTION -- registration",
      "detection -- CONJUNCTION -- registration",
      "one-to-one mapping -- FEATURE-OF -- non-rigid objects",
      "discrim-inative power -- CONJUNCTION -- rigidity constraints",
      "recognition -- CONJUNCTION -- detection",
      "features -- CONJUNCTION -- mapping constraints",
      "geometry re-ranking -- PART-OF -- search engine",
      "registration -- CONJUNCTION -- large scale image retrieval",
      "relaxed matching process -- USED-FOR -- geometry re-ranking",
      "geometric invariance -- CONJUNCTION -- rigidity constraints",
      "discrim-inative power -- CONJUNCTION -- geometric invariance",
      "rigidity constraints -- CONJUNCTION -- mapping constraints"
    ],
    "abstract": "a wide range of properties and <otherscientificterm_17> determine the most appropriate <method_2> for an application , e.g. <task_19> , <task_22> , <task_18> , or <task_0> . most notably , these include <otherscientificterm_12> , <otherscientificterm_7> , <otherscientificterm_5> , <otherscientificterm_10> , <otherscientificterm_17> made on the underlying <otherscientificterm_21> or <otherscientificterm_20> and , of course , <metric_8> . having <task_4> in mind , we present a very simple model inspired by <method_13> in the <otherscientificterm_14> , where votes arise from <material_3> . a <method_1> allows for multiple matching surfaces or <otherscientificterm_15> under <method_16> , yet is linear in the number of correspondences . we apply <method_1> to <task_9> in a <method_6> , yielding superior performance with the same <otherscientificterm_11> but a dramatic speed-up compared to the state of the art .",
    "abstract_og": "a wide range of properties and assumptions determine the most appropriate spatial matching model for an application , e.g. recognition , detection , registration , or large scale image retrieval . most notably , these include discrim-inative power , geometric invariance , rigidity constraints , mapping constraints , assumptions made on the underlying features or descriptors and , of course , computational complexity . having image retrieval in mind , we present a very simple model inspired by hough voting in the transformation space , where votes arise from single feature correspondences . a relaxed matching process allows for multiple matching surfaces or non-rigid objects under one-to-one mapping , yet is linear in the number of correspondences . we apply relaxed matching process to geometry re-ranking in a search engine , yielding superior performance with the same space requirements but a dramatic speed-up compared to the state of the art ."
  },
  {
    "title": "ReACTR : Realtime Algorithm Configuration through Tournament Rankings .",
    "entities": [
      "automated algorithm configuration",
      "demonstrative instances",
      "static config-urators",
      "parameter space",
      "steady stream",
      "parameter settings",
      "non-dynamic setting",
      "react system",
      "problem domain",
      "static environment",
      "configuration tools",
      "ranking scheme",
      "gga",
      "smac",
      "trueskill",
      "paramils"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method> <method> <method> <method> <method> <method>",
    "relations": [
      "gga -- HYPONYM-OF -- configuration tools",
      "trueskill -- HYPONYM-OF -- ranking scheme",
      "smac -- CONJUNCTION -- gga",
      "paramils -- CONJUNCTION -- smac",
      "smac -- HYPONYM-OF -- configuration tools",
      "non-dynamic setting -- USED-FOR -- static config-urators",
      "paramils -- HYPONYM-OF -- configuration tools",
      "smac -- HYPONYM-OF -- static config-urators",
      "smac -- PART-OF -- non-dynamic setting"
    ],
    "abstract": "it is now readily accepted that <method_0> is a necessity for ensuring optimized performance of solvers on a particular <material_8> . even the best developers who have carefully designed their solver are not always able to manually find the best <otherscientificterm_5> for it . yet , the opportunity for improving performance has been repeatedly demonstrated by <method_10> like <method_15> , <method_13> , and <method_12> . however , all these techniques currently assume a <otherscientificterm_9> , where <otherscientificterm_1> are procured beforehand , potentially unlimited time is provided to adequately search the <otherscientificterm_3> , and the solver would never need to be retrained . this is not always the case in practice . the <method_7> , proposed in 2014 , demonstrated that a solver could be configured during runtime as new instances arrive in a <otherscientificterm_4> . this paper further develops that approach and shows how a <method_11> , like <method_14> , can further improve the configurator 's performance , making it able to quickly find good parameterizations without adding any overhead on the time needed to solve any new instance , and then continuously improve as new instances are evaluated . the enhancements to <method_7> that we present enable us to even outperform existing <method_2> like <method_13> in a <otherscientificterm_6> .",
    "abstract_og": "it is now readily accepted that automated algorithm configuration is a necessity for ensuring optimized performance of solvers on a particular problem domain . even the best developers who have carefully designed their solver are not always able to manually find the best parameter settings for it . yet , the opportunity for improving performance has been repeatedly demonstrated by configuration tools like paramils , smac , and gga . however , all these techniques currently assume a static environment , where demonstrative instances are procured beforehand , potentially unlimited time is provided to adequately search the parameter space , and the solver would never need to be retrained . this is not always the case in practice . the react system , proposed in 2014 , demonstrated that a solver could be configured during runtime as new instances arrive in a steady stream . this paper further develops that approach and shows how a ranking scheme , like trueskill , can further improve the configurator 's performance , making it able to quickly find good parameterizations without adding any overhead on the time needed to solve any new instance , and then continuously improve as new instances are evaluated . the enhancements to react system that we present enable us to even outperform existing static config-urators like smac in a non-dynamic setting ."
  },
  {
    "title": "A semi-blind MIMO channel estimation scheme for MRT .",
    "entities": [
      "closed-form semi-blind solution",
      "maximum ratio transmission",
      "mean squared error",
      "matrix perturbation theory",
      "semi-blind channel estimation",
      "channel matrix",
      "training lengths",
      "beamforming vectors",
      "beamforming vector",
      "estimation technique",
      "snr",
      "snrs"
    ],
    "types": "<method> <method> <metric> <method> <task> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm>",
    "relations": [
      "estimation technique -- USED-FOR -- training lengths",
      "estimation technique -- USED-FOR -- closed-form semi-blind solution"
    ],
    "abstract": "in this paper , we investigate <task_4> for multiple input multiple output -lrb- mimo -rrb- quasi-static flat fading channels when <method_1> is employed . we propose a <method_0> for estimating the optimum transmit and receive <otherscientificterm_7> of the <otherscientificterm_5> . employing <method_3> , we develop expressions for the <metric_2> in the <otherscientificterm_8> and average received <metric_10> of both the semi-blind and the conventional least squares estimation -lrb- <method_0> -rrb- schemes . it is found that the proposed <method_9> outperforms <method_0> for a wide range of <metric_6> and training <otherscientificterm_11> .",
    "abstract_og": "in this paper , we investigate semi-blind channel estimation for multiple input multiple output -lrb- mimo -rrb- quasi-static flat fading channels when maximum ratio transmission is employed . we propose a closed-form semi-blind solution for estimating the optimum transmit and receive beamforming vectors of the channel matrix . employing matrix perturbation theory , we develop expressions for the mean squared error in the beamforming vector and average received snr of both the semi-blind and the conventional least squares estimation -lrb- closed-form semi-blind solution -rrb- schemes . it is found that the proposed estimation technique outperforms closed-form semi-blind solution for a wide range of training lengths and training snrs ."
  },
  {
    "title": "Acquiring Comparative Commonsense Knowledge from the Web .",
    "entities": [
      "open information extraction methods",
      "disambiguated commonsense assertions",
      "joint optimization model",
      "integer linear programming",
      "semantic coherence scores",
      "cleaning",
      "wordnet",
      "bears",
      "web",
      "commonsense"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <metric> <task> <material> <otherscientificterm> <material> <otherscientificterm>",
    "relations": [
      "web -- USED-FOR -- open information extraction methods",
      "integer linear programming -- USED-FOR -- joint optimization model",
      "integer linear programming -- CONJUNCTION -- semantic coherence scores",
      "semantic coherence scores -- EVALUATE-FOR -- joint optimization model"
    ],
    "abstract": "applications are increasingly expected to make smart decisions based on what humans consider basic <otherscientificterm_9> . an often overlooked but essential form of <otherscientificterm_9> involves comparisons , e.g. the fact that <otherscientificterm_7> are typically more dangerous than dogs , that tables are heavier than chairs , or that ice is colder than water . in this paper , we first rely on <method_0> to obtain large amounts of comparisons from the <material_8> . we then develop a <method_2> for <task_5> and disambiguating this knowledge with respect to <material_6> . this <method_2> relies on <method_3> and <metric_4> . experiments show that our <method_2> outperforms strong baselines and allows us to obtain a large knowledge base of <otherscientificterm_1> .",
    "abstract_og": "applications are increasingly expected to make smart decisions based on what humans consider basic commonsense . an often overlooked but essential form of commonsense involves comparisons , e.g. the fact that bears are typically more dangerous than dogs , that tables are heavier than chairs , or that ice is colder than water . in this paper , we first rely on open information extraction methods to obtain large amounts of comparisons from the web . we then develop a joint optimization model for cleaning and disambiguating this knowledge with respect to wordnet . this joint optimization model relies on integer linear programming and semantic coherence scores . experiments show that our joint optimization model outperforms strong baselines and allows us to obtain a large knowledge base of disambiguated commonsense assertions ."
  },
  {
    "title": "Resources for Urdu Language Processing .",
    "entities": [
      "corpus and lexical resources",
      "crulp",
      "urdu"
    ],
    "types": "<material> <material> <material>",
    "relations": [
      "crulp -- USED-FOR -- urdu"
    ],
    "abstract": "urdu is spoken by more than 100 million speakers . this paper summarizes the <material_0> being developed for <material_2> by the <material_1> , in pakistan .",
    "abstract_og": "urdu is spoken by more than 100 million speakers . this paper summarizes the corpus and lexical resources being developed for urdu by the crulp , in pakistan ."
  },
  {
    "title": "Minimizing the Reprojection Error in Surface Reconstruction from Images .",
    "entities": [
      "contour generators of the surface",
      "gradient descent surface evolution",
      "reprojection error functional",
      "apparent contour constraints",
      "computation of derivative",
      "image-based surface reconstruction",
      "surface moves",
      "visibility changes",
      "contour generators",
      "reprojection error"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "reprojection error -- EVALUATE-FOR -- contour generators"
    ],
    "abstract": "this paper addresses the problem of <task_5> . the main contribution is the computation of the exact derivative of the <otherscientificterm_2> . this allows its rigorous minimization via <method_1> . the main difficulty has been to correctly take into account the <otherscientificterm_7> that occur when the <otherscientificterm_6> . a geometric and analytical study of these changes is presented and used for the <task_4> . our analysis shows the strong influence that the movement of the <method_8> has on the <otherscientificterm_9> . as a consequence , during the proper minimization of the <otherscientificterm_9> , the <otherscientificterm_0> are automatically moved to their correct location in the images . therefore , current methods adding additional silhouettes or <otherscientificterm_3> to ensure this alignment can now be understood and justified by a single criterion : the <otherscientificterm_9> .",
    "abstract_og": "this paper addresses the problem of image-based surface reconstruction . the main contribution is the computation of the exact derivative of the reprojection error functional . this allows its rigorous minimization via gradient descent surface evolution . the main difficulty has been to correctly take into account the visibility changes that occur when the surface moves . a geometric and analytical study of these changes is presented and used for the computation of derivative . our analysis shows the strong influence that the movement of the contour generators has on the reprojection error . as a consequence , during the proper minimization of the reprojection error , the contour generators of the surface are automatically moved to their correct location in the images . therefore , current methods adding additional silhouettes or apparent contour constraints to ensure this alignment can now be understood and justified by a single criterion : the reprojection error ."
  },
  {
    "title": "Fast Structural Binary Coding .",
    "entities": [
      "fast structural binary coding",
      "geometric relationships between database examples",
      "supervised binary coding approach",
      "hamming distance ranking list",
      "stochastic gradient descent method",
      "discrete optimization objective",
      "lever-aging supervised information",
      "short binary codes",
      "binary coding techniques",
      "high-dimensional data samples",
      "coding functions",
      "information retrieval",
      "continuous surrogate",
      "hamming space",
      "image search",
      "euclidean space",
      "image datasets",
      "coding quality",
      "coding function",
      "precision",
      "states-of-the-art"
    ],
    "types": "<method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <metric> <otherscientificterm> <metric> <material>",
    "relations": [
      "hamming space -- FEATURE-OF -- euclidean space",
      "continuous surrogate -- FEATURE-OF -- discrete optimization objective",
      "image search -- EVALUATE-FOR -- supervised binary coding approach",
      "image search -- EVALUATE-FOR -- states-of-the-art",
      "discrete optimization objective -- USED-FOR -- coding function",
      "fast structural binary coding -- HYPONYM-OF -- supervised binary coding approach",
      "continuous surrogate -- USED-FOR -- coding function"
    ],
    "abstract": "binary coding techniques , which compress originally <material_9> into <otherscientificterm_7> , are becoming increasingly popular due to their efficiency for <task_11> . <otherscientificterm_6> can dramatically enhance the <metric_17> , and hence improve search performance . there are few methods , however , that efficiently learn <otherscientificterm_10> that optimize the <metric_19> at the top of the <otherscientificterm_3> while approximately preserving the <otherscientificterm_1> . in this paper , we propose a novel <method_2> , namely <method_0> , to optimize the <metric_19> at the top of a <otherscientificterm_3> and ensure that similar images can be returned as a whole . the key idea is to train disciplined <otherscientificterm_10> by optimizing a lower bound of the area under the roc -lrb- receiver operating characteristic -rrb- curve -lrb- auc -rrb- and penalize this objective so that the <otherscientificterm_1> in the original <otherscientificterm_15> are approximately preserved in the <otherscientificterm_13> . to find such a <otherscientificterm_18> , we relax the original <otherscientificterm_5> with a <otherscientificterm_12> , and then derive a <method_4> to optimize the surrogate objective efficiently . empirical studies based upon two <material_16> demonstrate that the proposed <method_2> achieve superior <task_14> performance to the <material_20> .",
    "abstract_og": "binary coding techniques , which compress originally high-dimensional data samples into short binary codes , are becoming increasingly popular due to their efficiency for information retrieval . lever-aging supervised information can dramatically enhance the coding quality , and hence improve search performance . there are few methods , however , that efficiently learn coding functions that optimize the precision at the top of the hamming distance ranking list while approximately preserving the geometric relationships between database examples . in this paper , we propose a novel supervised binary coding approach , namely fast structural binary coding , to optimize the precision at the top of a hamming distance ranking list and ensure that similar images can be returned as a whole . the key idea is to train disciplined coding functions by optimizing a lower bound of the area under the roc -lrb- receiver operating characteristic -rrb- curve -lrb- auc -rrb- and penalize this objective so that the geometric relationships between database examples in the original euclidean space are approximately preserved in the hamming space . to find such a coding function , we relax the original discrete optimization objective with a continuous surrogate , and then derive a stochastic gradient descent method to optimize the surrogate objective efficiently . empirical studies based upon two image datasets demonstrate that the proposed supervised binary coding approach achieve superior image search performance to the states-of-the-art ."
  },
  {
    "title": "Convolutional Neural Fabrics .",
    "entities": [
      "sparse homogeneous local connectivity pattern",
      "part labels dataset",
      "response maps",
      "3d trellis",
      "embedded architectures",
      "fabric size",
      "image classification",
      "semantic segmentation",
      "parameters",
      "layers",
      "mnist",
      "cnns",
      "back-propagation"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <material> <method> <method>",
    "relations": [
      "3d trellis -- USED-FOR -- response maps",
      "back-propagation -- USED-FOR -- parameters",
      "part labels dataset -- EVALUATE-FOR -- semantic segmentation"
    ],
    "abstract": "despite the success of <method_11> , selecting the optimal architecture for a given task remains an open problem . instead of aiming to select a single optimal architecture , we propose a '' <method_11> '' that embeds an exponentially large number of architectures . the <method_11> consists of a <otherscientificterm_3> that connects <otherscientificterm_2> at different <otherscientificterm_9> , scales , and channels with a <otherscientificterm_0> . the only hyper-parameters of a <method_11> are the number of channels and <otherscientificterm_9> . while individual architectures can be recovered as paths , the <method_11> can in addition ensemble all <otherscientificterm_4> together , sharing their weights where their paths overlap . <otherscientificterm_8> can be learned using standard methods based on <method_12> , at a cost that scales linearly in the <otherscientificterm_5> . we present benchmark results competitive with the state of the art for <task_6> on <material_10> and cifar10 , and for <task_7> on the <material_1> .",
    "abstract_og": "despite the success of cnns , selecting the optimal architecture for a given task remains an open problem . instead of aiming to select a single optimal architecture , we propose a '' cnns '' that embeds an exponentially large number of architectures . the cnns consists of a 3d trellis that connects response maps at different layers , scales , and channels with a sparse homogeneous local connectivity pattern . the only hyper-parameters of a cnns are the number of channels and layers . while individual architectures can be recovered as paths , the cnns can in addition ensemble all embedded architectures together , sharing their weights where their paths overlap . parameters can be learned using standard methods based on back-propagation , at a cost that scales linearly in the fabric size . we present benchmark results competitive with the state of the art for image classification on mnist and cifar10 , and for semantic segmentation on the part labels dataset ."
  },
  {
    "title": "Transforming HMMs for speaker-independent hands-free speech recognition in the car .",
    "entities": [
      "linear regression-based model adaptation procedure",
      "digit error rate",
      "quiet office",
      "adaptation utterances",
      "recording condition",
      "acoustic conditions",
      "hands-free recognition",
      "hmm",
      "hmms"
    ],
    "types": "<method> <metric> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "linear regression-based model adaptation procedure -- USED-FOR -- hands-free recognition",
      "linear regression-based model adaptation procedure -- USED-FOR -- hmms"
    ],
    "abstract": "in the absence of <method_8> trained with speech collected in the target environment , one may use <method_8> trained with a large amount of speech collected in another <otherscientificterm_4> -lrb- e.g. , <otherscientificterm_2> , with high quality microphone -rrb- . however , this may result in poor performance because of the mismatch between the two <otherscientificterm_5> . we propose a <method_0> to reduce such a mismatch . with some <material_3> collected for the target environment , the <method_0> transforms the <method_8> trained in a quiet condition to maximize the likelihood of observing the <material_3> . the transformation must be designed to maintain speaker-independence of the <method_7> . our speaker-independent test results show that with this <method_0> about 1 % <metric_1> can be achieved for <task_6> , using target environment speech from only 20 speakers .",
    "abstract_og": "in the absence of hmms trained with speech collected in the target environment , one may use hmms trained with a large amount of speech collected in another recording condition -lrb- e.g. , quiet office , with high quality microphone -rrb- . however , this may result in poor performance because of the mismatch between the two acoustic conditions . we propose a linear regression-based model adaptation procedure to reduce such a mismatch . with some adaptation utterances collected for the target environment , the linear regression-based model adaptation procedure transforms the hmms trained in a quiet condition to maximize the likelihood of observing the adaptation utterances . the transformation must be designed to maintain speaker-independence of the hmm . our speaker-independent test results show that with this linear regression-based model adaptation procedure about 1 % digit error rate can be achieved for hands-free recognition , using target environment speech from only 20 speakers ."
  },
  {
    "title": "Cross-Validation Optimization for Large Scale Hierarchical Classification Kernel Methods .",
    "entities": [
      "large scale text classification tasks",
      "cross-validation log likelihood",
      "kernel multi-class models",
      "hierarchical class structure",
      "predictive probabilities",
      "kernel parameters"
    ],
    "types": "<task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "cross-validation log likelihood -- USED-FOR -- kernel parameters"
    ],
    "abstract": "we propose a highly efficient framework for <method_2> with a large and structured set of classes . <otherscientificterm_5> are learned automatically by maximizing the <otherscientificterm_1> , and <otherscientificterm_4> are estimated . we demonstrate our approach on <task_0> with <otherscientificterm_3> , achieving state-of-the-art results in an order of magnitude less time than previous work .",
    "abstract_og": "we propose a highly efficient framework for kernel multi-class models with a large and structured set of classes . kernel parameters are learned automatically by maximizing the cross-validation log likelihood , and predictive probabilities are estimated . we demonstrate our approach on large scale text classification tasks with hierarchical class structure , achieving state-of-the-art results in an order of magnitude less time than previous work ."
  },
  {
    "title": "Combination of speech features using smoothed heteroscedastic linear discriminant analysis .",
    "entities": [
      "feature combination techniques",
      "feature combination",
      "statistic estimation",
      "lda",
      "recognition",
      "hlda",
      "pca"
    ],
    "types": "<method> <method> <method> <method> <task> <method> <method>",
    "relations": [
      "pca -- CONJUNCTION -- lda",
      "lda -- CONJUNCTION -- hlda",
      "pca -- USED-FOR -- feature combination techniques"
    ],
    "abstract": "feature combination techniques based on <method_6> , <method_3> and <method_5> are compared in experiments where limited amount of training data is available . success with <method_1> can be quite dependent on proper estimation of statistics required by the used technique . insufficiency of training data is , therefore , an important problem , which has to be taken in to account in our experiments . besides of some standard approaches increasing robustness of <method_2> , methods based on combination of <method_3> and <method_5> are proposed . an improved <task_4> performance obtained using these methods is demonstrated in experiments .",
    "abstract_og": "feature combination techniques based on pca , lda and hlda are compared in experiments where limited amount of training data is available . success with feature combination can be quite dependent on proper estimation of statistics required by the used technique . insufficiency of training data is , therefore , an important problem , which has to be taken in to account in our experiments . besides of some standard approaches increasing robustness of statistic estimation , methods based on combination of lda and hlda are proposed . an improved recognition performance obtained using these methods is demonstrated in experiments ."
  },
  {
    "title": "Efficient multi-label ranking for multi-class learning : Application to object recognition .",
    "entities": [
      "pascal voc 2006 and 2007 data sets",
      "im-balanced data distributions",
      "binary classification problems",
      "block coordinate descent",
      "visual object recognition",
      "multi-label learning",
      "ranking approach",
      "multi-label ranking",
      "multi-label learning"
    ],
    "types": "<material> <otherscientificterm> <task> <otherscientificterm> <task> <task> <method> <method> <method>",
    "relations": [
      "multi-label learning -- USED-FOR -- visual object recognition",
      "ranking approach -- USED-FOR -- multi-label learning"
    ],
    "abstract": "multi-label learning is useful in <task_4> when several objects are present in an image . conventional approaches implement <method_8> as a set of <task_2> , but <method_8> suffer from <otherscientificterm_1> when the number of classes is large . in this paper , we address <method_8> with many classes via a <method_6> , termed <method_7> . given a test image , the proposed scheme aims to order all the object classes such that the relevant classes are ranked higher than the irrelevant ones . we present an efficient algorithm for <method_7> based on the idea of <otherscientificterm_3> . the proposed algorithm is applied to <task_4> . empirical results on the <material_0> show promising results in comparison to the state-of-the-art algorithms for <method_8> .",
    "abstract_og": "multi-label learning is useful in visual object recognition when several objects are present in an image . conventional approaches implement multi-label learning as a set of binary classification problems , but multi-label learning suffer from im-balanced data distributions when the number of classes is large . in this paper , we address multi-label learning with many classes via a ranking approach , termed multi-label ranking . given a test image , the proposed scheme aims to order all the object classes such that the relevant classes are ranked higher than the irrelevant ones . we present an efficient algorithm for multi-label ranking based on the idea of block coordinate descent . the proposed algorithm is applied to visual object recognition . empirical results on the pascal voc 2006 and 2007 data sets show promising results in comparison to the state-of-the-art algorithms for multi-label learning ."
  },
  {
    "title": "Distributional vectors encode referential attributes .",
    "entities": [
      "supervised regression model",
      "structured knowledge bases",
      "distributional representations",
      "distribu-tional representations",
      "distributional vectors",
      "word referents",
      "distributional methods",
      "baseline error",
      "referential attributes",
      "spain",
      "accuracy"
    ],
    "types": "<method> <material> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <material> <metric>",
    "relations": [
      "supervised regression model -- USED-FOR -- distributional representations",
      "distributional vectors -- USED-FOR -- referential attributes"
    ],
    "abstract": "distributional methods have proven to excel at capturing fuzzy , graded aspects of meaning -lrb- italy is more similar to <material_9> than to germany -rrb- . in contrast , it is difficult to extract the values of more specific attributes of <otherscientificterm_5> from <method_3> , attributes of the kind typically found in <material_1> -lrb- italy has 60 million inhabitants -rrb- . in this paper , we pursue the hypothesis that <otherscientificterm_4> also implicitly encode <otherscientificterm_8> . we show that a standard <method_0> is in fact sufficient to retrieve such attributes to a reasonable degree of <metric_10> : when evaluated on the prediction of both categorical and numeric attributes of countries and cities , the <method_0> consistently reduces <otherscientificterm_7> by 30 % , and is not far from the upper bound . further analysis suggests that our <method_0> is able to '' ob-jectify '' <method_2> for entities , anchoring <method_2> more firmly in the external world in measurable ways .",
    "abstract_og": "distributional methods have proven to excel at capturing fuzzy , graded aspects of meaning -lrb- italy is more similar to spain than to germany -rrb- . in contrast , it is difficult to extract the values of more specific attributes of word referents from distribu-tional representations , attributes of the kind typically found in structured knowledge bases -lrb- italy has 60 million inhabitants -rrb- . in this paper , we pursue the hypothesis that distributional vectors also implicitly encode referential attributes . we show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy : when evaluated on the prediction of both categorical and numeric attributes of countries and cities , the supervised regression model consistently reduces baseline error by 30 % , and is not far from the upper bound . further analysis suggests that our supervised regression model is able to '' ob-jectify '' distributional representations for entities , anchoring distributional representations more firmly in the external world in measurable ways ."
  },
  {
    "title": "Robust Bayesian Analysis applied to Wiener filtering of speech .",
    "entities": [
      "objective quality measures",
      "robust bayesian analysis",
      "speech enhancement algorithms",
      "speech activity decision",
      "power spectral density",
      "undesirable artifacts",
      "enhanced speech",
      "noise estimates",
      "wiener algorithm",
      "mitigating artifacts",
      "clean speech",
      "heuristic methods",
      "noise estimate",
      "signal-to-noise ratio"
    ],
    "types": "<metric> <method> <method> <task> <otherscientificterm> <otherscientificterm> <material> <task> <method> <task> <material> <method> <task> <metric>",
    "relations": [
      "speech enhancement algorithms -- USED-FOR -- power spectral density",
      "noise estimate -- CONJUNCTION -- speech activity decision"
    ],
    "abstract": "commonly used <method_2> estimate the <otherscientificterm_4> of the noise to be removed , or make a decision about the presence of speech in a particular frame , and estimate the <material_10> based on these . errors in a <task_12> or <task_3> may result in <otherscientificterm_5> , and some errors may be more damaging than others . <method_1> is used to analyze the sensitivity of algorithms to errors in <task_7> and improve <metric_13> while <task_9> in the <material_6> . the findings explain why some common heuristic changes to the wiener filter algorithm are effective . a standard <method_8> is used for comparison , <metric_0> are used to quantify improvement , and insights into the underlying mechanisms of <method_11> are offered .",
    "abstract_og": "commonly used speech enhancement algorithms estimate the power spectral density of the noise to be removed , or make a decision about the presence of speech in a particular frame , and estimate the clean speech based on these . errors in a noise estimate or speech activity decision may result in undesirable artifacts , and some errors may be more damaging than others . robust bayesian analysis is used to analyze the sensitivity of algorithms to errors in noise estimates and improve signal-to-noise ratio while mitigating artifacts in the enhanced speech . the findings explain why some common heuristic changes to the wiener filter algorithm are effective . a standard wiener algorithm is used for comparison , objective quality measures are used to quantify improvement , and insights into the underlying mechanisms of heuristic methods are offered ."
  },
  {
    "title": "Heterogeneous Visual Features Fusion via Sparse Multimodal Machine .",
    "entities": [
      "object categorization and scene understanding image data sets",
      "sparse multimodal learning approach",
      "single-label and multi-label image classification tasks",
      "group-wise and individual point of views",
      "scene and object cat-egorization methods",
      "joint structured sparsity regularizations",
      "image and video information",
      "visual feature descriptors",
      "heterogeneous visual features",
      "elementary visual characteristics",
      "global convergence",
      "vision tasks",
      "optimization algorithm",
      "image features",
      "heterogeneous features",
      "non-smooth objective",
      "shape",
      "features"
    ],
    "types": "<material> <method> <task> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "heterogeneous visual features -- USED-FOR -- vision tasks",
      "sparse multimodal learning approach -- USED-FOR -- heterogeneous features",
      "joint structured sparsity regularizations -- USED-FOR -- heterogeneous features",
      "visual feature descriptors -- USED-FOR -- image and video information",
      "optimization algorithm -- USED-FOR -- non-smooth objective"
    ],
    "abstract": "to better understand , search , and classify <task_6> , many <otherscientificterm_7> have been proposed to describe <otherscientificterm_9> , such as the <otherscientificterm_16> , the color , the texture , etc. . how to integrate these <otherscientificterm_8> and identify the important ones from them for specific <task_11> has become an increasingly critical problem . in this paper , we propose a novel <method_1> to integrate such <otherscientificterm_14> by using the <method_5> to learn the feature importance of for the <task_11> from both <otherscientificterm_3> . a new <method_12> is also introduced to solve the <otherscientificterm_15> with rigorously proved <otherscientificterm_10> . we applied our <method_1> to five broadly used <material_0> for both <task_2> . for each data set we integrate six different types of popularly used <otherscientificterm_13> . compared to existing <method_4> using either single modality or multi-modalities of <otherscientificterm_17> , our <method_1> always achieves better performances measured .",
    "abstract_og": "to better understand , search , and classify image and video information , many visual feature descriptors have been proposed to describe elementary visual characteristics , such as the shape , the color , the texture , etc. . how to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem . in this paper , we propose a novel sparse multimodal learning approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views . a new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence . we applied our sparse multimodal learning approach to five broadly used object categorization and scene understanding image data sets for both single-label and multi-label image classification tasks . for each data set we integrate six different types of popularly used image features . compared to existing scene and object cat-egorization methods using either single modality or multi-modalities of features , our sparse multimodal learning approach always achieves better performances measured ."
  },
  {
    "title": "Information retrieval-based dynamic time warping .",
    "entities": [
      "information retrieval-based dynamic time warping",
      "dynamic time warping approaches",
      "baseline subsequence-dtw implementation",
      "non-linearly matching subsequences",
      "pure diagonal matching",
      "dynamic programming algorithm",
      "large scale implementations",
      "qbe-std task",
      "audio matching",
      "matching accuracy",
      "search collection",
      "memory footprint",
      "indexing techniques",
      "ir-dtw",
      "s-dtw"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <method> <task> <task> <task> <metric> <method> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "ir-dtw -- COMPARE -- dynamic time warping approaches",
      "dynamic programming algorithm -- USED-FOR -- non-linearly matching subsequences",
      "indexing techniques -- USED-FOR -- search collection",
      "dynamic programming algorithm -- USED-FOR -- audio matching",
      "matching accuracy -- COMPARE -- pure diagonal matching",
      "dynamic programming algorithm -- USED-FOR -- search collection",
      "information retrieval-based dynamic time warping -- HYPONYM-OF -- dynamic programming algorithm"
    ],
    "abstract": "in this paper we introduce a novel <method_5> called <method_0> used to find <otherscientificterm_3> between two time series where matching start and end points are not known a priori . in this paper our <method_5> is applied for <task_8> within the query by example -lrb- qbe -rrb- spoken term detection -lrb- std -rrb- task , although <method_5> is applicable to many other problems . the main advantages of the proposed <method_5> in comparison to similar approaches are twofold . on the one hand , <method_13> requires a much smaller <otherscientificterm_11> than standard <method_1> . on the other hand , <method_5> allows for the application of <method_12> to the <method_10> for increased matching speed , which makes <method_13> suitable for application in <task_6> . we show through preliminary experimentation with a <task_7> that the <otherscientificterm_11> is greatly reduced in comparison to a <method_2> and that its <metric_9> is much better than that of <task_4> and just slightly worse than that of <method_14> .",
    "abstract_og": "in this paper we introduce a novel dynamic programming algorithm called information retrieval-based dynamic time warping used to find non-linearly matching subsequences between two time series where matching start and end points are not known a priori . in this paper our dynamic programming algorithm is applied for audio matching within the query by example -lrb- qbe -rrb- spoken term detection -lrb- std -rrb- task , although dynamic programming algorithm is applicable to many other problems . the main advantages of the proposed dynamic programming algorithm in comparison to similar approaches are twofold . on the one hand , ir-dtw requires a much smaller memory footprint than standard dynamic time warping approaches . on the other hand , dynamic programming algorithm allows for the application of indexing techniques to the search collection for increased matching speed , which makes ir-dtw suitable for application in large scale implementations . we show through preliminary experimentation with a qbe-std task that the memory footprint is greatly reduced in comparison to a baseline subsequence-dtw implementation and that its matching accuracy is much better than that of pure diagonal matching and just slightly worse than that of s-dtw ."
  },
  {
    "title": "Dependence Minimizing Regression with Model Selection for Non-Linear Causal Inference under Non-Gaussian Noise .",
    "entities": [
      "least-squares independence regression",
      "additive non-gaussian noise models",
      "non-linear causal relationship",
      "causal inference method",
      "squared-loss mutual information",
      "causal inference algorithm",
      "additive noise model",
      "tuning parameters",
      "real-world datasets",
      "kernel width",
      "regularization parameter",
      "data-dependent fashion",
      "estimator",
      "overfitting",
      "cross-validation"
    ],
    "types": "<method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "kernel width -- CONJUNCTION -- regularization parameter",
      "kernel width -- HYPONYM-OF -- tuning parameters",
      "least-squares independence regression -- COMPARE -- causal inference method",
      "least-squares independence regression -- HYPONYM-OF -- causal inference algorithm",
      "least-squares independence regression -- USED-FOR -- additive noise model"
    ],
    "abstract": "the discovery of <otherscientificterm_2> under <method_1> has attracted considerable attention recently because of their high flexibility . in this paper , we propose a novel <method_5> called <method_0> . <method_0> learns the <method_6> through minimization of an <method_12> of the <otherscientificterm_4> between inputs and residuals . a notable advantage of <method_0> over existing approaches is that <otherscientificterm_7> such as the <otherscientificterm_9> and the <otherscientificterm_10> can be naturally optimized by <method_14> , allowing us to avoid <method_13> in a <otherscientificterm_11> . through experiments with <material_8> , we show that <method_0> compares favorably with the state-of-the-art <method_3> .",
    "abstract_og": "the discovery of non-linear causal relationship under additive non-gaussian noise models has attracted considerable attention recently because of their high flexibility . in this paper , we propose a novel causal inference algorithm called least-squares independence regression . least-squares independence regression learns the additive noise model through minimization of an estimator of the squared-loss mutual information between inputs and residuals . a notable advantage of least-squares independence regression over existing approaches is that tuning parameters such as the kernel width and the regularization parameter can be naturally optimized by cross-validation , allowing us to avoid overfitting in a data-dependent fashion . through experiments with real-world datasets , we show that least-squares independence regression compares favorably with the state-of-the-art causal inference method ."
  },
  {
    "title": "An efficient algorithm for Co-segmentation .",
    "entities": [
      "markov random field based segmen-tation",
      "maximum flow procedure",
      "optimization model",
      "histogram differences",
      "optimization problems",
      "foreground appearance",
      "polynomial time",
      "foreground pixels",
      "gaussian prior",
      "simultaneous segmentation",
      "co-segmentation problem",
      "images"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <material>",
    "relations": [
      "maximum flow procedure -- USED-FOR -- polynomial time",
      "markov random field based segmen-tation -- USED-FOR -- co-segmentation problem"
    ],
    "abstract": "this paper is focused on the <task_10> -lsb- 1 -rsb- -- where the objective is to segment a similar object from a pair of <material_11> . the background in the two <material_11> may be arbitrary ; therefore , <method_9> of both <material_11> must be performed with a requirement that the appearance of the two sets of <otherscientificterm_7> in the respective <material_11> are consistent . existing approaches -lsb- 1 , 2 -rsb- cast this <task_10> as a <method_0> of the image pair with a regularized difference of the two histograms -- assuming a <otherscientificterm_8> on the <otherscientificterm_5> -lsb- 1 -rsb- or by calculating the sum of squared differences -lsb- 2 -rsb- . both are interesting formulations but lead to difficult <task_4> , due to the presence of the second -lrb- histogram difference -rrb- term . the model proposed here bypasses measurement of the <otherscientificterm_3> in a direct fashion ; we show that this enables obtaining efficient solutions to the underlying <method_2> . our new algorithm is similar to the existing methods in spirit , but differs substantially in that it can be solved to optimal-ity in <otherscientificterm_6> using a <method_1> on an appropriately constructed graph . we discuss our ideas and present promising experimental results .",
    "abstract_og": "this paper is focused on the co-segmentation problem -lsb- 1 -rsb- -- where the objective is to segment a similar object from a pair of images . the background in the two images may be arbitrary ; therefore , simultaneous segmentation of both images must be performed with a requirement that the appearance of the two sets of foreground pixels in the respective images are consistent . existing approaches -lsb- 1 , 2 -rsb- cast this co-segmentation problem as a markov random field based segmen-tation of the image pair with a regularized difference of the two histograms -- assuming a gaussian prior on the foreground appearance -lsb- 1 -rsb- or by calculating the sum of squared differences -lsb- 2 -rsb- . both are interesting formulations but lead to difficult optimization problems , due to the presence of the second -lrb- histogram difference -rrb- term . the model proposed here bypasses measurement of the histogram differences in a direct fashion ; we show that this enables obtaining efficient solutions to the underlying optimization model . our new algorithm is similar to the existing methods in spirit , but differs substantially in that it can be solved to optimal-ity in polynomial time using a maximum flow procedure on an appropriately constructed graph . we discuss our ideas and present promising experimental results ."
  },
  {
    "title": "Database Pruning for Unsupervised Building of Text-To-Speech Voices .",
    "entities": [
      "unit selection speech synthesis techniques",
      "speech recognition confidence measures",
      "automatic segmentation of databases",
      "manually supervised tasks",
      "quality systems",
      "segmentation processes",
      "phonetic transcription",
      "segmentation errors"
    ],
    "types": "<method> <method> <task> <task> <method> <method> <task> <otherscientificterm>",
    "relations": [
      "phonetic transcription -- HYPONYM-OF -- manually supervised tasks"
    ],
    "abstract": "unit selection speech synthesis techniques lead the speech synthesis state of the art . <task_2> is necessary in order to build new voices . they may contain errors and <method_5> may introduce some more . <method_4> require a significant effort to find and correct these <otherscientificterm_7> . <task_6> is crucial and is one of the <task_3> . the possibility to automatically remove incorrectly transcribed units from the inventory will help to make the process more automatic . here we present a new technique based on <method_1> that reaches to remove 90 % of incorrectly transcribed units from a database . the cost for it is loosing only a 10 % of correctly transcribed units .",
    "abstract_og": "unit selection speech synthesis techniques lead the speech synthesis state of the art . automatic segmentation of databases is necessary in order to build new voices . they may contain errors and segmentation processes may introduce some more . quality systems require a significant effort to find and correct these segmentation errors . phonetic transcription is crucial and is one of the manually supervised tasks . the possibility to automatically remove incorrectly transcribed units from the inventory will help to make the process more automatic . here we present a new technique based on speech recognition confidence measures that reaches to remove 90 % of incorrectly transcribed units from a database . the cost for it is loosing only a 10 % of correctly transcribed units ."
  },
  {
    "title": "A Hassle-Free Unsupervised Domain Adaptation Method Using Instance Similarity Features .",
    "entities": [
      "unlabeled target domain instances",
      "labeled source domain instances",
      "unsupervised domain adaptation method",
      "unsu-pervised domain adaptation method",
      "instance similarity features",
      "computational cost",
      "nlp tasks",
      "nlp",
      "scl"
    ],
    "types": "<otherscientificterm> <material> <method> <method> <otherscientificterm> <metric> <task> <task> <method>",
    "relations": [
      "instance similarity features -- USED-FOR -- labeled source domain instances",
      "unsu-pervised domain adaptation method -- COMPARE -- scl",
      "unlabeled target domain instances -- USED-FOR -- instance similarity features",
      "unsupervised domain adaptation method -- USED-FOR -- nlp",
      "computational cost -- EVALUATE-FOR -- scl",
      "unsu-pervised domain adaptation method -- USED-FOR -- nlp tasks",
      "scl -- HYPONYM-OF -- nlp tasks",
      "unlabeled target domain instances -- USED-FOR -- unsu-pervised domain adaptation method",
      "computational cost -- EVALUATE-FOR -- unsu-pervised domain adaptation method",
      "scl -- HYPONYM-OF -- unsupervised domain adaptation method",
      "nlp tasks -- EVALUATE-FOR -- unsu-pervised domain adaptation method"
    ],
    "abstract": "we present a simple yet effective <method_3> that can be generally applied for different <task_6> . our <method_3> uses <otherscientificterm_0> to induce a set of <otherscientificterm_4> . these <otherscientificterm_4> are then combined with the original <otherscientificterm_4> to represent <material_1> . using three <task_6> , we show that our <method_3> consistently out-performs a few baselines , including <method_8> , an existing general <method_2> widely used in <task_7> . more importantly , our <method_3> is very easy to implement and incurs much less <metric_5> than <method_8> .",
    "abstract_og": "we present a simple yet effective unsu-pervised domain adaptation method that can be generally applied for different nlp tasks . our unsu-pervised domain adaptation method uses unlabeled target domain instances to induce a set of instance similarity features . these instance similarity features are then combined with the original instance similarity features to represent labeled source domain instances . using three nlp tasks , we show that our unsu-pervised domain adaptation method consistently out-performs a few baselines , including scl , an existing general unsupervised domain adaptation method widely used in nlp . more importantly , our unsu-pervised domain adaptation method is very easy to implement and incurs much less computational cost than scl ."
  },
  {
    "title": "An evaluation of unsupervised acoustic model training for a dysarthric speech interface .",
    "entities": [
      "acoustic unit descriptors",
      "unsupervised acoustic model training approaches",
      "vector quantization",
      "speaker-independent phoneme recognition baseline",
      "unsupervised acoustic model training",
      "slot filling f-score",
      "hidden markov models",
      "home automation task",
      "frame-based gaussian posteriorgrams",
      "recognition rates",
      "gaussian posteri-orgram",
      "unsupervised fashion",
      "dysarthric speech",
      "posteriorgram-based representations",
      "dysarthric-speech recognition",
      "posteriorgrams"
    ],
    "types": "<method> <method> <method> <method> <method> <metric> <method> <task> <method> <metric> <method> <method> <material> <method> <task> <method>",
    "relations": [
      "unsupervised acoustic model training approaches -- USED-FOR -- dysarthric-speech recognition",
      "unsupervised fashion -- USED-FOR -- frame-based gaussian posteriorgrams",
      "acoustic unit descriptors -- HYPONYM-OF -- frame-based gaussian posteriorgrams",
      "unsupervised acoustic model training approaches -- COMPARE -- speaker-independent phoneme recognition baseline",
      "recognition rates -- EVALUATE-FOR -- unsupervised acoustic model training approaches",
      "posteriorgrams -- USED-FOR -- acoustic unit descriptors",
      "recognition rates -- EVALUATE-FOR -- speaker-independent phoneme recognition baseline",
      "gaussian posteri-orgram -- EVALUATE-FOR -- posteriorgram-based representations",
      "recognition rates -- EVALUATE-FOR -- posteriorgram-based representations",
      "vector quantization -- USED-FOR -- frame-based gaussian posteriorgrams",
      "unsupervised acoustic model training -- USED-FOR -- dysarthric speech",
      "recognition rates -- EVALUATE-FOR -- acoustic unit descriptors"
    ],
    "abstract": "in this paper , we investigate <method_1> for <task_14> . these <method_1> are first , <method_8> , obtained from <method_2> , second , so-called <method_0> , which are <method_6> of phone-like units , that are trained in an <method_11> , and , third , <method_15> computed on the <method_0> . experiments were carried out on a database collected from a <task_7> and containing nine speakers , of which seven are considered to utter <material_12> . all <method_1> delivered significantly better <metric_9> than a <method_3> , showing the suitability of <method_4> for <material_12> . while the <method_0> led to the most compact representation of an utterance for the subsequent semantic inference stage , <method_13> resulted in higher <metric_9> , with the <method_10> achieving the highest <metric_5> of 97.02 % .",
    "abstract_og": "in this paper , we investigate unsupervised acoustic model training approaches for dysarthric-speech recognition . these unsupervised acoustic model training approaches are first , frame-based gaussian posteriorgrams , obtained from vector quantization , second , so-called acoustic unit descriptors , which are hidden markov models of phone-like units , that are trained in an unsupervised fashion , and , third , posteriorgrams computed on the acoustic unit descriptors . experiments were carried out on a database collected from a home automation task and containing nine speakers , of which seven are considered to utter dysarthric speech . all unsupervised acoustic model training approaches delivered significantly better recognition rates than a speaker-independent phoneme recognition baseline , showing the suitability of unsupervised acoustic model training for dysarthric speech . while the acoustic unit descriptors led to the most compact representation of an utterance for the subsequent semantic inference stage , posteriorgram-based representations resulted in higher recognition rates , with the gaussian posteri-orgram achieving the highest slot filling f-score of 97.02 % ."
  },
  {
    "title": "Comparative evaluation of the dual transform domain echo canceller for DMT-based systems .",
    "entities": [
      "dual transform domain echo canceller",
      "time domain signals",
      "data toeplitz matrix",
      "dmt-based communication systems",
      "low error floor",
      "digital echo cancellers",
      "toeplitz matrix",
      "full-duplex transmission",
      "transmitted signal",
      "computational cost",
      "computational complexity",
      "adaptive filters",
      "convergence curves",
      "complexity",
      "echo"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <task> <otherscientificterm> <metric> <metric> <method> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "convergence curves -- EVALUATE-FOR -- dual transform domain echo canceller",
      "toeplitz matrix -- USED-FOR -- time domain signals",
      "digital echo cancellers -- USED-FOR -- echo",
      "transmitted signal -- PART-OF -- toeplitz matrix",
      "adaptive filters -- USED-FOR -- echo",
      "convergence curves -- CONJUNCTION -- computational cost",
      "full-duplex transmission -- FEATURE-OF -- dmt-based communication systems",
      "computational cost -- EVALUATE-FOR -- dual transform domain echo canceller"
    ],
    "abstract": "in <method_3> where <task_7> is required , <method_5> are employed to cancel <otherscientificterm_14> by means of <method_11> . in order to reduce the <metric_10> of these cancellers , the structure of the <otherscientificterm_6> containing the <otherscientificterm_8> is usually exploited to transform the <otherscientificterm_1> and perform the emulation and adaptive update in a more convenient domain -lrb- e.g. frequency domain -rrb- . in this paper , we consider a recently proposed <method_0> , which is based on the general decomposition of the <otherscientificterm_2> . a comprehensive comparative performance evaluation of the proposed <method_0> with the existing methods is provided . this evaluation includes the comparison of the <otherscientificterm_12> and <metric_9> of the <method_0> . the comparison shows that the proposed <method_0> achieves a faster convergence with a <otherscientificterm_4> with no increase in the <metric_13> .",
    "abstract_og": "in dmt-based communication systems where full-duplex transmission is required , digital echo cancellers are employed to cancel echo by means of adaptive filters . in order to reduce the computational complexity of these cancellers , the structure of the toeplitz matrix containing the transmitted signal is usually exploited to transform the time domain signals and perform the emulation and adaptive update in a more convenient domain -lrb- e.g. frequency domain -rrb- . in this paper , we consider a recently proposed dual transform domain echo canceller , which is based on the general decomposition of the data toeplitz matrix . a comprehensive comparative performance evaluation of the proposed dual transform domain echo canceller with the existing methods is provided . this evaluation includes the comparison of the convergence curves and computational cost of the dual transform domain echo canceller . the comparison shows that the proposed dual transform domain echo canceller achieves a faster convergence with a low error floor with no increase in the complexity ."
  },
  {
    "title": "Multiple classifiers by constrained minimization .",
    "entities": [
      "nasal/oral vowel classification task",
      "combination strategy",
      "classification accuracy",
      "artificial data",
      "classification errors",
      "error rate",
      "ensemble",
      "classifier",
      "classifiers"
    ],
    "types": "<task> <method> <metric> <material> <otherscientificterm> <metric> <method> <method> <method>",
    "relations": [
      "combination strategy -- USED-FOR -- classifiers",
      "classification accuracy -- EVALUATE-FOR -- classifier",
      "error rate -- EVALUATE-FOR -- classifiers",
      "classifiers -- USED-FOR -- classification accuracy",
      "classifiers -- COMPARE -- classifier",
      "artificial data -- CONJUNCTION -- nasal/oral vowel classification task"
    ],
    "abstract": "the paper describes an approach to combining multiple <method_8> in order to improve <metric_2> . since individual <method_8> in the <method_6> should somehow be uncorrelated to yield higher <metric_2> than a single <method_7> , we propose to train <method_8> by minimizing the correlation between their <otherscientificterm_4> . a simple <method_1> for three <method_8> is then proposed and its achievable <metric_5> is analyzed and compared to individual single <method_7> performance . the proposed approach has been evaluated on <material_3> and a <task_0> . theoretical analyses and experimental results illustrate the effectiveness of the proposed approach .",
    "abstract_og": "the paper describes an approach to combining multiple classifiers in order to improve classification accuracy . since individual classifiers in the ensemble should somehow be uncorrelated to yield higher classification accuracy than a single classifier , we propose to train classifiers by minimizing the correlation between their classification errors . a simple combination strategy for three classifiers is then proposed and its achievable error rate is analyzed and compared to individual single classifier performance . the proposed approach has been evaluated on artificial data and a nasal/oral vowel classification task . theoretical analyses and experimental results illustrate the effectiveness of the proposed approach ."
  },
  {
    "title": "Model Checking Probabilistic Knowledge : A PSPACE Case .",
    "entities": [
      "model checking probabilistic knowledge of memory-ful semantics",
      "reachability of probabilistic knowledge",
      "undecidable model checking problems",
      "pspace-complete case",
      "limit-sure knowledge",
      "atomic propositions",
      "syntactic restrictions",
      "logic language",
      "ltl"
    ],
    "types": "<task> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "syntactic restrictions -- USED-FOR -- logic language"
    ],
    "abstract": "model checking probabilistic knowledge of memory-ful semantics is undecidable , even for a simple formula concerning the <otherscientificterm_1> of a single agent . this result suggests that the usual approach of tackling <task_2> , by finding <otherscientificterm_6> over the <otherscientificterm_7> , may not suffice . in this paper , we propose to work with an additional restriction that agent 's knowledge concerns a special class of <otherscientificterm_5> . a <material_3> is identified with this additional restriction , for a <otherscientificterm_7> combining <method_8> with <otherscientificterm_4> of a single agent .",
    "abstract_og": "model checking probabilistic knowledge of memory-ful semantics is undecidable , even for a simple formula concerning the reachability of probabilistic knowledge of a single agent . this result suggests that the usual approach of tackling undecidable model checking problems , by finding syntactic restrictions over the logic language , may not suffice . in this paper , we propose to work with an additional restriction that agent 's knowledge concerns a special class of atomic propositions . a pspace-complete case is identified with this additional restriction , for a logic language combining ltl with limit-sure knowledge of a single agent ."
  },
  {
    "title": "Optimized distributed 2D transforms for irregularly sampled sensor network grids using wavelet lifting .",
    "entities": [
      "dynamic programming algorithms",
      "minimum cost coding scheme assignment",
      "energy-efficient lifting-based 2d transform",
      "irregular spatial sampling",
      "wireless sensor networks",
      "recursive dp formulation",
      "2d transforms",
      "data decorrelation",
      "wavelet decomposition",
      "backward transmissions",
      "unidirectional computation"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "dynamic programming algorithms -- USED-FOR -- energy-efficient lifting-based 2d transform",
      "energy-efficient lifting-based 2d transform -- USED-FOR -- wireless sensor networks",
      "energy-efficient lifting-based 2d transform -- USED-FOR -- unidirectional computation",
      "minimum cost coding scheme assignment -- USED-FOR -- energy-efficient lifting-based 2d transform"
    ],
    "abstract": "we address the design and optimization of an <method_2> for <method_4> with <method_3> . the <method_2> is designed to allow for <task_10> found in existing path-wise transforms , thereby eliminating costly <otherscientificterm_9> often required by existing <otherscientificterm_6> , while simultaneously achieving greater <otherscientificterm_7> than those path-wise transforms . we also propose a framework for optimizing the <method_2> via an extension of standard <method_0> , where a selection is made among alternative coding schemes -lrb- e.g. , different number of levels in the <otherscientificterm_8> -rrb- . a <method_5> is provided and an algorithm is given that finds the <method_1> for our proposed <method_2> .",
    "abstract_og": "we address the design and optimization of an energy-efficient lifting-based 2d transform for wireless sensor networks with irregular spatial sampling . the energy-efficient lifting-based 2d transform is designed to allow for unidirectional computation found in existing path-wise transforms , thereby eliminating costly backward transmissions often required by existing 2d transforms , while simultaneously achieving greater data decorrelation than those path-wise transforms . we also propose a framework for optimizing the energy-efficient lifting-based 2d transform via an extension of standard dynamic programming algorithms , where a selection is made among alternative coding schemes -lrb- e.g. , different number of levels in the wavelet decomposition -rrb- . a recursive dp formulation is provided and an algorithm is given that finds the minimum cost coding scheme assignment for our proposed energy-efficient lifting-based 2d transform ."
  },
  {
    "title": "LPQP for MAP : Putting LP Solvers to Better Use .",
    "entities": [
      "linear programming based relaxation",
      "quadratic programming relaxation",
      "lp pairwise auxiliary variables",
      "synthetic and real-world data",
      "qp equivalent terms",
      "non-convex objective",
      "map inference",
      "lp relaxation",
      "kullback-leibler divergence",
      "belief propagation",
      "dual decomposition"
    ],
    "types": "<method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "belief propagation -- CONJUNCTION -- dual decomposition"
    ],
    "abstract": "map inference for general energy functions remains a challenging problem . while most efforts are channeled towards improving the <method_0> , this work is motivated by the <method_1> . we propose a novel <task_6> that penalizes the <otherscientificterm_8> between the <otherscientificterm_2> , and <otherscientificterm_4> given by the product of the unar-ies . we develop two efficient algorithms based on variants of this relaxation . the algorithms minimize the <otherscientificterm_5> using <method_9> and <otherscientificterm_10> as building blocks . experiments on <material_3> show that the solutions returned by our algorithms substantially improve over the <otherscientificterm_7> .",
    "abstract_og": "map inference for general energy functions remains a challenging problem . while most efforts are channeled towards improving the linear programming based relaxation , this work is motivated by the quadratic programming relaxation . we propose a novel map inference that penalizes the kullback-leibler divergence between the lp pairwise auxiliary variables , and qp equivalent terms given by the product of the unar-ies . we develop two efficient algorithms based on variants of this relaxation . the algorithms minimize the non-convex objective using belief propagation and dual decomposition as building blocks . experiments on synthetic and real-world data show that the solutions returned by our algorithms substantially improve over the lp relaxation ."
  },
  {
    "title": "Robust speech dereverberation based on non-negativity and sparse nature of speech spectrograms .",
    "entities": [
      "sparse nature of speech spectrograms",
      "non-negative matrix factor deconvolution",
      "blind deconvolution problem",
      "blind dereverberation method",
      "iterative algorithm",
      "subband envelope",
      "reverberant version",
      "non-negative constraints",
      "speaker movement",
      "optimization"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task>",
    "relations": [
      "non-negative constraints -- FEATURE-OF -- blind deconvolution problem",
      "blind dereverberation method -- USED-FOR -- speaker movement",
      "blind dereverberation method -- USED-FOR -- subband envelope",
      "iterative algorithm -- USED-FOR -- optimization"
    ],
    "abstract": "this paper presents a <method_3> designed to recover the <otherscientificterm_5> of an original speech signal from its <otherscientificterm_6> . the problem is formulated as a <task_2> with <otherscientificterm_7> , regularized by the <otherscientificterm_0> . we derive an <method_4> for its <task_9> , which can be seen as a special case of the <otherscientificterm_1> . we confirmed through experiments that the <method_3> is fast and robust to <task_8> .",
    "abstract_og": "this paper presents a blind dereverberation method designed to recover the subband envelope of an original speech signal from its reverberant version . the problem is formulated as a blind deconvolution problem with non-negative constraints , regularized by the sparse nature of speech spectrograms . we derive an iterative algorithm for its optimization , which can be seen as a special case of the non-negative matrix factor deconvolution . we confirmed through experiments that the blind dereverberation method is fast and robust to speaker movement ."
  },
  {
    "title": "Object recognition and segmentation by non-rigid quasi-dense matching .",
    "entities": [
      "non-rigid deformations of the imaged surfaces",
      "local affine transformation estimates",
      "non-rigid quasi-dense matching method",
      "match propagation algorithm",
      "global geometric constraints",
      "local image gradients",
      "geometrically consistent matches",
      "non-rigid image registration",
      "geometrically consistent groups",
      "quasi-dense pixel matches",
      "partial occlusion",
      "quasi-dense matching",
      "background clutter",
      "viewpoint changes",
      "local properties",
      "object recognition",
      "recognition criterion",
      "geometric deformations",
      "propagation"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "local image gradients -- USED-FOR -- match propagation algorithm",
      "local affine transformation estimates -- USED-FOR -- non-rigid quasi-dense matching method",
      "background clutter -- CONJUNCTION -- partial occlusion",
      "non-rigid quasi-dense matching method -- USED-FOR -- quasi-dense pixel matches",
      "partial occlusion -- CONJUNCTION -- viewpoint changes",
      "non-rigid quasi-dense matching method -- USED-FOR -- non-rigid image registration",
      "match propagation algorithm -- USED-FOR -- non-rigid quasi-dense matching method",
      "quasi-dense matching -- USED-FOR -- non-rigid quasi-dense matching method",
      "viewpoint changes -- CONJUNCTION -- geometric deformations",
      "local image gradients -- USED-FOR -- non-rigid quasi-dense matching method",
      "non-rigid quasi-dense matching method -- USED-FOR -- object recognition"
    ],
    "abstract": "in this paper , we present a <method_2> and its application to <task_15> and segmentation . the <method_2> is based on the <method_3> which is here extended by using <otherscientificterm_5> for adapting the <task_18> to smooth <otherscientificterm_0> . the adaptation is based entirely on the <otherscientificterm_14> of the images and the <method_2> can be hence used in <task_7> where <otherscientificterm_4> are not available . our <method_2> for <task_15> and seg-mentation is directly built on the <task_11> . the <otherscientificterm_9> between the <method_2> and test images are grouped into <otherscientificterm_8> using a <method_2> which utilizes the <otherscientificterm_1> obtained during the <task_18> . the number and quality of <otherscientificterm_6> is used as a <otherscientificterm_16> and the location of the matching pixels directly provides the segmentation . the experiments demonstrate that our <method_2> is able to deal with extensive <otherscientificterm_12> , <otherscientificterm_10> , large scale and <otherscientificterm_13> , and notable <otherscientificterm_17> .",
    "abstract_og": "in this paper , we present a non-rigid quasi-dense matching method and its application to object recognition and segmentation . the non-rigid quasi-dense matching method is based on the match propagation algorithm which is here extended by using local image gradients for adapting the propagation to smooth non-rigid deformations of the imaged surfaces . the adaptation is based entirely on the local properties of the images and the non-rigid quasi-dense matching method can be hence used in non-rigid image registration where global geometric constraints are not available . our non-rigid quasi-dense matching method for object recognition and seg-mentation is directly built on the quasi-dense matching . the quasi-dense pixel matches between the non-rigid quasi-dense matching method and test images are grouped into geometrically consistent groups using a non-rigid quasi-dense matching method which utilizes the local affine transformation estimates obtained during the propagation . the number and quality of geometrically consistent matches is used as a recognition criterion and the location of the matching pixels directly provides the segmentation . the experiments demonstrate that our non-rigid quasi-dense matching method is able to deal with extensive background clutter , partial occlusion , large scale and viewpoint changes , and notable geometric deformations ."
  },
  {
    "title": "A Scalable Interdependent Multi-Issue Negotiation Protocol for Energy Exchange .",
    "entities": [
      "interdependent multi-issue negotiation",
      "renewable energy generation",
      "off-grid homes",
      "negotiation protocol",
      "strategy profile",
      "pareto-optimal outcomes",
      "electricity storage"
    ],
    "types": "<task> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "negotiation protocol -- USED-FOR -- off-grid homes",
      "renewable energy generation -- CONJUNCTION -- electricity storage"
    ],
    "abstract": "we present a novel <method_3> to facilitate energy exchange between <otherscientificterm_2> that are equipped with <task_1> and <method_6> . our <method_3> imposes restrictions over negotiation such that <method_3> reduces the complex <task_0> to one where agents have a <otherscientificterm_4> in subgame perfect nash equilibrium . we show that our <method_3> is concurrent , scalable and ; under certain conditions ; leads to <otherscientificterm_5> .",
    "abstract_og": "we present a novel negotiation protocol to facilitate energy exchange between off-grid homes that are equipped with renewable energy generation and electricity storage . our negotiation protocol imposes restrictions over negotiation such that negotiation protocol reduces the complex interdependent multi-issue negotiation to one where agents have a strategy profile in subgame perfect nash equilibrium . we show that our negotiation protocol is concurrent , scalable and ; under certain conditions ; leads to pareto-optimal outcomes ."
  },
  {
    "title": "Continuous Relaxations for Discrete Hamiltonian Monte Carlo .",
    "entities": [
      "estimating normalization constants -lrb- partition functions",
      "continuous relaxation inference algorithms",
      "gradient-based hamiltonian monte carlo",
      "discrete variable undirected models",
      "gaussian integral trick",
      "approximate probabilistic inference",
      "continuous representation",
      "discrete optimization",
      "continuous systems",
      "illustrative problems",
      "continuous relaxations",
      "inference"
    ],
    "types": "<task> <method> <method> <method> <method> <task> <method> <task> <method> <task> <otherscientificterm> <task>",
    "relations": [
      "illustrative problems -- USED-FOR -- continuous relaxation inference algorithms",
      "gaussian integral trick -- USED-FOR -- discrete variable undirected models",
      "gradient-based hamiltonian monte carlo -- USED-FOR -- inference",
      "continuous representation -- USED-FOR -- inference",
      "continuous relaxations -- USED-FOR -- discrete optimization"
    ],
    "abstract": "continuous relaxations play an important role in <task_7> , but have not seen much use in <task_5> . here we show that a general form of the <method_4> makes it possible to transform a wide class of <method_3> into fully <method_8> . the <method_6> allows the use of <method_2> for <task_11> , results in new ways of <task_0> -rrb- , and in general opens up a number of new avenues for <task_11> in difficult discrete systems . we demonstrate some of these <method_1> on a number of <task_9> .",
    "abstract_og": "continuous relaxations play an important role in discrete optimization , but have not seen much use in approximate probabilistic inference . here we show that a general form of the gaussian integral trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems . the continuous representation allows the use of gradient-based hamiltonian monte carlo for inference , results in new ways of estimating normalization constants -lrb- partition functions -rrb- , and in general opens up a number of new avenues for inference in difficult discrete systems . we demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems ."
  },
  {
    "title": "Detection of OOV words using generalized word models and a semantic class language model .",
    "entities": [
      "recall and precision rates",
      "generalized word models",
      "context-independent crossword models",
      "spontaneous speech",
      "language model",
      "semantic categories",
      "theoretic optimum",
      "out-of-vocabulary words",
      "oov word",
      "recognition accuracy",
      "oov-detection",
      "crossword"
    ],
    "types": "<metric> <method> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <task> <method>",
    "relations": [
      "semantic categories -- USED-FOR -- language model",
      "recognition accuracy -- EVALUATE-FOR -- generalized word models",
      "generalized word models -- USED-FOR -- language model",
      "language model -- USED-FOR -- spontaneous speech"
    ],
    "abstract": "this paper describes an approach to detect <otherscientificterm_7> in <material_3> using a <method_4> built on <otherscientificterm_5> and a new type of <method_1> consisting of a mixture of specific and general acoustic units . we demonstrate the construction of the <method_1> as replacements for surnames in a german spontaneous travel planning task gsst -lsb- 1 -rsb- . we show that the use of our <method_1> improves <metric_9> in cases where <otherscientificterm_7> appear and does not lead to a degradation of the overall <metric_9> . in our experiments we measured <metric_0> of <task_10> which are close to their <otherscientificterm_6> . furthermore , we compared the effect of using <method_11> - triphones vs. using <method_2> . we show that when using <method_1> with <method_11> - triphones , the expected number of consequential errors following an <otherscientificterm_8> can be reduced significantly by 37 % .",
    "abstract_og": "this paper describes an approach to detect out-of-vocabulary words in spontaneous speech using a language model built on semantic categories and a new type of generalized word models consisting of a mixture of specific and general acoustic units . we demonstrate the construction of the generalized word models as replacements for surnames in a german spontaneous travel planning task gsst -lsb- 1 -rsb- . we show that the use of our generalized word models improves recognition accuracy in cases where out-of-vocabulary words appear and does not lead to a degradation of the overall recognition accuracy . in our experiments we measured recall and precision rates of oov-detection which are close to their theoretic optimum . furthermore , we compared the effect of using crossword - triphones vs. using context-independent crossword models . we show that when using generalized word models with crossword - triphones , the expected number of consequential errors following an oov word can be reduced significantly by 37 % ."
  },
  {
    "title": "A Study on Bootstrapping Bilingual Vector Spaces from Non-Parallel Data -LRB- and Nothing Else -RRB- .",
    "entities": [
      "bilingual lexicon extraction",
      "induced bilingual vector spaces",
      "language pair agnostic approach",
      "bilingual vector spaces",
      "bootstrapping fashion",
      "confidence estimation",
      "non-parallel data",
      "seed lexicon"
    ],
    "types": "<task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric> <material> <otherscientificterm>",
    "relations": [
      "language pair agnostic approach -- USED-FOR -- bilingual lexicon extraction",
      "non-parallel data -- USED-FOR -- language pair agnostic approach"
    ],
    "abstract": "we present a new <method_2> to inducing <otherscientificterm_3> from <material_6> without any other resource in a <otherscientificterm_4> . the paper systematically introduces and describes all key elements of the <method_2> : -lrb- 1 -rrb- starting point or <otherscientificterm_7> , -lrb- 2 -rrb- the <metric_5> and selection of new dimensions of the space , and -lrb- 3 -rrb- convergence . we test the quality of the <otherscientificterm_1> , and analyze the influence of the different components of the <method_2> in the task of <task_0> for two language pairs . results reveal that , contrary to conclusions from prior work , the seeding of the <method_2> has a heavy impact on the quality of the learned lexicons . we also show that our <method_2> outperforms the best performing fully corpus-based ble methods on these test sets .",
    "abstract_og": "we present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion . the paper systematically introduces and describes all key elements of the language pair agnostic approach : -lrb- 1 -rrb- starting point or seed lexicon , -lrb- 2 -rrb- the confidence estimation and selection of new dimensions of the space , and -lrb- 3 -rrb- convergence . we test the quality of the induced bilingual vector spaces , and analyze the influence of the different components of the language pair agnostic approach in the task of bilingual lexicon extraction for two language pairs . results reveal that , contrary to conclusions from prior work , the seeding of the language pair agnostic approach has a heavy impact on the quality of the learned lexicons . we also show that our language pair agnostic approach outperforms the best performing fully corpus-based ble methods on these test sets ."
  },
  {
    "title": "Halftone visual cryptography by iterative halftoning .",
    "entities": [
      "halftone visual cryptography",
      "iterative halftoning method",
      "visual sharing scheme",
      "constrained iterative halftoning",
      "reconstructed secret images",
      "binary valued shares",
      "halftone shares",
      "natural images",
      "construction method",
      "image quality",
      "secret image"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material> <method> <metric> <material>",
    "relations": [
      "iterative halftoning method -- USED-FOR -- construction method",
      "construction method -- USED-FOR -- halftone shares",
      "iterative halftoning method -- USED-FOR -- halftone visual cryptography",
      "halftone visual cryptography -- HYPONYM-OF -- visual sharing scheme",
      "construction method -- USED-FOR -- halftone visual cryptography",
      "secret image -- PART-OF -- binary valued shares"
    ],
    "abstract": "halftone visual cryptography -lrb- <method_0> -rrb- is a <method_2> where a <material_10> is encoded into <otherscientificterm_6> taking meaningful visual information . in this paper , novel <method_8> of <method_0> based on an <method_1> is proposed . the <material_10> is concurrently embedded into <otherscientificterm_5> while these shares are halftoned by <otherscientificterm_3> . the proposed <method_8> is able to generate <otherscientificterm_6> showing <material_7> with high <metric_9> . <material_4> , obtained by stacking qualified shares together , does not suffer from cross interference of share images . simulations are provided to show the effectiveness of our proposed <method_8> .",
    "abstract_og": "halftone visual cryptography -lrb- halftone visual cryptography -rrb- is a visual sharing scheme where a secret image is encoded into halftone shares taking meaningful visual information . in this paper , novel construction method of halftone visual cryptography based on an iterative halftoning method is proposed . the secret image is concurrently embedded into binary valued shares while these shares are halftoned by constrained iterative halftoning . the proposed construction method is able to generate halftone shares showing natural images with high image quality . reconstructed secret images , obtained by stacking qualified shares together , does not suffer from cross interference of share images . simulations are provided to show the effectiveness of our proposed construction method ."
  },
  {
    "title": "Bi-orthogonal filter banks with directional vanishing moments -LSB- image representation applications -RSB- .",
    "entities": [
      "directional vanishing moments",
      "2-d nonseparable filter banks",
      "snr and visual quality",
      "contourlet transform",
      "nonlinear approximation",
      "mapping technique",
      "design problem",
      "design procedure",
      "filters"
    ],
    "types": "<otherscientificterm> <method> <metric> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm>",
    "relations": [
      "mapping technique -- USED-FOR -- design procedure"
    ],
    "abstract": "in this paper we study <method_1> that annihilate information along a certain discrete direction . this is done by having <otherscientificterm_8> with <otherscientificterm_0> . we study the approximation property of such <otherscientificterm_8> and the <task_6> providing conditions for its solvability . in particular we completely characterize the solution and propose a <method_7> utilizing the <method_5> . <method_4> experiments with the <otherscientificterm_3> indicate that compared with the traditional <otherscientificterm_8> , the new <otherscientificterm_8> designed with <otherscientificterm_0> provide gains in <metric_2> due to their short size .",
    "abstract_og": "in this paper we study 2-d nonseparable filter banks that annihilate information along a certain discrete direction . this is done by having filters with directional vanishing moments . we study the approximation property of such filters and the design problem providing conditions for its solvability . in particular we completely characterize the solution and propose a design procedure utilizing the mapping technique . nonlinear approximation experiments with the contourlet transform indicate that compared with the traditional filters , the new filters designed with directional vanishing moments provide gains in snr and visual quality due to their short size ."
  },
  {
    "title": "Extended Discriminative Random Walk : A Hypergraph Approach to Multi-View Multi-Relational Transductive Learning .",
    "entities": [
      "dis-criminative random walk framework",
      "multi-view , multi-relational data",
      "random walk operator",
      "in-network classification setting",
      "real-life data sources",
      "non-network data",
      "reaction networks",
      "single graphs",
      "random walks",
      "biological data",
      "gene networks",
      "rela-tional information",
      "collaboration networks",
      "multi-way relations",
      "class imbalance",
      "drw framework",
      "class-imbalanced data",
      "multi-view data",
      "transductive inference",
      "multi-way interactions",
      "inference",
      "graph",
      "hypergraphs",
      "edges"
    ],
    "types": "<method> <material> <otherscientificterm> <task> <material> <material> <method> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <material> <material> <task> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "gene networks -- HYPONYM-OF -- real-life data sources",
      "biological data -- CONJUNCTION -- gene networks",
      "collaboration networks -- CONJUNCTION -- reaction networks",
      "reaction networks -- HYPONYM-OF -- multi-way interactions",
      "collaboration networks -- HYPONYM-OF -- multi-way interactions",
      "transductive inference -- USED-FOR -- single graphs",
      "drw framework -- USED-FOR -- inference",
      "biological data -- HYPONYM-OF -- real-life data sources",
      "dis-criminative random walk framework -- USED-FOR -- transductive inference"
    ],
    "abstract": "transductive <task_20> on graphs has been garner-ing increasing attention due to the connected nature of many <material_4> , such as online social media and <material_9> -lrb- protein-protein interaction network , <method_10> , etc. -rrb- . typically <otherscientificterm_11> in the data is encoded as <otherscientificterm_23> in a <otherscientificterm_21> but often it is important to model <otherscientificterm_19> , such as in <method_12> and <method_6> . in this work we model <otherscientificterm_13> as <otherscientificterm_22> and extend the <method_0> , originally proposed for <task_18> on <otherscientificterm_7> , to the case of multiple <otherscientificterm_22> . we use the extended <method_15> for <task_20> on <material_1> in a natural way , by representing attribute descriptions of the data also as <otherscientificterm_22> . we further exploit the structure of <otherscientificterm_22> to modify the <otherscientificterm_2> to take into account <otherscientificterm_14> in the data . this work is among very few approaches to explicitly address <otherscientificterm_14> in the <task_3> , using <otherscientificterm_8> . we compare our approach to methods proposed for <task_20> on <otherscientificterm_22> , and to methods proposed for <material_17> and show that empirically we achieve better performance . we also compare to methods specifically tailored for <material_16> and show that our approach achieves comparable performance even on <material_5> .",
    "abstract_og": "transductive inference on graphs has been garner-ing increasing attention due to the connected nature of many real-life data sources , such as online social media and biological data -lrb- protein-protein interaction network , gene networks , etc. -rrb- . typically rela-tional information in the data is encoded as edges in a graph but often it is important to model multi-way interactions , such as in collaboration networks and reaction networks . in this work we model multi-way relations as hypergraphs and extend the dis-criminative random walk framework , originally proposed for transductive inference on single graphs , to the case of multiple hypergraphs . we use the extended drw framework for inference on multi-view , multi-relational data in a natural way , by representing attribute descriptions of the data also as hypergraphs . we further exploit the structure of hypergraphs to modify the random walk operator to take into account class imbalance in the data . this work is among very few approaches to explicitly address class imbalance in the in-network classification setting , using random walks . we compare our approach to methods proposed for inference on hypergraphs , and to methods proposed for multi-view data and show that empirically we achieve better performance . we also compare to methods specifically tailored for class-imbalanced data and show that our approach achieves comparable performance even on non-network data ."
  },
  {
    "title": "Harnessing Cognitive Features for Sarcasm Detection .",
    "entities": [
      "eye-movement patterns of human readers",
      "sarcastic and non sarcastic sentences",
      "linguistic and stylistic features",
      "readers eye movement data",
      "cogni-tive features",
      "nlp applications",
      "f-score -rrb-",
      "cognitive features",
      "review summarization",
      "sarcasm detection",
      "feature vector",
      "statistical classification",
      "dialog systems",
      "sentiment analysis",
      "sarcasm"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <task> <metric> <otherscientificterm> <task> <task> <method> <task> <task> <task> <otherscientificterm>",
    "relations": [
      "sentiment analysis -- HYPONYM-OF -- nlp applications",
      "review summarization -- CONJUNCTION -- dialog systems",
      "dialog systems -- HYPONYM-OF -- nlp applications",
      "eye-movement patterns of human readers -- USED-FOR -- cogni-tive features",
      "linguistic and stylistic features -- USED-FOR -- sarcasm detection",
      "readers eye movement data -- USED-FOR -- cognitive features",
      "dialog systems -- CONJUNCTION -- sentiment analysis",
      "review summarization -- HYPONYM-OF -- nlp applications",
      "feature vector -- USED-FOR -- sarcasm detection"
    ],
    "abstract": "in this paper , we propose a novel mechanism for enriching the <method_10> , for the task of <task_9> , with <otherscientificterm_4> extracted from <otherscientificterm_0> . <task_9> has been a challenging research problem , and its importance for <task_5> such as <task_8> , <task_12> and <task_13> is well recognized . <otherscientificterm_14> can often be traced to incongruity that becomes apparent as the full sentence unfolds . this presence of incongruity-implicit or explicit-affects the way readers eyes move through the text . we observe the difference in the behaviour of the eye , while reading <material_1> . motivated by this observation , we augment traditional <otherscientificterm_2> for <task_9> with the <otherscientificterm_7> obtained from <material_3> . we perform <task_11> using the enhanced feature set so obtained . the <otherscientificterm_2> improve <task_9> by 3.7 % -lrb- in terms of <metric_6> , over the performance of the best reported system .",
    "abstract_og": "in this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cogni-tive features extracted from eye-movement patterns of human readers . sarcasm detection has been a challenging research problem , and its importance for nlp applications such as review summarization , dialog systems and sentiment analysis is well recognized . sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds . this presence of incongruity-implicit or explicit-affects the way readers eyes move through the text . we observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences . motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data . we perform statistical classification using the enhanced feature set so obtained . the linguistic and stylistic features improve sarcasm detection by 3.7 % -lrb- in terms of f-score -rrb- , over the performance of the best reported system ."
  },
  {
    "title": "A light transport model for mitigating multipath interference in Time-of-flight sensors .",
    "entities": [
      "continuous-wave time-of-flight range imaging",
      "direct and global light transport",
      "multipath interference",
      "scene dependent errors",
      "depth accuracy",
      "kinect sensor",
      "optical reflections",
      "computer vision",
      "tof camera",
      "spatial location",
      "imaging sensor",
      "tof cameras",
      "depth images",
      "graphics"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <material>",
    "relations": [
      "scene dependent errors -- FEATURE-OF -- tof cameras",
      "direct and global light transport -- USED-FOR -- multipath interference",
      "tof cameras -- USED-FOR -- depth images",
      "direct and global light transport -- CONJUNCTION -- tof camera"
    ],
    "abstract": "continuous-wave time-of-flight -lrb- tof -rrb- range imaging has become a commercially viable technology with many applications in <task_7> and <material_13> . however , the <otherscientificterm_12> obtained from <material_11> contain <otherscientificterm_3> due to <otherscientificterm_2> . specifically , <otherscientificterm_2> occurs when multiple <otherscientificterm_6> return to a single <otherscientificterm_9> on the <otherscientificterm_10> . many prior approaches to rectifying <otherscientificterm_2> rely on sparsity in <otherscientificterm_6> , which is an extreme simplification . in this paper , we correct <otherscientificterm_2> by combining the standard measurements from a <method_8> with information from <material_1> . we report results on both simulated experiments and physical experiments -lrb- using the <otherscientificterm_5> -rrb- . our results , evaluated against ground truth , demonstrate a quantitative improvement in <metric_4> .",
    "abstract_og": "continuous-wave time-of-flight -lrb- tof -rrb- range imaging has become a commercially viable technology with many applications in computer vision and graphics . however , the depth images obtained from tof cameras contain scene dependent errors due to multipath interference . specifically , multipath interference occurs when multiple optical reflections return to a single spatial location on the imaging sensor . many prior approaches to rectifying multipath interference rely on sparsity in optical reflections , which is an extreme simplification . in this paper , we correct multipath interference by combining the standard measurements from a tof camera with information from direct and global light transport . we report results on both simulated experiments and physical experiments -lrb- using the kinect sensor -rrb- . our results , evaluated against ground truth , demonstrate a quantitative improvement in depth accuracy ."
  },
  {
    "title": "A Spectral Algorithm for Latent Tree Graphical Models .",
    "entities": [
      "expectation maximization",
      "synthetic and real datasets",
      "local-minimum-free spectral algorithm",
      "latent variable models",
      "parameter learning algorithms",
      "latent variable models",
      "local search heuris-tics",
      "arbitrary tree topologies",
      "probabilistic modeling",
      "joint distribution",
      "speech analysis",
      "observed variables",
      "em",
      "bioinformatics",
      "tree"
    ],
    "types": "<method> <material> <method> <method> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "local search heuris-tics -- USED-FOR -- parameter learning algorithms",
      "parameter learning algorithms -- USED-FOR -- latent variable models",
      "local-minimum-free spectral algorithm -- COMPARE -- em",
      "speech analysis -- CONJUNCTION -- bioinformatics",
      "synthetic and real datasets -- EVALUATE-FOR -- local-minimum-free spectral algorithm",
      "arbitrary tree topologies -- USED-FOR -- local-minimum-free spectral algorithm",
      "expectation maximization -- HYPONYM-OF -- local search heuris-tics",
      "latent variable models -- USED-FOR -- probabilistic modeling",
      "local-minimum-free spectral algorithm -- USED-FOR -- latent variable models"
    ],
    "abstract": "latent variable models are powerful tools for <task_8> , and have been successfully applied to various domains , such as <task_10> and <material_13> . however , <method_4> for <method_5> have predominantly relied on <method_6> such as <method_0> . we propose a fast , <method_2> for learning <method_5> with <otherscientificterm_7> , and show that the <otherscientificterm_9> of the <otherscientificterm_11> can be reconstructed from the marginals of triples of <otherscientificterm_11> irrespective of the maximum degree of the <otherscientificterm_14> . we demonstrate the performance of our <method_2> on <material_1> ; for large training sizes , our <method_2> performs comparable to or better than <method_12> while being orders of magnitude faster .",
    "abstract_og": "latent variable models are powerful tools for probabilistic modeling , and have been successfully applied to various domains , such as speech analysis and bioinformatics . however , parameter learning algorithms for latent variable models have predominantly relied on local search heuris-tics such as expectation maximization . we propose a fast , local-minimum-free spectral algorithm for learning latent variable models with arbitrary tree topologies , and show that the joint distribution of the observed variables can be reconstructed from the marginals of triples of observed variables irrespective of the maximum degree of the tree . we demonstrate the performance of our local-minimum-free spectral algorithm on synthetic and real datasets ; for large training sizes , our local-minimum-free spectral algorithm performs comparable to or better than em while being orders of magnitude faster ."
  },
  {
    "title": "Robust transmission of compressed images over noisy Gaussian channels .",
    "entities": [
      "list-based iterative trellis decoder",
      "uncompressed raw image data",
      "image communication systems",
      "compressed image formats",
      "viterbi decoder",
      "channel errors",
      "reconstructed image",
      "power",
      "post-processor"
    ],
    "types": "<method> <material> <method> <method> <method> <otherscientificterm> <material> <otherscientificterm> <method>",
    "relations": [
      "post-processor -- USED-FOR -- list-based iterative trellis decoder",
      "list-based iterative trellis decoder -- COMPARE -- viterbi decoder"
    ],
    "abstract": "many <method_2> have constraints on bandwidth , <otherscientificterm_7> and time which prohibit transmission of <material_1> . <method_3> , however , are extremely sensitive to bit errors which can seriously degrade the quality of the image at the receiver . a new <method_0> is proposed which accepts feedback from a <method_8> which can detect <otherscientificterm_5> in the <material_6> . experimental results are shown which indicate the new <method_0> provides signiicant improvement over the standard <method_4> .",
    "abstract_og": "many image communication systems have constraints on bandwidth , power and time which prohibit transmission of uncompressed raw image data . compressed image formats , however , are extremely sensitive to bit errors which can seriously degrade the quality of the image at the receiver . a new list-based iterative trellis decoder is proposed which accepts feedback from a post-processor which can detect channel errors in the reconstructed image . experimental results are shown which indicate the new list-based iterative trellis decoder provides signiicant improvement over the standard viterbi decoder ."
  },
  {
    "title": "Quality-fair HTTP adaptive streaming over LTE network .",
    "entities": [
      "quality-fair adaptive streaming solution",
      "http adaptive streaming applications",
      "video content complexity",
      "fair video quality",
      "heterogeneous has users",
      "ssim quality metric",
      "video content characteristics",
      "end-user video quality",
      "channel condition",
      "quality fairness",
      "lte cell",
      "radio resource",
      "wireless channel",
      "video qualities",
      "channel conditions"
    ],
    "types": "<method> <task> <metric> <metric> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <material> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "quality-fair adaptive streaming solution -- USED-FOR -- fair video quality",
      "ssim quality metric -- EVALUATE-FOR -- quality-fair adaptive streaming solution",
      "video content characteristics -- CONJUNCTION -- channel condition",
      "quality fairness -- EVALUATE-FOR -- quality-fair adaptive streaming solution"
    ],
    "abstract": "in <task_1> multiple video clients sharing the same <material_12> may experience different <otherscientificterm_13> as result of both different <metric_2> and different <otherscientificterm_14> . this causes unfairness in the <metric_7> . in this paper , we propose a <method_0> to deliver <metric_3> to has clients competing for the same resources in an <otherscientificterm_10> . in the <method_0> the share of <material_11> is optimized according to <otherscientificterm_6> and <otherscientificterm_8> . the proposed <method_0> is compared with other state-of-the-art strategies and numerical results in terms of <metric_5> shows that <method_0> significantly improves the <metric_9> among <otherscientificterm_4> .",
    "abstract_og": "in http adaptive streaming applications multiple video clients sharing the same wireless channel may experience different video qualities as result of both different video content complexity and different channel conditions . this causes unfairness in the end-user video quality . in this paper , we propose a quality-fair adaptive streaming solution to deliver fair video quality to has clients competing for the same resources in an lte cell . in the quality-fair adaptive streaming solution the share of radio resource is optimized according to video content characteristics and channel condition . the proposed quality-fair adaptive streaming solution is compared with other state-of-the-art strategies and numerical results in terms of ssim quality metric shows that quality-fair adaptive streaming solution significantly improves the quality fairness among heterogeneous has users ."
  },
  {
    "title": "MKPM : A multiclass extension to the kernel projection machine .",
    "entities": [
      "multiclass kernel projection machines",
      "kernel projection machine framework",
      "pattern recognition problems",
      "dynamic programming approach",
      "multiclass case",
      "output codes",
      "global classifier",
      "optimization problem",
      "numerical simulations",
      "co-regularization scheme",
      "projection dimensions"
    ],
    "types": "<method> <method> <task> <method> <task> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm>",
    "relations": [
      "multiclass kernel projection machines -- USED-FOR -- optimization problem",
      "output codes -- USED-FOR -- multiclass kernel projection machines"
    ],
    "abstract": "we introduce <method_0> , a new formalism that extends the <method_1> to the <task_4> . our <method_0> is based on the use of <otherscientificterm_5> and <method_0> implements a <method_9> by simultaneously constraining the <otherscientificterm_10> associated with the individual predictors that constitute the <method_6> . in order to solve the <task_7> posed by our <method_0> , we propose an efficient <method_3> . <method_8> conducted on a few <task_2> illustrate the soundness of our <method_0> .",
    "abstract_og": "we introduce multiclass kernel projection machines , a new formalism that extends the kernel projection machine framework to the multiclass case . our multiclass kernel projection machines is based on the use of output codes and multiclass kernel projection machines implements a co-regularization scheme by simultaneously constraining the projection dimensions associated with the individual predictors that constitute the global classifier . in order to solve the optimization problem posed by our multiclass kernel projection machines , we propose an efficient dynamic programming approach . numerical simulations conducted on a few pattern recognition problems illustrate the soundness of our multiclass kernel projection machines ."
  },
  {
    "title": "Defeasible Inclusions in Low-Complexity DLs : Preliminary Notes .",
    "entities": [
      "circumscribed dl-lite r complexity",
      "cir-cumscribed low-complexity dls",
      "nexp np",
      "polynomial hierarchy",
      "abnormality predicates",
      "existential restrictions",
      "el family",
      "dl-lite",
      "exptime-hard"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "el family -- HYPONYM-OF -- cir-cumscribed low-complexity dls",
      "dl-lite -- HYPONYM-OF -- cir-cumscribed low-complexity dls",
      "dl-lite -- CONJUNCTION -- el family",
      "nexp np -- USED-FOR -- circumscribed dl-lite r complexity"
    ],
    "abstract": "we analyze the complexity of reasoning with <method_1> such as <method_7> and the <method_6> , under suitable restrictions on the use of <otherscientificterm_4> . we prove that in <otherscientificterm_0> drops from <otherscientificterm_2> to the second level of the <otherscientificterm_3> . in el , reasoning remains <otherscientificterm_8> , in general . however , by restricting the possible occurrences of <otherscientificterm_5> , we obtain membership in \u03c3 p 2 and \u03c0 p 2 for an extension of el .",
    "abstract_og": "we analyze the complexity of reasoning with cir-cumscribed low-complexity dls such as dl-lite and the el family , under suitable restrictions on the use of abnormality predicates . we prove that in circumscribed dl-lite r complexity drops from nexp np to the second level of the polynomial hierarchy . in el , reasoning remains exptime-hard , in general . however , by restricting the possible occurrences of existential restrictions , we obtain membership in \u03c3 p 2 and \u03c0 p 2 for an extension of el ."
  },
  {
    "title": "Ordinal regression for interaction quality prediction .",
    "entities": [
      "spearman 's rank correlation coefficient",
      "euclidean and manhattan errors",
      "cohen 's agreement rate",
      "ordinal regression predictor",
      "spoken dialogue system",
      "ordinal regression problem",
      "corpus of dialogues",
      "evaluation metrics",
      "ordinal regression",
      "regression models",
      "classifiers"
    ],
    "types": "<metric> <otherscientificterm> <metric> <method> <method> <task> <material> <metric> <task> <method> <method>",
    "relations": [
      "spearman 's rank correlation coefficient -- CONJUNCTION -- euclidean and manhattan errors",
      "evaluation metrics -- EVALUATE-FOR -- ordinal regression predictor",
      "spearman 's rank correlation coefficient -- HYPONYM-OF -- evaluation metrics",
      "classifiers -- CONJUNCTION -- regression models",
      "euclidean and manhattan errors -- HYPONYM-OF -- evaluation metrics"
    ],
    "abstract": "the automatic prediction of the quality of a dialogue is useful to keep track of a <method_4> 's performance and , if necessary , adapt its behaviour . <method_10> and <method_9> have been suggested to make this prediction . the parameters of these models are learnt from a <material_6> evaluated by users or experts . in this paper , we propose to model this task as an <task_5> . we apply support vector machines for <task_8> on a <material_6> where each system-user exchange was given a rate on a scale of 1 to 5 by experts . compared to previous models proposed in the literature , the <method_3> has significantly better results according to the following <metric_7> : <metric_2> with experts ratings , <metric_0> , and <otherscientificterm_1> .",
    "abstract_og": "the automatic prediction of the quality of a dialogue is useful to keep track of a spoken dialogue system 's performance and , if necessary , adapt its behaviour . classifiers and regression models have been suggested to make this prediction . the parameters of these models are learnt from a corpus of dialogues evaluated by users or experts . in this paper , we propose to model this task as an ordinal regression problem . we apply support vector machines for ordinal regression on a corpus of dialogues where each system-user exchange was given a rate on a scale of 1 to 5 by experts . compared to previous models proposed in the literature , the ordinal regression predictor has significantly better results according to the following evaluation metrics : cohen 's agreement rate with experts ratings , spearman 's rank correlation coefficient , and euclidean and manhattan errors ."
  },
  {
    "title": "Named Entity Translation with Web Mining and Transliteration .",
    "entities": [
      "bilingual con-textual co-occurrence",
      "named entity translation",
      "maximum entropy model",
      "pronunciation similarity",
      "translit-eration approach",
      "translation candidates",
      "transliteration information",
      "web mining",
      "web information",
      "precision",
      "transliteration",
      "recall"
    ],
    "types": "<otherscientificterm> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <task> <metric>",
    "relations": [
      "web information -- USED-FOR -- translit-eration approach",
      "precision -- EVALUATE-FOR -- named entity translation",
      "transliteration information -- USED-FOR -- web mining",
      "precision -- CONJUNCTION -- recall",
      "pronunciation similarity -- USED-FOR -- maximum entropy model",
      "bilingual con-textual co-occurrence -- USED-FOR -- maximum entropy model",
      "recall -- EVALUATE-FOR -- named entity translation",
      "pronunciation similarity -- CONJUNCTION -- bilingual con-textual co-occurrence",
      "maximum entropy model -- USED-FOR -- translation candidates"
    ],
    "abstract": "this paper presents a novel approach to improve the <task_1> by combining a <method_4> with <method_7> , using <otherscientificterm_8> as a source to complement <task_10> , and using <otherscientificterm_6> to guide and enhance <method_7> . a <method_2> is employed to rank <otherscientificterm_5> by combining <otherscientificterm_3> and <otherscientificterm_0> . experimental results show that our approach effectively improves the <metric_9> and <metric_11> of the <task_1> by a large margin .",
    "abstract_og": "this paper presents a novel approach to improve the named entity translation by combining a translit-eration approach with web mining , using web information as a source to complement transliteration , and using transliteration information to guide and enhance web mining . a maximum entropy model is employed to rank translation candidates by combining pronunciation similarity and bilingual con-textual co-occurrence . experimental results show that our approach effectively improves the precision and recall of the named entity translation by a large margin ."
  },
  {
    "title": "A Pointwise Approach to Pronunciation Estimation for a TTS Front-End .",
    "entities": [
      "japanese pronunciation estimation",
      "pointwise prediction",
      "pronunciation estimator",
      "annotated words",
      "word boundaries",
      "joint n-gram",
      "japanese/chinese"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <method> <material>",
    "relations": [
      "pronunciation estimator -- USED-FOR -- word boundaries"
    ],
    "abstract": "overview \u25cf objective : create a <method_2> that is robust and adaptable to new domains \u25cf focus on <material_6> , where we must estimate <otherscientificterm_4> as well \u25cf approach : <method_1> , which tags all <otherscientificterm_4> and pronunciations independently \u25cf <method_1> : \u25cf robust : relies on dictionaries less than previous methods \u25cf adaptable : it can be learned from single <otherscientificterm_3> , not full sentences \u25cf evaluation on <task_0> shows improvement over traditional joint n-gram",
    "abstract_og": "overview \u25cf objective : create a pronunciation estimator that is robust and adaptable to new domains \u25cf focus on japanese/chinese , where we must estimate word boundaries as well \u25cf approach : pointwise prediction , which tags all word boundaries and pronunciations independently \u25cf pointwise prediction : \u25cf robust : relies on dictionaries less than previous methods \u25cf adaptable : it can be learned from single annotated words , not full sentences \u25cf evaluation on japanese pronunciation estimation shows improvement over traditional joint n-gram"
  },
  {
    "title": "Social Planning : Achieving Goals by Altering Others ' Mental States .",
    "entities": [
      "cognitive task of social planning",
      "flexible problem solver",
      "social plans",
      "social scenarios",
      "social planning",
      "computational approach",
      "sfps"
    ],
    "types": "<task> <method> <otherscientificterm> <task> <task> <method> <method>",
    "relations": [
      "sfps -- HYPONYM-OF -- flexible problem solver",
      "computational approach -- USED-FOR -- social planning",
      "flexible problem solver -- USED-FOR -- social plans"
    ],
    "abstract": "in this paper , we discuss a <method_5> to the <task_0> . first , we specify a class of planning problems that involve an agent who attempts to achieve its goals by altering other agents ' mental states . next , we describe <method_6> , a <method_1> that generates <otherscientificterm_2> of this sort , including ones that include deception and reasoning about other agents ' beliefs . we report the results for experiments on <task_3> that involve different levels of sophistication and that demonstrate both <method_6> 's capabilities and the sources of its power . finally , we discuss how our <method_5> to <task_4> has been informed by earlier work in the area and propose directions for additional research on the topic .",
    "abstract_og": "in this paper , we discuss a computational approach to the cognitive task of social planning . first , we specify a class of planning problems that involve an agent who attempts to achieve its goals by altering other agents ' mental states . next , we describe sfps , a flexible problem solver that generates social plans of this sort , including ones that include deception and reasoning about other agents ' beliefs . we report the results for experiments on social scenarios that involve different levels of sophistication and that demonstrate both sfps 's capabilities and the sources of its power . finally , we discuss how our computational approach to social planning has been informed by earlier work in the area and propose directions for additional research on the topic ."
  },
  {
    "title": "Low Rank Approximation using Error Correcting Coding Matrices .",
    "entities": [
      "low rank approximations of large matrices",
      "principal component analysis",
      "low rank approximations",
      "error correcting codes",
      "low-rank matrix approximation",
      "gaussian random matrices",
      "structured random matrices",
      "subspace of vectors",
      "low coherence",
      "web search",
      "computer vision",
      "text mining",
      "face recognition",
      "randomized algorithms",
      "hadamard matrices",
      "log factor",
      "approximation errors",
      "singular values",
      "rank-k approximation",
      "matrices"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <task> <task> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "text mining -- CONJUNCTION -- computer vision",
      "gaussian random matrices -- CONJUNCTION -- structured random matrices",
      "text mining -- CONJUNCTION -- face recognition",
      "randomized algorithms -- USED-FOR -- low rank approximations of large matrices",
      "face recognition -- CONJUNCTION -- computer vision",
      "web search -- CONJUNCTION -- text mining"
    ],
    "abstract": "low-rank matrix approximation is an integral component of tools such as <method_1> , as well as is an important instrument used in applications like <task_9> , <task_11> and <task_10> , e.g. , <task_12> . recently , <method_13> were proposed to effectively construct <otherscientificterm_0> . in this paper , we show how <otherscientificterm_19> from <method_3> can be used to find such <otherscientificterm_2> . the benefits of using these <method_3> are the following : -lrb- i -rrb- they are easy to generate and they reduce randomness significantly . -lrb- ii -rrb- code <otherscientificterm_19> have <otherscientificterm_8> and have a better chance of preserving the geometry of an entire <otherscientificterm_7> ; -lrb- iii -rrb- unlike fourier transforms or <otherscientificterm_14> , which require sampling o -lrb- k log k -rrb- columns for a <method_18> , the <otherscientificterm_15> is not necessary in the case of <method_3> . -lrb- iv -rrb- under certain conditions , the <otherscientificterm_16> can be better and the <otherscientificterm_17> obtained can be more accurate , than those obtained using <otherscientificterm_5> and other <material_6> .",
    "abstract_og": "low-rank matrix approximation is an integral component of tools such as principal component analysis , as well as is an important instrument used in applications like web search , text mining and computer vision , e.g. , face recognition . recently , randomized algorithms were proposed to effectively construct low rank approximations of large matrices . in this paper , we show how matrices from error correcting codes can be used to find such low rank approximations . the benefits of using these error correcting codes are the following : -lrb- i -rrb- they are easy to generate and they reduce randomness significantly . -lrb- ii -rrb- code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors ; -lrb- iii -rrb- unlike fourier transforms or hadamard matrices , which require sampling o -lrb- k log k -rrb- columns for a rank-k approximation , the log factor is not necessary in the case of error correcting codes . -lrb- iv -rrb- under certain conditions , the approximation errors can be better and the singular values obtained can be more accurate , than those obtained using gaussian random matrices and other structured random matrices ."
  },
  {
    "title": "Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement .",
    "entities": [
      "time-frequency structure of sound signals",
      "markov chain monte carlo methods",
      "audio signal processing approaches",
      "posterior distribution of interest",
      "time-varying filtering techniques",
      "speech enhancement",
      "bayesian paradigm",
      "audio waveforms",
      "bayesian model",
      "noise power",
      "time-frequency characteristics",
      "prior knowledge",
      "speech"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "time-varying filtering techniques -- USED-FOR -- speech enhancement",
      "speech -- HYPONYM-OF -- time-frequency structure of sound signals",
      "bayesian paradigm -- USED-FOR -- time-frequency structure of sound signals"
    ],
    "abstract": "the <method_6> provides a natural and effective means of exploiting <otherscientificterm_11> concerning the <otherscientificterm_0> such as <material_12> and music -- something which has often been overlooked in traditional <method_2> . here , after constructing a <method_8> and prior distributions capable of taking into account the <otherscientificterm_10> of typical <otherscientificterm_7> , we apply <method_1> in order to sample from the resultant <otherscientificterm_3> . we present <task_5> results which compare favourably in objective terms with standard <method_4> -lrb- and in several cases yield superior performance , both objectively and subjectively -rrb- ; moreover , in contrast to such methods , our results are obtained without an assumption of <otherscientificterm_11> of the <otherscientificterm_9> .",
    "abstract_og": "the bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music -- something which has often been overlooked in traditional audio signal processing approaches . here , after constructing a bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms , we apply markov chain monte carlo methods in order to sample from the resultant posterior distribution of interest . we present speech enhancement results which compare favourably in objective terms with standard time-varying filtering techniques -lrb- and in several cases yield superior performance , both objectively and subjectively -rrb- ; moreover , in contrast to such methods , our results are obtained without an assumption of prior knowledge of the noise power ."
  },
  {
    "title": "The temporal organisation of speech as gauged by speech synthesis .",
    "entities": [
      "quantitative and qualitative effects",
      "simulation of speech rhythm",
      "word grouping boundaries",
      "language models",
      "temporal component",
      "temporal issues",
      "language system",
      "speech styles",
      "statistical analysis",
      "speech synthesis",
      "speech coherence",
      "qualitative parameters",
      "speech rate"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "temporal component -- USED-FOR -- simulation of speech rhythm"
    ],
    "abstract": "the simulation of speech by means of <task_9> involves , among other things , the ability to mimic typical delivery for different <otherscientificterm_7> . this requires a realistic imitation of the manner in which speakers organize their information flow in time -lrb- i.e. , <otherscientificterm_2> -rrb- , as well their <metric_12> with its variations . the originality of our model is grounded in two levels . first , it is assumed that the <method_4> plays a dominant role in the <task_1> , whereas in traditional <method_3> , <otherscientificterm_5> are mostly put aside . second , the outcome of our temporal modeling , based on <method_8> and <otherscientificterm_11> , results from the harmonization of various layers -lrb- segmental , syllabic , phrasal -rrb- . the benefit of a multidimensional model is the possibility of imposing subtle <otherscientificterm_0> at various levels , which is a key for respecting a specific <method_6> as well as <otherscientificterm_10> and fluency for different <otherscientificterm_7> .",
    "abstract_og": "the simulation of speech by means of speech synthesis involves , among other things , the ability to mimic typical delivery for different speech styles . this requires a realistic imitation of the manner in which speakers organize their information flow in time -lrb- i.e. , word grouping boundaries -rrb- , as well their speech rate with its variations . the originality of our model is grounded in two levels . first , it is assumed that the temporal component plays a dominant role in the simulation of speech rhythm , whereas in traditional language models , temporal issues are mostly put aside . second , the outcome of our temporal modeling , based on statistical analysis and qualitative parameters , results from the harmonization of various layers -lrb- segmental , syllabic , phrasal -rrb- . the benefit of a multidimensional model is the possibility of imposing subtle quantitative and qualitative effects at various levels , which is a key for respecting a specific language system as well as speech coherence and fluency for different speech styles ."
  },
  {
    "title": "Selection of optimal dimensionality reduction method using chernoff bound for segmental unit input HMM .",
    "entities": [
      "power linear discriminant analysis",
      "heteroscedastic discriminant analysis",
      "linear discriminant analysis",
      "segmental unit input hmm",
      "performance comparison method",
      "dimensionality reduction method",
      "relative recognition",
      "chernoff bound",
      "speech recognition",
      "features",
      "hmms"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <task> <otherscientificterm> <task> <otherscientificterm> <method>",
    "relations": [
      "dimensionality reduction method -- CONJUNCTION -- segmental unit input hmm",
      "linear discriminant analysis -- CONJUNCTION -- heteroscedastic discriminant analysis",
      "relative recognition -- EVALUATE-FOR -- dimensionality reduction method",
      "dimensionality reduction method -- USED-FOR -- speech recognition",
      "power linear discriminant analysis -- HYPONYM-OF -- dimensionality reduction method",
      "segmental unit input hmm -- USED-FOR -- speech recognition",
      "dimensionality reduction method -- USED-FOR -- dimensionality reduction method"
    ],
    "abstract": "to precisely model the time dependency of <otherscientificterm_9> , <method_3> with a <method_5> has been widely used for <task_8> . <method_2> and <method_1> are popular approaches to reduce the dimen-sionality . we have proposed another <method_5> called <method_0> to select the best <method_5> that yields the highest <task_6> performance . this <method_5> on the basis of trial and error requires much time to train <method_10> and to test the <task_6> performance for each <method_5> . in this paper we propose a <method_4> without training or testing . we show that the proposed <method_4> using the <otherscientificterm_7> can rapidly and accurately evaluate the <task_6> performance .",
    "abstract_og": "to precisely model the time dependency of features , segmental unit input hmm with a dimensionality reduction method has been widely used for speech recognition . linear discriminant analysis and heteroscedastic discriminant analysis are popular approaches to reduce the dimen-sionality . we have proposed another dimensionality reduction method called power linear discriminant analysis to select the best dimensionality reduction method that yields the highest relative recognition performance . this dimensionality reduction method on the basis of trial and error requires much time to train hmms and to test the relative recognition performance for each dimensionality reduction method . in this paper we propose a performance comparison method without training or testing . we show that the proposed performance comparison method using the chernoff bound can rapidly and accurately evaluate the relative recognition performance ."
  },
  {
    "title": "A noise-robust ASR back-end technique based on weighted viterbi recognition .",
    "entities": [
      "weighted viterbi recognition algorithm",
      "acoustic front-end features",
      "viterbi decoding stage",
      "speech recognition systems",
      "weighted viterbi recognition",
      "acoustic models",
      "adaptation data",
      "noisy conditions",
      "environment adaptation",
      "confidence/robustness factor",
      "clean data",
      "speech frame",
      "front-end features",
      "snr estimate",
      "lpcc",
      "snr",
      "plp",
      "mfcc"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <material> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <material> <method>",
    "relations": [
      "viterbi decoding stage -- FEATURE-OF -- snr estimate",
      "plp -- HYPONYM-OF -- front-end features",
      "mfcc -- HYPONYM-OF -- front-end features",
      "mfcc -- CONJUNCTION -- lpcc",
      "plp -- USED-FOR -- environment adaptation",
      "front-end features -- FEATURE-OF -- weighted viterbi recognition",
      "weighted viterbi recognition algorithm -- USED-FOR -- snr",
      "clean data -- USED-FOR -- acoustic models",
      "mfcc -- HYPONYM-OF -- weighted viterbi recognition",
      "lpcc -- CONJUNCTION -- plp",
      "lpcc -- HYPONYM-OF -- front-end features",
      "plp -- HYPONYM-OF -- weighted viterbi recognition",
      "lpcc -- HYPONYM-OF -- weighted viterbi recognition",
      "plp -- USED-FOR -- weighted viterbi recognition algorithm",
      "weighted viterbi recognition algorithm -- COMPARE -- environment adaptation",
      "noisy conditions -- FEATURE-OF -- speech recognition systems"
    ],
    "abstract": "the performance of <method_3> trained in quiet degrades significantly under <otherscientificterm_7> . to address this problem , a <method_0> that is a function of the <otherscientificterm_15> of each <otherscientificterm_11> is proposed . <method_5> trained on <material_10> , and the <otherscientificterm_1> are kept unchanged in this <method_0> . instead , a <otherscientificterm_9> is assigned to the output observation probability of each <otherscientificterm_11> according to its <otherscientificterm_13> during the <otherscientificterm_2> . comparative experiments are conducted with <method_4> with different <otherscientificterm_12> such as <method_17> , <method_14> and <material_16> . results show consistent improvements with all three feature vectors . for a reasonable size of <material_6> , <method_0> outperforms <method_8> using <material_16> .",
    "abstract_og": "the performance of speech recognition systems trained in quiet degrades significantly under noisy conditions . to address this problem , a weighted viterbi recognition algorithm that is a function of the snr of each speech frame is proposed . acoustic models trained on clean data , and the acoustic front-end features are kept unchanged in this weighted viterbi recognition algorithm . instead , a confidence/robustness factor is assigned to the output observation probability of each speech frame according to its snr estimate during the viterbi decoding stage . comparative experiments are conducted with weighted viterbi recognition with different front-end features such as mfcc , lpcc and plp . results show consistent improvements with all three feature vectors . for a reasonable size of adaptation data , weighted viterbi recognition algorithm outperforms environment adaptation using plp ."
  },
  {
    "title": "Robust detection of nonstationary random signals belonging to p-point uncertainty classes .",
    "entities": [
      "p-point uncertainty classes",
      "reduced prior knowledge",
      "local cosine bases",
      "nonstationary random signals",
      "deflection criterion",
      "signal-adaptive operation",
      "robust detectors",
      "time-frequency implementation",
      "estimator-correlator approach"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "robust detectors -- USED-FOR -- nonstationary random signals"
    ],
    "abstract": "we present two <method_6> for <task_3> that belong to <otherscientificterm_0> , one being based on an <method_8> , the other using the <otherscientificterm_4> . apart of stable performance , these <method_6> have the advantage of requiring only <otherscientificterm_1> . using <method_2> , we provide an intuitive and highly efficient <method_7> of these <method_6> along with an extension that permits <otherscientificterm_5> . simulation results illustrate the robustness of the proposed <method_6> .",
    "abstract_og": "we present two robust detectors for nonstationary random signals that belong to p-point uncertainty classes , one being based on an estimator-correlator approach , the other using the deflection criterion . apart of stable performance , these robust detectors have the advantage of requiring only reduced prior knowledge . using local cosine bases , we provide an intuitive and highly efficient time-frequency implementation of these robust detectors along with an extension that permits signal-adaptive operation . simulation results illustrate the robustness of the proposed robust detectors ."
  },
  {
    "title": "Nonparametric estimation of the precision-recall curve .",
    "entities": [
      "statistical estimation of pr curves",
      "precision-recall curve",
      "theoretical and computational nature",
      "scoring functions",
      "asymptotic normality",
      "pr curve",
      "resampling procedure",
      "classification data",
      "sup norm",
      "confidence bands",
      "visual tool",
      "pr space",
      "bootstrap"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "asymptotic normality -- FEATURE-OF -- pr curve",
      "classification data -- USED-FOR -- statistical estimation of pr curves"
    ],
    "abstract": "the <method_1> is a widely used <method_10> to evaluate the performance of <otherscientificterm_3> in regards to their capacities to discriminate between two populations . the purpose of this paper is to examine both theoretical and practical issues related to the <task_0> based on <material_7> . consistency and <otherscientificterm_4> of the empirical counterpart of the <otherscientificterm_5> in <otherscientificterm_8> are rigorously established . eventually , the issue of building <otherscientificterm_9> in the <otherscientificterm_11> is considered and a specific <method_6> based on a smoothed and truncated version of the empirical distribution of the data is promoted . arguments of <otherscientificterm_2> are presented to explain why such a <otherscientificterm_12> is preferable to a `` naive '' <otherscientificterm_12> in this setup .",
    "abstract_og": "the precision-recall curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations . the purpose of this paper is to examine both theoretical and practical issues related to the statistical estimation of pr curves based on classification data . consistency and asymptotic normality of the empirical counterpart of the pr curve in sup norm are rigorously established . eventually , the issue of building confidence bands in the pr space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted . arguments of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a `` naive '' bootstrap in this setup ."
  },
  {
    "title": "How NOT To Evaluate Your Dialogue System : An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation .",
    "entities": [
      "technical and non-technical domains",
      "end-to-end dialogue systems",
      "automatic evaluation metrics",
      "text sum-marization",
      "supervised labels",
      "evaluation metrics",
      "task completion",
      "machine translation",
      "response quality"
    ],
    "types": "<material> <task> <metric> <task> <material> <metric> <metric> <task> <metric>",
    "relations": [
      "machine translation -- USED-FOR -- end-to-end dialogue systems",
      "machine translation -- CONJUNCTION -- text sum-marization",
      "text sum-marization -- USED-FOR -- end-to-end dialogue systems",
      "evaluation metrics -- USED-FOR -- end-to-end dialogue systems",
      "automatic evaluation metrics -- USED-FOR -- end-to-end dialogue systems"
    ],
    "abstract": "we investigate <metric_5> for <task_1> where <material_4> , such as <metric_6> , are not available . recent works in <task_1> have adopted metrics from <task_7> and <task_3> to compare a model 's generated response to a single target response . we show that these metrics correlate very weakly or not at all with human judgements of the <metric_8> in both <material_0> . we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better <metric_2> for <task_1> .",
    "abstract_og": "we investigate evaluation metrics for end-to-end dialogue systems where supervised labels , such as task completion , are not available . recent works in end-to-end dialogue systems have adopted metrics from machine translation and text sum-marization to compare a model 's generated response to a single target response . we show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains . we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for end-to-end dialogue systems ."
  },
  {
    "title": "Maximal Recursive Rule : A New Social Decision Scheme .",
    "entities": [
      "rsd -lrb- random serial dictatorship -rrb- rules",
      "maximal recursive rule",
      "randomized social choice functions",
      "ex post efficiency",
      "social choice settings",
      "random dictatorship rules",
      "random dictatorship",
      "stochastic dominance",
      "strict preferences",
      "strategyproofness",
      "indifferences",
      "rsd"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "strategyproofness -- CONJUNCTION -- ex post efficiency"
    ],
    "abstract": "in <otherscientificterm_4> with <otherscientificterm_8> , <otherscientificterm_5> were characterized by gibbard -lsb- 1977 -rsb- as the only <method_2> that satisfy <otherscientificterm_9> and <metric_3> . in the more general domain with <otherscientificterm_10> , <otherscientificterm_0> are the well-known and perhaps only known generalization of <otherscientificterm_6> . we present a new generalization of <otherscientificterm_6> for <otherscientificterm_10> called <otherscientificterm_1> as an alternative to <method_11> . we show that <otherscientificterm_1> is polynomial-time computable , weakly strategyproof with respect to <otherscientificterm_7> , and , in some respects , outperforms <method_11> on efficiency .",
    "abstract_og": "in social choice settings with strict preferences , random dictatorship rules were characterized by gibbard -lsb- 1977 -rsb- as the only randomized social choice functions that satisfy strategyproofness and ex post efficiency . in the more general domain with indifferences , rsd -lrb- random serial dictatorship -rrb- rules are the well-known and perhaps only known generalization of random dictatorship . we present a new generalization of random dictatorship for indifferences called maximal recursive rule as an alternative to rsd . we show that maximal recursive rule is polynomial-time computable , weakly strategyproof with respect to stochastic dominance , and , in some respects , outperforms rsd on efficiency ."
  },
  {
    "title": "Semi-Supervised Learning with Explicit Misclassification Modeling .",
    "entities": [
      "classification expectation maximization algorithm",
      "classification maximum likelihood",
      "probabilistic misclassification model",
      "labeled data",
      "semi-supervised algorithm",
      "labeled-unlabeled data",
      "labeling process",
      "discriminant classifiers",
      "un-labeled data",
      "unlabeled data",
      "classifier parameters",
      "originality",
      "imperfections"
    ],
    "types": "<method> <metric> <method> <material> <method> <material> <task> <method> <material> <material> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "un-labeled data -- CONJUNCTION -- probabilistic misclassification model"
    ],
    "abstract": "this paper investigates a new approach for training <method_7> when only a small set of <material_3> is available together with a large set of <material_9> . this algorithm optimizes the <metric_1> of a set of <material_5> , using a variant form of the <method_0> . its <metric_11> is that it makes use of both <material_8> and of a <method_2> for these data . the parameters of the label-error model are learned together with the <otherscientificterm_10> . we demonstrate the effectiveness of the approach on four data-sets and show the advantages of this method over a previously developed <method_4> which does not consider <otherscientificterm_12> in the <task_6> .",
    "abstract_og": "this paper investigates a new approach for training discriminant classifiers when only a small set of labeled data is available together with a large set of unlabeled data . this algorithm optimizes the classification maximum likelihood of a set of labeled-unlabeled data , using a variant form of the classification expectation maximization algorithm . its originality is that it makes use of both un-labeled data and of a probabilistic misclassification model for these data . the parameters of the label-error model are learned together with the classifier parameters . we demonstrate the effectiveness of the approach on four data-sets and show the advantages of this method over a previously developed semi-supervised algorithm which does not consider imperfections in the labeling process ."
  },
  {
    "title": "Prof-Life-Log : Personal interaction analysis for naturalistic audio streams .",
    "entities": [
      "speech activity detection",
      "contemporary speech and language processing techniques",
      "contemporary speech technology",
      "life logging applications",
      "naturalistic audio streams",
      "personal audio recordings",
      "environmental sniffing techniques",
      "prof-life-log corpus",
      "analysis system",
      "speaker diarization"
    ],
    "types": "<method> <method> <method> <task> <material> <material> <method> <material> <method> <task>",
    "relations": [
      "prof-life-log corpus -- USED-FOR -- naturalistic audio streams",
      "speaker diarization -- CONJUNCTION -- environmental sniffing techniques",
      "speaker diarization -- USED-FOR -- analysis system",
      "contemporary speech and language processing techniques -- USED-FOR -- personal audio recordings",
      "speech activity detection -- CONJUNCTION -- speaker diarization",
      "personal audio recordings -- USED-FOR -- analysis system",
      "environmental sniffing techniques -- USED-FOR -- analysis system",
      "contemporary speech technology -- USED-FOR -- life logging applications",
      "speech activity detection -- USED-FOR -- analysis system"
    ],
    "abstract": "analysis of <material_5> is a challenging and interesting subject . using <method_1> , it is possible to mine <material_5> for a wealth of information that can be used to measure a person 's engagement with their environment as well as other people . in this study , we propose an <method_8> that uses <material_5> to automatically estimate the number of unique people and environments which encompass the total engagement within the recording . the proposed <method_8> uses <method_0> , <task_9> and <method_6> , and is evaluated on <material_4> from the <material_7> . we also report performance of the individual systems , and also present a combined analysis which reveals the interaction of the subject with both people and environment . hence , this study establishes the efficacy and novelty of using <method_2> for <task_3> .",
    "abstract_og": "analysis of personal audio recordings is a challenging and interesting subject . using contemporary speech and language processing techniques , it is possible to mine personal audio recordings for a wealth of information that can be used to measure a person 's engagement with their environment as well as other people . in this study , we propose an analysis system that uses personal audio recordings to automatically estimate the number of unique people and environments which encompass the total engagement within the recording . the proposed analysis system uses speech activity detection , speaker diarization and environmental sniffing techniques , and is evaluated on naturalistic audio streams from the prof-life-log corpus . we also report performance of the individual systems , and also present a combined analysis which reveals the interaction of the subject with both people and environment . hence , this study establishes the efficacy and novelty of using contemporary speech technology for life logging applications ."
  },
  {
    "title": "Enhancing model-based skin color detection : From low-level RGB features to high-level discriminative binary-class features .",
    "entities": [
      "model-based skin color detection",
      "high-level binary-class features",
      "low-level rgb feature",
      "bayesian model adaptation",
      "non-skin rgb models",
      "log likelihood ratio",
      "baseline f1 scores",
      "discriminative feature",
      "feature fusion",
      "lighting conditions",
      "background-foreground correlation"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <method> <method> <metric> <metric> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "baseline f1 scores -- EVALUATE-FOR -- high-level binary-class features",
      "high-level binary-class features -- USED-FOR -- model-based skin color detection",
      "bayesian model adaptation -- CONJUNCTION -- feature fusion",
      "background-foreground correlation -- COMPARE -- low-level rgb feature"
    ],
    "abstract": "we propose two very effective <otherscientificterm_1> to enhance <task_0> . first we find that the <metric_5> of the testing data between skin and <method_4> can be a good <otherscientificterm_7> . we also find that namely the <otherscientificterm_10> provides another complementary feature compared to the conventional <otherscientificterm_2> . further improvement can be accomplished by <method_3> and <method_8> . by jointly considering both schemes of <method_3> and <method_8> , we attain the best system performance . experimental results show that the proposed <otherscientificterm_1> improves the 68 % to 84 % <metric_6> to as high as almost 90 % in a wide range of <otherscientificterm_9> .",
    "abstract_og": "we propose two very effective high-level binary-class features to enhance model-based skin color detection . first we find that the log likelihood ratio of the testing data between skin and non-skin rgb models can be a good discriminative feature . we also find that namely the background-foreground correlation provides another complementary feature compared to the conventional low-level rgb feature . further improvement can be accomplished by bayesian model adaptation and feature fusion . by jointly considering both schemes of bayesian model adaptation and feature fusion , we attain the best system performance . experimental results show that the proposed high-level binary-class features improves the 68 % to 84 % baseline f1 scores to as high as almost 90 % in a wide range of lighting conditions ."
  },
  {
    "title": "3D Variational Brain Tumor Segmentation using a High Dimensional Feature Set .",
    "entities": [
      "variational brain tumor seg-mentation algorithm",
      "tumor and normal tissue",
      "appearance of tumor tissue",
      "appearance of normal brain",
      "high dimensional feature set",
      "cancer patient mri scans",
      "manually segmented data",
      "texture segmentation",
      "tumor segmentation",
      "registered atlases",
      "generative models",
      "mri data",
      "prior information",
      "statistical model",
      "conditional model",
      "validation"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <task> <task> <material> <method> <material> <otherscientificterm> <method> <method> <task>",
    "relations": [
      "mri data -- USED-FOR -- texture segmentation",
      "statistical model -- USED-FOR -- tumor and normal tissue",
      "mri data -- USED-FOR -- tumor segmentation",
      "conditional model -- COMPARE -- generative models",
      "cancer patient mri scans -- EVALUATE-FOR -- validation",
      "texture segmentation -- USED-FOR -- variational brain tumor seg-mentation algorithm",
      "manually segmented data -- USED-FOR -- statistical model",
      "cancer patient mri scans -- EVALUATE-FOR -- variational brain tumor seg-mentation algorithm",
      "high dimensional feature set -- USED-FOR -- variational brain tumor seg-mentation algorithm",
      "high dimensional feature set -- USED-FOR -- texture segmentation",
      "mri data -- USED-FOR -- high dimensional feature set",
      "registered atlases -- USED-FOR -- high dimensional feature set",
      "mri data -- CONJUNCTION -- registered atlases"
    ],
    "abstract": "tumor segmentation from <material_11> is an important but time consuming task performed manually by medical experts . automating this process is challenging due to the high diversity in <otherscientificterm_2> , among different patients and , in many cases , similarity between <otherscientificterm_1> . one other challenge is how to make use of <otherscientificterm_12> about the <otherscientificterm_3> . in this paper we propose a <method_0> that extends current approaches from <task_7> by using a <otherscientificterm_4> calculated from <material_11> and <material_9> . using <material_6> we learn a <method_13> for <otherscientificterm_1> . we show that using a <method_14> to discriminate between normal and abnormal regions significantly improves the segmentation results compared to traditional <method_10> . <task_15> is performed by testing the <method_0> on several <material_5> .",
    "abstract_og": "tumor segmentation from mri data is an important but time consuming task performed manually by medical experts . automating this process is challenging due to the high diversity in appearance of tumor tissue , among different patients and , in many cases , similarity between tumor and normal tissue . one other challenge is how to make use of prior information about the appearance of normal brain . in this paper we propose a variational brain tumor seg-mentation algorithm that extends current approaches from texture segmentation by using a high dimensional feature set calculated from mri data and registered atlases . using manually segmented data we learn a statistical model for tumor and normal tissue . we show that using a conditional model to discriminate between normal and abnormal regions significantly improves the segmentation results compared to traditional generative models . validation is performed by testing the variational brain tumor seg-mentation algorithm on several cancer patient mri scans ."
  },
  {
    "title": "Transfer Learning Using Task-Level Features with Application to Information Retrieval .",
    "entities": [
      "probabilistic transfer learning model",
      "empirical bayes method",
      "transfer learning methods",
      "task mixture selection",
      "variational approximation techniques",
      "hierarchical bayesian model",
      "task-level features",
      "information retrieval",
      "model parameters"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "variational approximation techniques -- USED-FOR -- empirical bayes method",
      "information retrieval -- EVALUATE-FOR -- probabilistic transfer learning model",
      "probabilistic transfer learning model -- USED-FOR -- task mixture selection",
      "task-level features -- USED-FOR -- probabilistic transfer learning model",
      "probabilistic transfer learning model -- COMPARE -- transfer learning methods"
    ],
    "abstract": "we propose a <method_0> that uses <otherscientificterm_6> to control the <method_3> in a <method_5> . these <otherscientificterm_6> , although rarely used in existing approaches , can provide additional information to <method_0> complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available . to estimate the <otherscientificterm_8> , we develop an <method_1> based on <method_4> . our experiments on <task_7> show that the proposed <method_0> achieves significantly better performance compared with other <method_2> .",
    "abstract_og": "we propose a probabilistic transfer learning model that uses task-level features to control the task mixture selection in a hierarchical bayesian model . these task-level features , although rarely used in existing approaches , can provide additional information to probabilistic transfer learning model complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available . to estimate the model parameters , we develop an empirical bayes method based on variational approximation techniques . our experiments on information retrieval show that the proposed probabilistic transfer learning model achieves significantly better performance compared with other transfer learning methods ."
  },
  {
    "title": "Past and Future of DL-Lite .",
    "entities": [
      "minimal temporal description logics",
      "temporal conceptual data models",
      "temporal and atemporal constraints",
      "temporal knowledge bases",
      "satisfiability problem",
      "computational complexity"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <metric>",
    "relations": [
      "satisfiability problem -- USED-FOR -- temporal knowledge bases",
      "minimal temporal description logics -- USED-FOR -- temporal conceptual data models"
    ],
    "abstract": "we design <otherscientificterm_0> that are capable of expressing various aspects of <method_1> and investigate their <metric_5> . we show that , depending on the required types of <otherscientificterm_2> , the <task_4> for <otherscientificterm_3> in the resulting logics can be nlogspace - , np-and pspace-complete , as well as undecidable .",
    "abstract_og": "we design minimal temporal description logics that are capable of expressing various aspects of temporal conceptual data models and investigate their computational complexity . we show that , depending on the required types of temporal and atemporal constraints , the satisfiability problem for temporal knowledge bases in the resulting logics can be nlogspace - , np-and pspace-complete , as well as undecidable ."
  },
  {
    "title": "Modeling Social Causality and Responsibility Judgment in Multi-Agent Interactions : Extended Abstract .",
    "entities": [
      "social causality and responsibility judgment",
      "domain-independent computational model",
      "psychological attribution theory",
      "causal knowledge"
    ],
    "types": "<task> <method> <method> <otherscientificterm>",
    "relations": [
      "domain-independent computational model -- USED-FOR -- social causality and responsibility judgment"
    ],
    "abstract": "based on <method_2> , this paper presents a <method_1> to automate <task_0> according to an agent 's <otherscientificterm_3> and observations of interaction . the proposed <method_1> is also empirically validated via experimental study .",
    "abstract_og": "based on psychological attribution theory , this paper presents a domain-independent computational model to automate social causality and responsibility judgment according to an agent 's causal knowledge and observations of interaction . the proposed domain-independent computational model is also empirically validated via experimental study ."
  },
  {
    "title": "Multiagent Reinforcement Learning : Theoretical Framework and an Algorithm .",
    "entities": [
      "general-sum stochas-tic games",
      "multiagent q-learning method",
      "multiagent reinforcement learning",
      "zero-sum stochas-tic games",
      "nash equilibrium",
      "nash equilibria",
      "speciied conditions",
      "learning techniques"
    ],
    "types": "<method> <method> <task> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "general-sum stochas-tic games -- USED-FOR -- multiagent reinforcement learning",
      "multiagent q-learning method -- USED-FOR -- nash equilibrium",
      "speciied conditions -- FEATURE-OF -- nash equilibrium"
    ],
    "abstract": "in this paper , we adopt <method_0> as a framework for <task_2> . our work extends previous work by littman on <material_3> to a broader framework . we design a <method_1> under this framework , and prove that <method_1> converges to a <otherscientificterm_4> under <otherscientificterm_6> . this <method_1> is useful for nding the optimal strategy when there exists a unique <otherscientificterm_4> in the game . when there exist multiple <otherscientificterm_5> in the game , this <method_1> should be combined with other <method_7> to nd optimal strategies .",
    "abstract_og": "in this paper , we adopt general-sum stochas-tic games as a framework for multiagent reinforcement learning . our work extends previous work by littman on zero-sum stochas-tic games to a broader framework . we design a multiagent q-learning method under this framework , and prove that multiagent q-learning method converges to a nash equilibrium under speciied conditions . this multiagent q-learning method is useful for nding the optimal strategy when there exists a unique nash equilibrium in the game . when there exist multiple nash equilibria in the game , this multiagent q-learning method should be combined with other learning techniques to nd optimal strategies ."
  },
  {
    "title": "Colorization by Patch-Based Local Low-Rank Matrix Completion .",
    "entities": [
      "local matrix completion problem",
      "matrix completion",
      "natural images",
      "low-rank assumption",
      "low-rank structure",
      "patch-based approach",
      "benchmark images",
      "admm subproblem",
      "color pixels",
      "divide-and-conquer"
    ],
    "types": "<task> <method> <material> <otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "benchmark images -- EVALUATE-FOR -- patch-based approach",
      "divide-and-conquer -- USED-FOR -- admm subproblem"
    ],
    "abstract": "colorization aims at recovering the original color of a monochrome image from only a few <otherscientificterm_8> . a state-of-the-art approach is based on <method_1> , which assumes that the target color image is low-rank . however , this <otherscientificterm_3> is often invalid on <material_2> . in this paper , we propose a <method_5> that divides the image into patches and then imposes a <otherscientificterm_4> only on groups of similar patches . each <task_0> is solved by an accelerated version of alternating direction <method_5> of multipliers -lrb- admm -rrb- , and each <otherscientificterm_7> is solved efficiently by <method_9> . experiments on a number of <material_6> demonstrate that the proposed <method_5> outperforms existing approaches .",
    "abstract_og": "colorization aims at recovering the original color of a monochrome image from only a few color pixels . a state-of-the-art approach is based on matrix completion , which assumes that the target color image is low-rank . however , this low-rank assumption is often invalid on natural images . in this paper , we propose a patch-based approach that divides the image into patches and then imposes a low-rank structure only on groups of similar patches . each local matrix completion problem is solved by an accelerated version of alternating direction patch-based approach of multipliers -lrb- admm -rrb- , and each admm subproblem is solved efficiently by divide-and-conquer . experiments on a number of benchmark images demonstrate that the proposed patch-based approach outperforms existing approaches ."
  },
  {
    "title": "RL-TOPS : An Architecture for Modularity and Re-Use in Reinforcement Learning .",
    "entities": [
      "reinforcement learning techniques",
      "robot learning",
      "simulated environment",
      "teleo-reactive planning",
      "hybrid system",
      "rl-tops architecture"
    ],
    "types": "<method> <task> <task> <method> <method> <method>",
    "relations": [
      "rl-tops architecture -- USED-FOR -- robot learning",
      "teleo-reactive planning -- CONJUNCTION -- reinforcement learning techniques",
      "reinforcement learning techniques -- PART-OF -- hybrid system",
      "teleo-reactive planning -- PART-OF -- hybrid system"
    ],
    "abstract": "this paper introduces the <method_5> for <task_1> , a <method_4> combining <method_3> and <method_0> . the aim of this <method_5> is to speed up learning by decomposing complex tasks into hierarchies of simple behaviours which can be learnt more easily . behaviours learnt in this way can subsequently be re-used to solve a variety of problems , reducing the need to learn every new task from scratch . it is even possible to learn multiple behaviours simultaneously , thus making more eecient use of experience . we demonstrate these advantages in a simple <task_2> .",
    "abstract_og": "this paper introduces the rl-tops architecture for robot learning , a hybrid system combining teleo-reactive planning and reinforcement learning techniques . the aim of this rl-tops architecture is to speed up learning by decomposing complex tasks into hierarchies of simple behaviours which can be learnt more easily . behaviours learnt in this way can subsequently be re-used to solve a variety of problems , reducing the need to learn every new task from scratch . it is even possible to learn multiple behaviours simultaneously , thus making more eecient use of experience . we demonstrate these advantages in a simple simulated environment ."
  },
  {
    "title": "Bayesian Transduction .",
    "entities": [
      "posterior probability of labellings",
      "real world data",
      "support vector machine",
      "linear discriminant functions",
      "kernel space",
      "version space",
      "classification loss",
      "volume ratio",
      "inference principle",
      "confidence measure",
      "equivalence classes",
      "hypothesis space",
      "ergodic billiard",
      "bayesian analysis",
      "risk-sensitive applications",
      "binary classification",
      "classifiers-a feature",
      "posterior measure",
      "transduction",
      "induction"
    ],
    "types": "<otherscientificterm> <material> <method> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <metric> <otherscientificterm> <otherscientificterm> <method> <task> <task> <task> <otherscientificterm> <metric> <method> <task>",
    "relations": [
      "classifiers-a feature -- USED-FOR -- risk-sensitive applications",
      "kernel space -- FEATURE-OF -- binary classification",
      "bayesian analysis -- USED-FOR -- classification loss",
      "transduction -- HYPONYM-OF -- inference principle",
      "real world data -- EVALUATE-FOR -- bayesian analysis"
    ],
    "abstract": "transduction is an <method_8> that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for <task_19> . <method_18> provides a <metric_9> on single predictions rather than <otherscientificterm_16> particularly important for <task_14> . the possibly infinite number of functions is reduced to a finite number of <otherscientificterm_10> on the working sample . a rigorous <task_13> reveals that for standard <task_6> we can not benefit from considering more than one test point at a time . the probability of the label of a given test point is determined as the <metric_17> of the corresponding subset of <otherscientificterm_11> . we consider the pac setting of <task_15> by <method_3> -lrb- perceptrons -rrb- in <otherscientificterm_4> such that the probability of labels is determined by the <otherscientificterm_7> in <otherscientificterm_5> . we suggest to sample this region by an <method_12> . experimental results on <material_1> indicate that <task_13> compares favourably to the well-known <method_2> , in particular if the <otherscientificterm_0> is used as a <metric_9> to exclude test points of low confidence .",
    "abstract_og": "transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction . transduction provides a confidence measure on single predictions rather than classifiers-a feature particularly important for risk-sensitive applications . the possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample . a rigorous bayesian analysis reveals that for standard classification loss we can not benefit from considering more than one test point at a time . the probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space . we consider the pac setting of binary classification by linear discriminant functions -lrb- perceptrons -rrb- in kernel space such that the probability of labels is determined by the volume ratio in version space . we suggest to sample this region by an ergodic billiard . experimental results on real world data indicate that bayesian analysis compares favourably to the well-known support vector machine , in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence ."
  },
  {
    "title": "Laryngealization and features for Chinese tonal recognition .",
    "entities": [
      "corpora of tonal production data",
      "extraction of f0 features",
      "f0 feature extraction",
      "laryngeal-ization/creaky voice quality",
      "lowest tone",
      "tonal recognition",
      "contrastive phonation",
      "laryngealization",
      "feature",
      "cantonese",
      "mandarin"
    ],
    "types": "<material> <task> <task> <metric> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material>",
    "relations": [
      "laryngealization -- USED-FOR -- f0 feature extraction",
      "laryngealization -- USED-FOR -- tonal recognition",
      "f0 feature extraction -- USED-FOR -- tonal recognition"
    ],
    "abstract": "it is well known that the <otherscientificterm_4> in <material_10> , a language without <otherscientificterm_6> , often co-occurs with <metric_3> , and we provide evidence that this is also the case for the <otherscientificterm_4> in <material_9> . however , the effects of <otherscientificterm_7> on <task_2> for <task_5> , as well as the potential of <otherscientificterm_7> as a <otherscientificterm_8> for improving <task_5> , have not been well-discussed in the literature . we give evidence from a <material_0> for <material_9> and <material_10> that <otherscientificterm_7> is prevalent and significantly disturbs the <task_1> , and suggest that <otherscientificterm_7> may in fact be a <otherscientificterm_8> that could improve <task_5> .",
    "abstract_og": "it is well known that the lowest tone in mandarin , a language without contrastive phonation , often co-occurs with laryngeal-ization/creaky voice quality , and we provide evidence that this is also the case for the lowest tone in cantonese . however , the effects of laryngealization on f0 feature extraction for tonal recognition , as well as the potential of laryngealization as a feature for improving tonal recognition , have not been well-discussed in the literature . we give evidence from a corpora of tonal production data for cantonese and mandarin that laryngealization is prevalent and significantly disturbs the extraction of f0 features , and suggest that laryngealization may in fact be a feature that could improve tonal recognition ."
  },
  {
    "title": "Lattice-based System Combination for Statistical Machine Translation .",
    "entities": [
      "lattice-based system combination model",
      "chinese-to-english translation test sets",
      "system combination methods",
      "one-to-one mappings",
      "consensus translations",
      "candidate translations",
      "phrase alignments",
      "confusion networks",
      "lattices"
    ],
    "types": "<method> <material> <method> <method> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "chinese-to-english translation test sets -- EVALUATE-FOR -- lattice-based system combination model",
      "lattice-based system combination model -- USED-FOR -- phrase alignments",
      "system combination methods -- USED-FOR -- consensus translations",
      "confusion networks -- USED-FOR -- system combination methods",
      "lattices -- USED-FOR -- lattice-based system combination model"
    ],
    "abstract": "current <method_2> usually use <method_7> to find <otherscientificterm_4> among different systems . requiring <method_3> between the words in <task_5> , <method_7> have difficulty in handling more general situations in which several words are connected to another several words . instead , we propose a <method_0> that allows for such <otherscientificterm_6> and uses <otherscientificterm_8> to encode all <task_5> . experiments show that our <method_0> achieves significant improvements over the state-of-the-art baseline system on <material_1> .",
    "abstract_og": "current system combination methods usually use confusion networks to find consensus translations among different systems . requiring one-to-one mappings between the words in candidate translations , confusion networks have difficulty in handling more general situations in which several words are connected to another several words . instead , we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations . experiments show that our lattice-based system combination model achieves significant improvements over the state-of-the-art baseline system on chinese-to-english translation test sets ."
  },
  {
    "title": "A Deeper Look at Saliency : Feature Contrast , Semantics , and Beyond .",
    "entities": [
      "visual saliency modeling",
      "high level considerations",
      "visual saliency models",
      "salient object segmentation",
      "human gaze prediction",
      "deep learning model",
      "salient objects",
      "model behaviour",
      "training data",
      "fcns"
    ],
    "types": "<task> <otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <otherscientificterm> <material> <method>",
    "relations": [
      "human gaze prediction -- CONJUNCTION -- salient object segmentation"
    ],
    "abstract": "in this paper we consider the problem of <task_0> , including both <task_4> and <method_3> . the overarching goal of the paper is to identify <otherscientificterm_1> relevant to deriving more sophisticated <method_2> . a <method_5> based on fully convolutional networks -lrb- <method_9> -rrb- is presented , which shows very favorable performance across a wide variety of benchmarks relative to existing proposals . we also demonstrate that the manner in which <material_8> is selected , and ground truth treated is critical to resulting <otherscientificterm_7> . recent efforts have explored the relationship between human gaze and <otherscientificterm_6> , and we also examine this point further in the context of <method_9> . close examination of the proposed and alternative <method_5> serves as a vehicle for identifying problems important to developing more comprehensive <method_5> going forward .",
    "abstract_og": "in this paper we consider the problem of visual saliency modeling , including both human gaze prediction and salient object segmentation . the overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models . a deep learning model based on fully convolutional networks -lrb- fcns -rrb- is presented , which shows very favorable performance across a wide variety of benchmarks relative to existing proposals . we also demonstrate that the manner in which training data is selected , and ground truth treated is critical to resulting model behaviour . recent efforts have explored the relationship between human gaze and salient objects , and we also examine this point further in the context of fcns . close examination of the proposed and alternative deep learning model serves as a vehicle for identifying problems important to developing more comprehensive deep learning model going forward ."
  },
  {
    "title": "Efficient segmentation using feature-based graph partitioning active contours .",
    "entities": [
      "tile/block-based or superpixel-based multiscale grouping of the pixels",
      "graph partitioning active contours",
      "graph-based image segmentation problem",
      "quadratic memory requirements",
      "continuous optimization framework",
      "contour optimization process",
      "constant memory requirement",
      "image partitioning",
      "image size",
      "gpac algorithm",
      "graph-based approaches",
      "complexity",
      "approximations",
      "accuracy"
    ],
    "types": "<otherscientificterm> <method> <task> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <method> <method> <metric> <method> <metric>",
    "relations": [
      "graph partitioning active contours -- USED-FOR -- image partitioning",
      "graph partitioning active contours -- USED-FOR -- graph-based image segmentation problem",
      "graph partitioning active contours -- COMPARE -- graph-based approaches",
      "constant memory requirement -- USED-FOR -- gpac algorithm"
    ],
    "abstract": "graph partitioning active contours -lrb- <method_1> -rrb- is a recently introduced approach that elegantly embeds the <task_2> within a <method_4> . <method_1> can be used within parametric snake-based or implicit level set-based active contour continuous paradigms for <task_7> . however , <method_1> similar to many other <method_10> has <otherscientificterm_3> which severely limits the scalability of the algorithm to practical problem domains . an n xn image requires o -lrb- n -lrb- 4 -rrb- -rrb- computation and memory to create and store the full graph of pixel inter-relationships even before the start of the <method_5> . for example , an 1024x1024 grayscale image needs over one terabyte of memory . <method_12> using <otherscientificterm_0> reduces this <metric_11> by trading off <metric_13> . this paper describes a new algorithm that implements the exact <method_9> using a <otherscientificterm_6> of a few kilobytes , independent of <otherscientificterm_8> .",
    "abstract_og": "graph partitioning active contours -lrb- graph partitioning active contours -rrb- is a recently introduced approach that elegantly embeds the graph-based image segmentation problem within a continuous optimization framework . graph partitioning active contours can be used within parametric snake-based or implicit level set-based active contour continuous paradigms for image partitioning . however , graph partitioning active contours similar to many other graph-based approaches has quadratic memory requirements which severely limits the scalability of the algorithm to practical problem domains . an n xn image requires o -lrb- n -lrb- 4 -rrb- -rrb- computation and memory to create and store the full graph of pixel inter-relationships even before the start of the contour optimization process . for example , an 1024x1024 grayscale image needs over one terabyte of memory . approximations using tile/block-based or superpixel-based multiscale grouping of the pixels reduces this complexity by trading off accuracy . this paper describes a new algorithm that implements the exact gpac algorithm using a constant memory requirement of a few kilobytes , independent of image size ."
  },
  {
    "title": "Microsegment-based connected digit recognition .",
    "entities": [
      "word error rate",
      "acoustic phonetic models",
      "recognition system",
      "phonetic models",
      "orthographic supervision",
      "hmm engine",
      "hmm-based recognizer",
      "error analysis",
      "unlabeled data",
      "word-based models",
      "pronunciation"
    ],
    "types": "<metric> <method> <method> <method> <otherscientificterm> <method> <method> <method> <material> <method> <otherscientificterm>",
    "relations": [
      "orthographic supervision -- FEATURE-OF -- unlabeled data",
      "acoustic phonetic models -- USED-FOR -- recognition system",
      "hmm engine -- USED-FOR -- word-based models",
      "unlabeled data -- USED-FOR -- phonetic models",
      "word error rate -- EVALUATE-FOR -- recognition system"
    ],
    "abstract": "by building <method_1> which explicitly represent as much knowledge of <otherscientificterm_10> in a small domain -lrb- the digits -rrb- as possible , we can create a <method_2> which not only performs well but allows for meaningful <method_7> and improvement . an <method_6> for the digits and a few associated words was constructed in accord with these principles . about 65 <method_3> were trained on 140 carefully labeled utterances , then iteratively trained on <material_8> under <otherscientificterm_4> . the basic <method_2> achieved less than 3 % <metric_0> on digit strings of unknown length from unseen test speakers , and 1.4 % on 7-digit strings of known length . this is competitive with <method_9> using the same <method_5> and similar parameter settings . as an <method_2> , <method_2> allows meaningful analysis of errors and relatively straightforward means of improvement .",
    "abstract_og": "by building acoustic phonetic models which explicitly represent as much knowledge of pronunciation in a small domain -lrb- the digits -rrb- as possible , we can create a recognition system which not only performs well but allows for meaningful error analysis and improvement . an hmm-based recognizer for the digits and a few associated words was constructed in accord with these principles . about 65 phonetic models were trained on 140 carefully labeled utterances , then iteratively trained on unlabeled data under orthographic supervision . the basic recognition system achieved less than 3 % word error rate on digit strings of unknown length from unseen test speakers , and 1.4 % on 7-digit strings of known length . this is competitive with word-based models using the same hmm engine and similar parameter settings . as an recognition system , recognition system allows meaningful analysis of errors and relatively straightforward means of improvement ."
  },
  {
    "title": "Face Recognition via the Overlapping Energy Histogram .",
    "entities": [
      "eigen-faces , 2dpca and energy histogram",
      "parameter selection methods",
      "face recognition problem",
      "yale face database",
      "energy histogram approach",
      "overlapping energy histogram",
      "dct coefficients",
      "training dataset",
      "selecting threshold",
      "recognition"
    ],
    "types": "<method> <method> <task> <material> <method> <method> <otherscientificterm> <material> <task> <task>",
    "relations": [
      "overlapping energy histogram -- USED-FOR -- dct coefficients",
      "overlapping energy histogram -- USED-FOR -- face recognition problem",
      "dct coefficients -- USED-FOR -- face recognition problem",
      "yale face database -- EVALUATE-FOR -- parameter selection methods"
    ],
    "abstract": "in this paper we investigate the <task_2> via the <method_5> of the <otherscientificterm_6> . particularly , we investigate some important issues relating to the <task_9> performance , such as the issue of <task_8> and the number of bins . these <method_1> utilise information obtained from the <material_7> . experimentation is conducted on the <material_3> and results indicate that the proposed <method_1> perform well in selecting the threshold and number of bins . furthermore , we show that the proposed <method_5> approach outperforms the <method_0> significantly .",
    "abstract_og": "in this paper we investigate the face recognition problem via the overlapping energy histogram of the dct coefficients . particularly , we investigate some important issues relating to the recognition performance , such as the issue of selecting threshold and the number of bins . these parameter selection methods utilise information obtained from the training dataset . experimentation is conducted on the yale face database and results indicate that the proposed parameter selection methods perform well in selecting the threshold and number of bins . furthermore , we show that the proposed overlapping energy histogram approach outperforms the eigen-faces , 2dpca and energy histogram significantly ."
  },
  {
    "title": "Multi-layered Decomposition of Recurrent Scenes .",
    "entities": [
      "optical flow field of objects",
      "bounding box aspect ratio",
      "spatio-temporal grid of histograms",
      "road traffic junction",
      "autocovariance of self-similarity",
      "context gait recognition",
      "periodic spatio-temporal patterns",
      "global fundamental period",
      "timing device",
      "traffic accidents",
      "road junctions",
      "spatial appearance",
      "anomalous events",
      "broken-down vehicles",
      "pedestrians jay-walking",
      "traffic junctions",
      "temporal aspects",
      "adverse behaviour",
      "anomalies",
      "feature"
    ],
    "types": "<otherscientificterm> <metric> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "traffic accidents -- CONJUNCTION -- pedestrians jay-walking",
      "timing device -- USED-FOR -- traffic junctions",
      "broken-down vehicles -- CONJUNCTION -- traffic accidents",
      "pedestrians jay-walking -- HYPONYM-OF -- anomalous events",
      "bounding box aspect ratio -- CONJUNCTION -- optical flow field of objects",
      "traffic accidents -- HYPONYM-OF -- anomalous events",
      "broken-down vehicles -- HYPONYM-OF -- anomalous events"
    ],
    "abstract": "there is considerable interest in techniques capable of identifying <otherscientificterm_18> and unusual events in busy outdoor scenes , e.g. <otherscientificterm_10> . many approaches achieve this by exploiting deviations in <otherscientificterm_11> from some expected norm accumulated by a model over time . in this work we show that much can be gained from explicitly modelling <otherscientificterm_16> in detail . specifically , many <otherscientificterm_15> are regulated by lights controlled by a <method_8> of considerable precision , and it is in these situations that we advocate a model which learns <otherscientificterm_6> with a view to highlighting <otherscientificterm_12> such as <otherscientificterm_13> , <otherscientificterm_9> , or <method_14> . more specifically , by estimating <otherscientificterm_4> , used previously in the <task_5> , we characterize a scene by identifying a <otherscientificterm_7> . as our model , we introduce a <method_2> built in accordance with some chosen <otherscientificterm_19> . this model is then used to classify objects found in subsequent test data . in particular we demonstrate the effect of such characterization experimentally by monitoring the <metric_1> and <otherscientificterm_0> detected on a <otherscientificterm_3> , enabling our model to discriminate between people and cars sufficiently well to provide useful warnings of <otherscientificterm_17> in real time .",
    "abstract_og": "there is considerable interest in techniques capable of identifying anomalies and unusual events in busy outdoor scenes , e.g. road junctions . many approaches achieve this by exploiting deviations in spatial appearance from some expected norm accumulated by a model over time . in this work we show that much can be gained from explicitly modelling temporal aspects in detail . specifically , many traffic junctions are regulated by lights controlled by a timing device of considerable precision , and it is in these situations that we advocate a model which learns periodic spatio-temporal patterns with a view to highlighting anomalous events such as broken-down vehicles , traffic accidents , or pedestrians jay-walking . more specifically , by estimating autocovariance of self-similarity , used previously in the context gait recognition , we characterize a scene by identifying a global fundamental period . as our model , we introduce a spatio-temporal grid of histograms built in accordance with some chosen feature . this model is then used to classify objects found in subsequent test data . in particular we demonstrate the effect of such characterization experimentally by monitoring the bounding box aspect ratio and optical flow field of objects detected on a road traffic junction , enabling our model to discriminate between people and cars sufficiently well to provide useful warnings of adverse behaviour in real time ."
  },
  {
    "title": "An Extension of the ICP Algorithm for Modeling Nonrigid Objects with Mobile Robots .",
    "entities": [
      "iterative closest point algorithm",
      "3d models of objects",
      "rigid surface assumption",
      "nonrigid object models",
      "simultaneously registering scans",
      "modeling 3d objects",
      "model-ing nonrigid objects",
      "local scan patches",
      "high-dimensional optimization problems",
      "hierarchical method",
      "mathematical framework",
      "surface configuration",
      "mobile robot",
      "local deformations"
    ],
    "types": "<method> <task> <otherscientificterm> <method> <task> <task> <otherscientificterm> <method> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "mobile robot -- USED-FOR -- 3d models of objects",
      "hierarchical method -- USED-FOR -- high-dimensional optimization problems",
      "hierarchical method -- USED-FOR -- local scan patches",
      "hierarchical method -- USED-FOR -- mobile robot",
      "rigid surface assumption -- USED-FOR -- iterative closest point algorithm",
      "iterative closest point algorithm -- USED-FOR -- model-ing nonrigid objects"
    ],
    "abstract": "the <method_0> -lsb- 2 -rsb- is a popular method for <task_5> from range data . the classical <method_0> rests on a <otherscientificterm_2> . building on recent work on <method_3> -lsb- 5 ; 16 ; 9 -rsb- , this paper presents an <method_0> capable of <otherscientificterm_6> , where individual scans may be subject to <otherscientificterm_13> . we describe an integrated <method_10> for <task_4> and recovering the <otherscientificterm_11> . to tackle the resulting <task_8> , we introduce a <method_9> that first matches a coarse skeleton of scan points , then adapts <method_7> . the <method_9> is implemented for a <method_12> capable of acquiring <task_1> .",
    "abstract_og": "the iterative closest point algorithm -lsb- 2 -rsb- is a popular method for modeling 3d objects from range data . the classical iterative closest point algorithm rests on a rigid surface assumption . building on recent work on nonrigid object models -lsb- 5 ; 16 ; 9 -rsb- , this paper presents an iterative closest point algorithm capable of model-ing nonrigid objects , where individual scans may be subject to local deformations . we describe an integrated mathematical framework for simultaneously registering scans and recovering the surface configuration . to tackle the resulting high-dimensional optimization problems , we introduce a hierarchical method that first matches a coarse skeleton of scan points , then adapts local scan patches . the hierarchical method is implemented for a mobile robot capable of acquiring 3d models of objects ."
  },
  {
    "title": "Higher-order gradient descent by fusion-move graph cut .",
    "entities": [
      "optimization of higher-order potentials",
      "higher-order binary energy minimization",
      "higher-order graph cuts",
      "fusion move algorithm",
      "higher-order energies",
      "proposal labelings",
      "fusion move",
      "vision problems",
      "optimization",
      "first-order"
    ],
    "types": "<task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "higher-order graph cuts -- USED-FOR -- optimization of higher-order potentials"
    ],
    "abstract": "markov random field is now ubiquitous in many formulations of various <task_7> . recently , <task_0> became practical using <otherscientificterm_2> : the combination of i -rrb- the <method_3> , ii -rrb- the reduction of <method_1> to <method_9> , and iii -rrb- the <method_3> . in the <otherscientificterm_6> , it is crucial for the success and efficiency of the <method_8> to provide proposals that fits the energies being optimized . for <otherscientificterm_4> , it is even more so because they have richer class of null potentials . in this paper , we focus on the efficiency of the <otherscientificterm_2> and present a simple technique for generating <otherscientificterm_5> that makes the algorithm much more efficient , which we empirically show using examples in stereo and image denoising .",
    "abstract_og": "markov random field is now ubiquitous in many formulations of various vision problems . recently , optimization of higher-order potentials became practical using higher-order graph cuts : the combination of i -rrb- the fusion move algorithm , ii -rrb- the reduction of higher-order binary energy minimization to first-order , and iii -rrb- the fusion move algorithm . in the fusion move , it is crucial for the success and efficiency of the optimization to provide proposals that fits the energies being optimized . for higher-order energies , it is even more so because they have richer class of null potentials . in this paper , we focus on the efficiency of the higher-order graph cuts and present a simple technique for generating proposal labelings that makes the algorithm much more efficient , which we empirically show using examples in stereo and image denoising ."
  },
  {
    "title": "Bayesian Unsupervised Signal Classification by Dirichlet Process Mixtures of Gaussian Processes .",
    "entities": [
      "real signals -lrb- mrna expression profiles",
      "monte carlo markov chain algorithm",
      "bayesian clustering approaches",
      "dirichlet process model",
      "gaussian processes parameters",
      "classifying signals",
      "gaussian processes",
      "bayesian technique",
      "mixture model",
      "noise",
      "clusters"
    ],
    "types": "<material> <method> <method> <method> <otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "bayesian technique -- USED-FOR -- classifying signals"
    ],
    "abstract": "this paper presents a <method_7> aimed at <task_5> without prior training -lrb- clustering -rrb- . the <method_7> consists of modelling the observed signals , known only through a finite set of samples corrupted by <otherscientificterm_9> , as <method_6> . as in many other <method_2> , the <otherscientificterm_10> are defined thanks to a <method_8> . in order to estimate the number of <otherscientificterm_10> , we assume a priori a countably infinite number of <otherscientificterm_10> , thanks to a <method_3> over the <otherscientificterm_4> . computations are performed thanks to a dedicated <method_1> , and results involving <material_0> -rrb- are presented .",
    "abstract_og": "this paper presents a bayesian technique aimed at classifying signals without prior training -lrb- clustering -rrb- . the bayesian technique consists of modelling the observed signals , known only through a finite set of samples corrupted by noise , as gaussian processes . as in many other bayesian clustering approaches , the clusters are defined thanks to a mixture model . in order to estimate the number of clusters , we assume a priori a countably infinite number of clusters , thanks to a dirichlet process model over the gaussian processes parameters . computations are performed thanks to a dedicated monte carlo markov chain algorithm , and results involving real signals -lrb- mrna expression profiles -rrb- are presented ."
  },
  {
    "title": "Information Capacity and Robustness of Stochastic Neuron Models .",
    "entities": [
      "hh ion channel density",
      "density of ion channels",
      "average firing rate",
      "ion channel stochasticity",
      "macroscopic behavior",
      "neuronal models",
      "spike trains",
      "information capacity",
      "neuron encodes",
      "information rate",
      "neuronal excitability",
      "precision",
      "membrane",
      "reliability",
      "accuracy",
      "neuron"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <metric> <otherscientificterm> <metric> <metric> <otherscientificterm>",
    "relations": [
      "ion channel stochasticity -- USED-FOR -- neuronal models",
      "information rate -- EVALUATE-FOR -- ion channel stochasticity",
      "accuracy -- EVALUATE-FOR -- spike trains",
      "information capacity -- FEATURE-OF -- density of ion channels"
    ],
    "abstract": "the <metric_13> and <metric_14> of <method_6> have been shown to depend on the nature of the stimulus that the <otherscientificterm_8> . adding <otherscientificterm_3> to <method_5> results with a <otherscientificterm_4> that replicates the input-dependent <metric_13> and <metric_11> of real neurons . we calculate the amount of information that an ion channel based stochastic hodgkin-huxley -lrb- hh -rrb- <otherscientificterm_15> model can encode about a wide set of stimuli . we show that both the <metric_9> and the information per spike of the <otherscientificterm_3> is similar to the values reported experimentally . moreover , the amount of information that the <otherscientificterm_8> is correlated with the amplitude of fluctuations in the input , and less so with the <metric_2> of the <otherscientificterm_15> . we also show that for the <otherscientificterm_0> , the <otherscientificterm_7> is robust to changes in the <otherscientificterm_1> in the <otherscientificterm_12> , whereas changing the ratio between the na + and k + ion channels has a considerable effect on the information that the <otherscientificterm_15> can encode . this suggests that neurons may maximize their <otherscientificterm_7> by appropriately balancing the density of the different ion channels that underlies <otherscientificterm_10> .",
    "abstract_og": "the reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes . adding ion channel stochasticity to neuronal models results with a macroscopic behavior that replicates the input-dependent reliability and precision of real neurons . we calculate the amount of information that an ion channel based stochastic hodgkin-huxley -lrb- hh -rrb- neuron model can encode about a wide set of stimuli . we show that both the information rate and the information per spike of the ion channel stochasticity is similar to the values reported experimentally . moreover , the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input , and less so with the average firing rate of the neuron . we also show that for the hh ion channel density , the information capacity is robust to changes in the density of ion channels in the membrane , whereas changing the ratio between the na + and k + ion channels has a considerable effect on the information that the neuron can encode . this suggests that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlies neuronal excitability ."
  },
  {
    "title": "Learning image representations from the pixel level via hierarchical sparse coding .",
    "entities": [
      "invariant and discriminative image representations",
      "two-layer sparse coding scheme",
      "high-order dependency among patterns",
      "rst layer codes",
      "handwritten digit recognition",
      "local image neighborhood",
      "sparse coding methods",
      "local patches",
      "hand-designed descriptors",
      "pixel level",
      "data encoding",
      "rst layer",
      "caltech101 benchmark",
      "object recognition",
      "codebook learning",
      "local regions",
      "image representations",
      "image",
      "features"
    ],
    "types": "<task> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material> <task> <task> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "rst layer -- USED-FOR -- local patches",
      "pixel level -- FEATURE-OF -- two-layer sparse coding scheme",
      "data encoding -- CONJUNCTION -- codebook learning",
      "sparse coding methods -- USED-FOR -- local patches",
      "two-layer sparse coding scheme -- USED-FOR -- image representations"
    ],
    "abstract": "we present a method for learning <method_16> using a <method_1> at the <otherscientificterm_9> . the <otherscientificterm_11> encodes <otherscientificterm_7> of an <material_17> . after pooling within <otherscientificterm_15> , the <otherscientificterm_3> are then passed to the second layer , which jointly encodes signals from the region . unlike traditional <method_6> that encode <otherscientificterm_7> independently , this approach accounts for <otherscientificterm_2> in a <otherscientificterm_5> . we develop algorithms for <task_10> and <task_14> , and show in experiments that the method leads to more <task_0> . the algorithm gives excellent results for <task_4> on mnist and <task_13> on the <material_12> . this marks the rst time that such accuracies have been achieved using automatically learned <otherscientificterm_18> from the <otherscientificterm_9> , rather than using <otherscientificterm_8> .",
    "abstract_og": "we present a method for learning image representations using a two-layer sparse coding scheme at the pixel level . the rst layer encodes local patches of an image . after pooling within local regions , the rst layer codes are then passed to the second layer , which jointly encodes signals from the region . unlike traditional sparse coding methods that encode local patches independently , this approach accounts for high-order dependency among patterns in a local image neighborhood . we develop algorithms for data encoding and codebook learning , and show in experiments that the method leads to more invariant and discriminative image representations . the algorithm gives excellent results for handwritten digit recognition on mnist and object recognition on the caltech101 benchmark . this marks the rst time that such accuracies have been achieved using automatically learned features from the pixel level , rather than using hand-designed descriptors ."
  },
  {
    "title": "Automatic intelligibility assessment of pathologic speech in head and neck cancer based on auditory-inspired spectro-temporal modulations .",
    "entities": [
      "interspeech 2012 speaker trait pathology sub-challenge",
      "auditory-inspired spectro-temporal modulation features",
      "speaker trait challenge problem",
      "automatic speech intelligibility assessment",
      "spectro-temporal modulation features",
      "non-intelligible speech",
      "radical surgery",
      "pathologic speech",
      "speech intelligibility",
      "tumor size",
      "anatomical structures",
      "chemotherapy",
      "svm",
      "gmm",
      "location",
      "radiology",
      "speech"
    ],
    "types": "<method> <otherscientificterm> <task> <task> <otherscientificterm> <material> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "auditory-inspired spectro-temporal modulation features -- USED-FOR -- pathologic speech",
      "anatomical structures -- USED-FOR -- speech",
      "location -- CONJUNCTION -- radiology",
      "radiology -- CONJUNCTION -- chemotherapy",
      "svm -- CONJUNCTION -- gmm",
      "radical surgery -- CONJUNCTION -- radiology",
      "auditory-inspired spectro-temporal modulation features -- USED-FOR -- automatic speech intelligibility assessment"
    ],
    "abstract": "oral , head and neck cancer represents 3 % of all cancers in the united states and is the 6th most common cancer worldwide . depending on the <otherscientificterm_9> , <otherscientificterm_14> and staging , patients are treated by <otherscientificterm_6> , <otherscientificterm_15> , <method_11> or a combination of those treatments . as a result , their <otherscientificterm_10> for <material_16> are impaired and this leads to some negative impact on their <otherscientificterm_8> . as a part of the <method_0> , this study explored the use of <otherscientificterm_1> for <task_3> of those <material_7> . the averaged spectro-temporal modulations of <material_16> considered as either intelligible or non-intelligible in the challenge database were analyzed and it was found that the <material_5> tends to have its modulation amplitude peaks shift towards a smaller rate and scale . based on <method_12> and <method_13> , variants of <otherscientificterm_4> were tested on the <task_2> and the resulting performances on both the development and the test datasets are comparable to the baseline performance .",
    "abstract_og": "oral , head and neck cancer represents 3 % of all cancers in the united states and is the 6th most common cancer worldwide . depending on the tumor size , location and staging , patients are treated by radical surgery , radiology , chemotherapy or a combination of those treatments . as a result , their anatomical structures for speech are impaired and this leads to some negative impact on their speech intelligibility . as a part of the interspeech 2012 speaker trait pathology sub-challenge , this study explored the use of auditory-inspired spectro-temporal modulation features for automatic speech intelligibility assessment of those pathologic speech . the averaged spectro-temporal modulations of speech considered as either intelligible or non-intelligible in the challenge database were analyzed and it was found that the non-intelligible speech tends to have its modulation amplitude peaks shift towards a smaller rate and scale . based on svm and gmm , variants of spectro-temporal modulation features were tested on the speaker trait challenge problem and the resulting performances on both the development and the test datasets are comparable to the baseline performance ."
  },
  {
    "title": "A general DSP processor at the cost of 23K gates and 1/2 a man-year design time .",
    "entities": [
      "16-bit fixed point dsp processor",
      "mac commercial dsp processors",
      "performance/gate count ratio",
      "communication infrastructure applications",
      "computational units",
      "clock frequency",
      "assembler instructions",
      "design time",
      "rtl code",
      "hardware accelerators",
      "i/o facilities",
      "heterogeneous processors",
      "debugger",
      "net-list",
      "as-sembler",
      "benchmarking"
    ],
    "types": "<method> <method> <metric> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method> <method> <method> <otherscientificterm> <metric> <otherscientificterm> <method>",
    "relations": [
      "as-sembler -- CONJUNCTION -- debugger",
      "16-bit fixed point dsp processor -- USED-FOR -- hardware accelerators",
      "heterogeneous processors -- USED-FOR -- communication infrastructure applications",
      "computational units -- CONJUNCTION -- assembler instructions",
      "debugger -- CONJUNCTION -- rtl code"
    ],
    "abstract": "this paper describes the design and implementation of a <method_0> . the <method_0> is intended as a platform for <method_9> and allows additional <otherscientificterm_4> and <otherscientificterm_6> to be added . the <method_10> can also be customized to the needs of a specific application . <method_15> has shown that the <method_0> , without any <method_9> , has a performance comparable to single <method_1> . the architecture has been successfully synthesized in a 0.13 m process , resulting in a <metric_13> of about 23000 gates , and a <otherscientificterm_5> of 195 mhz , making the <metric_2> very competitive . it is also small enough to integrate 100 <method_11> on a chip for example for <task_3> . the complete <metric_7> , including architecture and instruction set planning , <otherscientificterm_14> , <otherscientificterm_12> , instruction set simulator , <otherscientificterm_8> and complete verification was about half a person-year .",
    "abstract_og": "this paper describes the design and implementation of a 16-bit fixed point dsp processor . the 16-bit fixed point dsp processor is intended as a platform for hardware accelerators and allows additional computational units and assembler instructions to be added . the i/o facilities can also be customized to the needs of a specific application . benchmarking has shown that the 16-bit fixed point dsp processor , without any hardware accelerators , has a performance comparable to single mac commercial dsp processors . the architecture has been successfully synthesized in a 0.13 m process , resulting in a net-list of about 23000 gates , and a clock frequency of 195 mhz , making the performance/gate count ratio very competitive . it is also small enough to integrate 100 heterogeneous processors on a chip for example for communication infrastructure applications . the complete design time , including architecture and instruction set planning , as-sembler , debugger , instruction set simulator , rtl code and complete verification was about half a person-year ."
  },
  {
    "title": "Convolutional Neural Networks for Sentence Classification .",
    "entities": [
      "convolutional neural networks",
      "task-specific and static vectors",
      "learning task-specific vectors",
      "sentence-level classification tasks",
      "pre-trained word vectors",
      "question classification",
      "hyperparameter tuning",
      "sentiment analysis",
      "static vectors",
      "cnn models",
      "fine-tuning"
    ],
    "types": "<method> <otherscientificterm> <method> <task> <material> <task> <method> <task> <method> <method> <method>",
    "relations": [
      "static vectors -- USED-FOR -- convolutional neural networks",
      "sentiment analysis -- CONJUNCTION -- question classification",
      "pre-trained word vectors -- USED-FOR -- sentence-level classification tasks",
      "hyperparameter tuning -- CONJUNCTION -- static vectors"
    ],
    "abstract": "we report on a series of experiments with <method_0> trained on top of <material_4> for <task_3> . we show that a simple <method_0> with little <method_6> and <method_8> achieves excellent results on multiple benchmarks . <method_2> through <method_10> offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both <otherscientificterm_1> . the <method_9> discussed herein improve upon the state of the art on 4 out of 7 tasks , which include <task_7> and <task_5> .",
    "abstract_og": "we report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks . we show that a simple convolutional neural networks with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task-specific vectors through fine-tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification ."
  },
  {
    "title": "Learning Abstract Concept Embeddings from Multi-Modal Data : Since You Probably Ca n't See What I Mean .",
    "entities": [
      "commonly-occurring abstract lexical concepts",
      "linguistic and perceptual input",
      "abstract conceptual representation",
      "concrete noun concepts",
      "human language learning",
      "language-only models",
      "everyday language",
      "multi-modal approach",
      "concrete concepts",
      "semantic representations",
      "multi-modal embeddings",
      "perceptual information",
      "nlp"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <method> <material> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task>",
    "relations": [
      "linguistic and perceptual input -- USED-FOR -- semantic representations",
      "multi-modal approach -- COMPARE -- language-only models"
    ],
    "abstract": "models that acquire <method_9> from both <otherscientificterm_1> are of interest to researchers in <task_12> because of the obvious parallels with <task_4> . performance advantages of the <method_7> over <method_5> have been clearly established when models are required to learn <otherscientificterm_3> . however , such concepts are comparatively rare in <material_6> . in this work , we present a new means of extending the scope of <method_7> to more <otherscientificterm_0> via an approach that learns <otherscientificterm_10> . our architecture out-performs previous approaches in combining input from distinct modalities , and propagates <otherscientificterm_11> on <otherscientificterm_8> to abstract concepts more effectively than alternatives . we discuss the implications of our results both for optimizing the performance of <method_7> and for theories of <task_2> .",
    "abstract_og": "models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in nlp because of the obvious parallels with human language learning . performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts . however , such concepts are comparatively rare in everyday language . in this work , we present a new means of extending the scope of multi-modal approach to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings . our architecture out-performs previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives . we discuss the implications of our results both for optimizing the performance of multi-modal approach and for theories of abstract conceptual representation ."
  },
  {
    "title": "Belief Revision with General Epistemic States .",
    "entities": [
      "epistemic state",
      "classical iterated belief revision rules",
      "semantical characterisation of geps",
      "iterated belief revision",
      "mathematical structure",
      "epistemic states",
      "conditional beliefs",
      "epis-temic state",
      "belief algebra",
      "epistemic state",
      "belief set",
      "belief algebras",
      "1-1 correspondence",
      "ax-iomatic characterisation",
      "binary relation",
      "total preorder",
      "rules"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "ax-iomatic characterisation -- USED-FOR -- epistemic states",
      "belief algebra -- HYPONYM-OF -- mathematical structure",
      "conditional beliefs -- FEATURE-OF -- epistemic state",
      "conditional beliefs -- USED-FOR -- ax-iomatic characterisation"
    ],
    "abstract": "in order to properly regulate <method_3> , darwiche and pearl -lrb- 1997 -rrb- model belief revision as revising <otherscientificterm_5> by propositions . an <otherscientificterm_9> in their sense consists of a <otherscientificterm_10> and a set of <otherscientificterm_6> . although the denotation of an <otherscientificterm_7> can be indirectly captured by a <otherscientificterm_15> on the set of worlds , it is unclear how to directly capture the structure in terms of the beliefs and <otherscientificterm_6> it contains . in this paper , we first provide an <method_13> for <otherscientificterm_5> by using nine <otherscientificterm_16> about beliefs and <otherscientificterm_6> , and then argue that the last two <otherscientificterm_16> are too strong and should be eliminated for characterising the belief state of an agent . we call a structure which satisfies the first seven <otherscientificterm_16> a general <method_0> . to provide a <method_2> , we introduce a <otherscientificterm_4> called <method_8> , which is in essence a certain <otherscientificterm_14> defined on the power set of worlds . we then establish a <otherscientificterm_12> between <method_0> and <otherscientificterm_11> , and show that total preorders on worlds are special cases of <otherscientificterm_11> . furthermore , using the notion of <otherscientificterm_11> , we extend the <otherscientificterm_1> of darwiche and pearl to our setting of general <otherscientificterm_5> .",
    "abstract_og": "in order to properly regulate iterated belief revision , darwiche and pearl -lrb- 1997 -rrb- model belief revision as revising epistemic states by propositions . an epistemic state in their sense consists of a belief set and a set of conditional beliefs . although the denotation of an epis-temic state can be indirectly captured by a total preorder on the set of worlds , it is unclear how to directly capture the structure in terms of the beliefs and conditional beliefs it contains . in this paper , we first provide an ax-iomatic characterisation for epistemic states by using nine rules about beliefs and conditional beliefs , and then argue that the last two rules are too strong and should be eliminated for characterising the belief state of an agent . we call a structure which satisfies the first seven rules a general epistemic state . to provide a semantical characterisation of geps , we introduce a mathematical structure called belief algebra , which is in essence a certain binary relation defined on the power set of worlds . we then establish a 1-1 correspondence between epistemic state and belief algebras , and show that total preorders on worlds are special cases of belief algebras . furthermore , using the notion of belief algebras , we extend the classical iterated belief revision rules of darwiche and pearl to our setting of general epistemic states ."
  },
  {
    "title": "Prediction of time series using Yule-Walker equations with kernels .",
    "entities": [
      "autoregressive model",
      "ar model parameters",
      "pre-image problem",
      "yule-walker equations",
      "kernel machines",
      "covariance function",
      "input space",
      "feature space",
      "model parameters"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "ar model parameters -- CONJUNCTION -- covariance function",
      "yule-walker equations -- USED-FOR -- ar model parameters",
      "kernel machines -- USED-FOR -- autoregressive model",
      "feature space -- FEATURE-OF -- yule-walker equations"
    ],
    "abstract": "the <method_0> is a well-known technique to analyze time series . the <method_3> provide a straightforward connection between the <otherscientificterm_1> and the <otherscientificterm_5> of the process . in this paper , we propose a nonlinear extension of the <method_0> using <method_4> . to this end , we explore the <method_3> in the <otherscientificterm_7> , and show that the <otherscientificterm_8> can be estimated using the concept of expected kernels . finally , in order to predict once the <method_0> identified , we solve a <task_2> by getting back from the <otherscientificterm_7> to the <otherscientificterm_6> . we also give new insights into the convex-ity of the <task_2> . the relevance of the proposed method is evaluated on several time series .",
    "abstract_og": "the autoregressive model is a well-known technique to analyze time series . the yule-walker equations provide a straightforward connection between the ar model parameters and the covariance function of the process . in this paper , we propose a nonlinear extension of the autoregressive model using kernel machines . to this end , we explore the yule-walker equations in the feature space , and show that the model parameters can be estimated using the concept of expected kernels . finally , in order to predict once the autoregressive model identified , we solve a pre-image problem by getting back from the feature space to the input space . we also give new insights into the convex-ity of the pre-image problem . the relevance of the proposed method is evaluated on several time series ."
  },
  {
    "title": "DNNF-based Belief State Estimation .",
    "entities": [
      "best-first belief state update algorithm",
      "dnnf-based belief state estimation algorithm",
      "best-first trajec-tory enumeration algorithm",
      "probabilistic concurrent constraint automata",
      "mode estimation of pccas",
      "factored hidden markov models",
      "polynomial-time bounded algorithm",
      "mexec algorithm",
      "sd-dnnf-based representation",
      "embedded systems",
      "polynomial time",
      "declarative models",
      "probabilistic data",
      "physical plant",
      "cnf",
      "accuracy",
      "robustness"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <method> <method> <method> <otherscientificterm> <method> <material> <otherscientificterm> <method> <metric> <metric>",
    "relations": [
      "probabilistic concurrent constraint automata -- HYPONYM-OF -- factored hidden markov models",
      "best-first trajec-tory enumeration algorithm -- USED-FOR -- mode estimation of pccas",
      "mexec algorithm -- COMPARE -- best-first trajec-tory enumeration algorithm",
      "polynomial time -- CONJUNCTION -- mexec algorithm"
    ],
    "abstract": "as <method_9> grow increasingly complex , there is a pressing need for diagnosing and monitoring capabilities that estimate the system state robustly . this paper is based on approaches that address the problem of <metric_16> by reasoning over <method_11> of the <otherscientificterm_13> , represented as a variant of <method_5> , called <method_3> . prior work on <method_4> is based on a <method_2> . two algorithms have since made improvements to the <method_2> : 1 -rrb- the <method_0> has improved the <metric_15> of <method_2> and 2 -rrb- the <method_7> has introduced a <method_6> using a smooth deterministic decomposable negation normal form -lrb- sd-dnnf -rrb- representation . this paper introduces a new <method_1> that merges the <otherscientificterm_10> bound of the <method_7> with the <metric_15> of the <method_2> . this paper also presents an encoding of a <method_1> as a <method_14> with <material_12> , suitable for compilation into an <method_8> . the <method_8> supports computing k belief states from k previous belief states in the <method_2> .",
    "abstract_og": "as embedded systems grow increasingly complex , there is a pressing need for diagnosing and monitoring capabilities that estimate the system state robustly . this paper is based on approaches that address the problem of robustness by reasoning over declarative models of the physical plant , represented as a variant of factored hidden markov models , called probabilistic concurrent constraint automata . prior work on mode estimation of pccas is based on a best-first trajec-tory enumeration algorithm . two algorithms have since made improvements to the best-first trajec-tory enumeration algorithm : 1 -rrb- the best-first belief state update algorithm has improved the accuracy of best-first trajec-tory enumeration algorithm and 2 -rrb- the mexec algorithm has introduced a polynomial-time bounded algorithm using a smooth deterministic decomposable negation normal form -lrb- sd-dnnf -rrb- representation . this paper introduces a new dnnf-based belief state estimation algorithm that merges the polynomial time bound of the mexec algorithm with the accuracy of the best-first trajec-tory enumeration algorithm . this paper also presents an encoding of a dnnf-based belief state estimation algorithm as a cnf with probabilistic data , suitable for compilation into an sd-dnnf-based representation . the sd-dnnf-based representation supports computing k belief states from k previous belief states in the best-first trajec-tory enumeration algorithm ."
  },
  {
    "title": "Digital face makeup by example .",
    "entities": [
      "skin detail layer",
      "face structure layer",
      "color layer",
      "physical makeup",
      "face makeup",
      "transferring makeup",
      "face structure",
      "color"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "skin detail layer -- CONJUNCTION -- color layer",
      "face structure layer -- CONJUNCTION -- skin detail layer"
    ],
    "abstract": "this paper introduces an approach of creating <task_4> upon a face image with another image as the style example . our approach is analogous to <otherscientificterm_3> , as we modify the <otherscientificterm_7> and skin detail while preserving the <otherscientificterm_6> . more precisely , we first decompose the two images into three layers : <otherscientificterm_1> , <otherscientificterm_0> , and <otherscientificterm_2> . thereafter , we transfer information from each layer of one image to corresponding layer of the other image . one major advantage of the proposed method lies in that only one example image is required . this renders <task_4> by example very convenient and practical . equally , this enables some additional interesting applications , such as applying makeup by a portraiture . the experiment results demonstrate the effectiveness of the proposed approach in faithfully <task_5> .",
    "abstract_og": "this paper introduces an approach of creating face makeup upon a face image with another image as the style example . our approach is analogous to physical makeup , as we modify the color and skin detail while preserving the face structure . more precisely , we first decompose the two images into three layers : face structure layer , skin detail layer , and color layer . thereafter , we transfer information from each layer of one image to corresponding layer of the other image . one major advantage of the proposed method lies in that only one example image is required . this renders face makeup by example very convenient and practical . equally , this enables some additional interesting applications , such as applying makeup by a portraiture . the experiment results demonstrate the effectiveness of the proposed approach in faithfully transferring makeup ."
  },
  {
    "title": "Attention-based LSTM Network for Cross-Lingual Sentiment Classification .",
    "entities": [
      "long short term memory network",
      "attention-based bilingual representation learning model",
      "bilingual lstm network",
      "manually labeled data",
      "hierarchical attention mechanism",
      "sentence-level attention model",
      "word-level attention model",
      "supervised learning algorithms",
      "sentiment classification methods",
      "cross-lingual sentiment classification",
      "distributed semantics",
      "resource-poor languages",
      "sentiment resources",
      "word sequences",
      "resource-rich language",
      "english",
      "chinese"
    ],
    "types": "<method> <method> <task> <material> <method> <method> <method> <method> <method> <task> <otherscientificterm> <material> <otherscientificterm> <material> <material> <material> <material>",
    "relations": [
      "supervised learning algorithms -- USED-FOR -- sentiment classification methods",
      "manually labeled data -- USED-FOR -- supervised learning algorithms",
      "hierarchical attention mechanism -- USED-FOR -- bilingual lstm network",
      "english -- CONJUNCTION -- chinese",
      "english -- USED-FOR -- hierarchical attention mechanism"
    ],
    "abstract": "most of the state-of-the-art <method_8> are based on <method_7> which require large amounts of <material_3> . however , the labeled resources are usually imbalanced in different languages . <task_9> tackles the problem by adapting the <otherscientificterm_12> in a <material_14> to <material_11> . in this study , we propose an <method_1> which learns the <otherscientificterm_10> of the documents in both the source and the target languages . in each language , we use <method_0> to <method_4> the documents , which has been proved to be very effective for <material_13> . meanwhile , we propose a <method_4> for the <task_2> . the <method_5> learns which sentences of a document are more important for determining the overall sentiment while the <method_6> learns which words in each sentence are decisive . the proposed <method_4> achieves good results on a benchmark dataset using <material_15> as the source language and <material_16> as the target language .",
    "abstract_og": "most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data . however , the labeled resources are usually imbalanced in different languages . cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages . in this study , we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages . in each language , we use long short term memory network to hierarchical attention mechanism the documents , which has been proved to be very effective for word sequences . meanwhile , we propose a hierarchical attention mechanism for the bilingual lstm network . the sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive . the proposed hierarchical attention mechanism achieves good results on a benchmark dataset using english as the source language and chinese as the target language ."
  },
  {
    "title": "Catching heuristics are optimal control policies .",
    "entities": [
      "continuous partially observable markov decision process",
      "rational account of human ball-catching behavior",
      "generating reactive and predictive behavior",
      "stochastic optimal control theory",
      "stochastic optimal control",
      "ratio of system",
      "immediate visual feedback",
      "computational solutions",
      "control problem",
      "modeling catching",
      "observation noise",
      "reaction time",
      "task duration",
      "interception strategies",
      "ground contact",
      "airborne ball",
      "unifying explanation",
      "ball-catching agent",
      "perceptual latency",
      "ball trajectory",
      "model parameters",
      "heuristics",
      "noise"
    ],
    "types": "<method> <otherscientificterm> <task> <method> <otherscientificterm> <metric> <otherscientificterm> <method> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "heuristics -- USED-FOR -- control problem",
      "continuous partially observable markov decision process -- USED-FOR -- modeling catching",
      "interception strategies -- USED-FOR -- control problem",
      "reaction time -- CONJUNCTION -- task duration",
      "interception strategies -- USED-FOR -- heuristics",
      "stochastic optimal control theory -- USED-FOR -- modeling catching",
      "computational solutions -- USED-FOR -- control problem",
      "ground contact -- CONJUNCTION -- perceptual latency",
      "stochastic optimal control theory -- USED-FOR -- continuous partially observable markov decision process",
      "immediate visual feedback -- USED-FOR -- heuristics",
      "noise -- HYPONYM-OF -- model parameters"
    ],
    "abstract": "two seemingly contradictory theories attempt to explain how humans move to intercept an <otherscientificterm_15> . one theory posits that humans predict the <otherscientificterm_19> to optimally plan future actions ; the other claims that , instead of performing such complicated computations , humans employ <method_21> to reactively choose appropriate actions based on <otherscientificterm_6> . in this paper , we show that <method_13> appearing to be <method_21> can be understood as <method_7> to the optimal <task_8> faced by a <method_17> acting under uncertainty . <task_9> as a <method_0> and employing <method_3> , we discover that the four main <method_21> described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball . specifically , by varying <otherscientificterm_20> such as <otherscientificterm_22> , time to <otherscientificterm_14> , and <otherscientificterm_18> , we show that different strategies arise under different circumstances . the catcher 's policy switches between <task_2> based on the <metric_5> to <otherscientificterm_10> and the ratio between <otherscientificterm_11> and <otherscientificterm_12> . thus , we provide a <otherscientificterm_1> and a <method_16> for seemingly contradictory theories of target interception on the basis of <otherscientificterm_4> .",
    "abstract_og": "two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball . one theory posits that humans predict the ball trajectory to optimally plan future actions ; the other claims that , instead of performing such complicated computations , humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback . in this paper , we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty . modeling catching as a continuous partially observable markov decision process and employing stochastic optimal control theory , we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball . specifically , by varying model parameters such as noise , time to ground contact , and perceptual latency , we show that different strategies arise under different circumstances . the catcher 's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration . thus , we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control ."
  },
  {
    "title": "Non-lexical neural architecture for fine-grained POS Tagging .",
    "entities": [
      "pos and morphological tagging task",
      "meaningful word representations",
      "raw character stream",
      "character stream",
      "feature engineering",
      "prediction stage",
      "word representations",
      "neural architectures",
      "word representation",
      "convolutional network",
      "modelling stages",
      "german"
    ],
    "types": "<task> <method> <material> <otherscientificterm> <method> <otherscientificterm> <method> <method> <method> <method> <method> <material>",
    "relations": [
      "raw character stream -- USED-FOR -- word representations",
      "convolutional network -- USED-FOR -- word representation",
      "convolutional network -- HYPONYM-OF -- modelling stages",
      "convolutional network -- USED-FOR -- word representations",
      "neural architectures -- USED-FOR -- word representations",
      "raw character stream -- USED-FOR -- neural architectures",
      "convolutional network -- USED-FOR -- prediction stage"
    ],
    "abstract": "in this paper we explore a pos tagging application of <method_7> that can infer <method_6> from the <material_2> . it relies on two <method_10> that are jointly learnt : a <method_9> that infers a <method_8> directly from the <otherscientificterm_3> , followed by a <otherscientificterm_5> . models are evaluated on a <task_0> for <material_11> . experimental results show that the <method_9> can infer <method_1> , while for the <otherscientificterm_5> , a well designed and structured strategy allows the <method_9> to outperform state-of-the-art results , without any <method_4> .",
    "abstract_og": "in this paper we explore a pos tagging application of neural architectures that can infer word representations from the raw character stream . it relies on two modelling stages that are jointly learnt : a convolutional network that infers a word representation directly from the character stream , followed by a prediction stage . models are evaluated on a pos and morphological tagging task for german . experimental results show that the convolutional network can infer meaningful word representations , while for the prediction stage , a well designed and structured strategy allows the convolutional network to outperform state-of-the-art results , without any feature engineering ."
  },
  {
    "title": "Segmentation-Aware Deformable Part Models .",
    "entities": [
      "spatial support of slic superpixels",
      "object-specific and background changes",
      "enhanced , background-invariant features",
      "sliding window detectors",
      "dense sift descriptors",
      "soft segmentation masks",
      "low-level hog features",
      "bottom-up segmentation",
      "segmentation masks",
      "feature variation",
      "candidate window",
      "seg-mentation",
      "ap",
      "detection",
      "segmentation"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <metric> <task> <method>",
    "relations": [
      "segmentation masks -- USED-FOR -- enhanced , background-invariant features",
      "segmentation -- USED-FOR -- feature variation",
      "feature variation -- FEATURE-OF -- object-specific and background changes",
      "spatial support of slic superpixels -- USED-FOR -- low-level hog features"
    ],
    "abstract": "in this work we propose a technique to combine <method_7> , coming in the form of slic superpixels , with <method_3> , such as deformable part models -lrb- dpms -rrb- . the merit of our approach lies in ` cleaning up ' the <otherscientificterm_6> by exploiting the <method_0> ; this can be understood as using <method_14> to split the <otherscientificterm_9> into <otherscientificterm_1> . rather than committing to a single <method_11> we use a large pool of slic superpixels and combine them in a scale - , position-and object-dependent manner to build <method_5> . the <method_8> can be computed fast enough to repeat this process over every <otherscientificterm_10> , during training and <task_13> , for both the root and part filters of dpms . we use these <method_8> to construct <otherscientificterm_2> to train dpms . we test our approach on the pascal voc 2007 , outperforming the standard dpm in 17 out of 20 classes , yielding an average increase of 1.7 % <metric_12> . additionally , we demonstrate the robustness of this approach , extending it to <method_4> for large displacement optical flow .",
    "abstract_og": "in this work we propose a technique to combine bottom-up segmentation , coming in the form of slic superpixels , with sliding window detectors , such as deformable part models -lrb- dpms -rrb- . the merit of our approach lies in ` cleaning up ' the low-level hog features by exploiting the spatial support of slic superpixels ; this can be understood as using segmentation to split the feature variation into object-specific and background changes . rather than committing to a single seg-mentation we use a large pool of slic superpixels and combine them in a scale - , position-and object-dependent manner to build soft segmentation masks . the segmentation masks can be computed fast enough to repeat this process over every candidate window , during training and detection , for both the root and part filters of dpms . we use these segmentation masks to construct enhanced , background-invariant features to train dpms . we test our approach on the pascal voc 2007 , outperforming the standard dpm in 17 out of 20 classes , yielding an average increase of 1.7 % ap . additionally , we demonstrate the robustness of this approach , extending it to dense sift descriptors for large displacement optical flow ."
  },
  {
    "title": "Principal mixture speaker adaptation for improved continuous speech recognition .",
    "entities": [
      "speaker-independent speech recognition systems",
      "speaker adaptation",
      "principal mixture speaker adaptation method",
      "full multivariate mixture gaussian densities",
      "full mixture sa models",
      "speaker 's characteristics",
      "hmm observation density",
      "acoustic hmm",
      "gaussian densities",
      "recognition speed",
      "si models",
      "principle mixtures",
      "speaker variabilities",
      "speaker variation",
      "hmm complexity",
      "observation density",
      "recognition accuracy",
      "cdhmm"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <metric> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <metric> <method>",
    "relations": [
      "cdhmm -- USED-FOR -- speaker-independent speech recognition systems",
      "speaker adaptation -- USED-FOR -- speaker variation",
      "recognition accuracy -- EVALUATE-FOR -- principal mixture speaker adaptation method",
      "principal mixture speaker adaptation method -- USED-FOR -- hmm complexity",
      "gaussian densities -- FEATURE-OF -- acoustic hmm",
      "recognition speed -- EVALUATE-FOR -- si models",
      "recognition speed -- EVALUATE-FOR -- full mixture sa models",
      "recognition accuracy -- EVALUATE-FOR -- si models",
      "recognition speed -- EVALUATE-FOR -- principal mixture speaker adaptation method"
    ],
    "abstract": "nowadays , almost all <method_0> use <method_17> with multivariate mixture gaussian as <otherscientificterm_15> to cover <otherscientificterm_12> . it has been shown that given sufficient training data , the more mixtures are used in the <otherscientificterm_6> , the better the <method_0> 's perform . however , <method_7> with more <otherscientificterm_8> is more complex and slows down <metric_9> . another efficient way to handle <otherscientificterm_13> is to use <method_1> . yet , even though speaker adaptation of <otherscientificterm_3> can increase <metric_16> , it does not improve <metric_9> . in this paper , we introduce a <method_2> which reduces <metric_14> by choosing only the <otherscientificterm_11> corresponding to a particular <otherscientificterm_5> . we show that our <method_2> both improves <metric_16> by 31.8 % when compared to <method_10> , and reduces <metric_9> by 30 % , when compared to <method_4> .",
    "abstract_og": "nowadays , almost all speaker-independent speech recognition systems use cdhmm with multivariate mixture gaussian as observation density to cover speaker variabilities . it has been shown that given sufficient training data , the more mixtures are used in the hmm observation density , the better the speaker-independent speech recognition systems 's perform . however , acoustic hmm with more gaussian densities is more complex and slows down recognition speed . another efficient way to handle speaker variation is to use speaker adaptation . yet , even though speaker adaptation of full multivariate mixture gaussian densities can increase recognition accuracy , it does not improve recognition speed . in this paper , we introduce a principal mixture speaker adaptation method which reduces hmm complexity by choosing only the principle mixtures corresponding to a particular speaker 's characteristics . we show that our principal mixture speaker adaptation method both improves recognition accuracy by 31.8 % when compared to si models , and reduces recognition speed by 30 % , when compared to full mixture sa models ."
  },
  {
    "title": "Learning priors for calibrating families of stereo cameras .",
    "entities": [
      "online calibration of additional cameras",
      "stereo camera calibration",
      "unknown prior distribution",
      "online camera recalibration",
      "computer vision systems",
      "offline-calibrated cameras",
      "repeated texture",
      "point correspondences",
      "real-world scenes",
      "calibration problem",
      "recalibration information",
      "features",
      "planar",
      "robustness",
      "accuracy",
      "calibration"
    ],
    "types": "<task> <task> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <metric> <task>",
    "relations": [
      "online camera recalibration -- USED-FOR -- computer vision systems",
      "accuracy -- CONJUNCTION -- robustness"
    ],
    "abstract": "online camera recalibration is necessary for long-term deployment of <method_4> . existing algorithms assume that the source of <otherscientificterm_10> is a set of <otherscientificterm_11> in a general 3d scene ; and that enough <otherscientificterm_11> are observed that the <task_9> is well-constrained . however , these assumptions are frequently invalid outside the laboratory . <otherscientificterm_8> often lack texture , contain <otherscientificterm_6> , or are mostly <otherscientificterm_12> , making <task_15> difficult or impossible . in this paper we consider the <task_15> of families of stereo cameras , where each camera is assumed to have parameters drawn from a common but <otherscientificterm_2> . we show how estimation of this prior using a small-number of <otherscientificterm_5> -lrb- e.g. from the same production line -rrb- allows <task_0> using a small number of <otherscientificterm_7> ; and that using the estimated prior significantly increases the <metric_14> and <metric_13> of <task_1> .",
    "abstract_og": "online camera recalibration is necessary for long-term deployment of computer vision systems . existing algorithms assume that the source of recalibration information is a set of features in a general 3d scene ; and that enough features are observed that the calibration problem is well-constrained . however , these assumptions are frequently invalid outside the laboratory . real-world scenes often lack texture , contain repeated texture , or are mostly planar , making calibration difficult or impossible . in this paper we consider the calibration of families of stereo cameras , where each camera is assumed to have parameters drawn from a common but unknown prior distribution . we show how estimation of this prior using a small-number of offline-calibrated cameras -lrb- e.g. from the same production line -rrb- allows online calibration of additional cameras using a small number of point correspondences ; and that using the estimated prior significantly increases the accuracy and robustness of stereo camera calibration ."
  },
  {
    "title": "Friends and enemies : a novel initialization for speaker diarization .",
    "entities": [
      "bayesian information criterion",
      "absolute cluster purity improvement",
      "iterative cluster merging",
      "diarization error rate",
      "initialization algorithm",
      "rt05s evaluation",
      "agglomerative clustering",
      "speaker diarization",
      "clusters",
      "clustering",
      "robustness"
    ],
    "types": "<otherscientificterm> <metric> <method> <metric> <method> <metric> <method> <task> <otherscientificterm> <method> <metric>",
    "relations": [
      "iterative cluster merging -- USED-FOR -- clustering",
      "agglomerative clustering -- USED-FOR -- speaker diarization"
    ],
    "abstract": "the task of <task_7> consists of answering the question '' who spoke when ? '' . the most commonly used approach to <task_7> is <method_6> of multiple initial <otherscientificterm_8> . even though the initial <method_9> is greatly modified by <method_2> and possibly multiple resegmentations of the data , the <method_4> is a key module for system performance and <metric_10> . in this paper we present a novel approach that obtains a desired initial number of <otherscientificterm_8> in three steps . it first computes possible speaker change points via a standard technique based on the <otherscientificterm_0> . it then classifies the resulting segments into '' friend '' and '' enemy '' groups to finally creates an initial set of <otherscientificterm_8> for the system . we test this algorithm with the dataset used in the <metric_5> , where we show a 13 % <metric_3> relative improvement and a 2.5 % <metric_1> with respect to our previous algorithm .",
    "abstract_og": "the task of speaker diarization consists of answering the question '' who spoke when ? '' . the most commonly used approach to speaker diarization is agglomerative clustering of multiple initial clusters . even though the initial clustering is greatly modified by iterative cluster merging and possibly multiple resegmentations of the data , the initialization algorithm is a key module for system performance and robustness . in this paper we present a novel approach that obtains a desired initial number of clusters in three steps . it first computes possible speaker change points via a standard technique based on the bayesian information criterion . it then classifies the resulting segments into '' friend '' and '' enemy '' groups to finally creates an initial set of clusters for the system . we test this algorithm with the dataset used in the rt05s evaluation , where we show a 13 % diarization error rate relative improvement and a 2.5 % absolute cluster purity improvement with respect to our previous algorithm ."
  },
  {
    "title": "Computational Semantics of Noun Compounds in a Semantic Space Model .",
    "entities": [
      "meaning of noun compounds",
      "similarity judgment test",
      "semantic space model",
      "constituent word vectors",
      "computing compound vectors",
      "multiple-choice synonym test",
      "emergent meanings",
      "comparison algorithm",
      "noun compounds",
      "accuracy",
      "predication"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <metric> <otherscientificterm> <method> <otherscientificterm> <metric> <otherscientificterm>",
    "relations": [
      "predication -- CONJUNCTION -- similarity judgment test",
      "comparison algorithm -- USED-FOR -- meaning of noun compounds",
      "comparison algorithm -- USED-FOR -- noun compounds",
      "multiple-choice synonym test -- CONJUNCTION -- similarity judgment test",
      "semantic space model -- USED-FOR -- meaning of noun compounds"
    ],
    "abstract": "this study examines the ability of a <method_2> to represent the <otherscientificterm_0> such as '' information gathering '' or '' weather forecast '' . a new algorithm , comparison , is proposed for <task_4> from <otherscientificterm_3> , and compared with other algorithms -lrb- i.e. , <otherscientificterm_10> and centroid -rrb- in terms of <metric_9> of <metric_5> and <otherscientificterm_1> . the result of both tests is that the <method_7> is , on the whole , superior to other algorithms , and in particular achieves the best performance when <otherscientificterm_8> have <otherscientificterm_6> . furthermore , the <method_7> also works for novel <otherscientificterm_8> that do not occur in the corpus . these findings indicate that a <method_2> in general and the <method_7> in particular has sufficient ability to compute the <otherscientificterm_0> .",
    "abstract_og": "this study examines the ability of a semantic space model to represent the meaning of noun compounds such as '' information gathering '' or '' weather forecast '' . a new algorithm , comparison , is proposed for computing compound vectors from constituent word vectors , and compared with other algorithms -lrb- i.e. , predication and centroid -rrb- in terms of accuracy of multiple-choice synonym test and similarity judgment test . the result of both tests is that the comparison algorithm is , on the whole , superior to other algorithms , and in particular achieves the best performance when noun compounds have emergent meanings . furthermore , the comparison algorithm also works for novel noun compounds that do not occur in the corpus . these findings indicate that a semantic space model in general and the comparison algorithm in particular has sufficient ability to compute the meaning of noun compounds ."
  },
  {
    "title": "Probing the Linguistic Strengths and Limitations of Unsupervised Grammar Induction .",
    "entities": [
      "raw word or tag sequences",
      "unsupervised ccg parsers",
      "grammar induction algorithms",
      "unlabeled dependencies",
      "syntactic structure",
      "grammar induction",
      "supervision",
      "ccgbank"
    ],
    "types": "<material> <method> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <material>",
    "relations": [
      "grammar induction -- USED-FOR -- syntactic structure",
      "raw word or tag sequences -- USED-FOR -- syntactic structure",
      "grammar induction algorithms -- USED-FOR -- unlabeled dependencies"
    ],
    "abstract": "work in <task_5> should help shed light on the amount of <otherscientificterm_4> that is discoverable from <material_0> . but since most current <method_2> produce <otherscientificterm_3> , it is difficult to analyze what types of constructions these <method_2> can or can not capture , and , therefore , to identify where additional <otherscientificterm_6> may be necessary . this paper provides an in-depth analysis of the errors made by <method_1> by evaluating <method_1> against the labeled dependencies in <material_7> , hinting at new research directions necessary for progress in <task_5> .",
    "abstract_og": "work in grammar induction should help shed light on the amount of syntactic structure that is discoverable from raw word or tag sequences . but since most current grammar induction algorithms produce unlabeled dependencies , it is difficult to analyze what types of constructions these grammar induction algorithms can or can not capture , and , therefore , to identify where additional supervision may be necessary . this paper provides an in-depth analysis of the errors made by unsupervised ccg parsers by evaluating unsupervised ccg parsers against the labeled dependencies in ccgbank , hinting at new research directions necessary for progress in grammar induction ."
  },
  {
    "title": "Neurocalibration : A Neural Network That Can Tell Camera Calibration Parameters .",
    "entities": [
      "camera m o del parameters",
      "computer vision tasks",
      "automated active lenses",
      "random initial weights",
      "world 3d points",
      "2d image pixels",
      "camera c alibration",
      "camera calibration",
      "perspective-projection-transformation matrix",
      "rotational transformation",
      "calibrating cameras",
      "synthetic data",
      "noise conditions",
      "real images",
      "neural approach",
      "neural approaches",
      "calibration problems",
      "calibrating network",
      "orthogonality constraints",
      "accuracy"
    ],
    "types": "<otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task> <material> <otherscientificterm> <material> <method> <method> <task> <method> <otherscientificterm> <metric>",
    "relations": [
      "neural approach -- USED-FOR -- camera c alibration",
      "camera calibration -- HYPONYM-OF -- computer vision tasks",
      "neural approach -- USED-FOR -- calibrating cameras",
      "neural approaches -- COMPARE -- calibrating network",
      "automated active lenses -- FEATURE-OF -- calibrating cameras",
      "accuracy -- EVALUATE-FOR -- neural approach",
      "synthetic data -- EVALUATE-FOR -- neural approach",
      "neural approach -- USED-FOR -- computer vision tasks",
      "world 3d points -- CONJUNCTION -- 2d image pixels",
      "calibrating network -- USED-FOR -- perspective-projection-transformation matrix",
      "real images -- EVALUATE-FOR -- neural approach",
      "neural approach -- USED-FOR -- calibration problems"
    ],
    "abstract": "camera calibration is a primary crucial step in many <task_1> . in this paper we present a new <method_14> for <task_6> . unlike some existing <method_15> , our <method_17> can tell the <otherscientificterm_8> between the <otherscientificterm_4> and the corresponding <otherscientificterm_5> . starting from <otherscientificterm_3> , the net can specify the <otherscientificterm_0> satisfying the <otherscientificterm_18> on the <otherscientificterm_9> . the <method_14> is shown to solve four diierent types of <task_16> that are found in <task_1> . moreover , <method_14> can be extended to the more diicult problem of <task_10> with <otherscientificterm_2> . the validity and performance of our <method_14> are tested with both <material_11> under different <otherscientificterm_12> and with <material_13> . experiments have shown the <metric_19> and the eeciency of our <method_14> .",
    "abstract_og": "camera calibration is a primary crucial step in many computer vision tasks . in this paper we present a new neural approach for camera c alibration . unlike some existing neural approaches , our calibrating network can tell the perspective-projection-transformation matrix between the world 3d points and the corresponding 2d image pixels . starting from random initial weights , the net can specify the camera m o del parameters satisfying the orthogonality constraints on the rotational transformation . the neural approach is shown to solve four diierent types of calibration problems that are found in computer vision tasks . moreover , neural approach can be extended to the more diicult problem of calibrating cameras with automated active lenses . the validity and performance of our neural approach are tested with both synthetic data under different noise conditions and with real images . experiments have shown the accuracy and the eeciency of our neural approach ."
  },
  {
    "title": "Trading Accuracy for Numerical Stability : Orthogonalization , Biorthogonalization and Regularization .",
    "entities": [
      "geometric significance of biorthogonal bases",
      "homotopy or tuning parameter",
      "nonlinear projection operators",
      "signal processing applications",
      "real exponential signals",
      "ill-conditioned inverse problem",
      "signal processing literature",
      "tradeoff analysis",
      "regularization methods",
      "regularization techniques",
      "convex programming",
      "numerical stability",
      "continuation methods",
      "signal processing",
      "accuracy"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <task> <material> <task> <material> <task> <method> <method> <method> <otherscientificterm> <method> <task> <metric>",
    "relations": [
      "homotopy or tuning parameter -- USED-FOR -- tradeoff analysis",
      "signal processing -- CONJUNCTION -- convex programming",
      "convex programming -- CONJUNCTION -- nonlinear projection operators",
      "signal processing -- CONJUNCTION -- continuation methods",
      "accuracy -- CONJUNCTION -- numerical stability",
      "ill-conditioned inverse problem -- EVALUATE-FOR -- regularization methods",
      "geometric significance of biorthogonal bases -- USED-FOR -- regularization methods",
      "geometric significance of biorthogonal bases -- USED-FOR -- signal processing applications",
      "convex programming -- CONJUNCTION -- continuation methods",
      "continuation methods -- CONJUNCTION -- nonlinear projection operators"
    ],
    "abstract": "this paper presents two novel <method_8> motivated in part by the <otherscientificterm_0> in <task_3> . these <method_8> , in particular , draw upon the structural relevance of orthogonality and biorthogonality principles and are presented from the perspectives of <task_13> , <method_10> , <method_12> and <method_2> . each method is specifically endowed with either a <otherscientificterm_1> to facilitate <task_7> between <metric_14> and <otherscientificterm_11> . an example involving a basis comprised of <material_4> illustrates the utility of the proposed <method_8> on an <task_5> and the results are compared to standard <method_9> from the <material_6> .",
    "abstract_og": "this paper presents two novel regularization methods motivated in part by the geometric significance of biorthogonal bases in signal processing applications . these regularization methods , in particular , draw upon the structural relevance of orthogonality and biorthogonality principles and are presented from the perspectives of signal processing , convex programming , continuation methods and nonlinear projection operators . each method is specifically endowed with either a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy and numerical stability . an example involving a basis comprised of real exponential signals illustrates the utility of the proposed regularization methods on an ill-conditioned inverse problem and the results are compared to standard regularization techniques from the signal processing literature ."
  },
  {
    "title": "Robust Boltzmann Machines for recognition and denoising .",
    "entities": [
      "un-labeled noisy data",
      "unsupervised fashion",
      "visual recognition",
      "posterior inference",
      "multiplicative gating",
      "boltzmann machines",
      "image denoising",
      "spatial structure",
      "face databases",
      "denoising",
      "in-painting",
      "occlusions",
      "noise",
      "recognition",
      "corruptions"
    ],
    "types": "<material> <method> <task> <task> <method> <material> <task> <otherscientificterm> <material> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "recognition -- CONJUNCTION -- denoising",
      "occlusions -- CONJUNCTION -- noise",
      "un-labeled noisy data -- USED-FOR -- unsupervised fashion",
      "image denoising -- CONJUNCTION -- in-painting"
    ],
    "abstract": "while <material_5> have been successful at un-supervised learning and density modeling of images and speech data , <material_5> can be very sensitive to <otherscientificterm_12> in the data . in this paper , we introduce a novel model , the robust boltz-mann machine -lrb- robm -rrb- , which allows <material_5> to be robust to <otherscientificterm_14> . in the domain of <task_2> , the robm is able to accurately deal with <otherscientificterm_11> and <otherscientificterm_12> by using <method_4> to induce a scale mixture of gaussians over pixels . <task_6> and <otherscientificterm_10> correspond to <task_3> in the robm . our model is trained in an <method_1> with <material_0> and can learn the <otherscientificterm_7> of the occluders . compared to standard algorithms , the robm is significantly better at <task_13> and <task_9> on several <material_8> .",
    "abstract_og": "while boltzmann machines have been successful at un-supervised learning and density modeling of images and speech data , boltzmann machines can be very sensitive to noise in the data . in this paper , we introduce a novel model , the robust boltz-mann machine -lrb- robm -rrb- , which allows boltzmann machines to be robust to corruptions . in the domain of visual recognition , the robm is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of gaussians over pixels . image denoising and in-painting correspond to posterior inference in the robm . our model is trained in an unsupervised fashion with un-labeled noisy data and can learn the spatial structure of the occluders . compared to standard algorithms , the robm is significantly better at recognition and denoising on several face databases ."
  },
  {
    "title": "Adaptive Support Vector Machine for Time-Varying Data Streams Using Martingale .",
    "entities": [
      "adaptive support vector machine",
      "time-varying data streams",
      "adiabatic incremental learning",
      "one-pass incre-mental algorithm",
      "sliding window",
      "martingale approach",
      "data stream",
      "historical information",
      "hypothesis testing",
      "clas-sifier"
    ],
    "types": "<method> <material> <method> <method> <otherscientificterm> <method> <material> <otherscientificterm> <method> <method>",
    "relations": [
      "martingale approach -- USED-FOR -- adaptive support vector machine",
      "adaptive support vector machine -- USED-FOR -- time-varying data streams",
      "adaptive support vector machine -- HYPONYM-OF -- one-pass incre-mental algorithm"
    ],
    "abstract": "introduction in this paper we propose an efficient <method_0> for <material_1> based on the <method_5> -lsb- 2 -rsb- and using <method_2> -lsb- 1 -rsb- . when a new data point is observed , <method_8> decides whether any change has occurred . once a change is detected , <otherscientificterm_7> about previous data is removed from the memory . the <method_0> is a <method_3> that 1 . does not require a <otherscientificterm_4> on the <material_6> , 2 . does not require monitoring the performance of the <method_9> as data points are streaming , and 3 . works well for high dimensional , multi-class data streams .",
    "abstract_og": "introduction in this paper we propose an efficient adaptive support vector machine for time-varying data streams based on the martingale approach -lsb- 2 -rsb- and using adiabatic incremental learning -lsb- 1 -rsb- . when a new data point is observed , hypothesis testing decides whether any change has occurred . once a change is detected , historical information about previous data is removed from the memory . the adaptive support vector machine is a one-pass incre-mental algorithm that 1 . does not require a sliding window on the data stream , 2 . does not require monitoring the performance of the clas-sifier as data points are streaming , and 3 . works well for high dimensional , multi-class data streams ."
  },
  {
    "title": "Structured Prediction Energy Networks .",
    "entities": [
      "structured prediction energy networks",
      "feed-forward and iterative structured prediction techniques",
      "tractable learning and prediction problems",
      "energy function of candidate labels",
      "benchmark multi-label classification tasks",
      "minimal structural assumptions",
      "interpretable structure learning",
      "dis-criminative features",
      "multi-label classification",
      "multi-label problems",
      "high-order interactions",
      "learning features",
      "structured prediction",
      "deep architecture",
      "graphical models",
      "structure learning",
      "deep learning",
      "prediction problem",
      "dependencies",
      "back-propagation"
    ],
    "types": "<method> <method> <task> <otherscientificterm> <task> <otherscientificterm> <task> <otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task> <method> <method> <method> <method> <task> <otherscientificterm> <method>",
    "relations": [
      "structured prediction energy networks -- USED-FOR -- multi-label problems",
      "deep architecture -- USED-FOR -- energy function of candidate labels",
      "deep architecture -- USED-FOR -- graphical models",
      "structured prediction energy networks -- USED-FOR -- structured prediction",
      "deep learning -- USED-FOR -- prediction problem",
      "benchmark multi-label classification tasks -- USED-FOR -- interpretable structure learning"
    ],
    "abstract": "we introduce <method_0> , a flexible framework for <task_12> . a <method_13> is used to define an <otherscientificterm_3> , and then predictions are produced by using <method_19> to iteratively optimize the energy with respect to the labels . this <method_13> captures <otherscientificterm_18> between labels that would lead to intractable <method_14> , and performs <method_15> by automatically learning <otherscientificterm_7> of the structured output . one natural application of our technique is <task_8> , which traditionally has required strict prior assumptions about the interactions between labels to ensure <task_2> . we are able to apply <method_0> to <task_9> with substantially larger label sets than previous applications of <task_12> , while modeling <otherscientificterm_10> using <otherscientificterm_5> . overall , <method_16> provides remarkable tools for <otherscientificterm_11> of the inputs to a <task_17> , and this work extends these techniques to <otherscientificterm_11> of the outputs . our experiments provide impressive performance on a variety of <task_4> , demonstrate that our technique can be used to provide <task_6> , and illuminate fundamental trade-offs between <method_1> .",
    "abstract_og": "we introduce structured prediction energy networks , a flexible framework for structured prediction . a deep architecture is used to define an energy function of candidate labels , and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels . this deep architecture captures dependencies between labels that would lead to intractable graphical models , and performs structure learning by automatically learning dis-criminative features of the structured output . one natural application of our technique is multi-label classification , which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems . we are able to apply structured prediction energy networks to multi-label problems with substantially larger label sets than previous applications of structured prediction , while modeling high-order interactions using minimal structural assumptions . overall , deep learning provides remarkable tools for learning features of the inputs to a prediction problem , and this work extends these techniques to learning features of the outputs . our experiments provide impressive performance on a variety of benchmark multi-label classification tasks , demonstrate that our technique can be used to provide interpretable structure learning , and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques ."
  },
  {
    "title": "Theme identification in telephone service conversations using quaternions of speech features .",
    "entities": [
      "real-life telephone customer care service",
      "automatic speech recognition system",
      "quaternion algebra framework",
      "extracting word frequency",
      "casual customer calling",
      "theme classification accuracy",
      "human/human conversation",
      "features",
      "feature",
      "features"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <metric> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "theme classification accuracy -- EVALUATE-FOR -- features"
    ],
    "abstract": "the paper introduces new <otherscientificterm_7> for describing possible focus variation in a <material_6> . the application considered is a <method_0> . the purpose is to hypothesize the dominant theme of conversations between a <otherscientificterm_4> . conversations are processed by an <method_1> that provides hypotheses used for <task_3> . <otherscientificterm_9> are extracted in different , broadly defined and partially overlapped , time segments . combinations of each <otherscientificterm_8> in different segments are represented in a <method_2> . the advantage of the proposed <otherscientificterm_7> is made evident by the statistically significant improvements in <metric_5> .",
    "abstract_og": "the paper introduces new features for describing possible focus variation in a human/human conversation . the application considered is a real-life telephone customer care service . the purpose is to hypothesize the dominant theme of conversations between a casual customer calling . conversations are processed by an automatic speech recognition system that provides hypotheses used for extracting word frequency . features are extracted in different , broadly defined and partially overlapped , time segments . combinations of each feature in different segments are represented in a quaternion algebra framework . the advantage of the proposed features is made evident by the statistically significant improvements in theme classification accuracy ."
  },
  {
    "title": "Learning Mixtures of Ranking Models .",
    "entities": [
      "tensor decomposition techniques",
      "mallows mixture model",
      "polynomial time algorithm",
      "bad local optima",
      "ranking data",
      "mallows models",
      "top-k prefix",
      "heterogeneous population",
      "probabilistic models",
      "rankings"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "tensor decomposition techniques -- USED-FOR -- polynomial time algorithm",
      "heterogeneous population -- FEATURE-OF -- ranking data"
    ],
    "abstract": "this work concerns learning <method_8> for <task_4> in a <otherscientificterm_7> . the specific problem we study is learning the parameters of a <method_1> . despite being widely studied , current heuristics for this problem do not have theoretical guarantees and can get stuck in <otherscientificterm_3> . we present the first <method_2> which provably learns the parameters of a mixture of two <method_5> . a key component of our <method_2> is a novel use of <method_0> to learn the <otherscientificterm_6> in both the <otherscientificterm_9> . before this work , even the question of identifiability in the case of a mixture of two <method_5> was unresolved .",
    "abstract_og": "this work concerns learning probabilistic models for ranking data in a heterogeneous population . the specific problem we study is learning the parameters of a mallows mixture model . despite being widely studied , current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima . we present the first polynomial time algorithm which provably learns the parameters of a mixture of two mallows models . a key component of our polynomial time algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings . before this work , even the question of identifiability in the case of a mixture of two mallows models was unresolved ."
  },
  {
    "title": "Automatic Labeling of Voiced Consonants for Morphological Analysis of Modern Japanese Literature .",
    "entities": [
      "automatic labeling of voiced consonant marks",
      "compulsory voiced consonant marks",
      "japanese literary text",
      "binary classification problem",
      "voiced consonant mark",
      "annotated corpus",
      "meiji era",
      "literary japanese",
      "pre-processing step",
      "pointwise prediction",
      "training corpus",
      "morphological analysis",
      "morphological analyzers",
      "surface information",
      "dictionary-based approach",
      "learning"
    ],
    "types": "<task> <material> <material> <task> <material> <material> <material> <material> <method> <method> <material> <task> <method> <otherscientificterm> <method> <task>",
    "relations": [
      "pre-processing step -- USED-FOR -- morphological analysis",
      "meiji era -- FEATURE-OF -- japanese literary text",
      "pre-processing step -- CONJUNCTION -- dictionary-based approach",
      "pointwise prediction -- COMPARE -- dictionary-based approach"
    ],
    "abstract": "since the present-day japanese use of <material_4> had established in the <material_6> , modern <material_2> written in the <material_6> often lacks <material_1> . this deteriorates the performance of <method_12> using ordinary dictionary . in this paper , we propose an approach for <task_0> for modern <material_7> . we formulate the <task_0> into a <task_3> . our point-wise prediction method uses as its feature set only <otherscientificterm_13> about the surrounding character strings . as a consequence , <material_10> is easy to obtain and maintain because we can exploit a partially <material_5> for <task_15> . we compared our proposed method as a <method_8> for <task_11> with a <method_14> , and confirmed that <method_9> out-performs <method_14> by a large margin .",
    "abstract_og": "since the present-day japanese use of voiced consonant mark had established in the meiji era , modern japanese literary text written in the meiji era often lacks compulsory voiced consonant marks . this deteriorates the performance of morphological analyzers using ordinary dictionary . in this paper , we propose an approach for automatic labeling of voiced consonant marks for modern literary japanese . we formulate the automatic labeling of voiced consonant marks into a binary classification problem . our point-wise prediction method uses as its feature set only surface information about the surrounding character strings . as a consequence , training corpus is easy to obtain and maintain because we can exploit a partially annotated corpus for learning . we compared our proposed method as a pre-processing step for morphological analysis with a dictionary-based approach , and confirmed that pointwise prediction out-performs dictionary-based approach by a large margin ."
  },
  {
    "title": "Bayesian Maximum Margin Principal Component Analysis .",
    "entities": [
      "principal component analysis",
      "weight and penalty parameter",
      "mean-field variational inference algorithm",
      "maximum likelihood framework",
      "supervised dimensionality reduction",
      "max-margin learning machine",
      "maximum margin principle",
      "posterior-regularized bayesian approach",
      "pca sub-space",
      "bayesian framework",
      "max-margin learning",
      "classification tasks",
      "predictive subspaces",
      "posterior"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <method> <method> <method> <method> <otherscientificterm> <method> <method> <task> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "classification tasks -- EVALUATE-FOR -- posterior-regularized bayesian approach",
      "mean-field variational inference algorithm -- USED-FOR -- posterior",
      "supervised dimensionality reduction -- USED-FOR -- predictive subspaces",
      "posterior-regularized bayesian approach -- USED-FOR -- pca sub-space"
    ],
    "abstract": "supervised dimensionality reduction has shown great advantages in finding <otherscientificterm_12> . previous methods rarely consider the popular <method_6> and are prone to overfitting to usually small training data , especially for those under the <method_3> . in this paper , we present a <method_7> to combine <method_0> with the <method_10> . based on the data augmentation idea for <method_10> and the probabilistic interpretation of <method_0> , our <method_7> can automatically infer the <otherscientificterm_1> of <method_5> , while finding the most appropriate <otherscientificterm_8> simultaneously under the <method_9> . we develop a fast <method_2> to approximate the <otherscientificterm_13> . experimental results on various <task_11> show that our <method_7> outperforms a number of competitors .",
    "abstract_og": "supervised dimensionality reduction has shown great advantages in finding predictive subspaces . previous methods rarely consider the popular maximum margin principle and are prone to overfitting to usually small training data , especially for those under the maximum likelihood framework . in this paper , we present a posterior-regularized bayesian approach to combine principal component analysis with the max-margin learning . based on the data augmentation idea for max-margin learning and the probabilistic interpretation of principal component analysis , our posterior-regularized bayesian approach can automatically infer the weight and penalty parameter of max-margin learning machine , while finding the most appropriate pca sub-space simultaneously under the bayesian framework . we develop a fast mean-field variational inference algorithm to approximate the posterior . experimental results on various classification tasks show that our posterior-regularized bayesian approach outperforms a number of competitors ."
  },
  {
    "title": "Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets .",
    "entities": [
      "forward and backward selection of features",
      "error correcting output coding",
      "combining algorithm",
      "rule learners",
      "uci repository",
      "neural networks",
      "knn classiier",
      "irrelevant features",
      "decision trees",
      "eeective technique",
      "nn classiiers",
      "features",
      "mfs",
      "classiiers",
      "bagging",
      "accuracy"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <material> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method> <method> <metric>",
    "relations": [
      "rule learners -- CONJUNCTION -- neural networks",
      "neural networks -- HYPONYM-OF -- classiiers",
      "irrelevant features -- COMPARE -- knn classiier",
      "bagging -- HYPONYM-OF -- classiiers",
      "nn classiiers -- PART-OF -- mfs",
      "decision trees -- HYPONYM-OF -- classiiers",
      "features -- USED-FOR -- nn classiiers",
      "bagging -- CONJUNCTION -- rule learners",
      "decision trees -- CONJUNCTION -- rule learners",
      "rule learners -- HYPONYM-OF -- classiiers",
      "features -- USED-FOR -- mfs"
    ],
    "abstract": "combining multiple <method_13> is an <method_9> for improving <metric_15> . there are many general combining algorithms , such as <method_14> or <method_1> , that signiicantly improve <method_13> like <otherscientificterm_8> , <method_3> , or <method_5> . unfortunately , many combining methods do not improve the nearest neighbor classiier . in this paper , we present <method_12> , a <method_2> designed to improve the <metric_15> of the nearest neighbor -lrb- nn -rrb- classiier . <method_12> combines multiple <method_10> each using only a random subset of <otherscientificterm_11> . the experimental results are encouraging : on 25 datasets from the <material_4> , <method_12> sig-niicantly improved upon the nn , k nearest neighbor -lrb- knn -rrb- , and <method_10> with <otherscientificterm_0> . <method_12> was also robust to corruption by <otherscientificterm_7> compared to the <method_6> . finally , we show that <method_12> is able to reduce both bias and variance components of error .",
    "abstract_og": "combining multiple classiiers is an eeective technique for improving accuracy . there are many general combining algorithms , such as bagging or error correcting output coding , that signiicantly improve classiiers like decision trees , rule learners , or neural networks . unfortunately , many combining methods do not improve the nearest neighbor classiier . in this paper , we present mfs , a combining algorithm designed to improve the accuracy of the nearest neighbor -lrb- nn -rrb- classiier . mfs combines multiple nn classiiers each using only a random subset of features . the experimental results are encouraging : on 25 datasets from the uci repository , mfs sig-niicantly improved upon the nn , k nearest neighbor -lrb- knn -rrb- , and nn classiiers with forward and backward selection of features . mfs was also robust to corruption by irrelevant features compared to the knn classiier . finally , we show that mfs is able to reduce both bias and variance components of error ."
  },
  {
    "title": "An Unsupervised Bayesian Modelling Approach for Storyline Detection on News Articles .",
    "entities": [
      "dynamic storyline detection model",
      "large scale news corpus",
      "unsupervised bayesian model",
      "construction of storylines",
      "structured representations",
      "hierarchical structures",
      "news stories",
      "storyline detection",
      "news articles",
      "storyline generation",
      "storyline"
    ],
    "types": "<method> <material> <method> <task> <otherscientificterm> <otherscientificterm> <material> <task> <material> <task> <otherscientificterm>",
    "relations": [
      "large scale news corpus -- EVALUATE-FOR -- dynamic storyline detection model",
      "unsupervised bayesian model -- USED-FOR -- structured representations",
      "hierarchical structures -- USED-FOR -- storyline generation",
      "news articles -- USED-FOR -- storyline detection"
    ],
    "abstract": "storyline detection from <material_8> aims at summarizing events described under a certain news topic and revealing how those events evolve over time . it is a difficult task because it requires first the detection of events from <material_8> published in different time periods and then the <task_3> by linking events into coherent <material_6> . moreover , each <otherscientificterm_10> has different <otherscientificterm_5> which are dependent across epochs . existing approaches often ignore the dependency of <otherscientificterm_5> in <task_9> . in this paper , we propose an <method_2> , called <method_0> , to extract <otherscientificterm_4> and evolution patterns of storylines . the proposed <method_0> is evaluated on a <material_1> . experimental results show that our proposed <method_0> outperforms several baseline approaches .",
    "abstract_og": "storyline detection from news articles aims at summarizing events described under a certain news topic and revealing how those events evolve over time . it is a difficult task because it requires first the detection of events from news articles published in different time periods and then the construction of storylines by linking events into coherent news stories . moreover , each storyline has different hierarchical structures which are dependent across epochs . existing approaches often ignore the dependency of hierarchical structures in storyline generation . in this paper , we propose an unsupervised bayesian model , called dynamic storyline detection model , to extract structured representations and evolution patterns of storylines . the proposed dynamic storyline detection model is evaluated on a large scale news corpus . experimental results show that our proposed dynamic storyline detection model outperforms several baseline approaches ."
  },
  {
    "title": "Active Surveying : A Probabilistic Approach for Identifying Key Opinion Leaders .",
    "entities": [
      "active surveying method",
      "information gathering process",
      "data acquisition",
      "partial knowledge",
      "opinion leaders",
      "secondary data",
      "medical field",
      "opinion leaders"
    ],
    "types": "<method> <task> <task> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm>",
    "relations": [
      "secondary data -- CONJUNCTION -- partial knowledge",
      "active surveying method -- USED-FOR -- opinion leaders",
      "partial knowledge -- USED-FOR -- active surveying method",
      "secondary data -- USED-FOR -- active surveying method"
    ],
    "abstract": "opinion leaders play an important role in influencing people 's beliefs , actions and behaviors . although a number of methods have been proposed for identifying influentials using secondary sources of information , the use of primary sources , such as surveys , is still favored in many domains . in this work we present a new <method_0> which combines <material_5> with <otherscientificterm_3> from primary sources to guide the <task_1> . we apply our proposed <method_0> to the problem of identifying key <otherscientificterm_4> in the <material_6> , and show how we are able to accurately identify the <otherscientificterm_4> while minimizing the amount of primary data required , which results in significant cost reduction in <task_2> without sacrificing its integrity .",
    "abstract_og": "opinion leaders play an important role in influencing people 's beliefs , actions and behaviors . although a number of methods have been proposed for identifying influentials using secondary sources of information , the use of primary sources , such as surveys , is still favored in many domains . in this work we present a new active surveying method which combines secondary data with partial knowledge from primary sources to guide the information gathering process . we apply our proposed active surveying method to the problem of identifying key opinion leaders in the medical field , and show how we are able to accurately identify the opinion leaders while minimizing the amount of primary data required , which results in significant cost reduction in data acquisition without sacrificing its integrity ."
  },
  {
    "title": "Consistent Knowledge Discovery from Evolving Ontologies .",
    "entities": [
      "-lrb- description logics -rrb- reasoning",
      "incomplete and dynamic data",
      "representative association semantic rules",
      "semantics of data",
      "real world applications",
      "inductive learning",
      "data incompleteness",
      "deductive reasoning",
      "consistent knowledge"
    ],
    "types": "<method> <material> <otherscientificterm> <otherscientificterm> <task> <method> <metric> <method> <otherscientificterm>",
    "relations": [
      "deductive reasoning -- CONJUNCTION -- inductive learning"
    ],
    "abstract": "deductive reasoning and <method_5> are the most common approaches for deriving knowledge . in <task_4> when data is dynamic and incomplete , especially those exposed by sensors , reasoning is limited by dynamics of data while learning is biased by <metric_6> . therefore discovering <otherscientificterm_8> from <material_1> is a challenging open problem . in our approach the <otherscientificterm_3> is captured through ontologies to empower learning -lrb- mining -rrb- with <method_0> . consistent knowledge discovery is achieved by applying generic , significative , <otherscientificterm_2> . the experiments have shown scalable , accurate and <otherscientificterm_8> discovery with data from dublin .",
    "abstract_og": "deductive reasoning and inductive learning are the most common approaches for deriving knowledge . in real world applications when data is dynamic and incomplete , especially those exposed by sensors , reasoning is limited by dynamics of data while learning is biased by data incompleteness . therefore discovering consistent knowledge from incomplete and dynamic data is a challenging open problem . in our approach the semantics of data is captured through ontologies to empower learning -lrb- mining -rrb- with -lrb- description logics -rrb- reasoning . consistent knowledge discovery is achieved by applying generic , significative , representative association semantic rules . the experiments have shown scalable , accurate and consistent knowledge discovery with data from dublin ."
  },
  {
    "title": "The complex Double Gaussian distribution .",
    "entities": [
      "independent complex gaussian random variables",
      "doubly-infinite summation of terms",
      "blind tr detection systems",
      "monte carlo simulations",
      "time reversal scenario",
      "double gaussian distribution",
      "neyman-pearson optimal detector",
      "theoretical analysis",
      "summation terms",
      "near-optimal detection"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <material> <otherscientificterm> <method> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "theoretical analysis -- CONJUNCTION -- monte carlo simulations",
      "neyman-pearson optimal detector -- USED-FOR -- blind tr detection systems"
    ],
    "abstract": "-- we present the complex <method_5> that describes the product of two independent , non-zero mean , complex gaussian random variables , a <otherscientificterm_1> . this distribution is useful in a wide array of problems . we discuss its application to <method_2> by deriving the <method_6> when the channel is modeled as the product of two <otherscientificterm_0> , such as in a <otherscientificterm_4> . we show that <method_9> performance can be achieved with as few as 25 <otherscientificterm_8> . <method_7> and <material_3> illustrate our results .",
    "abstract_og": "-- we present the complex double gaussian distribution that describes the product of two independent , non-zero mean , complex gaussian random variables , a doubly-infinite summation of terms . this distribution is useful in a wide array of problems . we discuss its application to blind tr detection systems by deriving the neyman-pearson optimal detector when the channel is modeled as the product of two independent complex gaussian random variables , such as in a time reversal scenario . we show that near-optimal detection performance can be achieved with as few as 25 summation terms . theoretical analysis and monte carlo simulations illustrate our results ."
  },
  {
    "title": "Manifold regularized deep neural networks .",
    "entities": [
      "automatic speech recognition tasks",
      "asr word error rates",
      "deep neural networks",
      "manifold based locality preserving constraints",
      "manifold learning based regularization framework",
      "hybrid acoustic mod-eling scenarios",
      "discriminative feature extraction",
      "low dimensional manifold",
      "speech feature vectors",
      "estimating network parameters",
      "bottleneck dnn architecture",
      "regularization approaches",
      "speech-in-noise task",
      "loss functions",
      "dnn training",
      "mani-fold constraints",
      "feature extraction",
      "dnn-bottleneck networks",
      "optimization procedure",
      "tandem configuration"
    ],
    "types": "<task> <metric> <method> <otherscientificterm> <method> <task> <task> <otherscientificterm> <otherscientificterm> <task> <method> <method> <task> <method> <task> <otherscientificterm> <task> <method> <method> <otherscientificterm>",
    "relations": [
      "speech-in-noise task -- USED-FOR -- asr word error rates",
      "dnn-bottleneck networks -- USED-FOR -- speech-in-noise task",
      "dnn-bottleneck networks -- USED-FOR -- asr word error rates",
      "loss functions -- CONJUNCTION -- regularization approaches",
      "bottleneck dnn architecture -- USED-FOR -- feature extraction",
      "optimization procedure -- USED-FOR -- estimating network parameters",
      "manifold learning based regularization framework -- USED-FOR -- dnn training",
      "discriminative feature extraction -- CONJUNCTION -- hybrid acoustic mod-eling scenarios",
      "deep neural networks -- USED-FOR -- automatic speech recognition tasks"
    ],
    "abstract": "deep neural networks -lrb- dnns -rrb- have been successfully applied to a variety of <task_0> , both in <task_6> and <task_5> . the development of improved <method_13> and <method_11> have resulted in consistent reductions in <metric_1> . this paper presents a <method_4> for <task_14> . the associated techniques attempt to preserve the underlying <otherscientificterm_7> based relationships amongst <otherscientificterm_8> as part of the <method_18> for <task_9> . this is achieved by imposing <otherscientificterm_3> on the outputs of the network . the techniques are presented in the context of a <method_10> for <task_16> in a <otherscientificterm_19> . the <metric_1> obtained using these networks is evaluated on a <task_12> and compared to that obtained using <method_17> trained without <otherscientificterm_15> .",
    "abstract_og": "deep neural networks -lrb- dnns -rrb- have been successfully applied to a variety of automatic speech recognition tasks , both in discriminative feature extraction and hybrid acoustic mod-eling scenarios . the development of improved loss functions and regularization approaches have resulted in consistent reductions in asr word error rates . this paper presents a manifold learning based regularization framework for dnn training . the associated techniques attempt to preserve the underlying low dimensional manifold based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters . this is achieved by imposing manifold based locality preserving constraints on the outputs of the network . the techniques are presented in the context of a bottleneck dnn architecture for feature extraction in a tandem configuration . the asr word error rates obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using dnn-bottleneck networks trained without mani-fold constraints ."
  },
  {
    "title": "Google 's cross-dialect Arabic voice search .",
    "entities": [
      "commercial automatic speech recognition product",
      "united arab emirates",
      "word error rate",
      "diacritized vs. non-diacritized text",
      "voice control",
      "voice search",
      "arabic dialects",
      "saudi arabia",
      "arabic",
      "dictation",
      "recognizers"
    ],
    "types": "<method> <material> <metric> <material> <task> <task> <material> <material> <material> <task> <method>",
    "relations": [
      "dictation -- CONJUNCTION -- voice control",
      "saudi arabia -- CONJUNCTION -- united arab emirates",
      "voice search -- CONJUNCTION -- dictation",
      "recognizers -- USED-FOR -- arabic dialects"
    ],
    "abstract": "we present a large scale effort to build a <method_0> for <material_8> . our goal is to support <task_5> , <task_9> , and <task_4> for the general arabic-speaking public , including support for multiple <material_6> . we describe our <method_0> and compare <method_10> for five <material_6> , with the potential to reach more than 125 million people in egypt , jordan , lebanon , <material_7> , and the <material_1> . we compare systems built on <material_3> . we also conduct cross-dialect experiments , where we train on one dialect and test on the others . our average <metric_2> is 24.8 % for <task_5> .",
    "abstract_og": "we present a large scale effort to build a commercial automatic speech recognition product for arabic . our goal is to support voice search , dictation , and voice control for the general arabic-speaking public , including support for multiple arabic dialects . we describe our commercial automatic speech recognition product and compare recognizers for five arabic dialects , with the potential to reach more than 125 million people in egypt , jordan , lebanon , saudi arabia , and the united arab emirates . we compare systems built on diacritized vs. non-diacritized text . we also conduct cross-dialect experiments , where we train on one dialect and test on the others . our average word error rate is 24.8 % for voice search ."
  },
  {
    "title": "Why speech recognizers make errors ? a robustness view .",
    "entities": [
      "stationary signal-to-noise ratio",
      "tracking time-varying or nonstationary extraneous events",
      "natural language dialog services",
      "data selection algorithm",
      "field data",
      "background noise",
      "problematic speech",
      "robustness perspective",
      "recognition errors",
      "music"
    ],
    "types": "<metric> <task> <task> <method> <material> <otherscientificterm> <material> <otherscientificterm> <task> <material>",
    "relations": [
      "music -- CONJUNCTION -- background noise",
      "natural language dialog services -- USED-FOR -- field data",
      "natural language dialog services -- USED-FOR -- robustness perspective",
      "field data -- USED-FOR -- robustness perspective"
    ],
    "abstract": "the performance of large vocabulary speech recognizers often varies depending on the input speech and the quality of the trained models . the particular attributes that cause <task_8> are a research area that has not been well studied . this paper addresses this issue from a <otherscientificterm_7> using a large amount of <material_4> collected from <task_2> . in particular , we present a method for <task_1> , such as <material_9> , <otherscientificterm_5> , etc. . we show that this measure is a better predictor of <task_8> than a standard measure of <metric_0> . combining the two measures provides a <method_3> for detecting <material_6> .",
    "abstract_og": "the performance of large vocabulary speech recognizers often varies depending on the input speech and the quality of the trained models . the particular attributes that cause recognition errors are a research area that has not been well studied . this paper addresses this issue from a robustness perspective using a large amount of field data collected from natural language dialog services . in particular , we present a method for tracking time-varying or nonstationary extraneous events , such as music , background noise , etc. . we show that this measure is a better predictor of recognition errors than a standard measure of stationary signal-to-noise ratio . combining the two measures provides a data selection algorithm for detecting problematic speech ."
  },
  {
    "title": "Hierarchical Bayesian learning for electrical transient classification .",
    "entities": [
      "markov-chain monte carlo algorithm",
      "posterior distributions of the features",
      "real-world electrical transients signals",
      "non-intrusive load monitoring",
      "class-specific distribution parameters",
      "supervised signal classification",
      "hierarchical bayesian method",
      "feature extraction step",
      "electrical transient classification",
      "class-specific posterior distribution",
      "learning step",
      "learning signals",
      "features"
    ],
    "types": "<method> <otherscientificterm> <material> <task> <otherscientificterm> <task> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "electrical transient classification -- USED-FOR -- non-intrusive load monitoring",
      "hierarchical bayesian method -- USED-FOR -- supervised signal classification",
      "feature extraction step -- CONJUNCTION -- learning step",
      "markov-chain monte carlo algorithm -- USED-FOR -- posterior distributions of the features"
    ],
    "abstract": "this paper addresses the problem of the <task_5> , by using a <method_6> . each signal is characterized by a set of parameters , the <otherscientificterm_12> , which are estimated from a set of <otherscientificterm_11> . moreover , these parameters are distributed according to a <otherscientificterm_9> which allows one to capture the variability of the <otherscientificterm_12> within the same class . within the <method_6> , the <method_7> and the <method_10> can be performed jointly . unfortunately , the estimation of the <otherscientificterm_4> requires the computation of intractable multi-dimensional integrals . then a <method_0> is used to sample the <otherscientificterm_1> over all the training signals of each class . an application to <task_8> for <task_3> is introduced . simulations over <material_2> are driven and show the capacity of the proposed <method_0> to discriminate two classes of transients .",
    "abstract_og": "this paper addresses the problem of the supervised signal classification , by using a hierarchical bayesian method . each signal is characterized by a set of parameters , the features , which are estimated from a set of learning signals . moreover , these parameters are distributed according to a class-specific posterior distribution which allows one to capture the variability of the features within the same class . within the hierarchical bayesian method , the feature extraction step and the learning step can be performed jointly . unfortunately , the estimation of the class-specific distribution parameters requires the computation of intractable multi-dimensional integrals . then a markov-chain monte carlo algorithm is used to sample the posterior distributions of the features over all the training signals of each class . an application to electrical transient classification for non-intrusive load monitoring is introduced . simulations over real-world electrical transients signals are driven and show the capacity of the proposed markov-chain monte carlo algorithm to discriminate two classes of transients ."
  },
  {
    "title": "Collusion-resistant fingerprinting for multimedia .",
    "entities": [
      "o -lrb- \u221a n -rrb- bits",
      "anti-collusion codes",
      "binary code vectors",
      "block matrix structure",
      "antipodal cdma-type watermarking",
      "digital fingerprinting",
      "trace colluders",
      "detection capability",
      "digital fingerprints",
      "combinatorial designs",
      "computational complexity",
      "designing fingerprints",
      "correlation contributions",
      "multimedia content",
      "watermarking techniques",
      "collusion",
      "colluders",
      "images",
      "watermarks"
    ],
    "types": "<otherscientificterm> <method> <otherscientificterm> <method> <method> <task> <method> <metric> <material> <method> <metric> <task> <otherscientificterm> <material> <method> <otherscientificterm> <method> <material> <otherscientificterm>",
    "relations": [
      "images -- HYPONYM-OF -- multimedia content",
      "block matrix structure -- USED-FOR -- anti-collusion codes",
      "watermarking techniques -- USED-FOR -- digital fingerprinting",
      "correlation contributions -- USED-FOR -- antipodal cdma-type watermarking"
    ],
    "abstract": "digital fingerprinting is an effective method to identify users who might try to redistribute <material_13> , such as <material_17> and video . these <task_5> are typically embedded into the content using <method_14> that are designed to be robust to a variety of attacks . a cheap and effective attack against such <material_8> is <otherscientificterm_15> , where several differently marked copies of the same content are averaged or combined to disrupt the underlying fingerprint . in this paper , we study the problem of <task_11> that can withstand <otherscientificterm_15> , yet <method_6> . since , in <method_4> , the <otherscientificterm_12> only decrease where <otherscientificterm_18> differ , by constructing <otherscientificterm_2> where any subset of k or fewer of these vectors have unique overlap , we may identify groups of k or less <method_16> . our construction of such <method_1> uses the theory of <method_9> , and for n users requires <otherscientificterm_0> . further , we explore a <method_3> for the <method_1> that reduces the <metric_10> for identifying <method_16> and improves the <metric_7> when <method_16> belong to the same subgroup .",
    "abstract_og": "digital fingerprinting is an effective method to identify users who might try to redistribute multimedia content , such as images and video . these digital fingerprinting are typically embedded into the content using watermarking techniques that are designed to be robust to a variety of attacks . a cheap and effective attack against such digital fingerprints is collusion , where several differently marked copies of the same content are averaged or combined to disrupt the underlying fingerprint . in this paper , we study the problem of designing fingerprints that can withstand collusion , yet trace colluders . since , in antipodal cdma-type watermarking , the correlation contributions only decrease where watermarks differ , by constructing binary code vectors where any subset of k or fewer of these vectors have unique overlap , we may identify groups of k or less colluders . our construction of such anti-collusion codes uses the theory of combinatorial designs , and for n users requires o -lrb- \u221a n -rrb- bits . further , we explore a block matrix structure for the anti-collusion codes that reduces the computational complexity for identifying colluders and improves the detection capability when colluders belong to the same subgroup ."
  },
  {
    "title": "Online Streaming Feature Selection .",
    "entities": [
      "online streaming feature selection",
      "streaming feature selection algorithms",
      "standard feature selection methods",
      "fast-osfs algorithm",
      "selection efficiency",
      "candidate features",
      "streaming features",
      "feature relevance",
      "features",
      "compactness",
      "accuracy"
    ],
    "types": "<task> <method> <method> <method> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <metric> <metric>",
    "relations": [
      "selection efficiency -- EVALUATE-FOR -- fast-osfs algorithm",
      "accuracy -- EVALUATE-FOR -- streaming feature selection algorithms"
    ],
    "abstract": "we study an interesting and challenging problem , <task_0> , in which the size of the feature set is unknown , and not all <otherscientificterm_8> are available for learning while leaving the number of observations constant . in this problem , the <otherscientificterm_5> arrive one at a time , and the learner 's task is to select a '' best so far '' set of <otherscientificterm_8> from <otherscientificterm_6> . <method_2> can not perform well in this scenario . thus , we present a novel framework based on <otherscientificterm_7> . under this framework , a promising alternative method , online streaming feature selection -lrb- osfs -rrb- , is presented to online select strongly relevant and non-redundant <otherscientificterm_8> . in addition to osfs , a faster <method_3> is proposed to further improve the <metric_4> . experimental results show that our algorithms achieve more <metric_9> and better <metric_10> than existing <method_1> on various datasets .",
    "abstract_og": "we study an interesting and challenging problem , online streaming feature selection , in which the size of the feature set is unknown , and not all features are available for learning while leaving the number of observations constant . in this problem , the candidate features arrive one at a time , and the learner 's task is to select a '' best so far '' set of features from streaming features . standard feature selection methods can not perform well in this scenario . thus , we present a novel framework based on feature relevance . under this framework , a promising alternative method , online streaming feature selection -lrb- osfs -rrb- , is presented to online select strongly relevant and non-redundant features . in addition to osfs , a faster fast-osfs algorithm is proposed to further improve the selection efficiency . experimental results show that our algorithms achieve more compactness and better accuracy than existing streaming feature selection algorithms on various datasets ."
  },
  {
    "title": "Distributed Dual Averaging In Networks .",
    "entities": [
      "local -lrb- possibly nonsmooth -rrb- convex functions",
      "dual averaging of subgradients",
      "theoretical lower bounds",
      "spectral gap",
      "decentralized optimization",
      "convergence rates",
      "network size",
      "optimization algorithm",
      "distributed algorithms",
      "local computation",
      "network structure",
      "global objective",
      "communication constraints",
      "iterations",
      "topology"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <metric> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "network structure -- USED-FOR -- communication constraints",
      "dual averaging of subgradients -- USED-FOR -- distributed algorithms",
      "network size -- CONJUNCTION -- topology",
      "decentralized optimization -- USED-FOR -- global objective"
    ],
    "abstract": "the goal of <method_4> over a network is to optimize a <otherscientificterm_11> formed by a sum of <otherscientificterm_0> using only <otherscientificterm_9> and communication . we develop and analyze <method_8> based on <otherscientificterm_1> , and provide sharp bounds on their <metric_5> as a function of the <otherscientificterm_6> and <otherscientificterm_14> . our analysis clearly separates the convergence of the <method_7> itself from the effects of <otherscientificterm_12> arising from the <otherscientificterm_10> . we show that the number of <otherscientificterm_13> required by our algorithm scales inversely in the <otherscientificterm_3> of the network . the sharpness of this prediction is confirmed both by <otherscientificterm_2> and simulations for various networks .",
    "abstract_og": "the goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local -lrb- possibly nonsmooth -rrb- convex functions using only local computation and communication . we develop and analyze distributed algorithms based on dual averaging of subgradients , and provide sharp bounds on their convergence rates as a function of the network size and topology . our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure . we show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network . the sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks ."
  },
  {
    "title": "Higher Order Whitening of Natural Images .",
    "entities": [
      "power law -rrb- linear processing",
      "second order spatial correlations",
      "higher order whitening",
      "linear processing",
      "non-linear method",
      "natural images",
      "power law",
      "power spectrum",
      "con-volution",
      "redundancy",
      "images",
      "coefficients",
      "image"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <method> <method> <material> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "con-volution -- HYPONYM-OF -- linear processing",
      "con-volution -- HYPONYM-OF -- power law -rrb- linear processing"
    ],
    "abstract": "natural <material_10> are approximately scale invariant resulting in long range statistical regularities that typically obey a <method_6> . for example , <material_10> have considerable regularity in their <otherscientificterm_1> as measured by the <otherscientificterm_7> . processing <material_10> to remove these expected correlations is known as whitening an <otherscientificterm_12> . because the expected value of the <otherscientificterm_7> has a regular form -lrb- a <method_0> such as <otherscientificterm_8> can be used to whiten an <otherscientificterm_12> . after whitening an <otherscientificterm_12> , higher order regularities that can not be removed with <method_3> still exist in the form of correlations in the magnitude . in this paper it is shown that these correlations also obey a <method_6> and a <method_4> is used to remove them , a process referred to as <otherscientificterm_2> . the method is invertible demonstrating that while <otherscientificterm_9> is removed no information is lost . experiments are given showing that after <otherscientificterm_2> the <otherscientificterm_11> can be severely quantized yet a good reconstruction is possible despite the nonlinearities .",
    "abstract_og": "natural images are approximately scale invariant resulting in long range statistical regularities that typically obey a power law . for example , images have considerable regularity in their second order spatial correlations as measured by the power spectrum . processing images to remove these expected correlations is known as whitening an image . because the expected value of the power spectrum has a regular form -lrb- a power law -rrb- linear processing such as con-volution can be used to whiten an image . after whitening an image , higher order regularities that can not be removed with linear processing still exist in the form of correlations in the magnitude . in this paper it is shown that these correlations also obey a power law and a non-linear method is used to remove them , a process referred to as higher order whitening . the method is invertible demonstrating that while redundancy is removed no information is lost . experiments are given showing that after higher order whitening the coefficients can be severely quantized yet a good reconstruction is possible despite the nonlinearities ."
  },
  {
    "title": "Insights into machine lip reading .",
    "entities": [
      "active appearance models",
      "professional human lip-readers",
      "linear predictive trackers",
      "signal processing challenges",
      "connected words",
      "human speech",
      "viseme accuracy",
      "multiview dataset",
      "automatic systems",
      "computer lipreading",
      "human lip-readers",
      "fallibil-ity"
    ],
    "types": "<method> <method> <method> <task> <otherscientificterm> <material> <metric> <method> <method> <task> <method> <otherscientificterm>",
    "relations": [
      "active appearance models -- CONJUNCTION -- human lip-readers",
      "linear predictive trackers -- CONJUNCTION -- active appearance models",
      "computer lipreading -- HYPONYM-OF -- signal processing challenges",
      "viseme accuracy -- EVALUATE-FOR -- automatic systems",
      "connected words -- USED-FOR -- multiview dataset",
      "active appearance models -- USED-FOR -- automatic systems",
      "linear predictive trackers -- USED-FOR -- automatic systems",
      "linear predictive trackers -- CONJUNCTION -- human lip-readers",
      "connected words -- USED-FOR -- automatic systems"
    ],
    "abstract": "computer lipreading is one of the great <task_3> . not only is the signal noisy , it is variable . however it is almost unknown to compare the performance with <method_10> . partly this is because of the paucity of <method_10> and partly because most <method_8> only handle data that are trivial and therefore not representative of <material_5> . here we generate a <method_7> using <otherscientificterm_4> that can be analysed by an <method_8> , based on <method_2> and <method_0> , and <method_10> . the <method_8> we devise has a <metric_6> of \u2248 46 % which is comparable to poor <method_1> . however , unlike <method_10> our <method_8> is good at guessing its <otherscientificterm_11> .",
    "abstract_og": "computer lipreading is one of the great signal processing challenges . not only is the signal noisy , it is variable . however it is almost unknown to compare the performance with human lip-readers . partly this is because of the paucity of human lip-readers and partly because most automatic systems only handle data that are trivial and therefore not representative of human speech . here we generate a multiview dataset using connected words that can be analysed by an automatic systems , based on linear predictive trackers and active appearance models , and human lip-readers . the automatic systems we devise has a viseme accuracy of \u2248 46 % which is comparable to poor professional human lip-readers . however , unlike human lip-readers our automatic systems is good at guessing its fallibil-ity ."
  },
  {
    "title": "Context-aware Argumentative Relation Mining .",
    "entities": [
      "context-aware argumentative relation mining",
      "argumentative relation classification tasks",
      "argument mining methods",
      "contex-tual features",
      "argumentative relations",
      "student essays",
      "writing topics",
      "context"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm>",
    "relations": [
      "contex-tual features -- USED-FOR -- argument mining methods",
      "context -- USED-FOR -- argumentative relations",
      "context-aware argumentative relation mining -- USED-FOR -- context-aware argumentative relation mining",
      "writing topics -- USED-FOR -- context-aware argumentative relation mining"
    ],
    "abstract": "context is crucial for identifying <otherscientificterm_4> in text , but many <method_2> make little use of <otherscientificterm_3> . this paper presents <method_0> that uses <method_0> extracted from <material_6> as well as from windows of context sentences . experiments on <material_5> demonstrate that the proposed <method_0> improve predictive performance in two <task_1> .",
    "abstract_og": "context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contex-tual features . this paper presents context-aware argumentative relation mining that uses context-aware argumentative relation mining extracted from writing topics as well as from windows of context sentences . experiments on student essays demonstrate that the proposed context-aware argumentative relation mining improve predictive performance in two argumentative relation classification tasks ."
  },
  {
    "title": "Mondrian Forests : Efficient Online Random Forests .",
    "entities": [
      "ensembles of random decision trees",
      "distribution of online mondrian forests",
      "computation vs accuracy tradeoff",
      "randomized decision trees",
      "real-world prediction tasks",
      "online random forests",
      "random forest variants",
      "batch mondrian forests",
      "online methods",
      "machine learning",
      "mondrian processes",
      "mondrian forests",
      "incremental/online fashion",
      "batch counterpart",
      "random forests",
      "random forests",
      "statistics",
      "classification"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <task>",
    "relations": [
      "random forests -- USED-FOR -- real-world prediction tasks",
      "mondrian processes -- USED-FOR -- ensembles of random decision trees",
      "distribution of online mondrian forests -- COMPARE -- batch mondrian forests"
    ],
    "abstract": "ensembles of <otherscientificterm_3> , usually referred to as <otherscientificterm_15> , are widely used for <task_17> and regression tasks in <task_9> and <material_16> . <otherscientificterm_14> achieve competitive predictive performance and are computationally efficient to train and test , making <otherscientificterm_14> excellent candidates for <task_4> . the most popular <method_6> -lrb- such as breiman 's random forest and extremely randomized trees -rrb- operate on batches of training data . <method_8> are now in greater demand . existing <otherscientificterm_5> , however , require more training data than their <otherscientificterm_13> to achieve comparable predictive performance . in this work , we use <method_10> -lrb- roy and teh , 2009 -rrb- to construct <otherscientificterm_0> we call <otherscientificterm_11> . <otherscientificterm_11> can be grown in an <method_12> and remarkably , the <otherscientificterm_1> is the same as that of <otherscientificterm_7> . <otherscientificterm_11> achieve competitive predictive performance comparable with existing <otherscientificterm_5> and periodically retrained batch <otherscientificterm_15> , while being more than an order of magnitude faster , thus representing a better <metric_2> .",
    "abstract_og": "ensembles of randomized decision trees , usually referred to as random forests , are widely used for classification and regression tasks in machine learning and statistics . random forests achieve competitive predictive performance and are computationally efficient to train and test , making random forests excellent candidates for real-world prediction tasks . the most popular random forest variants -lrb- such as breiman 's random forest and extremely randomized trees -rrb- operate on batches of training data . online methods are now in greater demand . existing online random forests , however , require more training data than their batch counterpart to achieve comparable predictive performance . in this work , we use mondrian processes -lrb- roy and teh , 2009 -rrb- to construct ensembles of random decision trees we call mondrian forests . mondrian forests can be grown in an incremental/online fashion and remarkably , the distribution of online mondrian forests is the same as that of batch mondrian forests . mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically retrained batch random forests , while being more than an order of magnitude faster , thus representing a better computation vs accuracy tradeoff ."
  },
  {
    "title": "Voicebuilder : a framework for automatic speech application development .",
    "entities": [
      "speech user interface specialists",
      "mixed initiative dialogue strategies",
      "stand-alone gui application",
      "speech ui",
      "markup language",
      "automatic coding",
      "system initiative",
      "speech applications",
      "suiml documents",
      "web-based interface",
      "voicexml code",
      "e-mail reader",
      "voice toolkits",
      "flight reservations",
      "suiml",
      "macro-processor",
      "voicebuilder",
      "ui"
    ],
    "types": "<method> <method> <method> <material> <otherscientificterm> <task> <method> <task> <material> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <method> <method> <method> <method>",
    "relations": [
      "system initiative -- CONJUNCTION -- mixed initiative dialogue strategies",
      "flight reservations -- HYPONYM-OF -- speech applications",
      "stand-alone gui application -- CONJUNCTION -- web-based interface",
      "suiml documents -- USED-FOR -- macro-processor",
      "e-mail reader -- HYPONYM-OF -- speech applications",
      "macro-processor -- USED-FOR -- automatic coding",
      "voicebuilder -- USED-FOR -- speech applications",
      "suiml documents -- USED-FOR -- voicexml code",
      "macro-processor -- USED-FOR -- voicexml code",
      "e-mail reader -- CONJUNCTION -- flight reservations"
    ],
    "abstract": "in this paper we present <method_16> , a <method_16> for automating the process of developing <task_7> . our <method_16> allows <method_0> to introduce <method_17> 's in two ways : a <method_2> , and a <otherscientificterm_9> ; in which <material_3> 's are stored in a <otherscientificterm_4> previously proposed called <method_14> -lsb- 1 -rsb- , supporting either <method_6> or <method_1> . for <task_5> , we propose an algorithm based on a <method_15> that generates <otherscientificterm_10> by parsing <material_8> . this algorithm was designed to generate various kinds of code with a minimal initial effort . we performed experiments considering both <method_6> and <method_1> with three different <task_7> : auto-attendant , <material_11> , and <otherscientificterm_13> . <method_16> is very useful for building <task_7> in new domains , requires no programming effort and could be incorporated into several <method_12> .",
    "abstract_og": "in this paper we present voicebuilder , a voicebuilder for automating the process of developing speech applications . our voicebuilder allows speech user interface specialists to introduce ui 's in two ways : a stand-alone gui application , and a web-based interface ; in which speech ui 's are stored in a markup language previously proposed called suiml -lsb- 1 -rsb- , supporting either system initiative or mixed initiative dialogue strategies . for automatic coding , we propose an algorithm based on a macro-processor that generates voicexml code by parsing suiml documents . this algorithm was designed to generate various kinds of code with a minimal initial effort . we performed experiments considering both system initiative and mixed initiative dialogue strategies with three different speech applications : auto-attendant , e-mail reader , and flight reservations . voicebuilder is very useful for building speech applications in new domains , requires no programming effort and could be incorporated into several voice toolkits ."
  },
  {
    "title": "Exploiting global connectivity constraints for reconstruction of 3D line segments from images .",
    "entities": [
      "automatic 3d reconstruction of man-made environments",
      "reconstruction of straight 3d line segments",
      "synthetic and real scenes",
      "global topologi-cal constraints",
      "independent reconstructions",
      "ground truth",
      "partial occlusion",
      "image noise",
      "base images",
      "2d images",
      "outliers"
    ],
    "types": "<task> <task> <material> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material> <otherscientificterm>",
    "relations": [
      "image noise -- CONJUNCTION -- partial occlusion"
    ],
    "abstract": "given a set of <material_9> , we propose a novel approach for the <task_1> that represent the underlying geometry of static 3d objects in the scene . such an algorithm is especially useful for the <task_0> . the main contribution of our approach is the generation of an improved <task_1> by imposing <otherscientificterm_3> given by connections between neighbouring lines . additionally , our approach does not employ explicit line matching between views , thus making it more robust against <otherscientificterm_7> and <otherscientificterm_6> . furthermore , we suggest a technique to merge <method_4> , that are generated from different <material_8> , which also helps to remove <otherscientificterm_10> . the proposed algorithm is evaluated on <material_2> by comparison with <otherscientificterm_5> .",
    "abstract_og": "given a set of 2d images , we propose a novel approach for the reconstruction of straight 3d line segments that represent the underlying geometry of static 3d objects in the scene . such an algorithm is especially useful for the automatic 3d reconstruction of man-made environments . the main contribution of our approach is the generation of an improved reconstruction of straight 3d line segments by imposing global topologi-cal constraints given by connections between neighbouring lines . additionally , our approach does not employ explicit line matching between views , thus making it more robust against image noise and partial occlusion . furthermore , we suggest a technique to merge independent reconstructions , that are generated from different base images , which also helps to remove outliers . the proposed algorithm is evaluated on synthetic and real scenes by comparison with ground truth ."
  },
  {
    "title": "Actions ~ Transformations .",
    "entities": [
      "high-level feature space",
      "action recognition datasets",
      "cross-category generalization",
      "act dataset",
      "video representation",
      "siamese network",
      "deep learning",
      "ucf101",
      "hmdb51"
    ],
    "types": "<otherscientificterm> <material> <task> <material> <method> <method> <method> <material> <material>",
    "relations": [
      "ucf101 -- HYPONYM-OF -- action recognition datasets",
      "ucf101 -- CONJUNCTION -- hmdb51",
      "hmdb51 -- HYPONYM-OF -- action recognition datasets",
      "deep learning -- USED-FOR -- video representation",
      "cross-category generalization -- EVALUATE-FOR -- siamese network",
      "action recognition datasets -- EVALUATE-FOR -- siamese network"
    ],
    "abstract": "what defines an action like '' kicking ball '' ? we argue that the true meaning of an action lies in the change or transformation an action brings to the environment . in this paper , we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens -lrb- precondition -rrb- to the state after the action -lrb- effect -rrb- . motivated by recent advancements of <method_4> using <method_6> , we design a <method_5> which models the action as a transformation on a <otherscientificterm_0> . we show that our <method_5> gives improvements on standard <material_1> including <material_7> and <material_8> . more importantly , our <method_5> is able to generalize beyond learned action categories and shows significant performance improvement on <task_2> on our new <material_3> .",
    "abstract_og": "what defines an action like '' kicking ball '' ? we argue that the true meaning of an action lies in the change or transformation an action brings to the environment . in this paper , we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens -lrb- precondition -rrb- to the state after the action -lrb- effect -rrb- . motivated by recent advancements of video representation using deep learning , we design a siamese network which models the action as a transformation on a high-level feature space . we show that our siamese network gives improvements on standard action recognition datasets including ucf101 and hmdb51 . more importantly , our siamese network is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new act dataset ."
  },
  {
    "title": "Space Kernel Analysis .",
    "entities": [
      "space kernel analysis",
      "weighted least squared cost function",
      "radial basis function network",
      "general regression neural network",
      "nonparametric modeling techniques",
      "space kernel matrix",
      "nonparametric modeling technique",
      "kernel-based learning methods",
      "similarity based modeling",
      "weight matrix",
      "bias/variance dilemma",
      "kernel regression",
      "accuracy",
      "robustness"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <metric> <metric>",
    "relations": [
      "general regression neural network -- CONJUNCTION -- radial basis function network",
      "general regression neural network -- CONJUNCTION -- similarity based modeling",
      "kernel regression -- CONJUNCTION -- similarity based modeling",
      "space kernel analysis -- COMPARE -- kernel-based learning methods",
      "kernel regression -- CONJUNCTION -- general regression neural network",
      "space kernel analysis -- CONJUNCTION -- nonparametric modeling techniques",
      "kernel regression -- CONJUNCTION -- radial basis function network",
      "accuracy -- EVALUATE-FOR -- kernel-based learning methods",
      "space kernel analysis -- HYPONYM-OF -- nonparametric modeling technique",
      "similarity based modeling -- CONJUNCTION -- radial basis function network",
      "space kernel analysis -- USED-FOR -- weighted least squared cost function"
    ],
    "abstract": "in this paper , we propose a novel <method_6> , namely <method_0> , as a result of the definition of the space kernel . we analyze the uncertainty of <method_0> and show that <method_0> is subjected to the <otherscientificterm_10> . nevertheless , we demonstrate that , by a proper choice of the <method_5> , <method_0> is able to balance between the <metric_13> and <metric_12> and hence outperforms other <method_7> . the cost function of <method_0> is derived , and it proves that <method_0> minimizes the <otherscientificterm_1> whose <otherscientificterm_9> is diagonal and determined by the <method_5> . the parallels between <method_0> and several other <method_4> are examined . study shows that the traditional <method_11> , <method_3> , <method_8> and <method_2> are examples of <method_0> with specified space kernel matrices .",
    "abstract_og": "in this paper , we propose a novel nonparametric modeling technique , namely space kernel analysis , as a result of the definition of the space kernel . we analyze the uncertainty of space kernel analysis and show that space kernel analysis is subjected to the bias/variance dilemma . nevertheless , we demonstrate that , by a proper choice of the space kernel matrix , space kernel analysis is able to balance between the robustness and accuracy and hence outperforms other kernel-based learning methods . the cost function of space kernel analysis is derived , and it proves that space kernel analysis minimizes the weighted least squared cost function whose weight matrix is diagonal and determined by the space kernel matrix . the parallels between space kernel analysis and several other nonparametric modeling techniques are examined . study shows that the traditional kernel regression , general regression neural network , similarity based modeling and radial basis function network are examples of space kernel analysis with specified space kernel matrices ."
  },
  {
    "title": "A Metalogic Programming Approach to Reasoning about Time in Knowledge Bases .",
    "entities": [
      "historical time and belief time",
      "information models reality/belief time",
      "theory of time",
      "notions of time",
      "knowledge base applications",
      "theorem prover",
      "temporal reasoning",
      "legal reasoning",
      "historical time",
      "fold/unfold transformations",
      "prolog program",
      "multi-agent reasoning",
      "knowledge bases",
      "metalogic program",
      "logic",
      "metalanguage"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <method> <task> <otherscientificterm> <otherscientificterm> <method> <method> <material> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "metalogic program -- USED-FOR -- knowledge base applications",
      "metalanguage -- USED-FOR -- theory of time",
      "temporal reasoning -- USED-FOR -- knowledge base applications",
      "prolog program -- USED-FOR -- metalogic program",
      "metalogic program -- USED-FOR -- temporal reasoning",
      "logic -- USED-FOR -- theory of time",
      "fold/unfold transformations -- USED-FOR -- metalogic program",
      "knowledge bases -- FEATURE-OF -- notions of time"
    ],
    "abstract": "the problem of representing and reasoning about two <otherscientificterm_3> that are relevant in the context of <material_12> is addressed . these are called <otherscientificterm_0> respectively . <otherscientificterm_8> denotes the time for which <otherscientificterm_1> denotes the time lor which a belief is held -lrb- by an agent or a knowledge base -rrb- . we formalize an appropriate <otherscientificterm_2> using <otherscientificterm_14> as a <otherscientificterm_15> . we then present a <method_13> derived from this <otherscientificterm_2> through <otherscientificterm_9> . the <method_13> enables the <method_6> required for <task_4> to be carried out efficiently . the <method_13> is directly implementable as a <method_10> and hence the need for a more complex <method_5> is obviated . the <method_13> is applicable for such <task_4> as legislation and <task_7> and in the context of <method_11> where an agent reasons about the beliefs of another agent .",
    "abstract_og": "the problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed . these are called historical time and belief time respectively . historical time denotes the time for which information models reality/belief time denotes the time lor which a belief is held -lrb- by an agent or a knowledge base -rrb- . we formalize an appropriate theory of time using logic as a metalanguage . we then present a metalogic program derived from this theory of time through fold/unfold transformations . the metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently . the metalogic program is directly implementable as a prolog program and hence the need for a more complex theorem prover is obviated . the metalogic program is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent ."
  },
  {
    "title": "Beyond Doddington menagerie , a first step towards .",
    "entities": [
      "speaker verification systems",
      "open source system",
      "nist-sre 2008",
      "voice samples",
      "speaker factor",
      "speaker models",
      "error rate",
      "accuracy"
    ],
    "types": "<method> <method> <material> <material> <method> <method> <metric> <metric>",
    "relations": [
      "voice samples -- USED-FOR -- speaker models",
      "error rate -- EVALUATE-FOR -- speaker verification systems"
    ],
    "abstract": "during the last decade , <method_0> have shown significant progress and have reached a level of performance and <metric_7> that support their utilization in practical applications , including the forensic ones . this context emphasizes the importance of a deeper analysis of the <method_0> 's performance over basic <metric_6> . in this paper , the influence of the speaker -lrb- his/her ` voice ' -rrb- on the performance is studied and the effect of the model -lrb- the training excerpt -rrb- is investigated . the experimental setup is based on an <method_1> and the experimental context of <material_2> . the results confirm that the lower performances are obtained from a reduced number of speakers . even more than <method_4> , <method_0> performances are shown to be highly dependant on the <material_3> used to train <method_5> .",
    "abstract_og": "during the last decade , speaker verification systems have shown significant progress and have reached a level of performance and accuracy that support their utilization in practical applications , including the forensic ones . this context emphasizes the importance of a deeper analysis of the speaker verification systems 's performance over basic error rate . in this paper , the influence of the speaker -lrb- his/her ` voice ' -rrb- on the performance is studied and the effect of the model -lrb- the training excerpt -rrb- is investigated . the experimental setup is based on an open source system and the experimental context of nist-sre 2008 . the results confirm that the lower performances are obtained from a reduced number of speakers . even more than speaker factor , speaker verification systems performances are shown to be highly dependant on the voice samples used to train speaker models ."
  },
  {
    "title": "Musical Instrument Classification using Non-Negative Matrix Factorization Algorithms and Subset Feature Selection .",
    "entities": [
      "automatic classification of individual musical instrument sounds",
      "non-negative matrix factorization",
      "sound classification applications",
      "mel-frequency cepstral coefficient",
      "mpeg-7 descriptors",
      "audiospectrumflatness descrip-tor",
      "audiospectrumspread descriptors",
      "perceptual features",
      "branch-and-bound search",
      "nmf algorithms",
      "discriminant nmf",
      "feature subsets",
      "features",
      "classifiers",
      "classification",
      "accuracy"
    ],
    "types": "<task> <method> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <task> <metric>",
    "relations": [
      "mel-frequency cepstral coefficient -- CONJUNCTION -- audiospectrumflatness descrip-tor",
      "audiospectrumflatness descrip-tor -- CONJUNCTION -- audiospectrumspread descriptors",
      "non-negative matrix factorization -- USED-FOR -- classifiers",
      "features -- USED-FOR -- classification",
      "perceptual features -- USED-FOR -- sound classification applications"
    ],
    "abstract": "in this paper , a class of algorithms for <task_0> is presented . several <otherscientificterm_7> used in <task_2> as well as <material_4> were measured for 300 sound recordings consisting of 6 different musical instrument classes . subsets of the feature set are selected using <method_8> , obtaining the most suitable <otherscientificterm_12> for <task_14> . a class of <method_13> is developed based on the <method_1> . the standard <method_1> is examined as well as its modifications : the local , the sparse , and the <method_10> . the experimental results compare <otherscientificterm_11> of varying sizes alongside the various <method_9> . it has been found that a subset containing the mean and the variance of the first <otherscientificterm_3> and the <otherscientificterm_5> along with the means of the audiospectrumenvelope and the <otherscientificterm_6> when is fed to a standard <method_1> yields an <metric_15> exceeding 95 % .",
    "abstract_og": "in this paper , a class of algorithms for automatic classification of individual musical instrument sounds is presented . several perceptual features used in sound classification applications as well as mpeg-7 descriptors were measured for 300 sound recordings consisting of 6 different musical instrument classes . subsets of the feature set are selected using branch-and-bound search , obtaining the most suitable features for classification . a class of classifiers is developed based on the non-negative matrix factorization . the standard non-negative matrix factorization is examined as well as its modifications : the local , the sparse , and the discriminant nmf . the experimental results compare feature subsets of varying sizes alongside the various nmf algorithms . it has been found that a subset containing the mean and the variance of the first mel-frequency cepstral coefficient and the audiospectrumflatness descrip-tor along with the means of the audiospectrumenvelope and the audiospectrumspread descriptors when is fed to a standard non-negative matrix factorization yields an accuracy exceeding 95 % ."
  },
  {
    "title": "Near Real-Time Reliable Stereo Matching Using Programmable Graphics Hardware .",
    "entities": [
      "middlebury stereo vision research website",
      "reliability-based dynamic programming algorithm",
      "near-real-time stereo matching technique",
      "dynamic programming based technique",
      "dense disparity maps",
      "programmable graphics hardware",
      "graph cuts approaches",
      "semi-dense disparity maps",
      "middlebury stereo datasets",
      "dynamic programming passes",
      "computation time",
      "variable window",
      "reference images",
      "processing speed",
      "error rate",
      "accuracy"
    ],
    "types": "<material> <method> <method> <method> <otherscientificterm> <task> <method> <method> <material> <method> <otherscientificterm> <otherscientificterm> <material> <metric> <metric> <metric>",
    "relations": [
      "variable window -- CONJUNCTION -- graph cuts approaches",
      "graph cuts approaches -- HYPONYM-OF -- dynamic programming based technique"
    ],
    "abstract": "a <method_2> is presented in this paper , which is based on the <method_1> we proposed earlier . the new algorithm can generate <method_7> using only two <method_9> , while our previous approach requires 20 ~ 30 passes . we also implement the algorithm on <task_5> , which further improves the <metric_13> . the experiments on the four <material_8> show that the new algorithm can produce dense -lrb- > 85 % of the pixels -rrb- and reliable -lrb- <metric_14> < 0.3 % -rrb- matches in near real-time -lrb- 0.05 ~ 0.1 sec -rrb- . if needed , it can also be used to generate <otherscientificterm_4> . based on the evaluation conducted by the <material_0> , the new algorithm is ranked between the <otherscientificterm_11> and the <method_6> and currently is the most accurate <method_3> . when more than one <material_12> are available , the <metric_15> can be further improved with little extra <otherscientificterm_10> .",
    "abstract_og": "a near-real-time stereo matching technique is presented in this paper , which is based on the reliability-based dynamic programming algorithm we proposed earlier . the new algorithm can generate semi-dense disparity maps using only two dynamic programming passes , while our previous approach requires 20 ~ 30 passes . we also implement the algorithm on programmable graphics hardware , which further improves the processing speed . the experiments on the four middlebury stereo datasets show that the new algorithm can produce dense -lrb- > 85 % of the pixels -rrb- and reliable -lrb- error rate < 0.3 % -rrb- matches in near real-time -lrb- 0.05 ~ 0.1 sec -rrb- . if needed , it can also be used to generate dense disparity maps . based on the evaluation conducted by the middlebury stereo vision research website , the new algorithm is ranked between the variable window and the graph cuts approaches and currently is the most accurate dynamic programming based technique . when more than one reference images are available , the accuracy can be further improved with little extra computation time ."
  },
  {
    "title": "Multi-Step Stochastic ADMM in High Dimensions : Applications to Sparse Optimization and Matrix Decomposition .",
    "entities": [
      "o rate",
      "natural noise models",
      "stochastic admm method",
      "tight convergence guarantees",
      "general loss function",
      "sparse optimization problem",
      "matrix decomposition problems",
      "multi-step version",
      "matrix decomposition",
      "convergence rate",
      "t steps",
      "multi-step admm",
      "high-dimensional problems",
      "optimization problem",
      "loss function",
      "s-sparse problems",
      "minimax rate",
      "multi-block setting",
      "sparse optimization",
      "multi-block admm",
      "scaling",
      "matrix"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <otherscientificterm> <task> <task> <method> <otherscientificterm> <metric> <otherscientificterm> <method> <task> <task> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "general loss function -- USED-FOR -- sparse optimization problem",
      "minimax rate -- EVALUATE-FOR -- stochastic admm method",
      "loss function -- PART-OF -- optimization problem",
      "stochastic admm method -- USED-FOR -- high-dimensional problems",
      "sparse optimization -- CONJUNCTION -- matrix decomposition problems",
      "tight convergence guarantees -- USED-FOR -- multi-block admm"
    ],
    "abstract": "in this paper , we consider a <method_7> of the <method_2> with efficient guarantees for <task_12> . we first analyze the simple setting , where the <task_13> consists of a <otherscientificterm_14> and a single regularizer -lrb- e.g. <otherscientificterm_18> -rrb- , and then extend to the <otherscientificterm_17> with multiple regularizers and multiple variables -lrb- e.g. <otherscientificterm_8> into sparse and low rank components -rrb- . for the <task_5> , our <method_2> achieves the <metric_16> of o -lrb- s log d/t -rrb- for <otherscientificterm_15> in d dimensions in <otherscientificterm_10> , and is thus , unimprovable by any <method_2> up to constant factors . for the <task_5> with a <otherscientificterm_4> , we analyze the <method_11> with multiple blocks . we establish <method_0> and efficient <method_20> as the size of <otherscientificterm_21> grows . for <method_1> -lrb- e.g. independent noise -rrb- , our <metric_9> is minimax-optimal . thus , we establish <otherscientificterm_3> for <otherscientificterm_19> in high dimensions . experiments show that for both <otherscientificterm_18> and <task_6> , our <method_2> outperforms the state-of-the-art methods .",
    "abstract_og": "in this paper , we consider a multi-step version of the stochastic admm method with efficient guarantees for high-dimensional problems . we first analyze the simple setting , where the optimization problem consists of a loss function and a single regularizer -lrb- e.g. sparse optimization -rrb- , and then extend to the multi-block setting with multiple regularizers and multiple variables -lrb- e.g. matrix decomposition into sparse and low rank components -rrb- . for the sparse optimization problem , our stochastic admm method achieves the minimax rate of o -lrb- s log d/t -rrb- for s-sparse problems in d dimensions in t steps , and is thus , unimprovable by any stochastic admm method up to constant factors . for the sparse optimization problem with a general loss function , we analyze the multi-step admm with multiple blocks . we establish o rate and efficient scaling as the size of matrix grows . for natural noise models -lrb- e.g. independent noise -rrb- , our convergence rate is minimax-optimal . thus , we establish tight convergence guarantees for multi-block admm in high dimensions . experiments show that for both sparse optimization and matrix decomposition problems , our stochastic admm method outperforms the state-of-the-art methods ."
  },
  {
    "title": "A comparative study of some discriminative feature reduction algorithms on the AURORA 2000 and the daimlerchrysler in-car ASR tasks .",
    "entities": [
      "linear discriminant analysis mapping",
      "neural nets",
      "aurora 2000 digit task",
      "approximating class posterior probabilities",
      "poorly trained parameters",
      "consecutive feature frames",
      "feature reduction problem",
      "contextual information",
      "lda classes",
      "nn topology",
      "acoustic modelling",
      "in-car task",
      "nn-based approaches",
      "processing time",
      "asr",
      "feature",
      "hmm-states"
    ],
    "types": "<method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <method> <task> <method> <metric> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "processing time -- PART-OF -- acoustic modelling",
      "in-car task -- EVALUATE-FOR -- neural nets",
      "in-car task -- EVALUATE-FOR -- nn-based approaches"
    ],
    "abstract": "a common practice in <method_14> to add <otherscientificterm_7> is to append <otherscientificterm_5> in a single large <otherscientificterm_15> vector . however , this increases the <metric_13> in the <method_10> and may lead to <otherscientificterm_4> . a possible solution is to use a <method_0> to reduce the dimensionality of the <otherscientificterm_15> , but this is not optimal , at least in the case where the <otherscientificterm_8> are <otherscientificterm_16> . it is shown in this paper that the <task_6> is essentially a problem of <otherscientificterm_3> . these can be approximated using <otherscientificterm_1> . some approaches using different choices for the classes and <method_9> are presented and tested on the <material_2> and on our <task_11> . results on <otherscientificterm_1> show a significant performance increase compared to <otherscientificterm_1> , but none of the <method_12> outperforms <otherscientificterm_1> on our <task_11> .",
    "abstract_og": "a common practice in asr to add contextual information is to append consecutive feature frames in a single large feature vector . however , this increases the processing time in the acoustic modelling and may lead to poorly trained parameters . a possible solution is to use a linear discriminant analysis mapping to reduce the dimensionality of the feature , but this is not optimal , at least in the case where the lda classes are hmm-states . it is shown in this paper that the feature reduction problem is essentially a problem of approximating class posterior probabilities . these can be approximated using neural nets . some approaches using different choices for the classes and nn topology are presented and tested on the aurora 2000 digit task and on our in-car task . results on neural nets show a significant performance increase compared to neural nets , but none of the nn-based approaches outperforms neural nets on our in-car task ."
  },
  {
    "title": "The AMI System for the Transcription of Speech in Meetings .",
    "entities": [
      "discriminative and speaker adaptive training",
      "nist rt '06 evaluations",
      "heteroscedastic linear discriminant analysis",
      "maximum likelihood linear regression",
      "vocal tract length normalisation",
      "phone posterior based features",
      "word error rate",
      "ami transcription system",
      "channel adaptive training",
      "meeting data",
      "generic techniques",
      "domain adaptation",
      "web-data collection",
      "cross-talk suppression",
      "beam-forming",
      "segmentation"
    ],
    "types": "<method> <metric> <method> <method> <method> <otherscientificterm> <metric> <method> <method> <material> <method> <task> <task> <task> <method> <task>",
    "relations": [
      "cross-talk suppression -- CONJUNCTION -- beam-forming",
      "discriminative and speaker adaptive training -- HYPONYM-OF -- generic techniques",
      "cross-talk suppression -- CONJUNCTION -- domain adaptation",
      "segmentation -- CONJUNCTION -- cross-talk suppression",
      "beam-forming -- CONJUNCTION -- domain adaptation",
      "segmentation -- CONJUNCTION -- domain adaptation",
      "web-data collection -- CONJUNCTION -- channel adaptive training",
      "heteroscedastic linear discriminant analysis -- CONJUNCTION -- maximum likelihood linear regression",
      "phone posterior based features -- HYPONYM-OF -- generic techniques",
      "word error rate -- EVALUATE-FOR -- ami transcription system",
      "vocal tract length normalisation -- CONJUNCTION -- maximum likelihood linear regression",
      "vocal tract length normalisation -- CONJUNCTION -- heteroscedastic linear discriminant analysis",
      "heteroscedastic linear discriminant analysis -- CONJUNCTION -- phone posterior based features",
      "heteroscedastic linear discriminant analysis -- HYPONYM-OF -- generic techniques",
      "maximum likelihood linear regression -- CONJUNCTION -- phone posterior based features",
      "beam-forming -- CONJUNCTION -- web-data collection",
      "generic techniques -- PART-OF -- ami transcription system",
      "nist rt '06 evaluations -- EVALUATE-FOR -- ami transcription system",
      "discriminative and speaker adaptive training -- PART-OF -- ami transcription system",
      "heteroscedastic linear discriminant analysis -- PART-OF -- ami transcription system",
      "segmentation -- CONJUNCTION -- beam-forming",
      "discriminative and speaker adaptive training -- CONJUNCTION -- vocal tract length normalisation",
      "maximum likelihood linear regression -- HYPONYM-OF -- generic techniques",
      "domain adaptation -- CONJUNCTION -- web-data collection",
      "maximum likelihood linear regression -- PART-OF -- ami transcription system",
      "domain adaptation -- CONJUNCTION -- channel adaptive training",
      "vocal tract length normalisation -- HYPONYM-OF -- generic techniques",
      "discriminative and speaker adaptive training -- CONJUNCTION -- heteroscedastic linear discriminant analysis",
      "phone posterior based features -- PART-OF -- ami transcription system"
    ],
    "abstract": "this paper describes the <method_7> for speech in meetings developed in collaboration by five research groups . the <method_7> includes <method_10> such as <method_0> , <method_4> , <method_2> , <method_3> , and <otherscientificterm_5> , as well as techniques specifically designed for <material_9> . these include <task_15> and <task_13> , <method_14> , <task_11> , <task_12> , and <method_8> . the <method_7> was improved by more than 20 % relative in <metric_6> compared to our previous <method_7> and was usd in the <metric_1> where it was found to yield competitive performance .",
    "abstract_og": "this paper describes the ami transcription system for speech in meetings developed in collaboration by five research groups . the ami transcription system includes generic techniques such as discriminative and speaker adaptive training , vocal tract length normalisation , heteroscedastic linear discriminant analysis , maximum likelihood linear regression , and phone posterior based features , as well as techniques specifically designed for meeting data . these include segmentation and cross-talk suppression , beam-forming , domain adaptation , web-data collection , and channel adaptive training . the ami transcription system was improved by more than 20 % relative in word error rate compared to our previous ami transcription system and was usd in the nist rt '06 evaluations where it was found to yield competitive performance ."
  },
  {
    "title": "Polyphase filters - A model for teaching the art of discovery in DSP .",
    "entities": [
      "mathematical developments",
      "polyphase decimation",
      "mathematical approaches",
      "dsp topics",
      "dsp",
      "interpolation"
    ],
    "types": "<otherscientificterm> <task> <method> <material> <method> <task>",
    "relations": [
      "polyphase decimation -- CONJUNCTION -- interpolation"
    ],
    "abstract": "by its very nature <method_4> is a mathematically heavy topic and to fully understand it students need to understand the <otherscientificterm_0> underlying <material_3> . however , relying solely on <otherscientificterm_0> often clouds the true nature of the foundation of a result . it is likely that students who master the mathematics may still not truly grasp the key ideas of a topic . furthermore , teaching <material_3> by merely '' going through the mathematics '' deprives students of learning the art of discovery that will make them good researchers . this paper uses the topic of <task_1> and <task_5> to illustrate how it is possible to maintain rigor yet teach using less <method_2> that show students how researchers think when developing new ideas .",
    "abstract_og": "by its very nature dsp is a mathematically heavy topic and to fully understand it students need to understand the mathematical developments underlying dsp topics . however , relying solely on mathematical developments often clouds the true nature of the foundation of a result . it is likely that students who master the mathematics may still not truly grasp the key ideas of a topic . furthermore , teaching dsp topics by merely '' going through the mathematics '' deprives students of learning the art of discovery that will make them good researchers . this paper uses the topic of polyphase decimation and interpolation to illustrate how it is possible to maintain rigor yet teach using less mathematical approaches that show students how researchers think when developing new ideas ."
  },
  {
    "title": "Switching Hypothesized Measurements : A Dynamic Model with Applications to Occlusion Adaptive Joint Tracking .",
    "entities": [
      "switching hypothesized measurements model",
      "multimodal state space probability distributions",
      "history of measurement data",
      "tracking multiple objects",
      "online joint tracking",
      "dynamic model",
      "occlusion relationship",
      "visual occlusions",
      "hypothesized measurements",
      "filtering algorithms",
      "dynamic processes",
      "propagation"
    ],
    "types": "<method> <task> <material> <task> <task> <method> <otherscientificterm> <task> <otherscientificterm> <method> <method> <otherscientificterm>",
    "relations": [
      "dynamic model -- USED-FOR -- tracking multiple objects",
      "dynamic model -- USED-FOR -- multimodal state space probability distributions",
      "filtering algorithms -- USED-FOR -- online joint tracking",
      "dynamic model -- USED-FOR -- visual occlusions",
      "hypothesized measurements -- USED-FOR -- dynamic model"
    ],
    "abstract": "this paper proposes a <method_5> supporting <task_1> and presents the application of the <method_5> in dealing with <task_7> when <task_3> jointly . for a set of hypotheses , multiple measurements are acquired at each time instant . the <method_5> switches among a set of <otherscientificterm_8> during the <otherscientificterm_11> . two computationally efficient <method_9> are derived for <task_4> . both the <otherscientificterm_6> and state of the objects are recursively estimated from the <material_2> . the <method_0> is generally applicable to describe various <method_10> with multiple alternative measurement methods .",
    "abstract_og": "this paper proposes a dynamic model supporting multimodal state space probability distributions and presents the application of the dynamic model in dealing with visual occlusions when tracking multiple objects jointly . for a set of hypotheses , multiple measurements are acquired at each time instant . the dynamic model switches among a set of hypothesized measurements during the propagation . two computationally efficient filtering algorithms are derived for online joint tracking . both the occlusion relationship and state of the objects are recursively estimated from the history of measurement data . the switching hypothesized measurements model is generally applicable to describe various dynamic processes with multiple alternative measurement methods ."
  },
  {
    "title": "Effects of Sampling and Compression on Human IRIS Verification .",
    "entities": [
      "subsampling and compression of human iris images",
      "m1 biometric data interchange format",
      "identity verification systems",
      "normalized iris images",
      "radial fourier coefficients",
      "fourier domain processing",
      "noise reduction",
      "identity verification",
      "rectangular format",
      "file size",
      "verification",
      "compression"
    ],
    "types": "<task> <method> <method> <material> <otherscientificterm> <method> <task> <task> <otherscientificterm> <metric> <task> <method>",
    "relations": [
      "file size -- EVALUATE-FOR -- m1 biometric data interchange format"
    ],
    "abstract": "the resilience of <method_2> to <task_0> is investigated for three high performance iris matching algorithms . for evaluation , 2156 images from 308 eyes are mapped into a <otherscientificterm_8> with 512 pixels circumferentially and 80 radially . for <task_7> , the 48 rows nearest the pupil were taken and the images were subsampled by <method_5> . negligible degradation in <task_10> is observed if at least 171 circumferential and 16 <otherscientificterm_4> are preserved , corresponding to sampling at 342 by 32 pixels . with <method_11> by jpeg 2000 , improved performance is observed down to 0.3 bpp , attributed to <task_6> without significant loss of texture . to ensure that no algorithm is degraded , it is recommended that <material_3> should be exchanged at 512 x 80 pixel resolution , compressed by jpeg 2000 to 0.5 bpp . this achieves a smaller <metric_9> than the proposed <method_1> .",
    "abstract_og": "the resilience of identity verification systems to subsampling and compression of human iris images is investigated for three high performance iris matching algorithms . for evaluation , 2156 images from 308 eyes are mapped into a rectangular format with 512 pixels circumferentially and 80 radially . for identity verification , the 48 rows nearest the pupil were taken and the images were subsampled by fourier domain processing . negligible degradation in verification is observed if at least 171 circumferential and 16 radial fourier coefficients are preserved , corresponding to sampling at 342 by 32 pixels . with compression by jpeg 2000 , improved performance is observed down to 0.3 bpp , attributed to noise reduction without significant loss of texture . to ensure that no algorithm is degraded , it is recommended that normalized iris images should be exchanged at 512 x 80 pixel resolution , compressed by jpeg 2000 to 0.5 bpp . this achieves a smaller file size than the proposed m1 biometric data interchange format ."
  },
  {
    "title": "Coupled information-theoretic encoding for face photo-sketch recognition .",
    "entities": [
      "inter-modality face recognition approach",
      "automatic face photo-sketch recognition",
      "coupled information-theoretic projection tree",
      "discriminative local face structures",
      "face sketch database",
      "feature extraction stage",
      "quantized feature spaces",
      "large scale dataset",
      "classification algorithms",
      "information-theoretic encoding",
      "random-ized forest",
      "feret database",
      "law enforcement",
      "mutual information",
      "face descriptor",
      "modality gap",
      "coupled encoding",
      "features",
      "photos"
    ],
    "types": "<method> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material> <method> <method> <otherscientificterm> <material> <task> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material>",
    "relations": [
      "automatic face photo-sketch recognition -- USED-FOR -- law enforcement",
      "classification algorithms -- USED-FOR -- modality gap",
      "large scale dataset -- EVALUATE-FOR -- inter-modality face recognition approach",
      "modality gap -- FEATURE-OF -- feature extraction stage",
      "face descriptor -- USED-FOR -- discriminative local face structures",
      "modality gap -- USED-FOR -- inter-modality face recognition approach",
      "coupled information-theoretic projection tree -- USED-FOR -- coupled encoding",
      "coupled information-theoretic projection tree -- USED-FOR -- random-ized forest"
    ],
    "abstract": "automatic face photo-sketch recognition has important applications for <task_12> . recent research has fo-cused on transforming <material_18> and sketches into the same modality for matching or developing advanced <method_8> to reduce the <otherscientificterm_15> between <otherscientificterm_17> extracted from <material_18> and sketches . in this paper , we propose a new <method_0> by reducing the <otherscientificterm_15> at the <otherscientificterm_5> . a new <method_14> based on coupled <method_9> is used to capture <otherscientificterm_3> and to effectively match <material_18> and sketches . guided by maximizing the <otherscientificterm_13> between <material_18> and sketches in the <otherscientificterm_6> , the <otherscientificterm_16> is achieved by the proposed <otherscientificterm_2> , which is extended to the <otherscientificterm_10> to further boost the performance . we create the largest <material_4> including sketches of 1 , 194 people from the <material_11> . experiments on this <material_7> show that our <method_0> significantly outper-forms the state-of-the-art methods .",
    "abstract_og": "automatic face photo-sketch recognition has important applications for law enforcement . recent research has fo-cused on transforming photos and sketches into the same modality for matching or developing advanced classification algorithms to reduce the modality gap between features extracted from photos and sketches . in this paper , we propose a new inter-modality face recognition approach by reducing the modality gap at the feature extraction stage . a new face descriptor based on coupled information-theoretic encoding is used to capture discriminative local face structures and to effectively match photos and sketches . guided by maximizing the mutual information between photos and sketches in the quantized feature spaces , the coupled encoding is achieved by the proposed coupled information-theoretic projection tree , which is extended to the random-ized forest to further boost the performance . we create the largest face sketch database including sketches of 1 , 194 people from the feret database . experiments on this large scale dataset show that our inter-modality face recognition approach significantly outper-forms the state-of-the-art methods ."
  },
  {
    "title": "Learning Exemplar-Based Categorization for the Detection of Multi-View Multi-Pose Objects .",
    "entities": [
      "multi-view multi-pose people and vehicle data",
      "manually clustering multi-view multi-pose training data",
      "adaboost-based object detection framework",
      "multi-view multi-pose object detection",
      "nested adaboost loops",
      "discriminative shape-based exemplars",
      "real-time implementation",
      "labeling ambiguity",
      "two-class classifiers",
      "discriminative features",
      "inner adaboost",
      "objective function",
      "discriminative exemplars",
      "exemplar selection",
      "intra-class category",
      "categorization",
      "classifier",
      "classifiers"
    ],
    "types": "<material> <material> <method> <task> <otherscientificterm> <method> <task> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <task> <method> <method>",
    "relations": [
      "inner adaboost -- USED-FOR -- discriminative features",
      "classifiers -- PART-OF -- classifier",
      "two-class classifiers -- USED-FOR -- multi-view multi-pose object detection",
      "nested adaboost loops -- USED-FOR -- objective function"
    ],
    "abstract": "this paper proposes a novel approach for <task_3> using <method_5> . the key idea underlying this method is motivated by numerous previous observations that <material_1> into different categories and then combining the separately trained <method_8> greatly improved the <task_3> performance . a novel computational framework is proposed to unify different processes of <task_15> , training individual <method_16> for each <otherscientificterm_14> , and training a strong <method_16> combining the individual <method_17> . the individual processes employ a single <otherscientificterm_11> that is optimized using two <otherscientificterm_4> . the outer adaboost loop is used to select <otherscientificterm_12> and the <method_10> is used to select <otherscientificterm_9> on the selected exemplars . the proposed approach replaces the manual time-consuming process of <task_13> as well as addresses the problem of <task_7> inherent in this process . also , our approach fully complies with the standard <method_2> in terms of <task_6> . experiments on <material_0> demonstrate the efficacy of the proposed approach .",
    "abstract_og": "this paper proposes a novel approach for multi-view multi-pose object detection using discriminative shape-based exemplars . the key idea underlying this method is motivated by numerous previous observations that manually clustering multi-view multi-pose training data into different categories and then combining the separately trained two-class classifiers greatly improved the multi-view multi-pose object detection performance . a novel computational framework is proposed to unify different processes of categorization , training individual classifier for each intra-class category , and training a strong classifier combining the individual classifiers . the individual processes employ a single objective function that is optimized using two nested adaboost loops . the outer adaboost loop is used to select discriminative exemplars and the inner adaboost is used to select discriminative features on the selected exemplars . the proposed approach replaces the manual time-consuming process of exemplar selection as well as addresses the problem of labeling ambiguity inherent in this process . also , our approach fully complies with the standard adaboost-based object detection framework in terms of real-time implementation . experiments on multi-view multi-pose people and vehicle data demonstrate the efficacy of the proposed approach ."
  },
  {
    "title": "Optimization of the Gaussian Mixture Model Evaluation on GPU .",
    "entities": [
      "cuda or opencl gpu programming frameworks",
      "gaussian mixture acoustic model evaluation algorithm",
      "optimization of acoustic likelihoods computation",
      "real-time speech recognition engines",
      "model selection techniques",
      "automatics speech recognizers",
      "acoustic models",
      "gpu resources",
      "low-end gpu",
      "fusion techniques",
      "conditional likelihoods",
      "gpu devices",
      "lvcsr decoder",
      "cuda",
      "opencl"
    ],
    "types": "<method> <method> <task> <method> <method> <method> <method> <material> <method> <method> <otherscientificterm> <method> <method> <method> <material>",
    "relations": [
      "fusion techniques -- CONJUNCTION -- model selection techniques",
      "acoustic models -- PART-OF -- real-time speech recognition engines",
      "fusion techniques -- USED-FOR -- acoustic models",
      "gaussian mixture acoustic model evaluation algorithm -- USED-FOR -- gpu devices",
      "cuda -- USED-FOR -- optimization of acoustic likelihoods computation",
      "cuda -- COMPARE -- opencl"
    ],
    "abstract": "in this paper we present a highly optimized implementation of <method_1> . evaluation of these likelihoods is one of the most computationally intensive parts of <method_5> but <method_1> can be well-parallelized and offloaded to <method_11> . our <method_1> offers significant speed-up compared to the recently published approaches , since <method_1> exploits the <method_11> better . all the recent implementations were programmed either in <method_0> . we present results for both ; <method_13> as well as <material_14> . results suggest that even very large <method_6> can be utilized in <method_3> on computers and laptops equipped with a <method_8> . <task_2> on <method_13> enables to use the remaining <material_7> for offloading of other compute-intensive parts of <method_12> . other possible use of the freed <material_7> is to evaluate several <method_6> at the same time and use <method_9> or <method_4> to improve the quality of resulting <otherscientificterm_10> under diverse conditions .",
    "abstract_og": "in this paper we present a highly optimized implementation of gaussian mixture acoustic model evaluation algorithm . evaluation of these likelihoods is one of the most computationally intensive parts of automatics speech recognizers but gaussian mixture acoustic model evaluation algorithm can be well-parallelized and offloaded to gpu devices . our gaussian mixture acoustic model evaluation algorithm offers significant speed-up compared to the recently published approaches , since gaussian mixture acoustic model evaluation algorithm exploits the gpu devices better . all the recent implementations were programmed either in cuda or opencl gpu programming frameworks . we present results for both ; cuda as well as opencl . results suggest that even very large acoustic models can be utilized in real-time speech recognition engines on computers and laptops equipped with a low-end gpu . optimization of acoustic likelihoods computation on cuda enables to use the remaining gpu resources for offloading of other compute-intensive parts of lvcsr decoder . other possible use of the freed gpu resources is to evaluate several acoustic models at the same time and use fusion techniques or model selection techniques to improve the quality of resulting conditional likelihoods under diverse conditions ."
  },
  {
    "title": "Modeling Cantonese pronunciation variation by acoustic model refinement .",
    "entities": [
      "lower , phonetic or subphonetic level",
      "surfaceform phone",
      "baseform phone",
      "relative word error rate",
      "cantonese speech recognition database",
      "gaussian mixture components",
      "pronunciation modeling algorithms",
      "canonical phone",
      "sound change",
      "mixture components",
      "surfaceform models",
      "pronunciation variations",
      "phone change",
      "acoustic models",
      "baseform"
    ],
    "types": "<otherscientificterm> <method> <method> <metric> <material> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "baseform phone -- USED-FOR -- surfaceform phone",
      "cantonese speech recognition database -- EVALUATE-FOR -- pronunciation modeling algorithms",
      "phone change -- CONJUNCTION -- sound change"
    ],
    "abstract": "pronunciation variations can be roughly classified into two types : a <otherscientificterm_12> or a <otherscientificterm_8> -lsb- 1 -rsb- -lsb- 2 -rsb- . a <otherscientificterm_12> happens when a <otherscientificterm_7> is produced as a different phone . such a change can be modeled by converting the <method_2> to a <method_1> . a <otherscientificterm_8> happens at a <otherscientificterm_0> within a phone and it can not be modeled well by either the <otherscientificterm_14> or the surfaceform phone alone . we propose here to refine the <method_13> to cope with sound changes by -lrb- 1 -rrb- sharing the <method_5> of hmm states in the <otherscientificterm_14> and the <method_10> ; -lrb- 2 -rrb- adapting the <method_9> of the <method_13> towards those of the <method_10> ; -lrb- 3 -rrb- selectively reconstructing new <method_13> through sharing or adapting . the proposed <method_6> are generic and can , in principle , be applied to different languages . specifically , <method_6> were tested in a <material_4> . <metric_3> reductions of 5.45 % , 2.53 % , and 3.04 % have been achieved using the three approaches , respectively .",
    "abstract_og": "pronunciation variations can be roughly classified into two types : a phone change or a sound change -lsb- 1 -rsb- -lsb- 2 -rsb- . a phone change happens when a canonical phone is produced as a different phone . such a change can be modeled by converting the baseform phone to a surfaceform phone . a sound change happens at a lower , phonetic or subphonetic level within a phone and it can not be modeled well by either the baseform or the surfaceform phone alone . we propose here to refine the acoustic models to cope with sound changes by -lrb- 1 -rrb- sharing the gaussian mixture components of hmm states in the baseform and the surfaceform models ; -lrb- 2 -rrb- adapting the mixture components of the acoustic models towards those of the surfaceform models ; -lrb- 3 -rrb- selectively reconstructing new acoustic models through sharing or adapting . the proposed pronunciation modeling algorithms are generic and can , in principle , be applied to different languages . specifically , pronunciation modeling algorithms were tested in a cantonese speech recognition database . relative word error rate reductions of 5.45 % , 2.53 % , and 3.04 % have been achieved using the three approaches , respectively ."
  },
  {
    "title": "Distribution-Free Learning of Bayesian Network Structure in Continuous Domains .",
    "entities": [
      "bayesian networks",
      "probability distribution of the domain",
      "independence-based bn structure learning algorithm",
      "conditional independence test",
      "parametric distribution families",
      "bn structure learning",
      "local conditional probabilities",
      "real-world data",
      "continuous variables",
      "statistical approaches",
      "statistical consistency",
      "independence-based methods",
      "graphical models",
      "bayesian networks",
      "distributional assumptions",
      "continuous domains",
      "non-parametric",
      "prediscretization"
    ],
    "types": "<method> <task> <method> <method> <otherscientificterm> <method> <otherscientificterm> <material> <otherscientificterm> <method> <metric> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method>",
    "relations": [
      "conditional independence test -- USED-FOR -- continuous variables",
      "conditional independence test -- COMPARE -- statistical approaches",
      "prediscretization -- USED-FOR -- statistical approaches"
    ],
    "abstract": "in this paper we present a method for learning the structure of <method_0> without making any assumptions on the <task_1> . this is mainly useful for <otherscientificterm_15> , where there is little guidance and many choices for the <otherscientificterm_4> to be used for the <otherscientificterm_6> of the <method_13> , and only a few have been examined analytically . we therefore focus on <method_5> in <otherscientificterm_15> . we address the problem by developing a <method_3> for <otherscientificterm_8> , which can be readily used by any existing <method_2> . our test is <method_16> , making no assumptions on the distribution of the domain . we also provide an effective and computationally efficient method for calculating it from data . we demonstrate the learning of the structure of <method_12> in <otherscientificterm_15> from <material_7> , to our knowledge for the first time using <method_11> and without <otherscientificterm_14> . we also experimentally show that our test compares favorably with existing <method_9> which use <method_17> , and verify desirable properties such as <metric_10> .",
    "abstract_og": "in this paper we present a method for learning the structure of bayesian networks without making any assumptions on the probability distribution of the domain . this is mainly useful for continuous domains , where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the bayesian networks , and only a few have been examined analytically . we therefore focus on bn structure learning in continuous domains . we address the problem by developing a conditional independence test for continuous variables , which can be readily used by any existing independence-based bn structure learning algorithm . our test is non-parametric , making no assumptions on the distribution of the domain . we also provide an effective and computationally efficient method for calculating it from data . we demonstrate the learning of the structure of graphical models in continuous domains from real-world data , to our knowledge for the first time using independence-based methods and without distributional assumptions . we also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization , and verify desirable properties such as statistical consistency ."
  },
  {
    "title": "3D Mixed Invariant and its Application on Object Classification .",
    "entities": [
      "classi \u00bf - cation procedure",
      "characteristic curves",
      "integro-differential invariant",
      "noise",
      "derivatives",
      "curves",
      "invariant"
    ],
    "types": "<method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "characteristic curves -- USED-FOR -- classi \u00bf - cation procedure"
    ],
    "abstract": "a new <otherscientificterm_2> for <otherscientificterm_5> in 3d transformed by af \u00bf ne group action is presented in this paper . the <otherscientificterm_4> involved are of the \u00bf rst order , and therefore this <otherscientificterm_6> is signi \u00bf - cantly less sensitive to <otherscientificterm_3> than classical af \u00bf ne differential invari-ants , the simplest of which involves <otherscientificterm_4> of order 5 . a <method_0> based on <otherscientificterm_1> of an object surface is considered using our proposed mixed invariants . substantiating examples are provided to verify ef \u00bf ciency and discriminant power of the characteristic spatial curve based 3d object classi \u00bf cation .",
    "abstract_og": "a new integro-differential invariant for curves in 3d transformed by af \u00bf ne group action is presented in this paper . the derivatives involved are of the \u00bf rst order , and therefore this invariant is signi \u00bf - cantly less sensitive to noise than classical af \u00bf ne differential invari-ants , the simplest of which involves derivatives of order 5 . a classi \u00bf - cation procedure based on characteristic curves of an object surface is considered using our proposed mixed invariants . substantiating examples are provided to verify ef \u00bf ciency and discriminant power of the characteristic spatial curve based 3d object classi \u00bf cation ."
  },
  {
    "title": "Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions .",
    "entities": [
      "syntactic analysis of the source side",
      "source-side parsed , word-aligned parallel corpus",
      "syntactic structure of the source side",
      "hierarchical phrase-based machine translation systems",
      "linguistically-guided latent syntactic categories",
      "linguistically motivated syntactic features",
      "soft syntactic constraints",
      "latent syntactic categories",
      "real-valued feature vector",
      "scfg rules",
      "feature vectors",
      "x non-terminal",
      "hierarchical structure",
      "scfg rule",
      "treebank categories"
    ],
    "types": "<otherscientificterm> <material> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "real-valued feature vector -- USED-FOR -- scfg rule"
    ],
    "abstract": "in this paper , we present a novel approach to enhance <task_3> with <otherscientificterm_5> . rather than directly using <otherscientificterm_14> as in previous studies , we learn a set of <otherscientificterm_4> automatically from a <material_1> , based on the <otherscientificterm_12> among phrase pairs as well as the <otherscientificterm_2> . in our model , each <otherscientificterm_11> in a <otherscientificterm_13> is decorated with a <otherscientificterm_8> computed based on its distribution of <otherscientificterm_7> . these <otherscientificterm_10> are utilized at decoding time to measure the similarity between the <otherscientificterm_0> and the syntax of the <otherscientificterm_9> that are applied to derive translations . our approach maintains the advantages of <task_3> while at the same time naturally incorporates <otherscientificterm_6> .",
    "abstract_og": "in this paper , we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features . rather than directly using treebank categories as in previous studies , we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed , word-aligned parallel corpus , based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side . in our model , each x non-terminal in a scfg rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories . these feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the scfg rules that are applied to derive translations . our approach maintains the advantages of hierarchical phrase-based machine translation systems while at the same time naturally incorporates soft syntactic constraints ."
  },
  {
    "title": "Approaching user capacity in a DSL system via harmonic mean-rate optimization .",
    "entities": [
      "digital subscriber line system",
      "n orthogonal narrowband tones",
      "computationally-efficient power allocation technique",
      "-lrb- non-convex -rrb- integer-program",
      "power allocation techniques",
      "` user capacity",
      "harmonic mean-rate objective",
      "alternate approach",
      "power allocation",
      "features"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "n orthogonal narrowband tones -- FEATURE-OF -- digital subscriber line system",
      "features -- USED-FOR -- computationally-efficient power allocation technique",
      "computationally-efficient power allocation technique -- USED-FOR -- digital subscriber line system",
      "digital subscriber line system -- COMPARE -- power allocation techniques"
    ],
    "abstract": "in this paper we consider a <method_0> with <otherscientificterm_1> . each user has a limited power budget , and our goal is to determine the <otherscientificterm_8> of each user that enables the <otherscientificterm_5> ' of the <method_0> to be approached . in this paper , we use <otherscientificterm_5> ' to denote the maximum number of users that can be supported by the <method_0> , provided that each user is guaranteed to have a data rate that lies within a prescribed range . finding a <otherscientificterm_8> that enables this capacity to be approached directly can be quite cumbersome because <otherscientificterm_8> involves solving a <method_3> . in order to circumvent this difficulty , in this paper we propose an <method_7> that is based on exploiting the fairness and per-tone convexity of the <otherscientificterm_6> . using these <otherscientificterm_9> , we devise a <method_2> that enables the user capacity of the <method_0> to be approached more closely than <method_4> that are more computationally demanding .",
    "abstract_og": "in this paper we consider a digital subscriber line system with n orthogonal narrowband tones . each user has a limited power budget , and our goal is to determine the power allocation of each user that enables the ` user capacity ' of the digital subscriber line system to be approached . in this paper , we use ` user capacity ' to denote the maximum number of users that can be supported by the digital subscriber line system , provided that each user is guaranteed to have a data rate that lies within a prescribed range . finding a power allocation that enables this capacity to be approached directly can be quite cumbersome because power allocation involves solving a -lrb- non-convex -rrb- integer-program . in order to circumvent this difficulty , in this paper we propose an alternate approach that is based on exploiting the fairness and per-tone convexity of the harmonic mean-rate objective . using these features , we devise a computationally-efficient power allocation technique that enables the user capacity of the digital subscriber line system to be approached more closely than power allocation techniques that are more computationally demanding ."
  },
  {
    "title": "Using text and acoustic features to diagnose progressive aphasia and its subtypes .",
    "entities": [
      "automatically diagnosing primary progressive aphasia",
      "progressive nonfluent aphasia",
      "semantic dementia",
      "acoustics of recorded narratives",
      "statistical significance",
      "classifier optimization",
      "textual analysis",
      "feature selection",
      "ppa",
      "features",
      "minimum-redundancy-maximum-relevance"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <method> <method> <method> <method> <otherscientificterm> <material>",
    "relations": [
      "semantic dementia -- CONJUNCTION -- progressive nonfluent aphasia",
      "acoustics of recorded narratives -- CONJUNCTION -- progressive nonfluent aphasia"
    ],
    "abstract": "this paper presents experiments in <task_0> and two of its subtypes , <otherscientificterm_2> and <otherscientificterm_1> , from the <material_3> and <method_6> of the resultant transcripts . in order to train each of three types of classifier -lrb- na \u00a8 \u0131ve bayes , support vector machine , random forest -rrb- , a large set of 81 available <otherscientificterm_9> must be reduced in size . two methods of <method_7> are therefore compared -- one based on <otherscientificterm_4> and the other based on <material_10> . after <method_5> , <method_8> -lrb- or absence thereof -rrb- is correctly diagnosed across 87.4 % of conditions , and the two subtypes of <method_8> are correctly classified 75.6 % of the time .",
    "abstract_og": "this paper presents experiments in automatically diagnosing primary progressive aphasia and two of its subtypes , semantic dementia and progressive nonfluent aphasia , from the acoustics of recorded narratives and textual analysis of the resultant transcripts . in order to train each of three types of classifier -lrb- na \u00a8 \u0131ve bayes , support vector machine , random forest -rrb- , a large set of 81 available features must be reduced in size . two methods of feature selection are therefore compared -- one based on statistical significance and the other based on minimum-redundancy-maximum-relevance . after classifier optimization , ppa -lrb- or absence thereof -rrb- is correctly diagnosed across 87.4 % of conditions , and the two subtypes of ppa are correctly classified 75.6 % of the time ."
  },
  {
    "title": "Importance of nasality measures for speaker recognition data selection and performance prediction .",
    "entities": [
      "equal error rates",
      "speaker recognition",
      "feature vectors of phones",
      "feature vector distributions",
      "computational costs",
      "performance prediction",
      "data-selection scheme",
      "features",
      "sre08",
      "nasality"
    ],
    "types": "<metric> <task> <otherscientificterm> <otherscientificterm> <metric> <task> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "feature vector distributions -- USED-FOR -- performance prediction",
      "feature vector distributions -- USED-FOR -- speaker recognition"
    ],
    "abstract": "we improve upon our measures relating <otherscientificterm_3> to <task_1> performances for <task_5> and potential arbitrary data selection for <task_1> , as described in -lsb- 1 -rsb- . in particular , we examine the means and variances of 11 <otherscientificterm_7> pertaining to <otherscientificterm_9> -lrb- each of which is denoted as a measure -rrb- , computing them on <otherscientificterm_2> to determine which measures give good sr <task_5> of phones . we 've found that the combination of <otherscientificterm_9> measures give a 0.917 correlation with the <metric_0> of phones on <method_8> , exceeding the correlation of our previous best measure -lrb- mutual information -rrb- by 12.7 % . when implemented in our <method_6> -lrb- which does not require a <task_1> to be run -rrb- , the <otherscientificterm_9> measures allow us to select data with combined <metric_0> better than data selected via running a <task_1> in certain cases , at a fortieth of the <metric_4> . the <otherscientificterm_9> measures also require a tenth of the <metric_4> to compute compared to our previous best measure .",
    "abstract_og": "we improve upon our measures relating feature vector distributions to speaker recognition performances for performance prediction and potential arbitrary data selection for speaker recognition , as described in -lsb- 1 -rsb- . in particular , we examine the means and variances of 11 features pertaining to nasality -lrb- each of which is denoted as a measure -rrb- , computing them on feature vectors of phones to determine which measures give good sr performance prediction of phones . we 've found that the combination of nasality measures give a 0.917 correlation with the equal error rates of phones on sre08 , exceeding the correlation of our previous best measure -lrb- mutual information -rrb- by 12.7 % . when implemented in our data-selection scheme -lrb- which does not require a speaker recognition to be run -rrb- , the nasality measures allow us to select data with combined equal error rates better than data selected via running a speaker recognition in certain cases , at a fortieth of the computational costs . the nasality measures also require a tenth of the computational costs to compute compared to our previous best measure ."
  },
  {
    "title": "Generalized interior-point method for constrained peak power minimization of OFDM signals .",
    "entities": [
      "orthogonal frequency division multiplexing symbols",
      "interior-point method algorithm",
      "constellation extension",
      "hybrid ce constraint",
      "optimal distortion set",
      "convex functions",
      "distortion"
    ],
    "types": "<otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "hybrid ce constraint -- USED-FOR -- constellation extension",
      "convex functions -- USED-FOR -- distortion",
      "constellation extension -- USED-FOR -- orthogonal frequency division multiplexing symbols",
      "interior-point method algorithm -- USED-FOR -- optimal distortion set"
    ],
    "abstract": "in this paper we present two results on reducing the peak power of <otherscientificterm_0> via <method_2> . the first result is a derivation of the <method_1> needed to find the <otherscientificterm_4> , where the <otherscientificterm_6> is constrained by <otherscientificterm_5> . next we optimize the parameters of a <otherscientificterm_3> set to minimize the <method_2> . numerical examples are provided to illustrate the findings .",
    "abstract_og": "in this paper we present two results on reducing the peak power of orthogonal frequency division multiplexing symbols via constellation extension . the first result is a derivation of the interior-point method algorithm needed to find the optimal distortion set , where the distortion is constrained by convex functions . next we optimize the parameters of a hybrid ce constraint set to minimize the constellation extension . numerical examples are provided to illustrate the findings ."
  },
  {
    "title": "Content Models with Attitude .",
    "entities": [
      "social media review snippets",
      "variational mean-field inference algorithm",
      "large snippet collections",
      "aggregate user sentiments",
      "probabilistic topic model",
      "yelp reviews"
    ],
    "types": "<material> <method> <material> <otherscientificterm> <method> <material>",
    "relations": [
      "large snippet collections -- USED-FOR -- variational mean-field inference algorithm",
      "probabilistic topic model -- USED-FOR -- aggregate user sentiments"
    ],
    "abstract": "we present a <method_4> for jointly identifying properties and attributes of <material_0> . our <method_4> simultaneously learns a set of properties of a product and captures <otherscientificterm_3> towards these properties . this <method_4> directly enables discovery of highly rated or inconsistent properties of a product . our <method_4> admits an efficient <method_1> which can be paral-lelized and run on <material_2> . we evaluate our <method_4> on a large corpus of snippets from <material_5> to assess property and attribute prediction . we demonstrate that <method_4> outperforms applicable baselines by a considerable margin .",
    "abstract_og": "we present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets . our probabilistic topic model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties . this probabilistic topic model directly enables discovery of highly rated or inconsistent properties of a product . our probabilistic topic model admits an efficient variational mean-field inference algorithm which can be paral-lelized and run on large snippet collections . we evaluate our probabilistic topic model on a large corpus of snippets from yelp reviews to assess property and attribute prediction . we demonstrate that probabilistic topic model outperforms applicable baselines by a considerable margin ."
  },
  {
    "title": "Resolving Event Noun Phrases to Their Verbal Mentions .",
    "entities": [
      "lexical , syntactic and positional features",
      "cascaded event template extraction",
      "event noun phrase resolution",
      "pair-wise candidate preference knowledge",
      "event noun phrases",
      "event anaphora resolution",
      "flat features baseline",
      "syntactic structural information",
      "noun phrase resolution",
      "twin-candidate based model",
      "event pronoun resolution",
      "composite kernel",
      "ontonotes corpus",
      "tree kernel",
      "flat features",
      "nlp study",
      "morphology relation",
      "parse tree",
      "f-score",
      "features",
      "synonym"
    ],
    "types": "<otherscientificterm> <task> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <method> <task> <method> <material> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "composite kernel -- USED-FOR -- syntactic structural information",
      "syntactic structural information -- USED-FOR -- parse tree",
      "event anaphora resolution -- USED-FOR -- cascaded event template extraction",
      "lexical , syntactic and positional features -- USED-FOR -- event noun phrase resolution",
      "twin-candidate based model -- USED-FOR -- pair-wise candidate preference knowledge",
      "synonym -- HYPONYM-OF -- lexical , syntactic and positional features",
      "cascaded event template extraction -- CONJUNCTION -- nlp study",
      "tree kernel -- FEATURE-OF -- parse tree",
      "morphology relation -- CONJUNCTION -- synonym",
      "tree kernel -- USED-FOR -- syntactic structural information"
    ],
    "abstract": "event anaphora resolution is an important <task_5> for <task_1> and other <task_15> . previous study only touched on <task_10> . in this paper , we provide the first systematic study to resolve <otherscientificterm_4> to their verbal mentions crossing long distances . our study shows various <otherscientificterm_0> are needed for <task_2> and most of <otherscientificterm_0> , such as <otherscientificterm_16> , <otherscientificterm_20> and etc , are different from those <otherscientificterm_19> used for conventional <task_8> . <otherscientificterm_7> in the <otherscientificterm_17> modeled with <otherscientificterm_13> is combined with the above diverse <otherscientificterm_14> using a <method_11> , which shows more than 10 % <metric_18> improvement over the <otherscientificterm_6> . in addition , we employed a <method_9> to capture the <otherscientificterm_3> , which further demonstrates a statistically significant improvement . all the above contributes to an encouraging performance of 61.36 % <metric_18> on <material_12> .",
    "abstract_og": "event anaphora resolution is an important event anaphora resolution for cascaded event template extraction and other nlp study . previous study only touched on event pronoun resolution . in this paper , we provide the first systematic study to resolve event noun phrases to their verbal mentions crossing long distances . our study shows various lexical , syntactic and positional features are needed for event noun phrase resolution and most of lexical , syntactic and positional features , such as morphology relation , synonym and etc , are different from those features used for conventional noun phrase resolution . syntactic structural information in the parse tree modeled with tree kernel is combined with the above diverse flat features using a composite kernel , which shows more than 10 % f-score improvement over the flat features baseline . in addition , we employed a twin-candidate based model to capture the pair-wise candidate preference knowledge , which further demonstrates a statistically significant improvement . all the above contributes to an encouraging performance of 61.36 % f-score on ontonotes corpus ."
  },
  {
    "title": "A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval .",
    "entities": [
      "natural language processing",
      "content word",
      "technical abstracts",
      "information retrieval",
      "modern mongolian",
      "lemmatization method",
      "indexing"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <task> <material> <method> <task>",
    "relations": [
      "lemmatization method -- USED-FOR -- information retrieval",
      "lemmatization method -- USED-FOR -- modern mongolian",
      "indexing -- USED-FOR -- information retrieval",
      "natural language processing -- CONJUNCTION -- information retrieval",
      "lemmatization method -- USED-FOR -- indexing",
      "technical abstracts -- USED-FOR -- lemmatization method"
    ],
    "abstract": "in <material_4> , a <otherscientificterm_1> can be inflected when concatenated with suffixes . identifying the original forms of content words is crucial for <task_0> and <task_3> . we propose a <method_5> for <material_4> and apply our <method_5> to <task_6> for <task_3> . we use <otherscientificterm_2> to show the effectiveness of our <method_5> experimentally .",
    "abstract_og": "in modern mongolian , a content word can be inflected when concatenated with suffixes . identifying the original forms of content words is crucial for natural language processing and information retrieval . we propose a lemmatization method for modern mongolian and apply our lemmatization method to indexing for information retrieval . we use technical abstracts to show the effectiveness of our lemmatization method experimentally ."
  },
  {
    "title": "Region-Based Segmentation via Non-Rigid Template Matching .",
    "entities": [
      "global regularization of the template variations",
      "segmentation of irregular shapes",
      "computed tomography images",
      "non-rigid template matching",
      "region segmentation method",
      "fluid registration model",
      "contour-based segmenta-tion techniques",
      "3d shape models",
      "posteriori distributions",
      "intensity distributions",
      "geometric deformation",
      "intensity model",
      "binary template",
      "leaks",
      "accuracy"
    ],
    "types": "<task> <task> <material> <method> <method> <method> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <metric>",
    "relations": [
      "region segmentation method -- USED-FOR -- segmentation of irregular shapes",
      "non-rigid template matching -- USED-FOR -- region segmentation method",
      "intensity model -- CONJUNCTION -- posteriori distributions",
      "fluid registration model -- USED-FOR -- binary template",
      "fluid registration model -- USED-FOR -- global regularization of the template variations",
      "fluid registration model -- USED-FOR -- geometric deformation"
    ],
    "abstract": "we propose a new <method_4> based on <method_3> . we align a <otherscientificterm_12> to an image by maximizing the likelihood of <otherscientificterm_9> within a region of interest and its background . the <method_11> and the corresponding a <otherscientificterm_8> are estimated and updated throughout the alignment . the <otherscientificterm_10> of the <otherscientificterm_12> is based on a <method_5> . unlike <method_6> , this <method_5> allows for a <task_0> . this enables the <task_1> while avoiding <otherscientificterm_13> . we apply our <method_4> to the <task_1> in <material_2> , a challenging task due to the high inter-patient variability in the shape of this organ . we show that our segmentation results are equivalent or superior in <metric_14> to results obtained using existing techniques based on <method_7> .",
    "abstract_og": "we propose a new region segmentation method based on non-rigid template matching . we align a binary template to an image by maximizing the likelihood of intensity distributions within a region of interest and its background . the intensity model and the corresponding a posteriori distributions are estimated and updated throughout the alignment . the geometric deformation of the binary template is based on a fluid registration model . unlike contour-based segmenta-tion techniques , this fluid registration model allows for a global regularization of the template variations . this enables the segmentation of irregular shapes while avoiding leaks . we apply our region segmentation method to the segmentation of irregular shapes in computed tomography images , a challenging task due to the high inter-patient variability in the shape of this organ . we show that our segmentation results are equivalent or superior in accuracy to results obtained using existing techniques based on 3d shape models ."
  },
  {
    "title": "Dynamic Depth Recovery from Multiple Synchronized Video Streams .",
    "entities": [
      "extracting depth information of non-rigid dynamic 3d scenes",
      "3d piecewise planar surface patches",
      "3d geometric , motion",
      "sharp depth discontinuity estimation",
      "spatial color consistency constraint",
      "color based image segmentation",
      "temporally consistent depth estimation",
      "smooth scene motion model",
      "global visibility constraint",
      "synchronized video streams",
      "global visibility constraints",
      "object boundaries",
      "cost function",
      "incremental formulation"
    ],
    "types": "<task> <otherscientificterm> <otherscientificterm> <metric> <otherscientificterm> <method> <task> <method> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "incremental formulation -- USED-FOR -- 3d piecewise planar surface patches",
      "temporally consistent depth estimation -- CONJUNCTION -- sharp depth discontinuity estimation",
      "color based image segmentation -- USED-FOR -- 3d piecewise planar surface patches",
      "spatial color consistency constraint -- CONJUNCTION -- smooth scene motion model",
      "synchronized video streams -- USED-FOR -- extracting depth information of non-rigid dynamic 3d scenes"
    ],
    "abstract": "this paper addresses the problem of <task_0> from multiple <material_9> . three main issues are discussed in this context : -lrb- i -rrb- <task_6> , -lrb- ii -rrb- <metric_3> around <otherscientificterm_11> , and -lrb- iii -rrb- enforcement of the <otherscientificterm_8> . we present a framework in which the scene is modeled as a collection of <otherscientificterm_1> induced by <method_5> . this <otherscientificterm_1> is continuously estimated using an <method_13> in which the <otherscientificterm_2> , and <otherscientificterm_10> are enforced over space and time . the proposed algorithm optimizes a <otherscientificterm_12> that incorporates the <otherscientificterm_4> and a <method_7> .",
    "abstract_og": "this paper addresses the problem of extracting depth information of non-rigid dynamic 3d scenes from multiple synchronized video streams . three main issues are discussed in this context : -lrb- i -rrb- temporally consistent depth estimation , -lrb- ii -rrb- sharp depth discontinuity estimation around object boundaries , and -lrb- iii -rrb- enforcement of the global visibility constraint . we present a framework in which the scene is modeled as a collection of 3d piecewise planar surface patches induced by color based image segmentation . this 3d piecewise planar surface patches is continuously estimated using an incremental formulation in which the 3d geometric , motion , and global visibility constraints are enforced over space and time . the proposed algorithm optimizes a cost function that incorporates the spatial color consistency constraint and a smooth scene motion model ."
  },
  {
    "title": "Hierarchically Gated Deep Networks for Semantic Segmentation .",
    "entities": [
      "multi-scale convolutional neural networks",
      "hierarchically gated deep networks",
      "learning feature representations",
      "semantic seg-mentation task",
      "multi-scale deep network",
      "multi-scale deep networks",
      "fine-grained local structures",
      "global scene structure",
      "spatial contexts",
      "image structures",
      "customized scales",
      "memory gates",
      "memory cells",
      "semantic segmentation",
      "pixels",
      "patches",
      "pix-el"
    ],
    "types": "<method> <method> <method> <task> <task> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "semantic seg-mentation task -- EVALUATE-FOR -- multi-scale deep networks",
      "pixels -- HYPONYM-OF -- fine-grained local structures",
      "pixels -- CONJUNCTION -- patches",
      "patches -- HYPONYM-OF -- fine-grained local structures",
      "hierarchically gated deep networks -- USED-FOR -- pix-el"
    ],
    "abstract": "semantic segmentation aims to parse the scene structure of images by annotating the labels to each pixel so that images can be segmented into different regions . while <otherscientificterm_9> usually have various scales , it is difficult to use a single scale to model the <otherscientificterm_8> for all individual <otherscientificterm_14> . <method_0> and their variants have made striking success for modeling the <otherscientificterm_7> for an image . however , <method_0> are limited in labeling <otherscientificterm_6> like <otherscientificterm_14> and <otherscientificterm_15> , since <otherscientificterm_8> might be blindly mixed up without appropriately customizing their scales . to address this challenge , we develop a novel paradigm of <task_4> to model <otherscientificterm_8> surrounding different <otherscientificterm_14> at various scales . <task_4> builds multiple layers of <otherscientificterm_12> , <method_2> for individual <otherscientificterm_14> at their <otherscientificterm_10> by hierarchically absorbing relevant <otherscientificterm_8> via <otherscientificterm_11> between layers . such <method_1> can customize a suitable scale for each <otherscientificterm_16> , thereby delivering better performance on labeling scene structures of various scales . we conduct the experiments on two datasets , and show competitive results compared with the other <method_5> on the <task_3> .",
    "abstract_og": "semantic segmentation aims to parse the scene structure of images by annotating the labels to each pixel so that images can be segmented into different regions . while image structures usually have various scales , it is difficult to use a single scale to model the spatial contexts for all individual pixels . multi-scale convolutional neural networks and their variants have made striking success for modeling the global scene structure for an image . however , multi-scale convolutional neural networks are limited in labeling fine-grained local structures like pixels and patches , since spatial contexts might be blindly mixed up without appropriately customizing their scales . to address this challenge , we develop a novel paradigm of multi-scale deep network to model spatial contexts surrounding different pixels at various scales . multi-scale deep network builds multiple layers of memory cells , learning feature representations for individual pixels at their customized scales by hierarchically absorbing relevant spatial contexts via memory gates between layers . such hierarchically gated deep networks can customize a suitable scale for each pix-el , thereby delivering better performance on labeling scene structures of various scales . we conduct the experiments on two datasets , and show competitive results compared with the other multi-scale deep networks on the semantic seg-mentation task ."
  },
  {
    "title": "Planar Structure Matching under Projective Uncertainty for Geolocation .",
    "entities": [
      "uncertainty of line segments",
      "human delineated line segments",
      "uncertainty based representation",
      "false candidate regions",
      "image based geolocation",
      "distance transform matching",
      "geometric matching framework",
      "ground image",
      "ortho images",
      "visual appearances",
      "projective transformations"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <task> <method> <method> <material> <material> <material> <otherscientificterm>",
    "relations": [
      "projective transformations -- USED-FOR -- uncertainty of line segments",
      "uncertainty based representation -- PART-OF -- geometric matching framework",
      "projective transformations -- USED-FOR -- uncertainty based representation"
    ],
    "abstract": "image based geolocation aims to answer the question : where was this ground photograph taken ? we present an approach to geoloca-lating a single image based on matching <otherscientificterm_1> in the <material_7> to automatically detected line segments in <material_8> . our approach is based on <method_5> . by observing that the <otherscientificterm_0> is non-linearly amplified by <otherscientificterm_10> , we develop an <method_2> and incorporate <method_2> into a <method_6> . we show that our approach is able to rule out a considerable portion of <otherscientificterm_3> even in a database composed of geographic areas with similar <material_9> .",
    "abstract_og": "image based geolocation aims to answer the question : where was this ground photograph taken ? we present an approach to geoloca-lating a single image based on matching human delineated line segments in the ground image to automatically detected line segments in ortho images . our approach is based on distance transform matching . by observing that the uncertainty of line segments is non-linearly amplified by projective transformations , we develop an uncertainty based representation and incorporate uncertainty based representation into a geometric matching framework . we show that our approach is able to rule out a considerable portion of false candidate regions even in a database composed of geographic areas with similar visual appearances ."
  },
  {
    "title": "Projective Factorization of Multiple Rigid-Body Motions .",
    "entities": [
      "subspace of dimension",
      "point correspondences",
      "known depths",
      "segmentation error",
      "affine methods",
      "motion segmentation",
      "hopkins155 database",
      "subspace separation",
      "point trajec-tories",
      "execution time",
      "lsa",
      "gpca",
      "segmentation"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <task> <material> <method> <otherscientificterm> <metric> <method> <method> <otherscientificterm>",
    "relations": [
      "gpca -- CONJUNCTION -- lsa",
      "segmentation error -- EVALUATE-FOR -- affine methods",
      "lsa -- HYPONYM-OF -- subspace separation",
      "segmentation error -- CONJUNCTION -- execution time",
      "gpca -- HYPONYM-OF -- subspace separation",
      "subspace separation -- USED-FOR -- motion segmentation",
      "execution time -- EVALUATE-FOR -- affine methods",
      "known depths -- USED-FOR -- motion segmentation"
    ],
    "abstract": "given <otherscientificterm_1> in multiple perspective views of a scene containing multiple rigid-body motions , we present an algorithm for segmenting the correspondences according to the multiple motions . we exploit the fact that when the depths of the points are known , the <otherscientificterm_8> associated with a single motion live in a <otherscientificterm_0> at most four . thus <task_5> with <otherscientificterm_2> can be achieved by methods of <method_7> , such as <method_11> or <method_10> . when the depths are unknown , we proceed iteratively . given the <otherscientificterm_12> , we compute the depths using standard techniques . given the depths , we use <method_11> or <method_10> to segment the scene into multiple motions . experiments on the <material_6> show that our method outperforms existing <method_4> in terms of <otherscientificterm_3> and <metric_9> . our methods achieves an error of 2.5 % on 155 sequences .",
    "abstract_og": "given point correspondences in multiple perspective views of a scene containing multiple rigid-body motions , we present an algorithm for segmenting the correspondences according to the multiple motions . we exploit the fact that when the depths of the points are known , the point trajec-tories associated with a single motion live in a subspace of dimension at most four . thus motion segmentation with known depths can be achieved by methods of subspace separation , such as gpca or lsa . when the depths are unknown , we proceed iteratively . given the segmentation , we compute the depths using standard techniques . given the depths , we use gpca or lsa to segment the scene into multiple motions . experiments on the hopkins155 database show that our method outperforms existing affine methods in terms of segmentation error and execution time . our methods achieves an error of 2.5 % on 155 sequences ."
  },
  {
    "title": "Practical Bayesian Optimization of Machine Learning Algorithms .",
    "entities": [
      "gaussian process",
      "human expert-level optimization",
      "latent dirichlet allocation",
      "convolutional neural networks",
      "machine learning algorithms",
      "rules of thumb",
      "automatic approaches",
      "bayesian optimization",
      "brute-force search",
      "parallel experimentation",
      "automatic procedures",
      "learning parameters",
      "model hyperparameters",
      "structured svms",
      "kernel",
      "tuning"
    ],
    "types": "<method> <task> <task> <method> <method> <otherscientificterm> <method> <method> <method> <task> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "latent dirichlet allocation -- CONJUNCTION -- structured svms",
      "gaussian process -- USED-FOR -- machine learning algorithms",
      "rules of thumb -- CONJUNCTION -- brute-force search",
      "learning parameters -- USED-FOR -- machine learning algorithms",
      "model hyperparameters -- USED-FOR -- machine learning algorithms",
      "structured svms -- CONJUNCTION -- convolutional neural networks",
      "kernel -- HYPONYM-OF -- gaussian process"
    ],
    "abstract": "the use of <method_4> frequently involves careful <method_15> of <otherscientificterm_11> and <method_12> . unfortunately , this <method_15> is often a '' black art '' requiring expert experience , <otherscientificterm_5> , or sometimes <method_8> . there is therefore great appeal for <method_6> that can optimize the performance of any given <method_4> to the problem at hand . in this work , we consider this problem through the framework of <method_7> , in which a <method_4> 's generalization performance is modeled as a sample from a <method_0> . we show that certain choices for the nature of the <method_0> , such as the type of <otherscientificterm_14> and the treatment of its hyperparame-ters , can play a crucial role in obtaining a good optimizer that can achieve expert-level performance . we describe new algorithms that take into account the variable cost -lrb- duration -rrb- of <method_4> experiments and that can leverage the presence of multiple cores for <task_9> . we show that these proposed algorithms improve on previous <method_10> and can reach or surpass <task_1> for many algorithms including <task_2> , <method_13> and <method_3> .",
    "abstract_og": "the use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters . unfortunately , this tuning is often a '' black art '' requiring expert experience , rules of thumb , or sometimes brute-force search . there is therefore great appeal for automatic approaches that can optimize the performance of any given machine learning algorithms to the problem at hand . in this work , we consider this problem through the framework of bayesian optimization , in which a machine learning algorithms 's generalization performance is modeled as a sample from a gaussian process . we show that certain choices for the nature of the gaussian process , such as the type of kernel and the treatment of its hyperparame-ters , can play a crucial role in obtaining a good optimizer that can achieve expert-level performance . we describe new algorithms that take into account the variable cost -lrb- duration -rrb- of machine learning algorithms experiments and that can leverage the presence of multiple cores for parallel experimentation . we show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent dirichlet allocation , structured svms and convolutional neural networks ."
  },
  {
    "title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems .",
    "entities": [
      "<i> conditional </i> distributions",
      "hilbert space embedding approach",
      "conditional embeddings",
      "conditional embeddings",
      "kernel estimate",
      "dynamical systems",
      "hilbert spaces",
      "nonparametric method",
      "conditional embedding",
      "embeddings"
    ],
    "types": "<otherscientificterm> <method> <method> <method> <method> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "hilbert space embedding approach -- USED-FOR -- <i> conditional </i> distributions",
      "nonparametric method -- USED-FOR -- dynamical systems",
      "kernel estimate -- USED-FOR -- conditional embedding"
    ],
    "abstract": "in this paper , we extend the <method_1> to handle <otherscientificterm_0> . we derive a <method_4> for the <otherscientificterm_8> , and show its connection to ordinary <otherscientificterm_9> . <method_2> largely extend our ability to manipulate distributions in <otherscientificterm_6> , and as an example , we derive a <method_7> for modeling <method_5> where the belief state of the system is maintained as a <otherscientificterm_8> . our <method_7> is very general in terms of both the domains and the types of distributions that it can handle , and we demonstrate the effectiveness of our <method_7> in various <method_5> . we expect that <method_3> will have wider applications beyond modeling <method_5> .",
    "abstract_og": "in this paper , we extend the hilbert space embedding approach to handle <i> conditional </i> distributions . we derive a kernel estimate for the conditional embedding , and show its connection to ordinary embeddings . conditional embeddings largely extend our ability to manipulate distributions in hilbert spaces , and as an example , we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding . our nonparametric method is very general in terms of both the domains and the types of distributions that it can handle , and we demonstrate the effectiveness of our nonparametric method in various dynamical systems . we expect that conditional embeddings will have wider applications beyond modeling dynamical systems ."
  },
  {
    "title": "Polar coordinate based nonlinear function for frequency-domain blind source separation .",
    "entities": [
      "frequency-domain blind source separation",
      "independent component analysis",
      "probability density function",
      "nonlinear function",
      "polar coordinates",
      "cartesian coordinates",
      "complex-valued signals",
      "speech signals"
    ],
    "types": "<task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <material>",
    "relations": [
      "polar coordinates -- USED-FOR -- nonlinear function",
      "nonlinear function -- USED-FOR -- independent component analysis",
      "independent component analysis -- USED-FOR -- complex-valued signals",
      "independent component analysis -- USED-FOR -- frequency-domain blind source separation",
      "nonlinear function -- USED-FOR -- frequency-domain blind source separation",
      "nonlinear function -- USED-FOR -- complex-valued signals"
    ],
    "abstract": "this paper presents a new type of <otherscientificterm_3> for <task_1> to process <material_6> , which is used in <task_0> . the new <otherscientificterm_3> is based on the <otherscientificterm_4> of a complex number , whereas the conventional one is based on the <otherscientificterm_5> . the new <otherscientificterm_3> is derived from the <otherscientificterm_2> of frequency-domain signals that are assumed to be independent of the phase . we show that the difference between the two types of functions is in the assumed densities of independent components . experimental results for separating <material_7> show that the new <otherscientificterm_3> behaves better than the conventional one .",
    "abstract_og": "this paper presents a new type of nonlinear function for independent component analysis to process complex-valued signals , which is used in frequency-domain blind source separation . the new nonlinear function is based on the polar coordinates of a complex number , whereas the conventional one is based on the cartesian coordinates . the new nonlinear function is derived from the probability density function of frequency-domain signals that are assumed to be independent of the phase . we show that the difference between the two types of functions is in the assumed densities of independent components . experimental results for separating speech signals show that the new nonlinear function behaves better than the conventional one ."
  },
  {
    "title": "Learning Non-Generative Grammatical Models for Document Analysis .",
    "entities": [
      "hierarchical seg-mentation and labeling of document layout structures",
      "uwiii document image database",
      "document image analysis tasks",
      "page layout structure extraction",
      "document analysis problems",
      "mathematical expression interpretation",
      "printed mathematical expressions",
      "grammatical cost function",
      "machine learning",
      "optimal parse",
      "document structure",
      "global search",
      "layout analysis",
      "page grammar",
      "parsing process",
      "document layout",
      "features",
      "grammars",
      "grammar",
      "latex"
    ],
    "types": "<task> <material> <task> <task> <task> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <method> <method> <otherscientificterm> <otherscientificterm> <method> <method> <method>",
    "relations": [
      "global search -- USED-FOR -- optimal parse",
      "page layout structure extraction -- HYPONYM-OF -- document image analysis tasks",
      "page layout structure extraction -- CONJUNCTION -- mathematical expression interpretation",
      "grammar -- USED-FOR -- document layout",
      "grammars -- USED-FOR -- document structure",
      "mathematical expression interpretation -- HYPONYM-OF -- document image analysis tasks"
    ],
    "abstract": "-- we present a general approach for the <task_0> . this approach models <otherscientificterm_15> as a <method_18> and performs a <otherscientificterm_11> for the <otherscientificterm_9> based on a <otherscientificterm_7> . our contribution is to utilize <method_8> to discriminatively select <otherscientificterm_16> and set all parameters in the <method_14> . therefore , and unlike many other approaches for <task_12> , ours can easily adapt itself to a variety of <task_4> . one need only specify the <method_13> and provide a set of correctly labeled pages . experiments demonstrate the effectiveness of this technique on two <task_2> : <task_3> and <task_5> . experiments demonstrate that the learned <method_17> can be used to extract the <otherscientificterm_10> in 57 files from the <material_1> . a second set of experiments demonstrate that the same framework can be used to automatically interpret <otherscientificterm_6> so as to recreate the original <method_19> .",
    "abstract_og": "-- we present a general approach for the hierarchical seg-mentation and labeling of document layout structures . this approach models document layout as a grammar and performs a global search for the optimal parse based on a grammatical cost function . our contribution is to utilize machine learning to discriminatively select features and set all parameters in the parsing process . therefore , and unlike many other approaches for layout analysis , ours can easily adapt itself to a variety of document analysis problems . one need only specify the page grammar and provide a set of correctly labeled pages . experiments demonstrate the effectiveness of this technique on two document image analysis tasks : page layout structure extraction and mathematical expression interpretation . experiments demonstrate that the learned grammars can be used to extract the document structure in 57 files from the uwiii document image database . a second set of experiments demonstrate that the same framework can be used to automatically interpret printed mathematical expressions so as to recreate the original latex ."
  },
  {
    "title": "Temporal episodic memory model : an evolution of minerva2 .",
    "entities": [
      "automatic speech recognition",
      "human episodic memory",
      "hmm/gmm baseline systems",
      "temporal sequence",
      "prediction mechanism",
      "asr task",
      "recognition"
    ],
    "types": "<task> <otherscientificterm> <method> <otherscientificterm> <method> <task> <task>",
    "relations": [
      "asr task -- EVALUATE-FOR -- hmm/gmm baseline systems",
      "temporal sequence -- USED-FOR -- recognition"
    ],
    "abstract": "this paper introduces a new model for <task_0> called temm-temporal episodic memory model . temm is derived from a simulation of <otherscientificterm_1> called minerva2 , and it not only overcomes the inability of minerva2 to use <otherscientificterm_3> for <task_6> flexibly , but it also employs a <method_4> as an additional source of information . the performance of temm on an <task_5> is compared to state-of-the-art <method_2> , and a first analysis shows both promising results and a need to further stabilise the consistency of the output of the new model .",
    "abstract_og": "this paper introduces a new model for automatic speech recognition called temm-temporal episodic memory model . temm is derived from a simulation of human episodic memory called minerva2 , and it not only overcomes the inability of minerva2 to use temporal sequence for recognition flexibly , but it also employs a prediction mechanism as an additional source of information . the performance of temm on an asr task is compared to state-of-the-art hmm/gmm baseline systems , and a first analysis shows both promising results and a need to further stabilise the consistency of the output of the new model ."
  },
  {
    "title": "Nonbinary LDPC decoding by min-sum with Adaptive Message Control .",
    "entities": [
      "low-complexity decoding of nonbinary ldpc codes",
      "adaptive message control",
      "ms decoding",
      "message length of belief information",
      "nonbinary ldpc codes",
      "decoding iteration",
      "decoding algorithm",
      "arithmetic operations",
      "belief information",
      "decoding complexity",
      "non-truncated cases",
      "computation"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <task> <method> <task> <otherscientificterm> <metric> <material> <metric>",
    "relations": [
      "arithmetic operations -- EVALUATE-FOR -- decoding algorithm",
      "computation -- EVALUATE-FOR -- decoding algorithm",
      "decoding algorithm -- USED-FOR -- low-complexity decoding of nonbinary ldpc codes"
    ],
    "abstract": "-- a new <method_6> , referred to as min-sum with <method_1> , is proposed to reduce the <metric_9> of <otherscientificterm_4> . the proposed <method_6> adaptively trims the <otherscientificterm_3> to reduce the amount of <task_7> . exploiting the fact that during the <task_5> , the distribution of <otherscientificterm_8> will become more concentrated around the correct element in the case of convergence , the messages can be truncated accordingly by considering only a few entries with large likelihood . simulation results with a gf -lrb- 16 -rrb- nonbinary ldpc code indicate that the proposed <method_6> can reduce <task_7> by up to 65 % compared with <material_10> . compared with the state-of-the-art extended <method_2> , the proposed <method_6> can reduce the <metric_11> by up to 50 % , thereby enabling <task_0> .",
    "abstract_og": "-- a new decoding algorithm , referred to as min-sum with adaptive message control , is proposed to reduce the decoding complexity of nonbinary ldpc codes . the proposed decoding algorithm adaptively trims the message length of belief information to reduce the amount of arithmetic operations . exploiting the fact that during the decoding iteration , the distribution of belief information will become more concentrated around the correct element in the case of convergence , the messages can be truncated accordingly by considering only a few entries with large likelihood . simulation results with a gf -lrb- 16 -rrb- nonbinary ldpc code indicate that the proposed decoding algorithm can reduce arithmetic operations by up to 65 % compared with non-truncated cases . compared with the state-of-the-art extended ms decoding , the proposed decoding algorithm can reduce the computation by up to 50 % , thereby enabling low-complexity decoding of nonbinary ldpc codes ."
  },
  {
    "title": "Amplitude and gain error influence on time error estimation algorithm for time interleaved A/D converter system .",
    "entities": [
      "blind estimation of static time errors",
      "time inter-leaved a/d converters",
      "time error estimation",
      "gain errors",
      "amplitude"
    ],
    "types": "<task> <otherscientificterm> <metric> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "amplitude -- CONJUNCTION -- gain errors"
    ],
    "abstract": "a method for <task_0> in <otherscientificterm_1> is investigated . the method assumes that <otherscientificterm_4> and <otherscientificterm_3> are removed before the <metric_2> . even if the <otherscientificterm_4> and <otherscientificterm_3> are estimated and removed , there will be small errors left . in this paper , we investigate how the <otherscientificterm_4> and <otherscientificterm_3> influence the <metric_2> performance .",
    "abstract_og": "a method for blind estimation of static time errors in time inter-leaved a/d converters is investigated . the method assumes that amplitude and gain errors are removed before the time error estimation . even if the amplitude and gain errors are estimated and removed , there will be small errors left . in this paper , we investigate how the amplitude and gain errors influence the time error estimation performance ."
  },
  {
    "title": "Atom decomposition-based intonation modelling .",
    "entities": [
      "statistical parametric text-to-speech synthesis methods",
      "physiological aspects of prosody production",
      "extraction of physiologically meaningful atoms",
      "gamma distribution shaped atoms",
      "matching pursuit algorithm",
      "intonation decomposition",
      "intonation modelling",
      "tts system",
      "neutral speech",
      "prosody",
      "intonation"
    ],
    "types": "<method> <task> <task> <otherscientificterm> <method> <task> <task> <method> <material> <method> <task>",
    "relations": [
      "intonation -- USED-FOR -- tts system",
      "matching pursuit algorithm -- USED-FOR -- intonation decomposition",
      "physiological aspects of prosody production -- USED-FOR -- intonation modelling",
      "extraction of physiologically meaningful atoms -- USED-FOR -- tts system",
      "physiological aspects of prosody production -- USED-FOR -- statistical parametric text-to-speech synthesis methods",
      "extraction of physiologically meaningful atoms -- USED-FOR -- intonation",
      "intonation modelling -- USED-FOR -- statistical parametric text-to-speech synthesis methods"
    ],
    "abstract": "current <method_0> allow production of <material_8> with acceptable quality . however , <method_9> is often qualified as unsatisfactory and sounding too flat . in this paper , we address <task_6> for <method_0> based on <task_1> . a set of <otherscientificterm_3> is defined and then <task_5> is performed using a <method_4> . some preliminary experiments show that this model allows easy <task_2> that could be used to generate <task_10> in a <method_7> .",
    "abstract_og": "current statistical parametric text-to-speech synthesis methods allow production of neutral speech with acceptable quality . however , prosody is often qualified as unsatisfactory and sounding too flat . in this paper , we address intonation modelling for statistical parametric text-to-speech synthesis methods based on physiological aspects of prosody production . a set of gamma distribution shaped atoms is defined and then intonation decomposition is performed using a matching pursuit algorithm . some preliminary experiments show that this model allows easy extraction of physiologically meaningful atoms that could be used to generate intonation in a tts system ."
  },
  {
    "title": "Conducting Neuroscience to Guide the Development of AI .",
    "entities": [
      "human brain grounds language",
      "watching video stimuli",
      "artificial intelligence",
      "visual perception",
      "cross-modal studies",
      "computer-vision approaches",
      "brain activity",
      "activity recognition",
      "vice versa",
      "brain processing",
      "fmri decoding",
      "scanning",
      "accuracy",
      "ai",
      "fmri"
    ],
    "types": "<otherscientificterm> <task> <material> <otherscientificterm> <method> <method> <otherscientificterm> <task> <otherscientificterm> <task> <method> <task> <metric> <task> <method>",
    "relations": [
      "accuracy -- EVALUATE-FOR -- computer-vision approaches",
      "fmri decoding -- COMPARE -- computer-vision approaches",
      "fmri -- USED-FOR -- artificial intelligence",
      "cross-modal studies -- USED-FOR -- brain activity",
      "computer-vision approaches -- USED-FOR -- activity recognition"
    ],
    "abstract": "study of the human brain through <method_14> can potentially benefit the pursuit of <material_2> . four examples are presented . first , <method_10> of the <otherscientificterm_6> of subjects watching video clips yields higher <metric_12> than state-of-the-art <method_5> to <task_7> . second , novel methods are presented that decode aggregate representations of complex visual stimuli by decoding their independent constituents . third , <method_4> demonstrate the ability to decode the <otherscientificterm_6> induced in subjects <task_1> when trained on the <otherscientificterm_6> induced in subjects seeing text or hearing speech stimuli and <otherscientificterm_8> . fourth , the time course of <task_9> while <task_1> is probed with <task_11> that trades off the amount of the brain scanned for the frequency at which it is scanned . techniques like these can be used to study how the <otherscientificterm_0> in <otherscientificterm_3> and may motivate development of novel approaches in <task_13> .",
    "abstract_og": "study of the human brain through fmri can potentially benefit the pursuit of artificial intelligence . four examples are presented . first , fmri decoding of the brain activity of subjects watching video clips yields higher accuracy than state-of-the-art computer-vision approaches to activity recognition . second , novel methods are presented that decode aggregate representations of complex visual stimuli by decoding their independent constituents . third , cross-modal studies demonstrate the ability to decode the brain activity induced in subjects watching video stimuli when trained on the brain activity induced in subjects seeing text or hearing speech stimuli and vice versa . fourth , the time course of brain processing while watching video stimuli is probed with scanning that trades off the amount of the brain scanned for the frequency at which it is scanned . techniques like these can be used to study how the human brain grounds language in visual perception and may motivate development of novel approaches in ai ."
  },
  {
    "title": "Forward-backward retraining of recurrent neural networks .",
    "entities": [
      "letter posterior probability estimator",
      "oo-line handwriting recognition system",
      "hidden markov model",
      "supervised training algorithm",
      "recurrent neural network",
      "posterior distributions",
      "forward-backward algorithm",
      "handwritten word",
      "error rate",
      "recognizer"
    ],
    "types": "<method> <method> <method> <method> <method> <otherscientificterm> <method> <material> <metric> <method>",
    "relations": [
      "recurrent neural network -- USED-FOR -- posterior distributions",
      "letter posterior probability estimator -- USED-FOR -- recurrent neural network",
      "letter posterior probability estimator -- USED-FOR -- hidden markov model",
      "forward-backward algorithm -- USED-FOR -- recognizer",
      "recurrent neural network -- USED-FOR -- hidden markov model"
    ],
    "abstract": "this paper describes the training of a <method_4> as the <method_0> for a <method_2> , <method_1> . the <method_4> estimates <otherscientificterm_5> for each of a series of frames representing sections of a <material_7> . the <method_3> , backpropagation through time , requires target outputs to be provided for each frame . three methods for deriving these targets are presented . a novel method based upon the <method_6> is found to result in the <method_9> with the lowest <metric_8> .",
    "abstract_og": "this paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden markov model , oo-line handwriting recognition system . the recurrent neural network estimates posterior distributions for each of a series of frames representing sections of a handwritten word . the supervised training algorithm , backpropagation through time , requires target outputs to be provided for each frame . three methods for deriving these targets are presented . a novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate ."
  },
  {
    "title": "Phonology & the Interpretation of Fine Phonetic Detail in Berlin German .",
    "entities": [
      "reaction times",
      "interpretation of fine phonetic detail",
      "lax front vowel",
      "categorization of items",
      "perceptual divergence",
      "stigmatized vicinities",
      "perception studies",
      "older listeners",
      "phonetic input",
      "phonological generalization",
      "associative information",
      "zd"
    ],
    "types": "<metric> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method>",
    "relations": [
      "associative information -- USED-FOR -- phonetic input",
      "perceptual divergence -- FEATURE-OF -- associative information",
      "associative information -- USED-FOR -- phonological generalization"
    ],
    "abstract": "young multi-ethnolectal speakers of hamburg-german introduced an alternation of / \u00e7 / to -lsb- \u0283 -rsb- following a <otherscientificterm_2> / \u026a / -lsb- 1 -rsb- . we conducted <task_6> exploiting this contrast in berlin -lrb- germany -rrb- , a city with large multi-ethnic neighborhoods . this alternation is pervasive and noticeable , it is mocked and stigmatized and there is an awareness that many young speakers -lrb- including ethnically germans -rrb- from neighborhoods with larger migrant populations like kreuzberg -lrb- kb -rrb- substitute / \u00e7 / with / \u0283 / while speakers from less <otherscientificterm_5> like zehlendorf -lrb- <method_11> -rrb- do not . the <task_3> on two 14-step synthesized continua from fichte ` spruce ' to fischte ' 3 rd p. sg . to fish ' by 99 listeners shows that the <otherscientificterm_1> is strongly influenced by the co-presentation of the label kb or <method_11> in contrast to no label -lrb- control -rrb- . analyses of the <metric_0> show that significantly more time is needed to process stimuli in kb and less in <method_11> . moreover , younger listeners -lrb- below 30 years -rrb- perceive more / \u0283 / variants than <otherscientificterm_7> . <method_9> over <otherscientificterm_8> is dependent on <otherscientificterm_10> : <otherscientificterm_4> is found within the confines of a single large urban area -lsb- 2,3,4 -rsb- .",
    "abstract_og": "young multi-ethnolectal speakers of hamburg-german introduced an alternation of / \u00e7 / to -lsb- \u0283 -rsb- following a lax front vowel / \u026a / -lsb- 1 -rsb- . we conducted perception studies exploiting this contrast in berlin -lrb- germany -rrb- , a city with large multi-ethnic neighborhoods . this alternation is pervasive and noticeable , it is mocked and stigmatized and there is an awareness that many young speakers -lrb- including ethnically germans -rrb- from neighborhoods with larger migrant populations like kreuzberg -lrb- kb -rrb- substitute / \u00e7 / with / \u0283 / while speakers from less stigmatized vicinities like zehlendorf -lrb- zd -rrb- do not . the categorization of items on two 14-step synthesized continua from fichte ` spruce ' to fischte ' 3 rd p. sg . to fish ' by 99 listeners shows that the interpretation of fine phonetic detail is strongly influenced by the co-presentation of the label kb or zd in contrast to no label -lrb- control -rrb- . analyses of the reaction times show that significantly more time is needed to process stimuli in kb and less in zd . moreover , younger listeners -lrb- below 30 years -rrb- perceive more / \u0283 / variants than older listeners . phonological generalization over phonetic input is dependent on associative information : perceptual divergence is found within the confines of a single large urban area -lsb- 2,3,4 -rsb- ."
  },
  {
    "title": "Size Matters : Metric Visual Search Constraints from Monocular Metadata .",
    "entities": [
      "3-d size constraints",
      "purely monocular sources",
      "appearance gradient statistics",
      "associated exif metadata",
      "metric branch-and-bound algorithm",
      "category size information",
      "explicit 3-d sensing",
      "3-d sensor",
      "3-d sensing",
      "search task",
      "local window",
      "stereo rig",
      "metric constraints",
      "camera intrinstics",
      "online images",
      "online imagery",
      "training data",
      "reconstruction",
      "features"
    ],
    "types": "<otherscientificterm> <material> <method> <material> <method> <otherscientificterm> <method> <method> <method> <task> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <material> <material> <material> <task> <otherscientificterm>",
    "relations": [
      "3-d size constraints -- USED-FOR -- search task",
      "online images -- USED-FOR -- category size information",
      "metric branch-and-bound algorithm -- USED-FOR -- search task"
    ],
    "abstract": "metric constraints are known to be highly discriminative for many objects , but if training is limited to data captured from a particular <method_7> the quantity of <material_16> may be severly limited . in this paper , we show how a crucial aspect of <method_7> -- object and feature absolute size -- can be added to models learned from commonly available <material_15> , without use of any <method_8> or <task_17> at training time . such models can be utilized at test time together with <method_6> to perform robust search . our model uses a '' 2.1 d '' local feature , which combines traditional <method_2> with an estimate of average absolute depth within the <otherscientificterm_10> . we show how <otherscientificterm_5> can be obtained from <material_14> by exploiting relatively unbiquitous metadata fields specifying <otherscientificterm_13> . we develop an efficient <method_4> for our <task_9> , imposing <otherscientificterm_0> as part of an optimal search for a set of <otherscientificterm_18> which indicate the presence of a category . experiments on test scenes captured with a traditional <material_11> are shown , exploiting <material_16> from from <material_1> with <material_3> .",
    "abstract_og": "metric constraints are known to be highly discriminative for many objects , but if training is limited to data captured from a particular 3-d sensor the quantity of training data may be severly limited . in this paper , we show how a crucial aspect of 3-d sensor -- object and feature absolute size -- can be added to models learned from commonly available online imagery , without use of any 3-d sensing or reconstruction at training time . such models can be utilized at test time together with explicit 3-d sensing to perform robust search . our model uses a '' 2.1 d '' local feature , which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window . we show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics . we develop an efficient metric branch-and-bound algorithm for our search task , imposing 3-d size constraints as part of an optimal search for a set of features which indicate the presence of a category . experiments on test scenes captured with a traditional stereo rig are shown , exploiting training data from from purely monocular sources with associated exif metadata ."
  },
  {
    "title": "Automatic Detection of Cognates Using Orthographic Alignment .",
    "entities": [
      "automatically detecting pairs of cog-nates",
      "orthographic alignment method",
      "machine learning algorithms",
      "known cognates",
      "linguistic information",
      "language evolution",
      "sequence alignment",
      "aligned subsequences",
      "historical information",
      "computational biology",
      "linguistic changes",
      "rules",
      "features",
      "non-cognates"
    ],
    "types": "<task> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <task> <otherscientificterm> <otherscientificterm> <material> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "computational biology -- USED-FOR -- sequence alignment",
      "orthographic alignment method -- USED-FOR -- sequence alignment",
      "orthographic alignment method -- USED-FOR -- automatically detecting pairs of cog-nates",
      "features -- USED-FOR -- machine learning algorithms",
      "aligned subsequences -- USED-FOR -- machine learning algorithms",
      "machine learning algorithms -- USED-FOR -- linguistic changes",
      "aligned subsequences -- USED-FOR -- features"
    ],
    "abstract": "words undergo various changes when entering new languages . based on the assumption that these <otherscientificterm_10> follow certain <otherscientificterm_11> , we propose a method for <task_0> employing an <method_1> which proved relevant for <task_6> in <material_9> . we use <otherscientificterm_7> as <otherscientificterm_12> for <method_2> in order to infer <otherscientificterm_11> for <otherscientificterm_10> undergone by words when entering new languages and to discriminate between cognates and <otherscientificterm_13> . given a list of <otherscientificterm_3> , our approach does not require any other <otherscientificterm_4> . however , it can be customized to integrate <otherscientificterm_8> regarding <otherscientificterm_5> .",
    "abstract_og": "words undergo various changes when entering new languages . based on the assumption that these linguistic changes follow certain rules , we propose a method for automatically detecting pairs of cog-nates employing an orthographic alignment method which proved relevant for sequence alignment in computational biology . we use aligned subsequences as features for machine learning algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates . given a list of known cognates , our approach does not require any other linguistic information . however , it can be customized to integrate historical information regarding language evolution ."
  },
  {
    "title": "Dynamic Bayesian socio-situational setting classification .",
    "entities": [
      "bigram dynamic bayesian classification",
      "lexical and part-of-speech information",
      "dynamic bayesian classifier",
      "socio-situational setting",
      "static classifiers",
      "part-of-speech tags",
      "prediction accuracy",
      "context-dependent models",
      "speech recognition",
      "classification"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <method> <otherscientificterm> <metric> <method> <task> <task>",
    "relations": [
      "part-of-speech tags -- USED-FOR -- bigram dynamic bayesian classification",
      "prediction accuracy -- EVALUATE-FOR -- dynamic bayesian classifier",
      "context-dependent models -- USED-FOR -- speech recognition",
      "dynamic bayesian classifier -- USED-FOR -- socio-situational setting"
    ],
    "abstract": "we propose a <method_2> for the <method_3> of a conversation . knowledge of the <method_3> can be used to search for content recorded in a particular setting or to select <method_7> in <task_8> . the <method_2> has the advantage -- compared to <method_4> such a naive bayes and support vector machines -- that it can continuously update the <task_9> during a conversation . we experimented with several models that use <otherscientificterm_1> . our results show that the <metric_6> of the <method_2> using the first 25 % of a conversation is almost 98 % of the final <metric_6> , which is calculated on the entire conversation . the best final <metric_6> , 88.85 % , is obtained by <method_0> using words and <otherscientificterm_5> .",
    "abstract_og": "we propose a dynamic bayesian classifier for the socio-situational setting of a conversation . knowledge of the socio-situational setting can be used to search for content recorded in a particular setting or to select context-dependent models in speech recognition . the dynamic bayesian classifier has the advantage -- compared to static classifiers such a naive bayes and support vector machines -- that it can continuously update the classification during a conversation . we experimented with several models that use lexical and part-of-speech information . our results show that the prediction accuracy of the dynamic bayesian classifier using the first 25 % of a conversation is almost 98 % of the final prediction accuracy , which is calculated on the entire conversation . the best final prediction accuracy , 88.85 % , is obtained by bigram dynamic bayesian classification using words and part-of-speech tags ."
  },
  {
    "title": "Learning Optimal Commitment to Overcome Insecurity .",
    "entities": [
      "defender 's strategy",
      "game-theoretic algorithms",
      "game model",
      "prior information",
      "physical security",
      "stackelberg game"
    ],
    "types": "<method> <method> <method> <otherscientificterm> <task> <otherscientificterm>",
    "relations": [
      "game-theoretic algorithms -- USED-FOR -- physical security"
    ],
    "abstract": "game-theoretic <method_1> for <task_4> have made an impressive real-world impact . these <method_1> compute an optimal strategy for the defender to commit to in a <otherscientificterm_5> , where the attacker observes the <method_0> and best-responds . in order to build the <method_2> , though , the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies . we design an algorithm that optimizes the <method_0> with no <otherscientificterm_3> , by observing the attacker 's responses to randomized deployments of resources and learning his priorities . in contrast to previous work , our algorithm requires a number of queries that is polynomial in the representation of the game .",
    "abstract_og": "game-theoretic game-theoretic algorithms for physical security have made an impressive real-world impact . these game-theoretic algorithms compute an optimal strategy for the defender to commit to in a stackelberg game , where the attacker observes the defender 's strategy and best-responds . in order to build the game model , though , the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies . we design an algorithm that optimizes the defender 's strategy with no prior information , by observing the attacker 's responses to randomized deployments of resources and learning his priorities . in contrast to previous work , our algorithm requires a number of queries that is polynomial in the representation of the game ."
  },
  {
    "title": "Direct Code Access in Self-Organizing Neural Networks for Reinforcement Learning .",
    "entities": [
      "temporal difference methods",
      "direct code access procedure",
      "direct code access",
      "self-organizing neural network",
      "maximal reward values",
      "reinforcement learning",
      "decision cycle",
      "network complexity",
      "computation efficiency",
      "stable learning",
      "iterative process",
      "cognitive nodes",
      "td-falcon"
    ],
    "types": "<method> <method> <method> <method> <otherscientificterm> <task> <otherscientificterm> <metric> <metric> <method> <method> <otherscientificterm> <method>",
    "relations": [
      "direct code access -- CONJUNCTION -- td-falcon",
      "temporal difference methods -- PART-OF -- self-organizing neural network",
      "self-organizing neural network -- USED-FOR -- reinforcement learning",
      "td-falcon -- USED-FOR -- cognitive nodes",
      "direct code access -- USED-FOR -- td-falcon",
      "computation efficiency -- CONJUNCTION -- network complexity",
      "temporal difference methods -- USED-FOR -- reinforcement learning",
      "td-falcon -- HYPONYM-OF -- self-organizing neural network"
    ],
    "abstract": "td-falcon is a <method_3> that incorporates <method_0> for <task_5> . despite the advantages of fast and <method_9> , <method_12> still relies on an <method_10> to evaluate each available action in a <otherscientificterm_6> . to remove this deficiency , this paper presents a <method_1> whereby <method_12> conducts instantaneous searches for <otherscientificterm_11> that match with the current states and at the same time provide <otherscientificterm_4> . our comparative experiments show that <method_12> with <method_2> produces comparable performance with the original <method_12> while improving significantly in <metric_8> and <metric_7> .",
    "abstract_og": "td-falcon is a self-organizing neural network that incorporates temporal difference methods for reinforcement learning . despite the advantages of fast and stable learning , td-falcon still relies on an iterative process to evaluate each available action in a decision cycle . to remove this deficiency , this paper presents a direct code access procedure whereby td-falcon conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values . our comparative experiments show that td-falcon with direct code access produces comparable performance with the original td-falcon while improving significantly in computation efficiency and network complexity ."
  },
  {
    "title": "Fractional , canonical , and simplified fractional cosine transforms .",
    "entities": [
      "simplified fractional fourier transform",
      "simplified fractional cosine transform",
      "fractional cosine transform",
      "fractional fourier transform",
      "linear canonical transform",
      "canonical cosine transform",
      "space-variant pattem recognition",
      "optical system analysis",
      "fourier transform",
      "frft",
      "sfrft"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <task> <method> <otherscientificterm> <material> <method>",
    "relations": [
      "frft -- CONJUNCTION -- linear canonical transform",
      "linear canonical transform -- CONJUNCTION -- sfrft",
      "frft -- USED-FOR -- fourier transform",
      "linear canonical transform -- CONJUNCTION -- simplified fractional fourier transform",
      "frft -- CONJUNCTION -- sfrft",
      "canonical cosine transform -- CONJUNCTION -- simplified fractional cosine transform",
      "fractional cosine transform -- CONJUNCTION -- canonical cosine transform",
      "optical system analysis -- CONJUNCTION -- space-variant pattem recognition",
      "fractional fourier transform -- USED-FOR -- fourier transform",
      "fractional fourier transform -- CONJUNCTION -- linear canonical transform"
    ],
    "abstract": "fourier transform can be generalized into the <method_3> , <method_4> , and <method_0> . they extend the utilities of original <otherscientificterm_8> , and can solve many problems that ca n't be solved well by original <otherscientificterm_8> . in this paper , we will generalize the <otherscientificterm_8> . we will derive <method_2> , <method_5> , and <method_1> . we will show <method_2> are very similar to the <material_9> , <method_4> , and <method_10> , but <method_2> are much more efficient to deal with the even , real even functions . for <otherscientificterm_8> , <material_9> and <method_10> can save 112 of the real number multiplications , and <material_9> can save 314 . we also discuss their applications , such as <method_7> and <task_6> .",
    "abstract_og": "fourier transform can be generalized into the fractional fourier transform , linear canonical transform , and simplified fractional fourier transform . they extend the utilities of original fourier transform , and can solve many problems that ca n't be solved well by original fourier transform . in this paper , we will generalize the fourier transform . we will derive fractional cosine transform , canonical cosine transform , and simplified fractional cosine transform . we will show fractional cosine transform are very similar to the frft , linear canonical transform , and sfrft , but fractional cosine transform are much more efficient to deal with the even , real even functions . for fourier transform , frft and sfrft can save 112 of the real number multiplications , and frft can save 314 . we also discuss their applications , such as optical system analysis and space-variant pattem recognition ."
  },
  {
    "title": "Temporal decomposition : a promising approach to low rate wideband speech compression .",
    "entities": [
      "line spectral frequencies",
      "temporal decomposition",
      "dynamic programming search algorithm",
      "low bit rates",
      "split vector quantisation",
      "wideband speech",
      "unicast streaming"
    ],
    "types": "<method> <task> <method> <otherscientificterm> <method> <material> <task>",
    "relations": [
      "low bit rates -- EVALUATE-FOR -- split vector quantisation",
      "temporal decomposition -- USED-FOR -- wideband speech",
      "temporal decomposition -- COMPARE -- split vector quantisation",
      "temporal decomposition -- USED-FOR -- line spectral frequencies",
      "dynamic programming search algorithm -- USED-FOR -- temporal decomposition",
      "line spectral frequencies -- USED-FOR -- wideband speech"
    ],
    "abstract": "in this paper , we present new results on <task_1> applied to the <method_0> derived for <material_5> . the paper shows that by incorporating a <method_2> into <task_1> , near transparent quantisation of wideband lsfs can be obtained at approximately 1 kbps . we also show that <task_1> performs significantly better than <method_4> at <otherscientificterm_3> . we propose that <task_1> is a promising approach to low rate <material_5> coding for applications such as <task_6> .",
    "abstract_og": "in this paper , we present new results on temporal decomposition applied to the line spectral frequencies derived for wideband speech . the paper shows that by incorporating a dynamic programming search algorithm into temporal decomposition , near transparent quantisation of wideband lsfs can be obtained at approximately 1 kbps . we also show that temporal decomposition performs significantly better than split vector quantisation at low bit rates . we propose that temporal decomposition is a promising approach to low rate wideband speech coding for applications such as unicast streaming ."
  },
  {
    "title": "Component-Enhanced Chinese Character Embeddings .",
    "entities": [
      "component-enhanced chinese character embedding models",
      "en-glish word embeddings",
      "distributed word representations",
      "bi-gram extensions",
      "semantic information",
      "semantic in-dictors",
      "text classification",
      "word similarity",
      "nlp tasks",
      "en-glish"
    ],
    "types": "<method> <otherscientificterm> <method> <method> <otherscientificterm> <otherscientificterm> <task> <task> <task> <material>",
    "relations": [
      "word similarity -- EVALUATE-FOR -- component-enhanced chinese character embedding models",
      "text classification -- EVALUATE-FOR -- component-enhanced chinese character embedding models",
      "distributed word representations -- USED-FOR -- semantic information",
      "word similarity -- CONJUNCTION -- text classification",
      "component-enhanced chinese character embedding models -- CONJUNCTION -- bi-gram extensions",
      "en-glish -- HYPONYM-OF -- nlp tasks",
      "component-enhanced chinese character embedding models -- USED-FOR -- semantic in-dictors",
      "distributed word representations -- USED-FOR -- nlp tasks"
    ],
    "abstract": "distributed word representations are very useful for capturing <otherscientificterm_4> and have been successfully applied in a variety of <task_8> , especially on <material_9> . in this work , we innovatively develop two <method_0> and their <method_3> . distinguished from <otherscientificterm_1> , our <method_0> explore the compositions of chinese characters , which often serve as <otherscientificterm_5> inherently . the evaluations on both <task_7> and <task_6> demonstrate the effectiveness of our <method_0> .",
    "abstract_og": "distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of nlp tasks , especially on en-glish . in this work , we innovatively develop two component-enhanced chinese character embedding models and their bi-gram extensions . distinguished from en-glish word embeddings , our component-enhanced chinese character embedding models explore the compositions of chinese characters , which often serve as semantic in-dictors inherently . the evaluations on both word similarity and text classification demonstrate the effectiveness of our component-enhanced chinese character embedding models ."
  },
  {
    "title": "Active learning based automatic face segmentation for kinect video .",
    "entities": [
      "color and depth data",
      "semi-supervised spline regression model",
      "active learning framework",
      "commodity kinect camera",
      "face region",
      "labeling information",
      "human interactions",
      "real videos",
      "segmentation approach",
      "seg-mentation",
      "videos"
    ],
    "types": "<material> <method> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <material> <method> <otherscientificterm> <material>",
    "relations": [
      "semi-supervised spline regression model -- USED-FOR -- face region"
    ],
    "abstract": "this paper presents a novel <method_8> for extracting faces from <material_10> . under an <method_2> , the <otherscientificterm_9> is conducted automatically without <otherscientificterm_6> . a small portion of pixels are first labeled as face or non-face . given these labeled samples , a <method_1> is then applied to obtain the <otherscientificterm_4> . based on the segmentation result , new pixels are selected and labeled . these two steps perform iterately until convergence . the main novelty is that <material_0> are combined to provide the <otherscientificterm_5> . our <method_8> is validated via comparisons with state-of-the-art methods on <material_7> captured from the <otherscientificterm_3> .",
    "abstract_og": "this paper presents a novel segmentation approach for extracting faces from videos . under an active learning framework , the seg-mentation is conducted automatically without human interactions . a small portion of pixels are first labeled as face or non-face . given these labeled samples , a semi-supervised spline regression model is then applied to obtain the face region . based on the segmentation result , new pixels are selected and labeled . these two steps perform iterately until convergence . the main novelty is that color and depth data are combined to provide the labeling information . our segmentation approach is validated via comparisons with state-of-the-art methods on real videos captured from the commodity kinect camera ."
  },
  {
    "title": "Automata Theory for Reasoning About Actions .",
    "entities": [
      "second order quantiication over-nite",
      "domain closure assumption",
      "innnite trees",
      "uncountable domains",
      "situation calculus",
      "automata",
      "actions"
    ],
    "types": "<otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "automata -- USED-FOR -- domain closure assumption"
    ],
    "abstract": "in this paper , we show decidability of a rather expressive fragment of the <otherscientificterm_4> . we allow <otherscientificterm_0> and innnite sets of situations . we do not impose a <otherscientificterm_1> on <otherscientificterm_6> ; therefore , innnite and even <otherscientificterm_3> are allowed . the <otherscientificterm_1> is based on <method_5> accepting <otherscientificterm_2> .",
    "abstract_og": "in this paper , we show decidability of a rather expressive fragment of the situation calculus . we allow second order quantiication over-nite and innnite sets of situations . we do not impose a domain closure assumption on actions ; therefore , innnite and even uncountable domains are allowed . the domain closure assumption is based on automata accepting innnite trees ."
  },
  {
    "title": "Joint Emotion Analysis via Multi-task Gaussian Processes .",
    "entities": [
      "low-rank coregionalisation approach",
      "rich parameterisation scheme",
      "vector-valued gaussian process",
      "news headlines dataset",
      "natural language sentences",
      "multi-task approaches",
      "single-task baselines"
    ],
    "types": "<method> <method> <method> <material> <material> <method> <method>",
    "relations": [
      "single-task baselines -- CONJUNCTION -- multi-task approaches",
      "vector-valued gaussian process -- PART-OF -- low-rank coregionalisation approach",
      "vector-valued gaussian process -- CONJUNCTION -- rich parameterisation scheme",
      "rich parameterisation scheme -- PART-OF -- low-rank coregionalisation approach"
    ],
    "abstract": "we propose a model for jointly predicting multiple emotions in <material_4> . our model is based on a <method_0> , which combines a <method_2> with a <method_1> . we show that our approach is able to learn correlations and anti-correlations between emotions on a <material_3> . the proposed model outperforms both <method_6> and other <method_5> .",
    "abstract_og": "we propose a model for jointly predicting multiple emotions in natural language sentences . our model is based on a low-rank coregionalisation approach , which combines a vector-valued gaussian process with a rich parameterisation scheme . we show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset . the proposed model outperforms both single-task baselines and other multi-task approaches ."
  },
  {
    "title": "A General Regularization Framework for Domain Adaptation .",
    "entities": [
      "multi-task regularization framework",
      "domain adaptation framework",
      "feature augmentation technique",
      "negative transfer"
    ],
    "types": "<method> <method> <method> <otherscientificterm>",
    "relations": [
      "feature augmentation technique -- CONJUNCTION -- multi-task regularization framework"
    ],
    "abstract": "we propose a <method_1> , and formally prove that <method_1> generalizes the <method_2> in -lrb- daum\u00e9 iii , 2007 -rrb- and the <method_0> in -lrb- evgeniou and pontil , 2004 -rrb- . we show that our <method_1> is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid <otherscientificterm_3> between distant ones .",
    "abstract_og": "we propose a domain adaptation framework , and formally prove that domain adaptation framework generalizes the feature augmentation technique in -lrb- daum\u00e9 iii , 2007 -rrb- and the multi-task regularization framework in -lrb- evgeniou and pontil , 2004 -rrb- . we show that our domain adaptation framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones ."
  },
  {
    "title": "Max-Margin Multiple-Instance Dictionary Learning .",
    "entities": [
      "dictionary learning",
      "critical knowledge abstraction process",
      "image classification benchmarks",
      "weakly supervised learning",
      "codebook learning step",
      "representation problem",
      "machine learning",
      "randomized forests",
      "linear svm",
      "metric fusion",
      "dictionary learning",
      "dictionary learning",
      "classification margins",
      "multi-channel features",
      "mil",
      "classifier",
      "codes"
    ],
    "types": "<task> <method> <metric> <method> <method> <task> <task> <otherscientificterm> <method> <method> <task> <task> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "dictionary learning -- PART-OF -- machine learning",
      "dictionary learning -- USED-FOR -- representation problem",
      "weakly supervised learning -- USED-FOR -- dictionary learning",
      "linear svm -- HYPONYM-OF -- classifier",
      "metric fusion -- USED-FOR -- multi-channel features"
    ],
    "abstract": "dictionary learning has became an increasingly important task in <task_6> , as <task_10> is fundamental to the <task_5> . a number of emerging techniques specifically include a <method_4> , in which a <method_1> is carried out . existing approaches in <task_0> are either generative -lrb- unsupervised e.g. k-means -rrb- or discriminative -lrb- supervised e.g. extremely <otherscientificterm_7> -rrb- . in this paper , we propose a multiple instance learning -lrb- <otherscientificterm_14> -rrb- strategy -lrb- along the line of <method_3> -rrb- for <task_11> . each code is represented by a <method_15> , such as a <method_8> , which naturally performs <method_9> for <otherscientificterm_13> . we design a formulation to simultaneously learn mixtures of <otherscientificterm_16> by maximizing <otherscientificterm_12> in <otherscientificterm_14> . state-of-the-art results are observed in <metric_2> based on the learned codebooks , which observe both com-pactness and effectiveness .",
    "abstract_og": "dictionary learning has became an increasingly important task in machine learning , as dictionary learning is fundamental to the representation problem . a number of emerging techniques specifically include a codebook learning step , in which a critical knowledge abstraction process is carried out . existing approaches in dictionary learning are either generative -lrb- unsupervised e.g. k-means -rrb- or discriminative -lrb- supervised e.g. extremely randomized forests -rrb- . in this paper , we propose a multiple instance learning -lrb- mil -rrb- strategy -lrb- along the line of weakly supervised learning -rrb- for dictionary learning . each code is represented by a classifier , such as a linear svm , which naturally performs metric fusion for multi-channel features . we design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in mil . state-of-the-art results are observed in image classification benchmarks based on the learned codebooks , which observe both com-pactness and effectiveness ."
  },
  {
    "title": "Construction of discriminative Kernels from known and unknown non-targets for PLDA-SVM scoring .",
    "entities": [
      "impostor class data",
      "plda score space",
      "i-vector speaker verification",
      "speaker-and impostor-class data",
      "nist 2012 sre",
      "background speakers",
      "enrollment utterance",
      "resam-pling technique",
      "score vectors",
      "scoring process",
      "svm training",
      "plda-svm scoring",
      "known non-targets",
      "speaker-dependent svms",
      "resam-pling techniques",
      "target-speaker i-vectors",
      "plda scoring",
      "svms"
    ],
    "types": "<material> <otherscientificterm> <task> <material> <material> <otherscientificterm> <otherscientificterm> <method> <otherscientificterm> <method> <task> <task> <otherscientificterm> <method> <method> <otherscientificterm> <method> <method>",
    "relations": [
      "score vectors -- USED-FOR -- plda-svm scoring",
      "speaker-dependent svms -- USED-FOR -- scoring process",
      "plda scoring -- USED-FOR -- i-vector speaker verification",
      "speaker-and impostor-class data -- USED-FOR -- svm training",
      "background speakers -- PART-OF -- scoring process",
      "resam-pling technique -- USED-FOR -- target-speaker i-vectors"
    ],
    "abstract": "conventional <method_16> in <task_2> involves the i-vectors of target speakers and claimants only . we have previously demonstrated that better performance can be achieved by incorporating the information of <otherscientificterm_5> in the <method_9> via <method_13> . this is achieved by defining a <otherscientificterm_1> with dimension equal to the number of training i-vectors for each target speaker . the new protocol in <material_4> permits systems to use the information of other target-speakers -lrb- called <otherscientificterm_12> -rrb- in each verification trial . in this paper , we exploit this new protocol to enhance the performance of <task_11> by using the <otherscientificterm_8> of both known and unknown non-targets as the <material_0> to train the <method_13> . because some target speakers have one <otherscientificterm_6> only , which results in severe imbalance in the <material_3> for <task_10> . this paper shows that if the <otherscientificterm_6> is sufficiently long , a number of <otherscientificterm_15> can be generated by an utterance partitioning and <method_7> , resulting in much better scoring <method_17> . results on <material_4> demonstrate the advantages of pooling the known and unknown non-targets for training the <method_17> and that the <method_14> can help the <task_10> algorithm to find better decision boundaries for those speakers with only a small number of enrollment utterances .",
    "abstract_og": "conventional plda scoring in i-vector speaker verification involves the i-vectors of target speakers and claimants only . we have previously demonstrated that better performance can be achieved by incorporating the information of background speakers in the scoring process via speaker-dependent svms . this is achieved by defining a plda score space with dimension equal to the number of training i-vectors for each target speaker . the new protocol in nist 2012 sre permits systems to use the information of other target-speakers -lrb- called known non-targets -rrb- in each verification trial . in this paper , we exploit this new protocol to enhance the performance of plda-svm scoring by using the score vectors of both known and unknown non-targets as the impostor class data to train the speaker-dependent svms . because some target speakers have one enrollment utterance only , which results in severe imbalance in the speaker-and impostor-class data for svm training . this paper shows that if the enrollment utterance is sufficiently long , a number of target-speaker i-vectors can be generated by an utterance partitioning and resam-pling technique , resulting in much better scoring svms . results on nist 2012 sre demonstrate the advantages of pooling the known and unknown non-targets for training the svms and that the resam-pling techniques can help the svm training algorithm to find better decision boundaries for those speakers with only a small number of enrollment utterances ."
  },
  {
    "title": "One sentence voice adaptation using GMM-based frequency-warping and shift with a sub-band basis spectrum model .",
    "entities": [
      "sub-band basis spectrum model",
      "dp -lrb- dynamic programming -rrb- algorithm",
      "unit-selection based voice adaptation framework",
      "gmm.-based linear transformation of mel-cepstra",
      "unit-fusion based text-to-speech synthesizer",
      "rapid voice adaptation algorithm",
      "frequency domain representation",
      "rapid voice adaptation",
      "gmm-based frequency warping",
      "frequency warping function",
      "sub-band basis",
      "frequency warping",
      "mixture component",
      "shift",
      "gmm",
      "log-spectrum"
    ],
    "types": "<method> <method> <method> <method> <method> <method> <method> <task> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "shift -- USED-FOR -- rapid voice adaptation algorithm",
      "frequency warping -- USED-FOR -- sub-band basis spectrum model",
      "dp -lrb- dynamic programming -rrb- algorithm -- USED-FOR -- gmm",
      "sub-band basis -- USED-FOR -- rapid voice adaptation algorithm",
      "unit-selection based voice adaptation framework -- USED-FOR -- unit-fusion based text-to-speech synthesizer",
      "sub-band basis -- USED-FOR -- log-spectrum",
      "gmm-based frequency warping -- CONJUNCTION -- shift",
      "unit-selection based voice adaptation framework -- USED-FOR -- rapid voice adaptation algorithm",
      "rapid voice adaptation algorithm -- USED-FOR -- rapid voice adaptation",
      "sub-band basis spectrum model -- USED-FOR -- rapid voice adaptation algorithm",
      "gmm-based frequency warping -- USED-FOR -- rapid voice adaptation algorithm"
    ],
    "abstract": "this paper presents a <method_5> using <otherscientificterm_8> and <otherscientificterm_13> with parameters of a <method_0> -lsb- 1 -rsb- . the <method_0> represents a shape of a spectrum of speech . <method_5> is calculated by fitting a <otherscientificterm_10> to the <otherscientificterm_15> . since the <method_0> is the <method_6> , <method_11> can be directly applied to the <method_0> . a <method_9> that minimize the distance between source and target <method_0> pairs in each <method_12> of a <method_14> is derived using a <method_1> . the proposed <method_5> is evaluated in an <method_2> applied to a <method_4> . the experimental results show that the proposed <method_5> is effective for <task_7> using just one sentence , compared to the conventional <method_3> .",
    "abstract_og": "this paper presents a rapid voice adaptation algorithm using gmm-based frequency warping and shift with parameters of a sub-band basis spectrum model -lsb- 1 -rsb- . the sub-band basis spectrum model represents a shape of a spectrum of speech . rapid voice adaptation algorithm is calculated by fitting a sub-band basis to the log-spectrum . since the sub-band basis spectrum model is the frequency domain representation , frequency warping can be directly applied to the sub-band basis spectrum model . a frequency warping function that minimize the distance between source and target sub-band basis spectrum model pairs in each mixture component of a gmm is derived using a dp -lrb- dynamic programming -rrb- algorithm . the proposed rapid voice adaptation algorithm is evaluated in an unit-selection based voice adaptation framework applied to a unit-fusion based text-to-speech synthesizer . the experimental results show that the proposed rapid voice adaptation algorithm is effective for rapid voice adaptation using just one sentence , compared to the conventional gmm.-based linear transformation of mel-cepstra ."
  },
  {
    "title": "Improving Learning Performance Through Rational Resource Allocation .",
    "entities": [
      "parametric hypothesis selection problems",
      "synthetic and real-world problems",
      "resource optimization problem",
      "heuristic learning algorithm",
      "statistical learning problems",
      "rational analysis",
      "learning cost",
      "unknown parameters",
      "optimization problem",
      "effr-cient learning"
    ],
    "types": "<task> <task> <task> <method> <task> <method> <metric> <otherscientificterm> <task> <task>",
    "relations": [
      "learning cost -- USED-FOR -- statistical learning problems",
      "rational analysis -- USED-FOR -- statistical learning problems",
      "resource optimization problem -- USED-FOR -- effr-cient learning",
      "rational analysis -- USED-FOR -- learning cost",
      "heuristic learning algorithm -- USED-FOR -- optimization problem"
    ],
    "abstract": "this article shows how <method_5> can be used to minimize <metric_6> for a general class of <task_4> . we discuss the factors that influence <metric_6> and show that the problem of <task_9> can be cast as a <task_2> . solutions found in this way can be significantly more efficient than the best solutions that do not account for these factors . we introduce a <method_3> that approximately solves this <task_8> and document its performance improvements on <task_1> . se1191 -rsb- -rrb- of these factors to minimize <metric_6> . we discuss this in the context of <task_0> , an abstract class of <task_4> where a system must select one of a finite set of hypothesized courses of action , where the quality of each hypothesis is described as a function of some <otherscientificterm_7> -lrb- e.g. . a <method_3> determines and refines estimates of these parameters by '' paying for '' training examples .",
    "abstract_og": "this article shows how rational analysis can be used to minimize learning cost for a general class of statistical learning problems . we discuss the factors that influence learning cost and show that the problem of effr-cient learning can be cast as a resource optimization problem . solutions found in this way can be significantly more efficient than the best solutions that do not account for these factors . we introduce a heuristic learning algorithm that approximately solves this optimization problem and document its performance improvements on synthetic and real-world problems . se1191 -rsb- -rrb- of these factors to minimize learning cost . we discuss this in the context of parametric hypothesis selection problems , an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action , where the quality of each hypothesis is described as a function of some unknown parameters -lrb- e.g. . a heuristic learning algorithm determines and refines estimates of these parameters by '' paying for '' training examples ."
  },
  {
    "title": "Design and evaluation of a voice conversion algorithm based on spectral envelope mapping and residual prediction .",
    "entities": [
      "voice conversion system",
      "speaker discrimination of same/difference pairs",
      "speaker discrimination of natural speech",
      "spectral parameter conversion function",
      "perceived speaker identity",
      "lpc coded speech",
      "converted utterances",
      "speech utterances",
      "lpc coded",
      "lpc spectrum",
      "residual",
      "accuracy"
    ],
    "types": "<method> <material> <material> <otherscientificterm> <otherscientificterm> <material> <material> <material> <otherscientificterm> <method> <otherscientificterm> <metric>",
    "relations": [
      "voice conversion system -- USED-FOR -- perceived speaker identity",
      "voice conversion system -- COMPARE -- speaker discrimination of natural speech"
    ],
    "abstract": "the purpose of a <method_0> is to change the <otherscientificterm_4> of a speech signal . in this paper , we propose a new algorithm based on converting the <method_9> and predicting the <otherscientificterm_10> as a function of the target envelope parameters . we conduct listening tests based on <material_1> to measure the <metric_11> by which the converted voices match the desired target voices . to establish the level of human performance as a baseline , we first measure the ability of listeners to discriminate between original <material_7> under three conditions : normal , fundamental frequency and duration normalized , and <otherscientificterm_8> . additionally , the <otherscientificterm_3> is tested in isolation by listening to source , target , and converted speakers as <material_5> . the results show that the speaker identity of speech whose <method_9> has been converted can be recognized as the target speaker with the same level of performance as discriminating between <material_5> . however , the level of discrimination of <material_6> produced by the <method_0> is significantly below that of <material_2> .",
    "abstract_og": "the purpose of a voice conversion system is to change the perceived speaker identity of a speech signal . in this paper , we propose a new algorithm based on converting the lpc spectrum and predicting the residual as a function of the target envelope parameters . we conduct listening tests based on speaker discrimination of same/difference pairs to measure the accuracy by which the converted voices match the desired target voices . to establish the level of human performance as a baseline , we first measure the ability of listeners to discriminate between original speech utterances under three conditions : normal , fundamental frequency and duration normalized , and lpc coded . additionally , the spectral parameter conversion function is tested in isolation by listening to source , target , and converted speakers as lpc coded speech . the results show that the speaker identity of speech whose lpc spectrum has been converted can be recognized as the target speaker with the same level of performance as discriminating between lpc coded speech . however , the level of discrimination of converted utterances produced by the voice conversion system is significantly below that of speaker discrimination of natural speech ."
  },
  {
    "title": "Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM .",
    "entities": [
      "o communication cost",
      "non-strongly-convex linear svm dual problem",
      "box-constrained quadratic optimization algorithm",
      "training machine learning models",
      "distributed linear svm algorithms",
      "global linear convergence",
      "analytical solution",
      "distributed fashion",
      "large data",
      "fast convergence",
      "an-accurate solution",
      "disdca",
      "dsvm-ave",
      "tron"
    ],
    "types": "<otherscientificterm> <task> <method> <method> <method> <otherscientificterm> <method> <material> <material> <otherscientificterm> <task> <method> <method> <method>",
    "relations": [
      "tron -- HYPONYM-OF -- distributed linear svm algorithms",
      "dsvm-ave -- CONJUNCTION -- disdca",
      "analytical solution -- COMPARE -- distributed linear svm algorithms",
      "disdca -- HYPONYM-OF -- distributed linear svm algorithms",
      "dsvm-ave -- HYPONYM-OF -- distributed linear svm algorithms",
      "dsvm-ave -- CONJUNCTION -- tron",
      "disdca -- CONJUNCTION -- tron",
      "an-accurate solution -- USED-FOR -- non-strongly-convex linear svm dual problem",
      "o communication cost -- USED-FOR -- analytical solution"
    ],
    "abstract": "training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine , motivating recent works on developing algorithms that train in a <material_7> . this paper proposes an efficient <method_2> for distributedly training linear support vector machines -lrb- svms -rrb- with <material_8> . our key technical contribution is an <method_6> to the problem of computing the optimal step size at each iteration , using an efficient <method_6> that requires only <otherscientificterm_0> to ensure <otherscientificterm_9> . with this optimal step size , our <method_6> is superior to other methods by possessing <otherscientificterm_5> , or , equivalently , o -lrb- log -lrb- 1 / / -rrb- -rrb- iteration complexity for <task_10> , for dis-tributedly solving the <task_1> . experiments also show that our <method_6> is significantly faster than state-of-the-art <method_4> including <method_12> , <method_11> and <method_13> .",
    "abstract_og": "training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine , motivating recent works on developing algorithms that train in a distributed fashion . this paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines -lrb- svms -rrb- with large data . our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration , using an efficient analytical solution that requires only o communication cost to ensure fast convergence . with this optimal step size , our analytical solution is superior to other methods by possessing global linear convergence , or , equivalently , o -lrb- log -lrb- 1 / / -rrb- -rrb- iteration complexity for an-accurate solution , for dis-tributedly solving the non-strongly-convex linear svm dual problem . experiments also show that our analytical solution is significantly faster than state-of-the-art distributed linear svm algorithms including dsvm-ave , disdca and tron ."
  },
  {
    "title": "Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation .",
    "entities": [
      "soft source syntactic constraints",
      "discriminative rule selection model",
      "rule selection model",
      "syntax-based machine translation",
      "string-to-tree systems",
      "syntactic annotation",
      "rule selection",
      "translation rule",
      "rules",
      "features",
      "moses"
    ],
    "types": "<otherscientificterm> <method> <method> <task> <method> <otherscientificterm> <method> <otherscientificterm> <otherscientificterm> <otherscientificterm> <method>",
    "relations": [
      "rule selection -- USED-FOR -- syntax-based machine translation",
      "features -- PART-OF -- rule selection model"
    ],
    "abstract": "in <task_3> , <method_6> is the task of choosing the correct target side of a <otherscientificterm_7> among <otherscientificterm_8> with the same source side . we define a <method_1> for systems that have <otherscientificterm_5> on the target language side -lrb- string-to-tree -rrb- . this is a new and clean way to integrate <otherscientificterm_0> into <method_4> as <otherscientificterm_9> of the <method_2> . we release our implementation as part of <method_10> .",
    "abstract_og": "in syntax-based machine translation , rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side . we define a discriminative rule selection model for systems that have syntactic annotation on the target language side -lrb- string-to-tree -rrb- . this is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model . we release our implementation as part of moses ."
  },
  {
    "title": "Stochastic cross-layer resource allocation for wireless networks using orthogonal access : Optimality and delay analysis .",
    "entities": [
      "average end-to-end rates",
      "stochastic approximation tools",
      "average queue delays",
      "channel state information",
      "cross-layer algorithms",
      "physical layers",
      "stochastic schemes",
      "stochastic algorithm",
      "parallel channels",
      "convex optimization",
      "closed form",
      "wireless networks",
      "nodes",
      "focus",
      "interference"
    ],
    "types": "<metric> <method> <metric> <otherscientificterm> <method> <otherscientificterm> <method> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm> <method> <otherscientificterm>",
    "relations": [
      "channel state information -- USED-FOR -- cross-layer algorithms",
      "stochastic approximation tools -- USED-FOR -- stochastic algorithm",
      "convex optimization -- CONJUNCTION -- stochastic approximation tools",
      "convex optimization -- USED-FOR -- stochastic algorithm"
    ],
    "abstract": "efficient design of <method_11> requires implementation of <method_4> that exploit <otherscientificterm_3> . capitalizing on <method_9> and <method_1> , this paper develops a <method_7> that allocates resources at network , link , and <otherscientificterm_5> so that a sum-utility of the <metric_0> is maximized . <method_13> is placed on <method_11> where <otherscientificterm_14> is strong and <otherscientificterm_12> transmit orthogonally over a set of <otherscientificterm_8> . convergence of the developed <method_6> is characterized , and the <metric_2> are obtained in <otherscientificterm_10> .",
    "abstract_og": "efficient design of wireless networks requires implementation of cross-layer algorithms that exploit channel state information . capitalizing on convex optimization and stochastic approximation tools , this paper develops a stochastic algorithm that allocates resources at network , link , and physical layers so that a sum-utility of the average end-to-end rates is maximized . focus is placed on wireless networks where interference is strong and nodes transmit orthogonally over a set of parallel channels . convergence of the developed stochastic schemes is characterized , and the average queue delays are obtained in closed form ."
  },
  {
    "title": "Learning spectral mapping for speech dereverberation .",
    "entities": [
      "deep neural networks",
      "derever-ebrated speech signal",
      "automatic speech recognition",
      "daily listening environments",
      "speaker identification systems",
      "estimated spectral representation",
      "performance degradation",
      "dereverberation problem",
      "human speech",
      "spectral mapping",
      "reverberant speech",
      "hearing-impaired listeners",
      "speech intelligibility",
      "anechoic speech",
      "distortion",
      "reverberation"
    ],
    "types": "<method> <material> <task> <material> <task> <method> <metric> <task> <material> <method> <material> <otherscientificterm> <task> <material> <otherscientificterm> <otherscientificterm>",
    "relations": [
      "deep neural networks -- USED-FOR -- spectral mapping",
      "automatic speech recognition -- CONJUNCTION -- speaker identification systems",
      "reverberant speech -- USED-FOR -- spectral mapping"
    ],
    "abstract": "reverberation distorts <material_8> and usually has negative effects on <task_12> , especially for <otherscientificterm_11> . it also causes <metric_6> in <task_2> and <task_4> . therefore , the <task_7> must be dealt with in <material_3> . we propose to use <method_0> to learn a <method_9> from the <material_10> to the <material_13> . the trained <method_0> produces the <method_5> of the corresponding <material_13> . we demonstrate that <otherscientificterm_14> caused by <otherscientificterm_15> is substantially attenuated by the <method_0> whose outputs can be resynthesized to the <material_1> . the proposed <method_0> is simple , and our systematic evaluation shows promising dereverberation results , which are significantly better than those of related systems .",
    "abstract_og": "reverberation distorts human speech and usually has negative effects on speech intelligibility , especially for hearing-impaired listeners . it also causes performance degradation in automatic speech recognition and speaker identification systems . therefore , the dereverberation problem must be dealt with in daily listening environments . we propose to use deep neural networks to learn a spectral mapping from the reverberant speech to the anechoic speech . the trained deep neural networks produces the estimated spectral representation of the corresponding anechoic speech . we demonstrate that distortion caused by reverberation is substantially attenuated by the deep neural networks whose outputs can be resynthesized to the derever-ebrated speech signal . the proposed deep neural networks is simple , and our systematic evaluation shows promising dereverberation results , which are significantly better than those of related systems ."
  },
  {
    "title": "Learning Step Size Controllers for Robust Neural Network Training .",
    "entities": [
      "learning rate of neural networks",
      "stochastic gradient descent",
      "learning methods",
      "adaptive controller",
      "nn setting",
      "learning rate",
      "learning problem",
      "features"
    ],
    "types": "<task> <method> <method> <method> <method> <metric> <task> <otherscientificterm>",
    "relations": [
      "learning methods -- USED-FOR -- nn setting"
    ],
    "abstract": "this paper investigates algorithms to automatically adapt the <task_0> . starting with <method_1> , a large variety of <method_2> has been proposed for the <method_4> . however , these methods are usually sensitive to the initial <metric_5> which has to be chosen by the exper-imenter . we investigate several <otherscientificterm_7> and show how an <method_3> can adjust the <metric_5> without prior knowledge of the <task_6> at hand .",
    "abstract_og": "this paper investigates algorithms to automatically adapt the learning rate of neural networks . starting with stochastic gradient descent , a large variety of learning methods has been proposed for the nn setting . however , these methods are usually sensitive to the initial learning rate which has to be chosen by the exper-imenter . we investigate several features and show how an adaptive controller can adjust the learning rate without prior knowledge of the learning problem at hand ."
  }
]